{
  "dataset_revision": "14b13edfbc4046892f6011d114c29c0f83170589",
  "mteb_dataset_name": "TweetSentimentClassification",
  "mteb_version": "1.7.40",
  "test": {
    "arabic": {
      "accuracy": 0.48359375,
      "accuracy_stderr": 0.05119423414189336,
      "f1": 0.4665069540755282,
      "f1_stderr": 0.053537433628209126,
      "main_score": 0.48359375
    },
    "english": {
      "accuracy": 0.50625,
      "accuracy_stderr": 0.020758328524353786,
      "f1": 0.49122983557790834,
      "f1_stderr": 0.027050415221438424,
      "main_score": 0.50625
    },
    "evaluation_time": 9.21,
    "french": {
      "accuracy": 0.46796875,
      "accuracy_stderr": 0.03952074959356793,
      "f1": 0.4625026576869818,
      "f1_stderr": 0.04174274171094167,
      "main_score": 0.46796875
    },
    "german": {
      "accuracy": 0.542578125,
      "accuracy_stderr": 0.030879147146830738,
      "f1": 0.5402160159852935,
      "f1_stderr": 0.030994484379560573,
      "main_score": 0.542578125
    },
    "hindi": {
      "accuracy": 0.366015625,
      "accuracy_stderr": 0.049009039842952705,
      "f1": 0.3615475949350663,
      "f1_stderr": 0.04770803261081365,
      "main_score": 0.366015625
    },
    "italian": {
      "accuracy": 0.48671875,
      "accuracy_stderr": 0.0647589807357829,
      "f1": 0.4622386564468567,
      "f1_stderr": 0.0738871672919012,
      "main_score": 0.48671875
    },
    "portuguese": {
      "accuracy": 0.467578125,
      "accuracy_stderr": 0.019456889695956672,
      "f1": 0.46145513352481027,
      "f1_stderr": 0.020486063439540143,
      "main_score": 0.467578125
    },
    "spanish": {
      "accuracy": 0.45703125,
      "accuracy_stderr": 0.04141361962718304,
      "f1": 0.44253494659642734,
      "f1_stderr": 0.05122657838363042,
      "main_score": 0.45703125
    }
  }
}