{
    "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
    "evaluation_time": 21.549480199813843,
    "kg_co2_emissions": null,
    "mteb_version": "1.12.34",
    "scores": {
        "test": [
            {
                "cosine": {
                    "accuracy": 0.9954950495049505,
                    "accuracy_threshold": 0.907312273979187,
                    "ap": 0.8012273184915149,
                    "f1": 0.7572016460905351,
                    "f1_threshold": 0.9018044471740723,
                    "precision": 0.7796610169491526,
                    "recall": 0.736
                },
                "dot": {
                    "accuracy": 0.9902079207920792,
                    "accuracy_threshold": 0.47660601139068604,
                    "ap": 0.2218132363528724,
                    "f1": 0.3066424021838035,
                    "f1_threshold": 0.403484582901001,
                    "precision": 0.28130217028380633,
                    "recall": 0.337
                },
                "euclidean": {
                    "accuracy": 0.9954653465346535,
                    "accuracy_threshold": 0.26685065031051636,
                    "ap": 0.7727693506179343,
                    "f1": 0.7418677859391395,
                    "f1_threshold": 0.2830333113670349,
                    "precision": 0.7803532008830022,
                    "recall": 0.707
                },
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.8012273184915149,
                "manhattan": {
                    "accuracy": 0.9954455445544554,
                    "accuracy_threshold": 3.6830081939697266,
                    "ap": 0.7726615479510065,
                    "f1": 0.7445652173913043,
                    "f1_threshold": 3.8327877521514893,
                    "precision": 0.8154761904761905,
                    "recall": 0.685
                },
                "max": {
                    "accuracy": 0.9954950495049505,
                    "ap": 0.8012273184915149,
                    "f1": 0.7572016460905351
                },
                "similarity": {
                    "accuracy": 0.9954950495049505,
                    "accuracy_threshold": 0.9073122143745422,
                    "ap": 0.8012269960916356,
                    "f1": 0.7572016460905351,
                    "f1_threshold": 0.9018043875694275,
                    "precision": 0.7796610169491526,
                    "recall": 0.736
                }
            }
        ],
        "validation": [
            {
                "cosine": {
                    "accuracy": 0.9956237623762376,
                    "accuracy_threshold": 0.9099201560020447,
                    "ap": 0.794888046665419,
                    "f1": 0.7552602436323367,
                    "f1_threshold": 0.909598171710968,
                    "precision": 0.8461538461538461,
                    "recall": 0.682
                },
                "dot": {
                    "accuracy": 0.9902277227722772,
                    "accuracy_threshold": 0.5442271828651428,
                    "ap": 0.19330120389645145,
                    "f1": 0.2625298329355608,
                    "f1_threshold": 0.4011155664920807,
                    "precision": 0.21796565389696168,
                    "recall": 0.33
                },
                "euclidean": {
                    "accuracy": 0.9954950495049505,
                    "accuracy_threshold": 0.2651556134223938,
                    "ap": 0.7694435884693833,
                    "f1": 0.7494553376906317,
                    "f1_threshold": 0.27809423208236694,
                    "precision": 0.8229665071770335,
                    "recall": 0.688
                },
                "hf_subset": "default",
                "languages": [
                    "eng-Latn"
                ],
                "main_score": 0.794888046665419,
                "manhattan": {
                    "accuracy": 0.9955049504950495,
                    "accuracy_threshold": 3.766726493835449,
                    "ap": 0.7684690106594636,
                    "f1": 0.7508196721311474,
                    "f1_threshold": 3.8263633251190186,
                    "precision": 0.827710843373494,
                    "recall": 0.687
                },
                "max": {
                    "accuracy": 0.9956237623762376,
                    "ap": 0.794888046665419,
                    "f1": 0.7552602436323367
                },
                "similarity": {
                    "accuracy": 0.9956237623762376,
                    "accuracy_threshold": 0.9099200963973999,
                    "ap": 0.7948878387935692,
                    "f1": 0.7552602436323367,
                    "f1_threshold": 0.9095981121063232,
                    "precision": 0.8461538461538461,
                    "recall": 0.682
                }
            }
        ]
    },
    "task_name": "SprintDuplicateQuestions"
}
