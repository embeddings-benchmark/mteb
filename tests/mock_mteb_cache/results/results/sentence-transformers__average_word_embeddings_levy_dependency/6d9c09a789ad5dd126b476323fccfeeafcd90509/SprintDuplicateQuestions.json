{
  "dataset_revision": "d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46",
  "evaluation_time": 21.549480199813843,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.34",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.9954950495049505,
          "accuracy_threshold": 0.907312273979187,
          "ap": 0.8012273184915149,
          "f1": 0.7572016460905351,
          "f1_threshold": 0.9018044471740723,
          "precision": 0.7796610169491526,
          "recall": 0.736
        },
        "dot": {
          "accuracy": 0.9902079207920792,
          "accuracy_threshold": 0.47660601139068604,
          "ap": 0.2218132363528724,
          "f1": 0.3066424021838035,
          "f1_threshold": 0.403484582901001,
          "precision": 0.28130217028380633,
          "recall": 0.337
        },
        "euclidean": {
          "accuracy": 0.9954653465346535,
          "accuracy_threshold": 0.26685065031051636,
          "ap": 0.7727693506179343,
          "f1": 0.7418677859391395,
          "f1_threshold": 0.2830333113670349,
          "precision": 0.7803532008830022,
          "recall": 0.707
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8012273184915149,
        "manhattan": {
          "accuracy": 0.9954455445544554,
          "accuracy_threshold": 3.6830081939697266,
          "ap": 0.7726615479510065,
          "f1": 0.7445652173913043,
          "f1_threshold": 3.8327877521514893,
          "precision": 0.8154761904761905,
          "recall": 0.685
        },
        "max": {
          "accuracy": 0.9954950495049505,
          "ap": 0.8012273184915149,
          "f1": 0.7572016460905351
        },
        "similarity": {
          "accuracy": 0.9954950495049505,
          "accuracy_threshold": 0.9073122143745422,
          "ap": 0.8012269960916356,
          "f1": 0.7572016460905351,
          "f1_threshold": 0.9018043875694275,
          "precision": 0.7796610169491526,
          "recall": 0.736
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.9956237623762376,
          "accuracy_threshold": 0.9099201560020447,
          "ap": 0.794888046665419,
          "f1": 0.7552602436323367,
          "f1_threshold": 0.909598171710968,
          "precision": 0.8461538461538461,
          "recall": 0.682
        },
        "dot": {
          "accuracy": 0.9902277227722772,
          "accuracy_threshold": 0.5442271828651428,
          "ap": 0.19330120389645145,
          "f1": 0.2625298329355608,
          "f1_threshold": 0.4011155664920807,
          "precision": 0.21796565389696168,
          "recall": 0.33
        },
        "euclidean": {
          "accuracy": 0.9954950495049505,
          "accuracy_threshold": 0.2651556134223938,
          "ap": 0.7694435884693833,
          "f1": 0.7494553376906317,
          "f1_threshold": 0.27809423208236694,
          "precision": 0.8229665071770335,
          "recall": 0.688
        },
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.794888046665419,
        "manhattan": {
          "accuracy": 0.9955049504950495,
          "accuracy_threshold": 3.766726493835449,
          "ap": 0.7684690106594636,
          "f1": 0.7508196721311474,
          "f1_threshold": 3.8263633251190186,
          "precision": 0.827710843373494,
          "recall": 0.687
        },
        "max": {
          "accuracy": 0.9956237623762376,
          "ap": 0.794888046665419,
          "f1": 0.7552602436323367
        },
        "similarity": {
          "accuracy": 0.9956237623762376,
          "accuracy_threshold": 0.9099200963973999,
          "ap": 0.7948878387935692,
          "f1": 0.7552602436323367,
          "f1_threshold": 0.9095981121063232,
          "precision": 0.8461538461538461,
          "recall": 0.682
        }
      }
    ]
  },
  "task_name": "SprintDuplicateQuestions"
}