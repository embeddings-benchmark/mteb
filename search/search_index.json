{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MTEB Documentation","text":"<p>Info</p> <p>We recently released <code>mteb</code> version 2.0.0, to see what is new check of what is new and see how to upgrade your existing code.</p> <p>Welcome documentation of MTEB. <code>mteb</code> a package for benchmark and evaluating the quality of embeddings.</p> <p>MTEB is the go-to documentation for evaluating embeddings models across a variety of tasks, modalities and domains. MTEB covers more than a 1000 different tasks from covering a diverse set of tasks from historic Swedish patent classification to documentation retrieval for Python. These tasks spread across more than 1000 languages and cover both image and text tasks.</p> <p>This package was initially introduced as a package for evaluating text embeddings predominantly for English<sup>1</sup>, but have since been extended for broad languages coverage<sup>2</sup> and to support multiple modalities<sup>3</sup>.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is as simple as:</p> pipuv <pre><code>pip install mteb\n</code></pre> <pre><code>uv add mteb\n</code></pre> <p>To see more check out the installation guide.</p>"},{"location":"#quickstart","title":"Quickstart","text":"Using ScriptUsing the CLI <p>To evaluating a model simply select a model, select tasks and evaluate:</p> <pre><code>import mteb\nfrom sentence_transformers import SentenceTransformer\n\n# Select model\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel = mteb.get_model(model_name) # if the model is not implemented in MTEB it will be eq. to SentenceTransformer(model_name)\n\n# Select tasks\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\n\n# evaluate\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>To see more check out the usage documentation</p> <p>To run a model from the cli simply specify the <code>--model/-m</code> and the <code>--tasks/-t</code> <pre><code>mteb run \\\n    -m sentence-transformers/all-MiniLM-L6-v2 \\\n    -t Banking77Classification \\\n    --output-folder results\n</code></pre></p> <p>To read more about what you can do with the command line interface check out its documentation</p>"},{"location":"#citing","title":"Citing","text":"<p>MTEB was introduced in the paper \"MTEB: Massive Text Embedding Benchmark\"<sup>1</sup>, and heavily expanded in \"MMTEB: Massive Multilingual Text Embedding Benchmark\"<sup>2</sup>. When using <code>mteb</code>, we recommend that you cite both articles.</p> <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo\u00efc and Reimers, Nils},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2210.07316},\n  year = {2022}\n  url = {https://arxiv.org/abs/2210.07316},\n  doi = {10.48550/ARXIV.2210.07316},\n}\n\n@article{enevoldsen2025mmtebmassivemultilingualtext,\n  title={MMTEB: Massive Multilingual Text Embedding Benchmark},\n  author={Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2502.13595},\n  year={2025},\n  url={https://arxiv.org/abs/2502.13595},\n  doi = {10.48550/arXiv.2502.13595},\n}\n</code></pre> <p>If you use any of the specific benchmarks, we also recommend that you cite the paper, which you can obtain using:</p> <pre><code>benchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nbenchmark.citation # get citation for a specific benchmark\n\n# you can also create a table of the task for the appendix using:\nbenchmark.tasks.to_latex()\n</code></pre> <ol> <li> <p>Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2014\u20132037. Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL: https://aclanthology.org/2023.eacl-main.148, doi:10.18653/v1/2023.eacl-main.148.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. URL: https://arxiv.org/abs/2502.13595, doi:10.48550/arXiv.2502.13595.\u00a0\u21a9\u21a9</p> </li> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"CONTRIBUTING/","title":"Guidelines","text":""},{"location":"CONTRIBUTING/#contributing-to-mteb","title":"Contributing to MTEB","text":"<p>We welcome contributions. Please see the current open issues or open an issue yourself. Once you have decided on what you'd like to contribute, this document describes how to set up the repository for development.</p>"},{"location":"CONTRIBUTING/#development-installation","title":"Development Installation","text":"<p>If you want to submit a dataset or in other ways contribute to MTEB, you can install the package in development mode:</p> <pre><code># download the git repository\ngit clone https://github.com/embeddings-benchmark/mteb\ncd mteb\n\n# create your virtual environment and activate it\nmake install\n</code></pre> <p>This uses make to define the install command. You can see what each command does in the makefile.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>To run the tests, you can use the following command:</p> <pre><code>make test\n</code></pre> <p>This is also run by the CI pipeline, so you can be sure that your changes do not break the package. We recommend running the tests in the lowest version of Python supported by the package (see the pyproject.toml) to ensure compatibility.</p>"},{"location":"CONTRIBUTING/#running-linting","title":"Running linting","text":"<p>To run the linting before a PR, you can use the following command:</p> <pre><code>make lint\n</code></pre> <p>This command is equivalent to the command run during CI. It will check for code style and formatting issues.</p>"},{"location":"CONTRIBUTING/#semantic-versioning-and-releases","title":"Semantic Versioning and Releases","text":"<p>MTEB follows semantic versioning. This means that the version number of the package is composed of three numbers: <code>MAJOR.MINOR.PATCH</code>. This allows us to use existing tools to manage the versioning of the package automatically. For maintainers (and contributors), this means that commits with the following prefixes will automatically trigger a version bump:</p> <ul> <li><code>fix:</code> for patches</li> <li><code>model:</code> for new models</li> <li><code>dataset:</code> for new datasets and benchmarks</li> <li><code>feat:</code> for minor versions</li> <li><code>breaking:</code> for major versions</li> </ul> <p>Any commit with one of these prefixes will trigger a version bump upon merging to the main branch, as long as the tests pass. A version bump will then trigger a new release on PyPI as well as a new release on GitHub.</p> <p>Other prefixes will not trigger a version bump. For example, <code>docs:</code>, <code>chore:</code>, <code>refactor:</code>, etc., however they will structure the commit history and the changelog. You can find more information about this in the python-semantic-release documentation. If you do not intend to trigger a version bump, you're not required to follow this convention when contributing to MTEB.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> pipuv <pre><code>pip install mteb\n</code></pre> <pre><code>uv add mteb\n</code></pre>"},{"location":"installation/#model-specific-installations","title":"Model Specific Installations","text":"<p>If you want to run certain models implemented within mteb you will often need some additional dependencies. These can be installed using:</p> <pre><code>pip install mteb[cohere]\n</code></pre> <p>If a specific model requires a dependency it will raise an error with the recommended installation. To see full list of available models you can look at the models overview.</p>"},{"location":"whats_new/","title":"New in v2.0 \ud83c\udf89","text":"<p>This section goes through new features added in v2. Below we give an overview of changes following by detailed examples.</p> <p>Overview of changes:</p> <ul> <li>New in v2.0 \ud83c\udf89<ul> <li>Easier evaluation</li> <li>Better local and online caching</li> <li>Multimodal Input format</li> <li>Better support for CrossEncoders</li> <li>Unified Retrieval, Reranking and instruction variants</li> <li>Search Interface</li> <li>New Documentation</li> <li>Better support for loading and comparing results</li> <li>Descriptive Statistics</li> <li>Saving Predictions</li> <li>Support datasets v4</li> </ul> </li> <li>Upgrading from v1<ul> <li>Replacing <code>mteb.MTEB</code></li> <li>Replacing <code>mteb.load_results()</code></li> <li>Converting model to new format</li> <li>Reuploading datasets</li> <li>Converting Reranking datasets to new format</li> </ul> </li> </ul> <p>What are the reasons for the changes? Generally the many inconsistencies in the library made it hard to maintain without introducing breaking changes and we do think that there are multiple important areas to expand in, e.g. [adding new benchmark for image embeddings]<sup>1</sup>, support new model types in general making the library more accessible. We have already been able to add many new feature in v2.0, but hope that this new version allow us to keep doing so without breaking backward compatibility. See upgrading from v1 for specific deprecations and how to fix them.</p>"},{"location":"whats_new/#easier-evaluation","title":"Easier evaluation","text":"<p>Evaluations are now a lot easier using <code>mteb.evaluate</code>,</p> <pre><code>results = mteb.evaluate(model, tasks)\n</code></pre>"},{"location":"whats_new/#better-local-and-online-caching","title":"Better local and online caching","text":"<p>The new <code>mteb.ResultCache</code> makes managing the cache notably easier: <pre><code>from mteb.cache import ResultCache\n\nmodel = ...\ntasks = ...\n\ncache = ResultCache(cache_path=\"~/.cache/mteb\")  # default\n\n# simple evaluate with cache\nresults = mteb.evaluate(model, tasks, cache=cache)  # only runs if results not in cache\n</code></pre></p> <p>It allow you to access the online cache so you don't have to rerun existing models.</p> <pre><code># no need to rerun already public results\ncache.download_from_remote() # download the latest results from the remote repository\nresults = mteb.evaluate(model, tasks, cache=cache)\n</code></pre>"},{"location":"whats_new/#multimodal-input-format","title":"Multimodal Input format","text":"<p>Models in mteb who implements the <code>Encoder</code> protocol now supports multimodal input With the model protocol roughly looking like so:</p> <p><pre><code>class EncoderProtocol(Protocol):  # simplified\n    \"\"\"The interface for an encoder in MTEB.\"\"\"\n\n    def encode(self, inputs: DataLoader[BatchedInput], ...) -&gt; Array: ...\n</code></pre> Not only does this allow more efficient loading using the torch dataloader, but it also allows keys for multiple modalities:</p> <pre><code>batch_input: BatchedInput = {\n    \"text\": list[str],\n    \"images\": list[PIL.Image],\n    \"audio\": list[list[audio]], # upcoming\n    # + optional fields such as document title\n}\n</code></pre> <p>Where <code>text</code> is a batch of texts and <code>list[images]</code> is a batch for that texts. This e.g. allows markdown documents with multiple figures like so:</p> <p>As you see in the following figure figure 1 there is a correlation between A and B.</p> <p>Note</p> <p>More examples of new multimodal inputs you can find in BatchedInput documentation.</p> <p>However, this also allows no text, multi-image inputs (e.g. for PDFs). Overall this greatly expands the possible tasks that can now be evaluated in MTEB. To see how to convert a legacy model see the converting model section.</p>"},{"location":"whats_new/#better-support-for-crossencoders","title":"Better support for CrossEncoders","text":"<p>Also, we've introduced a new <code>CrossEncoderProtocol</code> for cross-encoders and now all cross-encoders have better support for evaluation:</p> <pre><code>class CrossEncoderProtocol(Protocol):\n    def predict(\n        self,\n        inputs1: DataLoader[BatchedInput],\n        inputs2: DataLoader[BatchedInput],\n        ...\n    ) -&gt; Array:\n</code></pre>"},{"location":"whats_new/#unified-retrieval-reranking-and-instruction-variants","title":"Unified Retrieval, Reranking and instruction variants","text":"<p>The retrieval tasks in MTEB now supports both retrieval and reranking using the same base task. The main difference now that Reranking tasks should have <code>top_ranked</code> subset to be evaluated on. New structure of retrieval tasks: <code>dataset[subset][split]</code> = RetrievalSplitData. On HF this dataset should these subsets:</p> <ol> <li><code>Corpus</code> - the corpus to retrieve from. Monolingual name: <code>corpus</code>, multilingual name: <code>{subset}-corpus</code>. Can contain columns:</li> <li><code>id</code>, <code>text</code>, <code>title</code> for text corpus</li> <li><code>id</code>, <code>image</code>, (<code>text</code> optionally) for image or multimodal corpus</li> <li><code>Queries</code> - the queries to retrieve with. Monolingual name: <code>queries</code>, multilingual name: <code>{subset}-queries</code>.</li> <li><code>id</code>, <code>text</code> for text queries. Where text can be str for single query or <code>list[str]</code> or <code>Conversation</code> for multi-turn dialogs queries.</li> <li><code>id</code>, <code>text</code>, <code>instructions</code> for instruction retrieval/reranking tasks</li> <li><code>id</code>, <code>image</code>, (<code>text</code> optionally) for image or multimodal queries</li> <li><code>Qrels</code> - the relevance judgements. Monolingual name: <code>qrels</code>, multilingual name: <code>{subset}-qrels</code>.       <code>query-id</code>, <code>corpus-id</code>, <code>score</code> (int or float) for relevance judgements.</li> <li><code>Top Ranked</code> - the top ranked documents to rerank. Only for reranking tasks. Monolingual name: <code>top_ranked</code>, multilingual name: <code>{subset}-top_ranked</code>.       <code>query-id</code>, <code>corpus-ids</code> (<code>list[str]</code>) - the top ranked documents for each query.</li> </ol>"},{"location":"whats_new/#search-interface","title":"Search Interface","text":"<p>To make it easier to use MTEB for search, we have added a simple search interface using the new <code>SearchProtocol</code>:</p> <pre><code>class SearchProtocol(Protocol):\n    \"\"\"Interface for searching models.\"\"\"\n\n    def index(\n        self,\n        corpus: CorpusDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, Any],\n    ) -&gt; None:\n        ...\n\n    def search(\n        self,\n        queries: QueryDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        top_k: int,\n        encode_kwargs: dict[str, Any],\n        top_ranked: TopRankedDocumentsType | None = None,\n    ) -&gt; RetrievalOutputType:\n        ...\n</code></pre> <p>We're automatically wrapping <code>Encoder</code> and <code>CrossEncoder</code> models support <code>SearchProtocol</code>. However, if your model needs a custom index you can implement this protocol directly, like was done for colbert-like models. </p>"},{"location":"whats_new/#new-documentation","title":"New Documentation","text":"<p>We've added a lot of new documentation to make it easier to get started with MTEB.</p> <ul> <li>You can see api of our models in tasks in API documentation.</li> <li>We've added a getting started guide to help you get started with MTEB.</li> <li>You can see implemented tasks and models in MTEB.</li> </ul>"},{"location":"whats_new/#better-support-for-loading-and-comparing-results","title":"Better support for loading and comparing results","text":"<p>The new <code>ResultCache</code> also makes it easier to load, inspect and compare both local and online results:</p> <pre><code>from mteb.cache import ResultCache\n\ncache = ResultCache(cache_path=\"~/.cache/mteb\") # default\ncache.download_from_remote() # download the latest results from the remote repository\n\n# load both local and online results\nresults = cache.load_results(models=[\"sentence-transformers/all-MiniLM-L6-v2\", ...], tasks=[\"STS12\"])\ndf = results.to_dataframe()\n</code></pre>"},{"location":"whats_new/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Descriptive statistics isn't a new thing in MTEB, however, now it is there for every task, to extract it simply run:</p> <pre><code>import mteb\ntask = mteb.get_task(\"MIRACLRetrievalHardNegatives\")\n\ntask.metadata.descriptive_stats\n</code></pre> <p>And you will get a highly detailed set of descriptive statistics covering everything from number of samples query lengths, duplicates, etc. These not only make it easier for you to examine tasks, but it also makes it easier for us to make quality checks on future tasks.</p> <p>Example for reranking task: <pre><code>{\n    \"test\": {\n        \"num_samples\": 160,\n        \"number_of_characters\": 310133,\n        \"documents_text_statistics\": {\n            \"total_text_length\": 307938,\n            \"min_text_length\": 0,\n            \"average_text_length\": 2199.557142857143,\n            \"max_text_length\": 2710,\n            \"unique_texts\": 140\n        },\n        \"documents_image_statistics\": null,\n        \"queries_text_statistics\": {\n            \"total_text_length\": 2195,\n            \"min_text_length\": 55,\n            \"average_text_length\": 109.75,\n            \"max_text_length\": 278,\n            \"unique_texts\": 20\n        },\n        \"queries_image_statistics\": null,\n        \"relevant_docs_statistics\": {\n            \"num_relevant_docs\": 60,\n            \"min_relevant_docs_per_query\": 7,\n            \"average_relevant_docs_per_query\": 3.0,\n            \"max_relevant_docs_per_query\": 7,\n            \"unique_relevant_docs\": 140\n        },\n        \"top_ranked_statistics\": {\n            \"num_top_ranked\": 140,\n            \"min_top_ranked_per_query\": 7,\n            \"average_top_ranked_per_query\": 7.0,\n            \"max_top_ranked_per_query\": 7\n        }\n    }\n}\n</code></pre></p> <p>Documentation for the descriptive statistics types.</p>"},{"location":"whats_new/#saving-predictions","title":"Saving Predictions","text":"<p>To support error analysis it is now possible to save the model prediction on a given task. You can do this simply as follows: <pre><code>import mteb\n\n# using a small model and small dataset\nencoder = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\ntask = mteb.get_task(\"NanoArguAnaRetrieval\")\n\nprediction_folder = \"path/to/model_predictions\"\n\nres = mteb.evaluate(\n    encoder,\n    task,\n    prediction_folder=prediction_folder,\n)\n</code></pre></p> <p>Result of prediction will be saved in <code>path/to/model_predictions/{task_name}_predictions.json</code> and will look like so for retrieval tasks:</p> <pre><code>{\n  \"test\": {\n        \"query1\": {\"document1\": 0.77, \"document2\": 0.12, ...},\n        \"query2\": {\"document2\": 0.87, \"document1\": 0.32, ...},\n        ...\n    }\n}\n</code></pre>"},{"location":"whats_new/#support-datasets-v4","title":"Support datasets v4","text":"<p>With the new functionality for reuploading datasets to the standard datasets Parquet format, we\u2019ve reuploaded all tasks with <code>trust_remote_code</code>, and MTEB now fully supports Datasets v4.</p>"},{"location":"whats_new/#upgrading-from-v1","title":"Upgrading from v1","text":"<p>This section gives an introduction of how to upgrade from v1 to v2.</p>"},{"location":"whats_new/#replacing-mtebmteb","title":"Replacing <code>mteb.MTEB</code>","text":"<p>The previous approach to evaluate would require you to first create <code>MTEB</code> object and then call <code>.run</code> on that object. The <code>MTEB</code> object was initially a sort of catch all object intended for both filtering tasks, selecting tasks, evaluating and few other cases.</p> <p>This overload of functionality made it hard to change. We have already for a while made it easier to filter and select tasks using <code>get_tasks</code> and <code>mteb.evaluate</code> now superseded <code>MTEB</code> as the method for evaluation.</p> <pre><code># Approach before 2.0.0:\neval = mteb.MTEB(tasks=tasks) # now throw a deprecation warning\nresults = eval.run(\n    model,\n    overwrite=True,\n    encode_kwargs={},\n    ...\n)\n\n# Recommended:\nmteb.evaluate(\n    model,\n    tasks,\n    overwrite_strategy=\"only-missing\", # only rerun missing splits\n    encode_kwargs={},\n    ...\n)\n</code></pre>"},{"location":"whats_new/#replacing-mtebload_results","title":"Replacing <code>mteb.load_results()</code>","text":"<p>Given the new <code>ResultCache</code> makes dealing with a results from both local and online caches a lot easier, it can now replace <code>mteb.load_results</code> it</p> <pre><code>tasks = mteb.get_tasks(tasks=[\"STS12\"])\nmodel_names = [\"intfloat/multilingual-e5-large\"]\n\n# Approach before 2.0.0:\nresults = mteb.load_results(models=model_names, tasks=tasks, download_latest=True)\n\n# Recommended:\ncache = ResultCache(\"~/.cache/mteb\")  # default\ncache.download_from_remote()  # downloads remote results\n\nresults = cache.load_results(models=model_names, tasks=tasks)\n</code></pre>"},{"location":"whats_new/#converting-model-to-new-format","title":"Converting model to new format","text":"<p>As mentioned in the above section MTEB v2, now supports multimodal input as the default. Luckily for you all models implemented in MTEB already supports this new format! However, if you have a local model that you would like to evaluate Here is a quick conversion guide. If you previous implementation looks like so:</p> <pre><code># v1.X.X\nclass MyDummyEncoder:\n    def __init__(self, **kwargs):\n        self.model = ...\n\n    def encode(self, sentences: list[str], **kwargs) -&gt; Array:\n        embeddings = self.model.encode(sentences)\n        return embeddings\n</code></pre> <p>You can simply unpack it to its text input like so:</p> <pre><code># v2.0.0\nclass MyDummyEncoder:\n    def __init__(self, **kwargs):\n        self.model = ...\n\n    def encode(self, input: DataLoader[BatchedInput], **kwargs) -&gt; Array:\n        # unpack to v1 format:\n        sentences = [text for batch in inputs for text in batch[\"text\"]]\n        # do as you did beforehand:\n        embeddings = self.model.encode(sentences)\n        return embeddings\n</code></pre> <p>Of course, it will be more efficient if you work directly with the dataloader.</p>"},{"location":"whats_new/#reuploading-datasets","title":"Reuploading datasets","text":"<p>If your dataset is in old format, or you want to reupload it to the new Parquet format, you can do so using the new <code>push_dataset_to_hub</code> method:</p> <pre><code>import mteb\n\ntask = mteb.get_task(\"MyOldTask\")\ntask.push_dataset_to_hub(\"my-username/my-new-task\")\n</code></pre>"},{"location":"whats_new/#converting-reranking-datasets-to-new-format","title":"Converting Reranking datasets to new format","text":"<p>If you have a reranking dataset, you can convert it to the retrieval format. To do this you need to add your task name to the <code>mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS</code> and after this it would be converted to the new format automatically. To reupload them in new reranking format you refer to the reuploading datasets section.</p> <pre><code>import mteb\nfrom mteb.abstasks.text.reranking import OLD_FORMAT_RERANKING_TASKS\n\nOLD_FORMAT_RERANKING_TASKS.append(\"MyOldRerankingTask\")\n\ntask = mteb.get_task(\"MyOldRerankingTask\")\nmodel = ...\nmteb.evaluate(model, task)\n</code></pre> <ol> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"advanced_usage/cache_embeddings/","title":"Cache embeddings","text":""},{"location":"advanced_usage/cache_embeddings/#caching-embeddings-to-re-use-them","title":"Caching Embeddings To Re-Use Them","text":"<p>There are times you may want to cache the embeddings so you can re-use them. This may be true if you have multiple query sets for the same corpus (e.g. Wikipedia) or are doing some optimization over the queries (e.g. prompting, other experiments). You can setup a cache by using a simple wrapper, which will save the cache per task in the <code>&lt;path_to_cache_dir&gt;/&lt;task_name&gt;</code> folder:</p> <pre><code># define your task(s) and model above as normal\ntask = mteb.get_task(\"LccSentimentClassification\")\nmodel = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\n\n# wrap the model with the cache wrapper\nfrom mteb.models.cache_wrapper import CachedEmbeddingWrapper\nmodel_with_cached_emb = CachedEmbeddingWrapper(model, cache_path='path_to_cache_dir')\n# run as normal\nresults = mteb.evaluate(model_with_cached_emb, tasks=[task])\n</code></pre> <p>If you want to directly access the cached embeddings (e.g. for subsequent analyses) follow this example:</p> <pre><code>import numpy as np\nfrom mteb.models.cache_wrapper import TextVectorMap\n\n# Access the memory-mapped file and convert to array\nvector_map = TextVectorMap(\"path_to_cache_dir/LccSentimentClassification\")\nvector_map.load(name=\"LccSentimentClassification\")\nvectors = np.asarray(vector_map.vectors)\n\n# Remove all \"placeholders\" in the embedding cache\nzero_mask = (vectors == 0).all(axis=1)\nvectors = vectors[~zero_mask]\n</code></pre>"},{"location":"advanced_usage/cache_embeddings/#different-cache-backends","title":"Different Cache backends","text":"<p>By default, the <code>CachedEmbeddingWrapper</code> uses a NumPy memmap backend (<code>NumpyCache</code>) to store embeddings. However, you can also use other backends. Currently, only <code>FAISS</code> is implemented, but you can provide your own custom backend that implements the <code>CacheBackendProtocol</code> by passing it as the <code>search_backend</code> parameter when initializing the <code>CachedEmbeddingWrapper</code>. For example:</p> <pre><code>import mteb\nfrom mteb.models.cache_wrappers.cache_backends import FaissCache\nfrom mteb.models import CachedEmbeddingWrapper\n\nmodel = mteb.get_model(...)\ncachedmodel = CachedEmbeddingWrapper(model, \"cache_dir\", cache_backend=FaissCache)\n</code></pre>"},{"location":"advanced_usage/two_stage_reranking/","title":"Two stage reranking","text":""},{"location":"advanced_usage/two_stage_reranking/#two-stage-reranking","title":"Two stage reranking","text":"<p>To use a cross encoder for reranking. The following code shows a two-stage run with the second stage reading results saved from the first stage.</p> <pre><code>from sentence_transformers import CrossEncoder\n\nimport mteb\n\nencoder = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\ntask = mteb.get_task(\"NanoArguAnaRetrieval\")\n\nprediction_folder = \"model_predictions\"\n\n# stage 1: retrieval\nres = mteb.evaluate(\n    encoder,\n    task,\n    prediction_folder=prediction_folder,\n)\n\n# convert task to retrieval\ntask = task.convert_to_reranking(prediction_folder, top_k=100)\n\n# stage 2: reranking\n# if model implemented in mteb it's better to use `mteb.get_model`\n# cross_encoder = mteb.get_model(\"jinaai/jina-reranker-v2-base-multilingual\")\n# or if model is't implemented you can pass CrossEncoder directly\ncross_encoder = CrossEncoder(\"cross-encoder/ms-marco-TinyBERT-L-2-v2\")\ncross_enc_results = mteb.evaluate(cross_encoder, task)\n\nprint(task.metadata.main_score) # NDCG@10\nres[0].get_score()  # 0.286\ncross_enc_results[0].get_score() # 0.338\n</code></pre>"},{"location":"api/","title":"Overview","text":"<p>This is the API documentation for <code>mteb</code> a package for benchmark and evaluating the quality of embeddings. This package was initially introduced as a package for evaluating text embeddings for English<sup>1</sup>, but have since been extended cover multiple languages<sup>2</sup> and multiple modalities<sup>3</sup>.</p>"},{"location":"api/#package-overview","title":"Package Overview","text":"<p>This package generally consists of three main concepts benchmarks, tasks and model implementations.</p>"},{"location":"api/#benchmarks","title":"Benchmarks","text":"<p>A benchmark is a tool to evaluate an embedding model for a given use case. For instance, <code>mteb(eng)</code> is intended to evaluate the quality of text embedding models for broad range of English use-cases such retrieval, classification, and reranking. A benchmark consist of a collection of tasks. When a model is run on a benchmark it is run on each task individually.</p> An overview of the benchmark within <code>mteb</code>"},{"location":"api/#task","title":"Task","text":"<p>A task is an implementation of a dataset for evaluation. It could for instance be the MIRACL dataset consisting of queries, a corpus of documents as well as the correct documents to retrieve for a given query. In addition to the dataset a task includes specification for how a model should be run on the dataset and how its output should be evaluation. We implement a variety of different tasks e.g. for evaluating classification, retrieval etc., We denote these task categories. Each task also come with extensive metadata including the license, who annotated the data and so on.</p> An overview of the tasks within <code>mteb</code>"},{"location":"api/#model-implementation","title":"Model Implementation","text":"<p>A model implementation is simply an implementation of an embedding model or API to ensure that others can reproduce the exact results on a given task. For instance, when running the OpenAI embedding API on a document larger than the maximum amount of tokens a user will have to decide how they want to deal with this limitations (e.g. by truncating the sequence). Having a shared implementation allow us to examine these implementation assumptions and allow for reproducible workflow. To ensure consistency we define a standard interface/protocol that models should follow to be implemented. These implementations additionally come with metadata, that for example include license, compatible frameworks, and whether the weight are public or not.</p> An overview of the model and its metadata within <code>mteb</code> <ol> <li> <p>Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2014\u20132037. Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL: https://aclanthology.org/2023.eacl-main.148, doi:10.18653/v1/2023.eacl-main.148.\u00a0\u21a9</p> </li> <li> <p>Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. URL: https://arxiv.org/abs/2502.13595, doi:10.48550/arXiv.2502.13595.\u00a0\u21a9</p> </li> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/benchmark/","title":"Benchmark","text":"<p>A benchmark within <code>mteb</code> is essentially just a list of tasks along with some metadata about the benchmark.</p> An overview of the benchmark within <code>mteb</code> <p>This metadata includes a short description of the benchmark's intention, the reference, and the citation. If you use a benchmark from <code>mteb</code>, we recommend that you cite it along with <code>mteb</code>.</p>"},{"location":"api/benchmark/#utilities","title":"Utilities","text":""},{"location":"api/benchmark/#mteb.get_benchmarks","title":"<code>mteb.get_benchmarks(names=None, display_on_leaderboard=None)</code>","text":"<p>Get a list of benchmarks by name.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str] | None</code> <p>A list of benchmark names to retrieve. If None, all benchmarks are returned.</p> <code>None</code> <code>display_on_leaderboard</code> <code>bool | None</code> <p>If specified, filters benchmarks by whether they are displayed on the leaderboard.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Benchmark]</code> <p>A list of Benchmark instances.</p> Source code in <code>mteb/benchmarks/get_benchmark.py</code> <pre><code>def get_benchmarks(\n    names: list[str] | None = None, display_on_leaderboard: bool | None = None\n) -&gt; list[Benchmark]:\n    \"\"\"Get a list of benchmarks by name.\n\n    Args:\n        names: A list of benchmark names to retrieve. If None, all benchmarks are returned.\n        display_on_leaderboard: If specified, filters benchmarks by whether they are displayed on the leaderboard.\n\n    Returns:\n        A list of Benchmark instances.\n    \"\"\"\n    benchmark_registry = _build_registry()\n\n    if names is None:\n        names = list(benchmark_registry.keys())\n    benchmarks = [get_benchmark(name) for name in names]\n    if display_on_leaderboard is not None:\n        benchmarks = [\n            b for b in benchmarks if b.display_on_leaderboard is display_on_leaderboard\n        ]\n    return benchmarks\n</code></pre>"},{"location":"api/benchmark/#mteb.get_benchmark","title":"<code>mteb.get_benchmark(benchmark_name)</code>","text":"<p>Get a benchmark by name.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_name</code> <code>str</code> <p>The name of the benchmark to retrieve.</p> required <p>Returns:</p> Type Description <code>Benchmark</code> <p>The Benchmark instance corresponding to the given name.</p> Source code in <code>mteb/benchmarks/get_benchmark.py</code> <pre><code>def get_benchmark(\n    benchmark_name: str,\n) -&gt; Benchmark:\n    \"\"\"Get a benchmark by name.\n\n    Args:\n        benchmark_name: The name of the benchmark to retrieve.\n\n    Returns:\n        The Benchmark instance corresponding to the given name.\n    \"\"\"\n    previous_benchmark_names = _get_previous_benchmark_names()\n    benchmark_registry = _build_registry()\n    if benchmark_name in previous_benchmark_names:\n        warnings.warn(\n            f\"Using the previous benchmark name '{benchmark_name}' is deprecated. Please use '{previous_benchmark_names[benchmark_name]}' instead.\",\n            DeprecationWarning,\n        )\n        benchmark_name = previous_benchmark_names[benchmark_name]\n    if benchmark_name not in benchmark_registry:\n        close_matches = difflib.get_close_matches(\n            benchmark_name, benchmark_registry.keys()\n        )\n        if close_matches:\n            suggestion = f\"KeyError: '{benchmark_name}' not found. Did you mean: {close_matches[0]}?\"\n        else:\n            suggestion = f\"KeyError: '{benchmark_name}' not found and no similar keys were found.\"\n        raise KeyError(suggestion)\n    return benchmark_registry[benchmark_name]\n</code></pre>"},{"location":"api/benchmark/#the-benchmark-object","title":"The Benchmark Object","text":""},{"location":"api/benchmark/#mteb.Benchmark","title":"<code>mteb.Benchmark</code>  <code>dataclass</code>","text":"<p>A benchmark object intended to run a certain benchmark within MTEB.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the benchmark</p> required <code>tasks</code> <code>Sequence[AbsTask]</code> <p>The tasks within the benchmark.</p> required <code>description</code> <code>str | None</code> <p>A description of the benchmark, should include its intended goal and potentially a description of its construction</p> <code>None</code> <code>reference</code> <code>StrURL | None</code> <p>A link reference, to a source containing additional information typically to a paper, leaderboard or github.</p> <code>None</code> <code>citation</code> <code>str | None</code> <p>A bibtex citation</p> <code>None</code> <code>contacts</code> <code>list[str] | None</code> <p>The people to contact in case of a problem in the benchmark, preferably a GitHub handle.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Benchmark(\n...     name=\"MTEB(custom)\",\n...     tasks=mteb.get_tasks(\n...         tasks=[\"AmazonCounterfactualClassification\", \"AmazonPolarityClassification\"],\n...         languages=[\"eng\"],\n...     ),\n...     description=\"A custom benchmark\"\n... )\n</code></pre> Source code in <code>mteb/benchmarks/benchmark.py</code> <pre><code>@dataclass\nclass Benchmark:\n    \"\"\"A benchmark object intended to run a certain benchmark within MTEB.\n\n    Args:\n        name: The name of the benchmark\n        tasks: The tasks within the benchmark.\n        description: A description of the benchmark, should include its intended goal and potentially a description of its construction\n        reference: A link reference, to a source containing additional information typically to a paper, leaderboard or github.\n        citation: A bibtex citation\n        contacts: The people to contact in case of a problem in the benchmark, preferably a GitHub handle.\n\n    Examples:\n        &gt;&gt;&gt; Benchmark(\n        ...     name=\"MTEB(custom)\",\n        ...     tasks=mteb.get_tasks(\n        ...         tasks=[\"AmazonCounterfactualClassification\", \"AmazonPolarityClassification\"],\n        ...         languages=[\"eng\"],\n        ...     ),\n        ...     description=\"A custom benchmark\"\n        ... )\n    \"\"\"\n\n    name: str\n    tasks: Sequence[\"AbsTask\"]\n    description: str | None = None\n    reference: StrURL | None = None\n    citation: str | None = None\n    contacts: list[str] | None = None\n    display_on_leaderboard: bool = True\n    icon: str | None = None\n    display_name: str | None = None\n\n    def __iter__(self) -&gt; Iterable[\"AbsTask\"]:\n        return iter(self.tasks)\n\n    def __len__(self) -&gt; int:\n        return len(self.tasks)\n\n    def __getitem__(self, index: int) -&gt; \"AbsTask\":\n        return self.tasks[index]\n\n    def _create_summary_table(\n        self, benchmark_results: BenchmarkResults\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create summary table. Called by the leaderboard app.\n\n        Returns:\n            A pandas DataFrame representing the summary results.\n        \"\"\"\n        return _create_summary_table_from_benchmark_results(benchmark_results)\n\n    def _create_per_task_table(\n        self, benchmark_results: BenchmarkResults\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create per-task table. Called by the leaderboard app.\n\n        Returns:\n            A pandas DataFrame representing the per-task results.\n        \"\"\"\n        return _create_per_task_table_from_benchmark_results(benchmark_results)\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation","text":""},{"location":"api/evaluation/#mteb.evaluate","title":"<code>mteb.evaluate</code>","text":""},{"location":"api/evaluation/#mteb.evaluate.OverwriteStrategy","title":"<code>OverwriteStrategy</code>","text":"<p>               Bases: <code>HelpfulStrEnum</code></p> <p>Enum for the overwrite strategy when running a task.</p> <ul> <li>\"always\": Always run the task, overwriting the results</li> <li>\"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.</li> <li>\"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has     changed.</li> <li>\"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the     cache.</li> </ul> Source code in <code>mteb/evaluate.py</code> <pre><code>class OverwriteStrategy(HelpfulStrEnum):\n    \"\"\"Enum for the overwrite strategy when running a task.\n\n    - \"always\": Always run the task, overwriting the results\n    - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.\n    - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has\n        changed.\n    - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the\n        cache.\n    \"\"\"\n\n    ALWAYS = \"always\"\n    NEVER = \"never\"\n    ONLY_MISSING = \"only-missing\"\n    ONLY_CACHE = \"only-cache\"\n</code></pre>"},{"location":"api/evaluation/#mteb.evaluate.evaluate","title":"<code>evaluate(model, tasks, *, co2_tracker=None, raise_error=True, encode_kwargs=None, cache=ResultCache(), overwrite_strategy='only-missing', prediction_folder=None, show_progress_bar=True)</code>","text":"<p>This function runs a model on a given task and returns the results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelMeta | MTEBModels | SentenceTransformer | CrossEncoder</code> <p>The model to use for encoding.</p> required <code>tasks</code> <code>AbsTask | Iterable[AbsTask]</code> <p>A task to run.</p> required <code>co2_tracker</code> <code>bool | None</code> <p>If True, track the CO\u2082 emissions of the evaluation, required codecarbon to be installed, which can be installed using <code>pip install mteb[codecarbon]</code>. If none is passed co2 tracking will only be run if codecarbon is installed.</p> <code>None</code> <code>encode_kwargs</code> <code>dict[str, Any] | None</code> <p>Additional keyword arguments passed to the models <code>encode</code> method.</p> <code>None</code> <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the task fails. If False, return an empty list.</p> <code>True</code> <code>cache</code> <code>ResultCache | None</code> <p>The cache to use for loading the results. If None, then no cache will be used. The default cache saved the cache in the <code>~/.cache/mteb</code> directory. It can be overridden by setting the <code>MTEB_CACHE</code> environment variable to a different directory or by directly passing a <code>ResultCache</code> object.</p> <code>ResultCache()</code> <code>overwrite_strategy</code> <code>str | OverwriteStrategy</code> <p>The strategy to use for run a task and overwrite the results. Can be: - \"always\": Always run the task, overwriting the results - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task. - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has     changed. - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the     cache.</p> <code>'only-missing'</code> <code>prediction_folder</code> <code>Path | str | None</code> <p>Optional folder in which to save model predictions for the task. Predictions of the tasks will be sabed in <code>prediction_folder/{task_name}_predictions.json</code></p> <code>None</code> <code>show_progress_bar</code> <code>bool</code> <p>Whether to show a progress bar when running the evaluation. Default is True. Setting this to False will also set the <code>encode_kwargs['show_progress_bar']</code> to False if encode_kwargs is unspecified.</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelResult</code> <p>The results of the evaluation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n&gt;&gt;&gt; task = mteb.get_task(\"STS12\")\n&gt;&gt;&gt; result = mteb.evaluate(ModelMeta, task)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with CO2 tracking\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, co2_tracker=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with encode kwargs\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, encode_kwargs={\"batch_size\": 16})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with online cache\n&gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; cache.download_from_remote()\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, cache=cache)\n</code></pre> Source code in <code>mteb/evaluate.py</code> <pre><code>def evaluate(\n    model: ModelMeta | MTEBModels | SentenceTransformer | CrossEncoder,\n    tasks: AbsTask | Iterable[AbsTask],\n    *,\n    co2_tracker: bool | None = None,\n    raise_error: bool = True,\n    encode_kwargs: dict[str, Any] | None = None,\n    cache: ResultCache | None = ResultCache(),\n    overwrite_strategy: str | OverwriteStrategy = \"only-missing\",\n    prediction_folder: Path | str | None = None,\n    show_progress_bar: bool = True,\n) -&gt; ModelResult:\n    \"\"\"This function runs a model on a given task and returns the results.\n\n    Args:\n        model: The model to use for encoding.\n        tasks: A task to run.\n        co2_tracker: If True, track the CO\u2082 emissions of the evaluation, required codecarbon to be installed, which can be installed using\n            `pip install mteb[codecarbon]`. If none is passed co2 tracking will only be run if codecarbon is installed.\n        encode_kwargs: Additional keyword arguments passed to the models `encode` method.\n        raise_error: If True, raise an error if the task fails. If False, return an empty list.\n        cache: The cache to use for loading the results. If None, then no cache will be used. The default cache saved the cache in the\n            `~/.cache/mteb` directory. It can be overridden by setting the `MTEB_CACHE` environment variable to a different directory or by directly\n            passing a `ResultCache` object.\n        overwrite_strategy: The strategy to use for run a task and overwrite the results. Can be:\n            - \"always\": Always run the task, overwriting the results\n            - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.\n            - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has\n                changed.\n            - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the\n                cache.\n        prediction_folder: Optional folder in which to save model predictions for the task. Predictions of the tasks will be sabed in `prediction_folder/{task_name}_predictions.json`\n        show_progress_bar: Whether to show a progress bar when running the evaluation. Default is True. Setting this to False will also set the\n            `encode_kwargs['show_progress_bar']` to False if encode_kwargs is unspecified.\n\n    Returns:\n        The results of the evaluation.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n        &gt;&gt;&gt; task = mteb.get_task(\"STS12\")\n        &gt;&gt;&gt; result = mteb.evaluate(ModelMeta, task)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with CO2 tracking\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, co2_tracker=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with encode kwargs\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, encode_kwargs={\"batch_size\": 16})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with online cache\n        &gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cache.download_from_remote()\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, cache=cache)\n    \"\"\"\n    if isinstance(prediction_folder, str):\n        prediction_folder = Path(prediction_folder)\n\n    if encode_kwargs is None:\n        encode_kwargs = (\n            {\"show_progress_bar\": False} if show_progress_bar is False else {}\n        )\n    if \"batch_size\" not in encode_kwargs:\n        encode_kwargs[\"batch_size\"] = 32\n        logger.info(\n            \"No batch size defined in encode_kwargs. Setting `encode_kwargs['batch_size'] = 32`. Explicitly set the batch size to silence this message.\"\n        )\n\n    model, meta, model_name, model_revision = _sanitize_model(model)\n    _check_model_modalities(meta, tasks)\n\n    # AbsTaskAggregate is a special case where we have to run multiple tasks and combine the results\n    if isinstance(tasks, AbsTaskAggregate):\n        task = cast(AbsTaskAggregate, tasks)\n        results = evaluate(\n            model,\n            task.metadata.tasks,\n            co2_tracker=co2_tracker,\n            raise_error=raise_error,\n            encode_kwargs=encode_kwargs,\n            cache=cache,\n            overwrite_strategy=overwrite_strategy,\n            prediction_folder=prediction_folder,\n            show_progress_bar=show_progress_bar,\n        )\n        result = task.combine_task_results(results.task_results)\n        return ModelResult(\n            model_name=results.model_name,\n            model_revision=results.model_revision,\n            task_results=[result],\n        )\n\n    if isinstance(tasks, AbsTask):\n        task = tasks\n    else:\n        results = []\n        tasks_tqdm = tqdm(\n            tasks,\n            desc=\"Evaluating tasks\",\n            disable=not show_progress_bar,\n        )\n        for i, task in enumerate(tasks_tqdm):\n            tasks_tqdm.set_description(f\"Evaluating task {task.metadata.name}\")\n            _res = evaluate(\n                model,\n                task,\n                co2_tracker=co2_tracker,\n                raise_error=raise_error,\n                encode_kwargs=encode_kwargs,\n                cache=cache,\n                overwrite_strategy=overwrite_strategy,\n                prediction_folder=prediction_folder,\n                show_progress_bar=False,\n            )\n            results.extend(_res.task_results)\n        return ModelResult(\n            model_name=_res.model_name,\n            model_revision=_res.model_revision,\n            task_results=results,\n        )\n\n    overwrite_strategy = OverwriteStrategy.from_str(overwrite_strategy)\n\n    existing_results = None\n    if cache and overwrite_strategy != OverwriteStrategy.ALWAYS:\n        results = cache.load_task_result(task.metadata.name, meta)\n        if results:\n            existing_results = results\n\n    if (\n        existing_results\n        and overwrite_strategy == \"only-missing\"\n        and overwrite_strategy == OverwriteStrategy.ONLY_MISSING\n        and existing_results.is_mergeable(task)\n    ):\n        missing_eval = existing_results.get_missing_evaluations(task)\n    else:\n        missing_eval = dict.fromkeys(task.eval_splits, task.hf_subsets)\n\n    if (\n        existing_results\n        and not missing_eval\n        and overwrite_strategy != OverwriteStrategy.ALWAYS\n    ):\n        # if there are no missing evals we can just return the results\n        logger.info(\n            f\"Results for {task.metadata.name} already exist in cache. Skipping evaluation and loading results.\"\n        )\n        return ModelResult(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_results=[existing_results],\n        )\n    if missing_eval and overwrite_strategy in [\n        OverwriteStrategy.NEVER,\n        OverwriteStrategy.ONLY_CACHE,\n    ]:\n        raise ValueError(\n            f\"overwrite_strategy is set to '{overwrite_strategy.value}' and the results file exists. However there are the following missing splits (and subsets): {missing_eval}. To rerun these set overwrite_strategy to 'only-missing'.\"\n        )\n\n    if existing_results:\n        logger.info(\n            f\"Found existing results for {task.metadata.name}, only running missing splits: {list(missing_eval.keys())}\"\n        )\n\n    if isinstance(model, ModelMeta):\n        logger.info(\n            f\"Loading model {model_name} with revision {model_revision} from ModelMeta.\"\n        )\n        model = model.load_model()\n        logger.info(\"\u2713 Model loaded\")\n\n    if raise_error is False:\n        try:\n            result = _evaluate_task(\n                model=model,\n                splits=missing_eval,\n                task=task,\n                co2_tracker=co2_tracker,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n            )\n        except Exception as e:\n            logger.error(\n                f\"Error while running task {task.metadata.name} on splits {list(missing_eval.keys())}: {e}\"\n            )\n            return ModelResult(\n                model_name=model_name,\n                model_revision=model_revision,\n                task_results=[],\n            )\n    else:\n        result = _evaluate_task(\n            model=model,\n            splits=missing_eval,\n            task=task,\n            co2_tracker=False,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n        )\n    logger.info(f\"\u2713 Finished evaluation for {task.metadata.name}\")\n\n    if existing_results:\n        result = result.merge(existing_results)\n\n    if cache:\n        cache.save_to_cache(result, meta)\n\n    return ModelResult(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_results=[result],\n    )\n</code></pre>"},{"location":"api/model/","title":"Models","text":"<p>A model in <code>mteb</code> covers two concepts: metadata and implementation. - Metadata contains information about the model such as maximum input length, valid frameworks, license, and degree of openness. - Implementation is a reproducible workflow, which allows others to run the same model again, using the same prompts, hyperparameters, aggregation strategies, etc.</p> An overview of the model and its metadata within <code>mteb</code>"},{"location":"api/model/#utilities","title":"Utilities","text":""},{"location":"api/model/#mteb.get_model_metas","title":"<code>mteb.get_model_metas(model_names=None, languages=None, open_weights=None, frameworks=None, n_parameters_range=(None, None), use_instructions=None, zero_shot_on=None)</code>","text":"<p>Load all models' metadata that fit the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>Iterable[str] | None</code> <p>A list of model names to filter by. If None, all models are included.</p> <code>None</code> <code>languages</code> <code>Iterable[str] | None</code> <p>A list of languages to filter by. If None, all languages are included.</p> <code>None</code> <code>open_weights</code> <code>bool | None</code> <p>Whether to filter by models with open weights. If None this filter is ignored.</p> <code>None</code> <code>frameworks</code> <code>Iterable[str] | None</code> <p>A list of frameworks to filter by. If None, all frameworks are included.</p> <code>None</code> <code>n_parameters_range</code> <code>tuple[int | None, int | None]</code> <p>A tuple of lower and upper bounds of the number of parameters to filter by. If (None, None), this filter is ignored.</p> <code>(None, None)</code> <code>use_instructions</code> <code>bool | None</code> <p>Whether to filter by models that use instructions. If None, all models are included.</p> <code>None</code> <code>zero_shot_on</code> <code>list[AbsTask] | None</code> <p>A list of tasks on which the model is zero-shot. If None this filter is ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ModelMeta]</code> <p>A list of model metadata objects that fit the specified criteria.</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model_metas(\n    model_names: Iterable[str] | None = None,\n    languages: Iterable[str] | None = None,\n    open_weights: bool | None = None,\n    frameworks: Iterable[str] | None = None,\n    n_parameters_range: tuple[int | None, int | None] = (None, None),\n    use_instructions: bool | None = None,\n    zero_shot_on: list[AbsTask] | None = None,\n) -&gt; list[ModelMeta]:\n    \"\"\"Load all models' metadata that fit the specified criteria.\n\n    Args:\n        model_names: A list of model names to filter by. If None, all models are included.\n        languages: A list of languages to filter by. If None, all languages are included.\n        open_weights: Whether to filter by models with open weights. If None this filter is ignored.\n        frameworks: A list of frameworks to filter by. If None, all frameworks are included.\n        n_parameters_range: A tuple of lower and upper bounds of the number of parameters to filter by.\n            If (None, None), this filter is ignored.\n        use_instructions: Whether to filter by models that use instructions. If None, all models are included.\n        zero_shot_on: A list of tasks on which the model is zero-shot. If None this filter is ignored.\n\n    Returns:\n        A list of model metadata objects that fit the specified criteria.\n    \"\"\"\n    res = []\n    model_names = set(model_names) if model_names is not None else None\n    languages = set(languages) if languages is not None else None\n    frameworks = set(frameworks) if frameworks is not None else None\n    for model_meta in MODEL_REGISTRY.values():\n        if (model_names is not None) and (model_meta.name not in model_names):\n            continue\n        if languages is not None:\n            if (model_meta.languages is None) or not (\n                languages &lt;= set(model_meta.languages)\n            ):\n                continue\n        if (open_weights is not None) and (model_meta.open_weights != open_weights):\n            continue\n        if (frameworks is not None) and not (frameworks &lt;= set(model_meta.framework)):\n            continue\n        if (use_instructions is not None) and (\n            model_meta.use_instructions != use_instructions\n        ):\n            continue\n\n        lower, upper = n_parameters_range\n        n_parameters = model_meta.n_parameters\n\n        if upper is not None:\n            if (n_parameters is None) or (n_parameters &gt; upper):\n                continue\n            if lower is not None and n_parameters &lt; lower:\n                continue\n\n        if zero_shot_on is not None:\n            if not model_meta.is_zero_shot_on(zero_shot_on):\n                continue\n        res.append(model_meta)\n    return res\n</code></pre>"},{"location":"api/model/#mteb.get_model_meta","title":"<code>mteb.get_model_meta(model_name, revision=None, fetch_from_hf=True)</code>","text":"<p>A function to fetch a model metadata object by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to fetch</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model to fetch</p> <code>None</code> <code>fetch_from_hf</code> <code>bool</code> <p>Whether to fetch the model from HuggingFace Hub if not found in the registry</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelMeta</code> <p>A model metadata object</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model_meta(\n    model_name: str, revision: str | None = None, fetch_from_hf: bool = True\n) -&gt; ModelMeta:\n    \"\"\"A function to fetch a model metadata object by name.\n\n    Args:\n        model_name: Name of the model to fetch\n        revision: Revision of the model to fetch\n        fetch_from_hf: Whether to fetch the model from HuggingFace Hub if not found in the registry\n\n    Returns:\n        A model metadata object\n    \"\"\"\n    if model_name in MODEL_REGISTRY:\n        model_meta = MODEL_REGISTRY[model_name]\n\n        if revision and (not model_meta.revision == revision):\n            raise ValueError(\n                f\"Model revision {revision} not found for model {model_name}. Expected {model_meta.revision}.\"\n            )\n        return model_meta\n    if fetch_from_hf:\n        logger.info(\n            \"Model not found in model registry. Attempting to extract metadata by loading the model ({model_name}) using HuggingFace.\"\n        )\n        try:\n            meta = _model_meta_from_hf_hub(model_name)\n            meta.revision = revision\n            return meta\n        except RepositoryNotFoundError:\n            pass\n\n    not_found_msg = f\"Model '{model_name}' not found in MTEB registry\"\n    not_found_msg += \" nor on the Huggingface Hub.\" if fetch_from_hf else \".\"\n\n    close_matches = difflib.get_close_matches(model_name, MODEL_REGISTRY.keys())\n    model_names_no_org = {mdl: mdl.split(\"/\")[-1] for mdl in MODEL_REGISTRY.keys()}\n    if model_name in model_names_no_org:\n        close_matches = [model_names_no_org[model_name]] + close_matches\n\n    suggestion = \"\"\n    if close_matches:\n        if len(close_matches) &gt; 1:\n            suggestion = f\" Did you mean: '{close_matches[0]}' or {close_matches[1]}?\"\n        else:\n            suggestion = f\" Did you mean: '{close_matches[0]}'?\"\n\n    raise KeyError(not_found_msg + suggestion)\n</code></pre>"},{"location":"api/model/#mteb.get_model","title":"<code>mteb.get_model(model_name, revision=None, **kwargs)</code>","text":"<p>A function to fetch and load model object by name.</p> <p>Note</p> <p>This function loads the model into memory. If you only want to fetch the metadata, use <code>get_model_meta</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to fetch</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model to fetch</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model loader</p> <code>{}</code> <p>Returns:</p> Type Description <code>MTEBModels</code> <p>A model object</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model(\n    model_name: str, revision: str | None = None, **kwargs: Any\n) -&gt; MTEBModels:\n    \"\"\"A function to fetch and load model object by name.\n\n    !!! note\n        This function loads the model into memory. If you only want to fetch the metadata, use [`get_model_meta`](#mteb.get_model_meta) instead.\n\n    Args:\n        model_name: Name of the model to fetch\n        revision: Revision of the model to fetch\n        **kwargs: Additional keyword arguments to pass to the model loader\n\n    Returns:\n        A model object\n    \"\"\"\n    from sentence_transformers import CrossEncoder, SentenceTransformer\n\n    meta = get_model_meta(model_name, revision)\n    model = meta.load_model(**kwargs)\n\n    # If revision not available in the modelmeta, try to extract it from sentence-transformers\n    if hasattr(model, \"model\") and isinstance(model.model, SentenceTransformer):  # type: ignore\n        _meta = _model_meta_from_sentence_transformers(model.model)  # type: ignore\n        if meta.revision is None:\n            meta.revision = _meta.revision if _meta.revision else meta.revision\n        if not meta.similarity_fn_name:\n            meta.similarity_fn_name = _meta.similarity_fn_name\n\n    elif isinstance(model, CrossEncoder):\n        _meta = _model_meta_from_cross_encoder(model.model)\n        if meta.revision is None:\n            meta.revision = _meta.revision if _meta.revision else meta.revision\n\n    model.mteb_model_meta = meta  # type: ignore\n    return model\n</code></pre>"},{"location":"api/model/#metadata","title":"Metadata","text":""},{"location":"api/model/#mteb.models.model_meta.ModelMeta","title":"<code>mteb.models.model_meta.ModelMeta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The model metadata object.</p> <p>Attributes:</p> Name Type Description <code>loader</code> <code>Callable[..., MTEBModels] | None</code> <p>The function that loads the model. If None it assumes that the model is not implemented.</p> <code>loader_kwargs</code> <code>dict[str, Any]</code> <p>The keyword arguments to pass to the loader function.</p> <code>name</code> <code>str | None</code> <p>The name of the model, ideally the name on huggingface. It should be in the format \"organization/model_name\".</p> <code>n_parameters</code> <code>int | None</code> <p>The number of parameters in the model, e.g. 7_000_000 for a 7M parameter model. Can be None if the number of parameters is not known (e.g. for proprietary models) or if the loader returns a SentenceTransformer model from which it can be derived.</p> <code>memory_usage_mb</code> <code>float | None</code> <p>The memory usage of the model in MB. Can be None if the memory usage is not known (e.g. for proprietary models). To calculate it use the <code>calculate_memory_usage_mb</code> method.</p> <code>max_tokens</code> <code>float | None</code> <p>The maximum number of tokens the model can handle. Can be None if the maximum number of tokens is not known (e.g. for proprietary models).</p> <code>embed_dim</code> <code>int | None</code> <p>The dimension of the embeddings produced by the model. Currently all models are assumed to produce fixed-size embeddings.</p> <code>revision</code> <code>str | None</code> <p>The revision number of the model. If None, it is assumed that the metadata (including the loader) is valid for all revisions of the model.</p> <code>release_date</code> <code>StrDate | None</code> <p>The date the model's revision was released.</p> <code>license</code> <code>Licenses | StrURL | None</code> <p>The license under which the model is released. Required if open_weights is True.</p> <code>open_weights</code> <code>bool | None</code> <p>Whether the model is open source or proprietary.</p> <code>public_training_code</code> <code>str | None</code> <p>A link to the publicly available training code. If None, it is assumed that the training code is not publicly available.</p> <code>public_training_data</code> <code>str | bool | None</code> <p>A link to the publicly available training data. If None, it is assumed that the training data is not publicly available.</p> <code>similarity_fn_name</code> <code>ScoringFunction | None</code> <p>The distance metric used by the model.</p> <code>framework</code> <code>list[FRAMEWORKS]</code> <p>The framework the model is implemented in, can be a list of frameworks e.g. <code>[\"Sentence Transformers\", \"PyTorch\"]</code>.</p> <code>reference</code> <code>StrURL | None</code> <p>A URL to the model's page on huggingface or another source.</p> <code>languages</code> <code>list[ISOLanguageScript] | None</code> <p>The languages the model is intended to be specified as a 3-letter language code followed by a script code e.g., \"eng-Latn\" for English in the Latin script.</p> <code>use_instructions</code> <code>bool | None</code> <p>Whether the model uses instructions E.g. for prompt-based models. This also includes models that require a specific format for input, such as \"query: {document}\" or \"passage: {document}\".</p> <code>citation</code> <code>str | None</code> <p>The citation for the model. This is a bibtex string.</p> <code>training_datasets</code> <code>set[str] | None</code> <p>A dictionary of datasets that the model was trained on. Names should be names as their appear in <code>mteb</code> for example {\"ArguAna\"} if the model is trained on the ArguAna test set. This field is used to determine if a model generalizes zero-shot to a benchmark as well as mark dataset contaminations.</p> <code>adapted_from</code> <code>str | None</code> <p>Name of the model from which this model is adapted. For quantizations, fine-tunes, long doc extensions, etc.</p> <code>superseded_by</code> <code>str | None</code> <p>Name of the model that supersedes this model, e.g., nvidia/NV-Embed-v2 supersedes v1.</p> <code>is_cross_encoder</code> <code>bool | None</code> <p>Whether the model can act as a cross-encoder or not.</p> <code>modalities</code> <code>list[Modalities]</code> <p>A list of strings representing the modalities the model supports. Default is [\"text\"].</p> <code>contacts</code> <code>list[str] | None</code> <p>The people to contact in case of a problem in the model, preferably a GitHub handle.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>class ModelMeta(BaseModel):\n    \"\"\"The model metadata object.\n\n    Attributes:\n        loader: The function that loads the model. If None it assumes that the model is not implemented.\n        loader_kwargs: The keyword arguments to pass to the loader function.\n        name: The name of the model, ideally the name on huggingface. It should be in the format \"organization/model_name\".\n        n_parameters: The number of parameters in the model, e.g. 7_000_000 for a 7M parameter model. Can be None if the number of parameters is not known (e.g. for proprietary models) or\n            if the loader returns a SentenceTransformer model from which it can be derived.\n        memory_usage_mb: The memory usage of the model in MB. Can be None if the memory usage is not known (e.g. for proprietary models). To calculate it use the `calculate_memory_usage_mb` method.\n        max_tokens: The maximum number of tokens the model can handle. Can be None if the maximum number of tokens is not known (e.g. for proprietary\n            models).\n        embed_dim: The dimension of the embeddings produced by the model. Currently all models are assumed to produce fixed-size embeddings.\n        revision: The revision number of the model. If None, it is assumed that the metadata (including the loader) is valid for all revisions of the model.\n        release_date: The date the model's revision was released.\n        license: The license under which the model is released. Required if open_weights is True.\n        open_weights: Whether the model is open source or proprietary.\n        public_training_code: A link to the publicly available training code. If None, it is assumed that the training code is not publicly available.\n        public_training_data: A link to the publicly available training data. If None, it is assumed that the training data is not publicly available.\n        similarity_fn_name: The distance metric used by the model.\n        framework: The framework the model is implemented in, can be a list of frameworks e.g. `[\"Sentence Transformers\", \"PyTorch\"]`.\n        reference: A URL to the model's page on huggingface or another source.\n        languages: The languages the model is intended to be specified as a 3-letter language code followed by a script code e.g., \"eng-Latn\" for English\n            in the Latin script.\n        use_instructions: Whether the model uses instructions E.g. for prompt-based models. This also includes models that require a specific format for\n            input, such as \"query: {document}\" or \"passage: {document}\".\n        citation: The citation for the model. This is a bibtex string.\n        training_datasets: A dictionary of datasets that the model was trained on. Names should be names as their appear in `mteb` for example\n            {\"ArguAna\"} if the model is trained on the ArguAna test set. This field is used to determine if a model generalizes zero-shot to\n            a benchmark as well as mark dataset contaminations.\n        adapted_from: Name of the model from which this model is adapted. For quantizations, fine-tunes, long doc extensions, etc.\n        superseded_by: Name of the model that supersedes this model, e.g., nvidia/NV-Embed-v2 supersedes v1.\n        is_cross_encoder: Whether the model can act as a cross-encoder or not.\n        modalities: A list of strings representing the modalities the model supports. Default is [\"text\"].\n        contacts: The people to contact in case of a problem in the model, preferably a GitHub handle.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    # loaders\n    loader: Callable[..., MTEBModels] | None\n    loader_kwargs: dict[str, Any] = field(default_factory=dict)\n    name: str | None\n    revision: str | None\n    release_date: StrDate | None\n    languages: list[ISOLanguageScript] | None\n    n_parameters: int | None\n    memory_usage_mb: float | None\n    max_tokens: float | None\n    embed_dim: int | None\n    license: Licenses | StrURL | None\n    open_weights: bool | None\n    public_training_code: str | None\n    public_training_data: str | bool | None\n    framework: list[FRAMEWORKS]\n    reference: StrURL | None = None\n    similarity_fn_name: ScoringFunction | None\n    use_instructions: bool | None\n    training_datasets: set[str] | None\n    adapted_from: str | None = None\n    superseded_by: str | None = None\n    modalities: list[Modalities] = [\"text\"]\n    is_cross_encoder: bool | None = None\n    citation: str | None = None\n    contacts: list[str] | None = None\n\n    @field_validator(\"similarity_fn_name\", mode=\"before\")\n    @classmethod\n    def _validate_similarity_fn_name(cls, value: str) -&gt; ScoringFunction | None:\n        \"\"\"Converts the similarity function name to the corresponding enum value.\n\n        Sentence_transformers uses Literal['cosine', 'dot', 'euclidean', 'manhattan'],\n        and pylate uses Literal['MaxSim']\n\n        Args:\n            value: The similarity function name as a string.\n\n        Returns:\n            The corresponding ScoringFunction enum value.\n        \"\"\"\n        if type(value) is ScoringFunction or value is None:\n            return value\n        mapping = {\n            \"cosine\": ScoringFunction.COSINE,\n            \"dot\": ScoringFunction.DOT_PRODUCT,\n            \"MaxSim\": ScoringFunction.MAX_SIM,\n        }\n        if value in mapping:\n            return mapping[value]\n        raise ValueError(f\"Invalid similarity function name: {value}\")\n\n    def to_dict(self):\n        \"\"\"Returns a dictionary representation of the model metadata.\"\"\"\n        dict_repr = self.model_dump()\n        loader = dict_repr.pop(\"loader\", None)\n        dict_repr[\"training_datasets\"] = (\n            list(dict_repr[\"training_datasets\"])\n            if isinstance(dict_repr[\"training_datasets\"], set)\n            else dict_repr[\"training_datasets\"]\n        )\n        dict_repr[\"loader\"] = _get_loader_name(loader)\n        return dict_repr\n\n    @field_validator(\"languages\")\n    @classmethod\n    def _languages_are_valid(\n        cls, languages: list[ISOLanguageScript] | None\n    ) -&gt; list[ISOLanguageScript] | None:\n        if languages is None:\n            return None\n\n        for code in languages:\n            check_language_code(code)\n        return languages\n\n    @field_validator(\"name\")\n    @classmethod\n    def _check_name(cls, v: str | None) -&gt; str | None:\n        if v is None or v in (\"bm25s\", \"Human\"):\n            return v\n        if \"/\" not in v:\n            raise ValueError(\n                \"Model name must be in the format 'organization/model_name'\"\n            )\n        return v\n\n    def load_model(self, **kwargs: Any) -&gt; MTEBModels:\n        \"\"\"Loads the model using the specified loader function.\"\"\"\n        if self.loader is None:\n            raise NotImplementedError(\n                \"No model implementation is available for this model.\"\n            )\n        if self.name is None:\n            raise ValueError(\"name is not set for ModelMeta. Cannot load model.\")\n\n        # Allow overwrites\n        _kwargs = self.loader_kwargs.copy()\n        _kwargs.update(kwargs)\n\n        model: EncoderProtocol = self.loader(\n            self.name, revision=self.revision, **_kwargs\n        )\n        model.mteb_model_meta = self  # type: ignore\n        return model\n\n    def model_name_as_path(self) -&gt; str:\n        \"\"\"Returns the model name in a format that can be used as a file path.\n\n        Replaces \"/\" with \"__\" and spaces with \"_\".\n        \"\"\"\n        if self.name is None:\n            raise ValueError(\"Model name is not set\")\n        return self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n    def is_zero_shot_on(\n        self, tasks: Sequence[\"AbsTask\"] | Sequence[str]\n    ) -&gt; bool | None:\n        \"\"\"Indicates whether the given model can be considered zero-shot or not on the given tasks.\n\n        Returns:\n             None if no training data is specified on the model.\n        \"\"\"\n        # If no tasks were specified, we're obviously zero-shot\n        if not tasks:\n            return True\n        training_datasets = self.get_training_datasets()\n        # If no tasks were specified, we're obviously zero-shot\n        if training_datasets is None:\n            return None\n\n        if isinstance(tasks[0], str):\n            benchmark_datasets = set(tasks)\n        else:\n            tasks = cast(Sequence[\"AbsTask\"], tasks)\n            benchmark_datasets = set()\n            for task in tasks:\n                benchmark_datasets.add(task.metadata.name)\n        intersection = training_datasets &amp; benchmark_datasets\n        return len(intersection) == 0\n\n    def get_training_datasets(self) -&gt; set[str] | None:\n        \"\"\"Returns all training datasets of the model including similar tasks.\"\"\"\n        import mteb\n\n        if self.training_datasets is None:\n            return None\n\n        training_datasets = self.training_datasets.copy()\n        if self.adapted_from is not None:\n            try:\n                adapted_from_model = mteb.get_model_meta(\n                    self.adapted_from, fetch_from_hf=False\n                )\n                adapted_training_datasets = adapted_from_model.get_training_datasets()\n                if adapted_training_datasets is not None:\n                    training_datasets |= adapted_training_datasets\n            except (ValueError, KeyError) as e:\n                logger.warning(f\"Could not get source model: {e} in MTEB\")\n\n        return_dataset = training_datasets.copy()\n        visited = set()\n\n        for dataset in training_datasets:\n            similar_tasks = _collect_similar_tasks(dataset, visited)\n            return_dataset |= similar_tasks\n\n        return return_dataset\n\n    def zero_shot_percentage(\n        self, tasks: Sequence[\"AbsTask\"] | Sequence[str]\n    ) -&gt; int | None:\n        \"\"\"Indicates how out-of-domain the selected tasks are for the given model.\n\n        Args:\n            tasks: A sequence of tasks or dataset names to evaluate against.\n\n        Returns:\n            An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.\n            Returns None if no training data is specified on the model or if no tasks are provided.\n        \"\"\"\n        training_datasets = self.get_training_datasets()\n        if (training_datasets is None) or (not tasks):\n            return None\n        if isinstance(tasks[0], str):\n            benchmark_datasets = set(tasks)\n        else:\n            tasks = cast(Sequence[\"AbsTask\"], tasks)\n            benchmark_datasets = {task.metadata.name for task in tasks}\n        overlap = training_datasets &amp; benchmark_datasets\n        perc_overlap = 100 * (len(overlap) / len(benchmark_datasets))\n        return int(100 - perc_overlap)\n\n    def calculate_memory_usage_mb(self) -&gt; int | None:\n        \"\"\"Calculates the memory usage (in FP32) of the model in MB.\n\n        Returns:\n            The memory usage of the model in MB, or None if it cannot be determined.\n        \"\"\"\n        if \"API\" in self.framework:\n            return None\n\n        MB = 1024**2  # noqa: N806\n        try:\n            safetensors_metadata = get_safetensors_metadata(self.name)  # type: ignore\n            if len(safetensors_metadata.parameter_count) &gt;= 0:\n                dtype_size_map = {\n                    \"F64\": 8,  # 64-bit float\n                    \"F32\": 4,  # 32-bit float (FP32)\n                    \"F16\": 2,  # 16-bit float (FP16)\n                    \"BF16\": 2,  # BFloat16\n                    \"I64\": 8,  # 64-bit integer\n                    \"I32\": 4,  # 32-bit integer\n                    \"I16\": 2,  # 16-bit integer\n                    \"I8\": 1,  # 8-bit integer\n                    \"U8\": 1,  # Unsigned 8-bit integer\n                    \"BOOL\": 1,  # Boolean (assuming 1 byte per value)\n                }\n                total_memory_bytes = sum(\n                    parameters * dtype_size_map.get(dtype, 4)\n                    for dtype, parameters in safetensors_metadata.parameter_count.items()\n                )\n                return round(total_memory_bytes / MB)  # Convert to MB\n\n        except (NotASafetensorsRepoError, SafetensorsParsingError, GatedRepoError):\n            pass\n        if self.n_parameters is None:\n            return None\n        # Model memory in bytes. For FP32 each parameter is 4 bytes.\n        model_memory_bytes = self.n_parameters * 4\n\n        # Convert to MB\n        model_memory_mb = model_memory_bytes / MB\n        return round(model_memory_mb)\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.calculate_memory_usage_mb","title":"<code>calculate_memory_usage_mb()</code>","text":"<p>Calculates the memory usage (in FP32) of the model in MB.</p> <p>Returns:</p> Type Description <code>int | None</code> <p>The memory usage of the model in MB, or None if it cannot be determined.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def calculate_memory_usage_mb(self) -&gt; int | None:\n    \"\"\"Calculates the memory usage (in FP32) of the model in MB.\n\n    Returns:\n        The memory usage of the model in MB, or None if it cannot be determined.\n    \"\"\"\n    if \"API\" in self.framework:\n        return None\n\n    MB = 1024**2  # noqa: N806\n    try:\n        safetensors_metadata = get_safetensors_metadata(self.name)  # type: ignore\n        if len(safetensors_metadata.parameter_count) &gt;= 0:\n            dtype_size_map = {\n                \"F64\": 8,  # 64-bit float\n                \"F32\": 4,  # 32-bit float (FP32)\n                \"F16\": 2,  # 16-bit float (FP16)\n                \"BF16\": 2,  # BFloat16\n                \"I64\": 8,  # 64-bit integer\n                \"I32\": 4,  # 32-bit integer\n                \"I16\": 2,  # 16-bit integer\n                \"I8\": 1,  # 8-bit integer\n                \"U8\": 1,  # Unsigned 8-bit integer\n                \"BOOL\": 1,  # Boolean (assuming 1 byte per value)\n            }\n            total_memory_bytes = sum(\n                parameters * dtype_size_map.get(dtype, 4)\n                for dtype, parameters in safetensors_metadata.parameter_count.items()\n            )\n            return round(total_memory_bytes / MB)  # Convert to MB\n\n    except (NotASafetensorsRepoError, SafetensorsParsingError, GatedRepoError):\n        pass\n    if self.n_parameters is None:\n        return None\n    # Model memory in bytes. For FP32 each parameter is 4 bytes.\n    model_memory_bytes = self.n_parameters * 4\n\n    # Convert to MB\n    model_memory_mb = model_memory_bytes / MB\n    return round(model_memory_mb)\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.get_training_datasets","title":"<code>get_training_datasets()</code>","text":"<p>Returns all training datasets of the model including similar tasks.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def get_training_datasets(self) -&gt; set[str] | None:\n    \"\"\"Returns all training datasets of the model including similar tasks.\"\"\"\n    import mteb\n\n    if self.training_datasets is None:\n        return None\n\n    training_datasets = self.training_datasets.copy()\n    if self.adapted_from is not None:\n        try:\n            adapted_from_model = mteb.get_model_meta(\n                self.adapted_from, fetch_from_hf=False\n            )\n            adapted_training_datasets = adapted_from_model.get_training_datasets()\n            if adapted_training_datasets is not None:\n                training_datasets |= adapted_training_datasets\n        except (ValueError, KeyError) as e:\n            logger.warning(f\"Could not get source model: {e} in MTEB\")\n\n    return_dataset = training_datasets.copy()\n    visited = set()\n\n    for dataset in training_datasets:\n        similar_tasks = _collect_similar_tasks(dataset, visited)\n        return_dataset |= similar_tasks\n\n    return return_dataset\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.is_zero_shot_on","title":"<code>is_zero_shot_on(tasks)</code>","text":"<p>Indicates whether the given model can be considered zero-shot or not on the given tasks.</p> <p>Returns:</p> Type Description <code>bool | None</code> <p>None if no training data is specified on the model.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def is_zero_shot_on(\n    self, tasks: Sequence[\"AbsTask\"] | Sequence[str]\n) -&gt; bool | None:\n    \"\"\"Indicates whether the given model can be considered zero-shot or not on the given tasks.\n\n    Returns:\n         None if no training data is specified on the model.\n    \"\"\"\n    # If no tasks were specified, we're obviously zero-shot\n    if not tasks:\n        return True\n    training_datasets = self.get_training_datasets()\n    # If no tasks were specified, we're obviously zero-shot\n    if training_datasets is None:\n        return None\n\n    if isinstance(tasks[0], str):\n        benchmark_datasets = set(tasks)\n    else:\n        tasks = cast(Sequence[\"AbsTask\"], tasks)\n        benchmark_datasets = set()\n        for task in tasks:\n            benchmark_datasets.add(task.metadata.name)\n    intersection = training_datasets &amp; benchmark_datasets\n    return len(intersection) == 0\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.load_model","title":"<code>load_model(**kwargs)</code>","text":"<p>Loads the model using the specified loader function.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def load_model(self, **kwargs: Any) -&gt; MTEBModels:\n    \"\"\"Loads the model using the specified loader function.\"\"\"\n    if self.loader is None:\n        raise NotImplementedError(\n            \"No model implementation is available for this model.\"\n        )\n    if self.name is None:\n        raise ValueError(\"name is not set for ModelMeta. Cannot load model.\")\n\n    # Allow overwrites\n    _kwargs = self.loader_kwargs.copy()\n    _kwargs.update(kwargs)\n\n    model: EncoderProtocol = self.loader(\n        self.name, revision=self.revision, **_kwargs\n    )\n    model.mteb_model_meta = self  # type: ignore\n    return model\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.model_name_as_path","title":"<code>model_name_as_path()</code>","text":"<p>Returns the model name in a format that can be used as a file path.</p> <p>Replaces \"/\" with \"__\" and spaces with \"_\".</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def model_name_as_path(self) -&gt; str:\n    \"\"\"Returns the model name in a format that can be used as a file path.\n\n    Replaces \"/\" with \"__\" and spaces with \"_\".\n    \"\"\"\n    if self.name is None:\n        raise ValueError(\"Model name is not set\")\n    return self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns a dictionary representation of the model metadata.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def to_dict(self):\n    \"\"\"Returns a dictionary representation of the model metadata.\"\"\"\n    dict_repr = self.model_dump()\n    loader = dict_repr.pop(\"loader\", None)\n    dict_repr[\"training_datasets\"] = (\n        list(dict_repr[\"training_datasets\"])\n        if isinstance(dict_repr[\"training_datasets\"], set)\n        else dict_repr[\"training_datasets\"]\n    )\n    dict_repr[\"loader\"] = _get_loader_name(loader)\n    return dict_repr\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.zero_shot_percentage","title":"<code>zero_shot_percentage(tasks)</code>","text":"<p>Indicates how out-of-domain the selected tasks are for the given model.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[AbsTask] | Sequence[str]</code> <p>A sequence of tasks or dataset names to evaluate against.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.</p> <code>int | None</code> <p>Returns None if no training data is specified on the model or if no tasks are provided.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def zero_shot_percentage(\n    self, tasks: Sequence[\"AbsTask\"] | Sequence[str]\n) -&gt; int | None:\n    \"\"\"Indicates how out-of-domain the selected tasks are for the given model.\n\n    Args:\n        tasks: A sequence of tasks or dataset names to evaluate against.\n\n    Returns:\n        An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.\n        Returns None if no training data is specified on the model or if no tasks are provided.\n    \"\"\"\n    training_datasets = self.get_training_datasets()\n    if (training_datasets is None) or (not tasks):\n        return None\n    if isinstance(tasks[0], str):\n        benchmark_datasets = set(tasks)\n    else:\n        tasks = cast(Sequence[\"AbsTask\"], tasks)\n        benchmark_datasets = {task.metadata.name for task in tasks}\n    overlap = training_datasets &amp; benchmark_datasets\n    perc_overlap = 100 * (len(overlap) / len(benchmark_datasets))\n    return int(100 - perc_overlap)\n</code></pre>"},{"location":"api/model/#model-protocols","title":"Model Protocols","text":""},{"location":"api/model/#mteb.models.EncoderProtocol","title":"<code>mteb.models.EncoderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for an encoder in MTEB.</p> <p>Besides the required functions specified below, the encoder can additionally specify the following signatures seen below. In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass EncoderProtocol(Protocol):\n    \"\"\"The interface for an encoder in MTEB.\n\n    Besides the required functions specified below, the encoder can additionally specify the following signatures seen below.\n    In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.\n    \"\"\"\n\n    def __init__(self, model_name: str, revision: str | None, **kwargs: Any) -&gt; None:\n        \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n        Args:\n            model_name: Name of the model\n            revision: revision of the model\n            kwargs: Any additional kwargs\n        \"\"\"\n        ...\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Any,\n    ) -&gt; Array:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: Batch of inputs to encode.\n            task_metadata: The metadata of the task. Encoders (e.g. SentenceTransformers) use to\n                select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.\n\n                The order of priorities for prompt selection are:\n                    1. Composed prompt of task name + prompt type (query or passage)\n                    2. Specific task prompt\n                    3. Composed prompt of task type + prompt type (query or passage)\n                    4. Specific task type prompt\n                    5. Specific prompt type (query or passage)\n            hf_split: Split of current task, allows to know some additional information about current split.\n                E.g. Current language\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            prompt_type: The name type of prompt. (query or passage)\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n        \"\"\"\n        ...\n\n    def similarity(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Compute the similarity between two collections of embeddings.\n\n        The output will be a matrix with the similarity scores between all embeddings from the first parameter and all\n        embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity\n        between corresponding pairs of embeddings.\n\n        Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity\n\n        Args:\n            embeddings1: [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n            embeddings2: [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n        Returns:\n            A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.\n        \"\"\"\n        ...\n\n    def similarity_pairwise(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.\n\n        Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise\n\n        Args:\n            embeddings1: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n            embeddings2: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n        Returns:\n            A [num_embeddings]-shaped torch tensor with pairwise similarity scores.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; \"ModelMeta\":\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.EncoderProtocol.__init__","title":"<code>__init__(model_name, revision, **kwargs)</code>","text":"<p>The initialization function for the encoder. Used when calling it from the mteb run CLI.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>revision</code> <code>str | None</code> <p>revision of the model</p> required <code>kwargs</code> <code>Any</code> <p>Any additional kwargs</p> <code>{}</code> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def __init__(self, model_name: str, revision: str | None, **kwargs: Any) -&gt; None:\n    \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n    Args:\n        model_name: Name of the model\n        revision: revision of the model\n        kwargs: Any additional kwargs\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.encode","title":"<code>encode(inputs, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Encodes the given sentences using the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader[BatchedInput]</code> <p>Batch of inputs to encode.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>The metadata of the task. Encoders (e.g. SentenceTransformers) use to select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.</p> <p>The order of priorities for prompt selection are:     1. Composed prompt of task name + prompt type (query or passage)     2. Specific task prompt     3. Composed prompt of task type + prompt type (query or passage)     4. Specific task type prompt     5. Specific prompt type (query or passage)</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split. E.g. Current language</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def encode(\n    self,\n    inputs: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Any,\n) -&gt; Array:\n    \"\"\"Encodes the given sentences using the encoder.\n\n    Args:\n        inputs: Batch of inputs to encode.\n        task_metadata: The metadata of the task. Encoders (e.g. SentenceTransformers) use to\n            select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.\n\n            The order of priorities for prompt selection are:\n                1. Composed prompt of task name + prompt type (query or passage)\n                2. Specific task prompt\n                3. Composed prompt of task type + prompt type (query or passage)\n                4. Specific task type prompt\n                5. Specific prompt type (query or passage)\n        hf_split: Split of current task, allows to know some additional information about current split.\n            E.g. Current language\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        prompt_type: The name type of prompt. (query or passage)\n        **kwargs: Additional arguments to pass to the encoder.\n\n    Returns:\n        The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.similarity","title":"<code>similarity(embeddings1, embeddings2)</code>","text":"<p>Compute the similarity between two collections of embeddings.</p> <p>The output will be a matrix with the similarity scores between all embeddings from the first parameter and all embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity between corresponding pairs of embeddings.</p> <p>Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity</p> <p>Parameters:</p> Name Type Description Default <code>embeddings1</code> <code>Array</code> <p>[num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <code>embeddings2</code> <code>Array</code> <p>[num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def similarity(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Compute the similarity between two collections of embeddings.\n\n    The output will be a matrix with the similarity scores between all embeddings from the first parameter and all\n    embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity\n    between corresponding pairs of embeddings.\n\n    Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity\n\n    Args:\n        embeddings1: [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n        embeddings2: [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n    Returns:\n        A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.similarity_pairwise","title":"<code>similarity_pairwise(embeddings1, embeddings2)</code>","text":"<p>Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.</p> <p>Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise</p> <p>Parameters:</p> Name Type Description Default <code>embeddings1</code> <code>Array</code> <p>[num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <code>embeddings2</code> <code>Array</code> <p>[num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A [num_embeddings]-shaped torch tensor with pairwise similarity scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def similarity_pairwise(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.\n\n    Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise\n\n    Args:\n        embeddings1: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n        embeddings2: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n    Returns:\n        A [num_embeddings]-shaped torch tensor with pairwise similarity scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol","title":"<code>mteb.models.SearchProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for searching models.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass SearchProtocol(Protocol):\n    \"\"\"Interface for searching models.\"\"\"\n\n    def index(\n        self,\n        corpus: CorpusDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, Any],\n    ) -&gt; None:\n        \"\"\"Index the corpus for retrieval.\n\n        Args:\n            corpus: Corpus dataset to index.\n            task_metadata: Metadata of the task, used to determine how to index the corpus.\n            hf_split: Split of current task, allows to know some additional information about current split.\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            encode_kwargs: Additional arguments to pass to the encoder during indexing.\n        \"\"\"\n        ...\n\n    def search(\n        self,\n        queries: QueryDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        top_k: int,\n        encode_kwargs: dict[str, Any],\n        top_ranked: TopRankedDocumentsType | None = None,\n    ) -&gt; RetrievalOutputType:\n        \"\"\"Search the corpus using the given queries.\n\n        Args:\n            queries: Queries to find\n            task_metadata: Task metadata\n            hf_split: split of the dataset\n            hf_subset: subset of the dataset\n            top_ranked: Top-ranked documents for each query, mapping query IDs to a list of document IDs.\n                Passed only from Reranking tasks.\n            top_k: Number of top documents to return for each query.\n            encode_kwargs: Additional arguments to pass to the encoder during indexing.\n\n        Returns:\n            Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; \"ModelMeta\":\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.SearchProtocol.index","title":"<code>index(corpus, *, task_metadata, hf_split, hf_subset, encode_kwargs)</code>","text":"<p>Index the corpus for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>CorpusDatasetType</code> <p>Corpus dataset to index.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Metadata of the task, used to determine how to index the corpus.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split.</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>encode_kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments to pass to the encoder during indexing.</p> required Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def index(\n    self,\n    corpus: CorpusDatasetType,\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    encode_kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Index the corpus for retrieval.\n\n    Args:\n        corpus: Corpus dataset to index.\n        task_metadata: Metadata of the task, used to determine how to index the corpus.\n        hf_split: Split of current task, allows to know some additional information about current split.\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        encode_kwargs: Additional arguments to pass to the encoder during indexing.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol.search","title":"<code>search(queries, *, task_metadata, hf_split, hf_subset, top_k, encode_kwargs, top_ranked=None)</code>","text":"<p>Search the corpus using the given queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>QueryDatasetType</code> <p>Queries to find</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Task metadata</p> required <code>hf_split</code> <code>str</code> <p>split of the dataset</p> required <code>hf_subset</code> <code>str</code> <p>subset of the dataset</p> required <code>top_ranked</code> <code>TopRankedDocumentsType | None</code> <p>Top-ranked documents for each query, mapping query IDs to a list of document IDs. Passed only from Reranking tasks.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Number of top documents to return for each query.</p> required <code>encode_kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments to pass to the encoder during indexing.</p> required <p>Returns:</p> Type Description <code>RetrievalOutputType</code> <p>Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def search(\n    self,\n    queries: QueryDatasetType,\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    top_k: int,\n    encode_kwargs: dict[str, Any],\n    top_ranked: TopRankedDocumentsType | None = None,\n) -&gt; RetrievalOutputType:\n    \"\"\"Search the corpus using the given queries.\n\n    Args:\n        queries: Queries to find\n        task_metadata: Task metadata\n        hf_split: split of the dataset\n        hf_subset: subset of the dataset\n        top_ranked: Top-ranked documents for each query, mapping query IDs to a list of document IDs.\n            Passed only from Reranking tasks.\n        top_k: Number of top documents to return for each query.\n        encode_kwargs: Additional arguments to pass to the encoder during indexing.\n\n    Returns:\n        Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol","title":"<code>mteb.models.CrossEncoderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for a CrossEncoder in MTEB.</p> <p>Besides the required functions specified below, the cross-encoder can additionally specify the following signatures seen below. In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass CrossEncoderProtocol(Protocol):\n    \"\"\"The interface for a CrossEncoder in MTEB.\n\n    Besides the required functions specified below, the cross-encoder can additionally specify the following signatures seen below.\n    In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.\n    \"\"\"\n\n    def __init__(self, model_name: str, revision: str | None, **kwargs: Any) -&gt; None:\n        \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n        Args:\n            model_name: Name of the model\n            revision: revision of the model\n            kwargs: Any additional kwargs\n        \"\"\"\n        ...\n\n    def predict(\n        self,\n        inputs1: DataLoader[BatchedInput],\n        inputs2: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Any,\n    ) -&gt; Array:\n        \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n        Args:\n            inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n            inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n            task_metadata: Metadata of the current task.\n            hf_split: Split of current task, allows to know some additional information about current split.\n                E.g. Current language\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            prompt_type: The name type of prompt. (query or passage)\n            **kwargs: Additional arguments to pass to the cross-encoder.\n\n        Returns:\n            The predicted relevance scores for each inputs pair.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; \"ModelMeta\":\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.__init__","title":"<code>__init__(model_name, revision, **kwargs)</code>","text":"<p>The initialization function for the encoder. Used when calling it from the mteb run CLI.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>revision</code> <code>str | None</code> <p>revision of the model</p> required <code>kwargs</code> <code>Any</code> <p>Any additional kwargs</p> <code>{}</code> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def __init__(self, model_name: str, revision: str | None, **kwargs: Any) -&gt; None:\n    \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n    Args:\n        model_name: Name of the model\n        revision: revision of the model\n        kwargs: Any additional kwargs\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.predict","title":"<code>predict(inputs1, inputs2, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs1</code> <code>DataLoader[BatchedInput]</code> <p>First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks <code>QueryDatasetType</code>).</p> required <code>inputs2</code> <code>DataLoader[BatchedInput]</code> <p>Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks <code>RetrievalOutputType</code>).</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Metadata of the current task.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split. E.g. Current language</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the cross-encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The predicted relevance scores for each inputs pair.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def predict(\n    self,\n    inputs1: DataLoader[BatchedInput],\n    inputs2: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Any,\n) -&gt; Array:\n    \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n    Args:\n        inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n        inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n        task_metadata: Metadata of the current task.\n        hf_split: Split of current task, allows to know some additional information about current split.\n            E.g. Current language\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        prompt_type: The name type of prompt. (query or passage)\n        **kwargs: Additional arguments to pass to the cross-encoder.\n\n    Returns:\n        The predicted relevance scores for each inputs pair.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.MTEBModels","title":"<code>mteb.models.MTEBModels = EncoderProtocol | CrossEncoderProtocol | SearchProtocol</code>  <code>module-attribute</code>","text":"<p>Type alias for all MTEB model types as many models implement multiple protocols and many tasks can be solved by multiple model types.</p>"},{"location":"api/results/","title":"Results","text":"<p>When a models is evaluated in MTEB it produces results. These results consist of:</p> <ul> <li><code>TaskResult</code>: Result for a single task</li> <li><code>ModelResult</code>: Result for a model on a set of tasks</li> <li><code>BenchmarkResults</code>: Result for a set of models on a set of tasks</li> </ul> <p></p> <p>In normal use these come up when running a model: <pre><code># ...\nmodels_results = mteb.evaluate(model, tasks)\ntype(models_results) # mteb.results.ModelResults\n\ntask_result = models_results.task_results\ntype(models_results) # mteb.results.TaskResult\n</code></pre></p>"},{"location":"api/results/#results-cache","title":"Results cache","text":""},{"location":"api/results/#mteb.cache.ResultCache","title":"<code>mteb.cache.ResultCache</code>","text":"<p>Class to handle the local cache of MTEB results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mteb.cache import ResultCache\n&gt;&gt;&gt; cache = ResultCache(cache_path=\"~/.cache/mteb\") # default\n&gt;&gt;&gt; cache.download_from_remote() # download the latest results from the remote repository\n&gt;&gt;&gt; result = cache.load_results(\"task_name\", \"model_name\")\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>class ResultCache:\n    \"\"\"Class to handle the local cache of MTEB results.\n\n    Examples:\n        &gt;&gt;&gt; from mteb.cache import ResultCache\n        &gt;&gt;&gt; cache = ResultCache(cache_path=\"~/.cache/mteb\") # default\n        &gt;&gt;&gt; cache.download_from_remote() # download the latest results from the remote repository\n        &gt;&gt;&gt; result = cache.load_results(\"task_name\", \"model_name\")\n    \"\"\"\n\n    cache_path: Path\n\n    def __init__(self, cache_path: Path | str | None = None) -&gt; None:\n        if cache_path is not None:\n            self.cache_path = Path(cache_path)\n        else:\n            self.cache_path = self.default_cache_path\n        self.cache_path.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def has_remote(self) -&gt; bool:\n        \"\"\"Check if the remote results repository exists in the cache directory.\n\n        Returns:\n            True if the remote results repository exists, False otherwise.\n        \"\"\"\n        return (self.cache_path / \"remote\").exists()\n\n    def get_task_result_path(\n        self,\n        task_name: str,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n        remote: bool = False,\n    ) -&gt; Path:\n        \"\"\"Get the path to the results of a specific task for a specific model and revision.\n\n        Args:\n            task_name: The name of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n            remote: If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.\n\n        Returns:\n            The path to the results of the task.\n        \"\"\"\n        results_folder = \"results\" if not remote else \"remote\"\n\n        if isinstance(model_name, ModelMeta):\n            if model_revision is not None:\n                logger.warning(\n                    \"model_revision is ignored when model_name is a ModelMeta object\"\n                )\n            model_revision = model_name.revision\n            model_name = model_name.model_name_as_path()\n        elif isinstance(model_name, str):\n            model_name = model_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n        model_path = self.cache_path / results_folder / model_name\n\n        if model_revision is None:\n            logger.warning(\n                \"model_revision is not specified, attempting to load the latest revision. To disable this behavior, specify model_revision explicitly.\"\n            )\n            # get revs from paths\n            revisions = [p for p in model_path.glob(\"*\") if p.is_dir()]\n            if not revisions:\n                model_revision = \"no_revision_available\"\n            else:\n                if len(revisions) &gt; 1:\n                    logger.warning(\n                        f\"Multiple revisions found for model {model_name}: {revisions}. Using the latest one (according to latest edit).\"\n                    )\n                    # sort folder by latest edit time\n                    revisions.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n                model_revision = revisions[0].name\n\n        return model_path / model_revision / f\"{task_name}.json\"\n\n    def load_task_result(\n        self,\n        task_name: str,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n        raise_if_not_found: bool = False,\n        prioritize_remote: bool = False,\n    ) -&gt; TaskResult | None:\n        \"\"\"Load the results from the local cache directory.\n\n        Args:\n            task_name: The name of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n            raise_if_not_found: If True, raise an error if the results are not found.\n            prioritize_remote: If True, it will first try to load the results from the remote repository, if available.\n\n        Returns:\n            The results of the task, or None if not found.\n        \"\"\"\n        result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_name,\n        )\n\n        if self.has_remote:\n            remote_result_path = self.get_task_result_path(\n                model_name=model_name,\n                model_revision=model_revision,\n                task_name=task_name,\n                remote=True,\n            )\n            if remote_result_path.exists() and prioritize_remote:\n                result_path = remote_result_path\n            elif not result_path.exists():\n                result_path = remote_result_path\n\n        if not result_path.exists():\n            msg = f\"Results for {model_name} on {task_name} not found in {result_path}\"\n            if raise_if_not_found:\n                raise FileNotFoundError(msg)\n            logger.debug(msg)\n            return None\n\n        return TaskResult.from_disk(result_path)\n\n    def save_to_cache(\n        self,\n        task_result: TaskResult,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n    ) -&gt; None:\n        \"\"\"Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.\n\n        Where model_name is a path-normalized model name.\n        In addition we also save a model_meta.json in the revision folder to preserve the model metadata.\n\n        Args:\n            task_result: The results of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n        \"\"\"\n        result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_result.task_name,\n        )\n        result_path.parent.mkdir(parents=True, exist_ok=True)\n        task_result.to_disk(result_path)\n\n        model_meta_path = result_path.parent / \"model_meta.json\"\n        if isinstance(model_name, ModelMeta):\n            meta = model_name\n            with model_meta_path.open(\"w\") as f:\n                json.dump(meta.to_dict(), f, default=str)\n\n    @property\n    def default_cache_path(self) -&gt; Path:\n        \"\"\"Get the local cache directory for MTEB results.\n\n        Returns:\n            The path to the local cache directory.\n        \"\"\"\n        default_cache_directory = Path.home() / \".cache\" / \"mteb\"\n\n        _cache_directory = os.environ.get(\"MTEB_CACHE\", None)\n        cache_directory = (\n            Path(_cache_directory) if _cache_directory else default_cache_directory\n        )\n        return cache_directory\n\n    def download_from_remote(\n        self,\n        remote: str = \"https://github.com/embeddings-benchmark/results\",\n        download_latest: bool = True,\n    ) -&gt; Path:\n        \"\"\"Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.\n\n        Args:\n            remote: The URL of the results repository on GitHub.\n            download_latest: If True it will download the latest version of the repository, otherwise it will only update the existing repository.\n\n        Returns:\n            The path to the local cache directory.\n        \"\"\"\n        if not self.cache_path.exists() and not self.cache_path.is_dir():\n            logger.info(\n                f\"Cache directory {self.cache_path} does not exist, creating it\"\n            )\n\n        # if \"results\" folder already exists update it\n        results_directory = self.cache_path / \"remote\"\n\n        if results_directory.exists():\n            # check repository in the directory is the same as the remote\n            remote_url = subprocess.run(\n                [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n                cwd=results_directory,\n                capture_output=True,\n                text=True,\n            ).stdout.strip()\n            if remote_url != remote:\n                msg = (\n                    f\"remote repository '{remote}' does not match the one in {results_directory},  which is '{remote_url}'.\"\n                    + \" Please remove the directory and try again.\"\n                )\n                raise ValueError(msg)\n\n            if download_latest:\n                logger.info(\n                    f\"remote repository already exists in {results_directory}, updating it using git pull\"\n                )\n                subprocess.run([\"git\", \"pull\"], cwd=results_directory)\n            else:\n                logger.debug(\n                    f\"Results repository already exists in {results_directory}, skipping update, set download_latest=True to update it\"\n                )\n            return results_directory\n\n        logger.info(\n            f\"No results repository found in {results_directory}, cloning it from {remote}\"\n        )\n\n        subprocess.run([\"git\", \"clone\", remote, \"remote\"], cwd=self.cache_path)\n\n        return results_directory\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear the local cache directory.\"\"\"\n        if self.cache_path.exists() and self.cache_path.is_dir():\n            shutil.rmtree(self.cache_path)\n            logger.info(f\"Cache directory {self.cache_path} cleared.\")\n        else:\n            logger.warning(f\"Cache directory {self.cache_path} does not exist.\")\n\n    def __repr__(self) -&gt; str:\n        return f\"ResultCache(cache_path={self.cache_path})\"\n\n    def get_cache_paths(\n        self,\n        models: Sequence[str] | Sequence[ModelMeta] | None = None,\n        tasks: Sequence[str] | Sequence[AbsTask] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n    ) -&gt; list[Path]:\n        \"\"\"Get all paths to result JSON files in the cache directory.\n\n        These paths can then be used to fetch task results, like:\n        ```python\n        for path in paths:\n            task_result = TaskResult.from_disk(path)\n        ```\n\n        Args:\n            models: A list of model names or ModelMeta objects to filter the paths.\n            tasks: A list of task names to filter the paths.\n            require_model_meta: If True, only return paths that have a model_meta.json file.\n            include_remote: If True, include remote results in the returned paths.\n\n        Returns:\n            A list of paths in the cache directory.\n\n        Examples:\n            &gt;&gt;&gt; from mteb.cache import ResultCache\n            &gt;&gt;&gt; cache = ResultCache()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths\n            &gt;&gt;&gt; paths = cache.get_cache_paths()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific task\n            &gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific model\n            &gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific model and revision\n            &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n            &gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n        \"\"\"\n        cache_paths = [\n            p\n            for p in (self.cache_path / \"results\").glob(\"**/*.json\")\n            if p.name != \"model_meta.json\"\n        ]\n        if include_remote:\n            cache_paths += [\n                p\n                for p in (self.cache_path / \"remote\" / \"results\").glob(\"**/*.json\")\n                if p.name != \"model_meta.json\"\n            ]\n\n        cache_paths = self._filter_paths_by_model_and_revision(\n            cache_paths,\n            models=models,\n        )\n        cache_paths = self._filter_paths_by_task(cache_paths, tasks=tasks)\n\n        if require_model_meta:\n            cache_paths = [\n                p for p in cache_paths if (p.parent / \"model_meta.json\").exists()\n            ]\n        return cache_paths\n\n    def get_models(\n        self,\n        tasks: Sequence[str] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n    ) -&gt; list[tuple[ModelName, Revision]]:\n        \"\"\"Get all models in the cache directory.\n\n        Args:\n            tasks: A list of task names to filter the models.\n            require_model_meta: If True, only return models that have a model_meta.json file.\n            include_remote: If True, include remote results in the returned models.\n\n        Returns:\n            A list of tuples containing the model name and revision.\n        \"\"\"\n        cache_paths = self.get_cache_paths(\n            tasks=tasks,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n        )\n        models = [(p.parent.parent.name, p.parent.name) for p in cache_paths]\n        return list(set(models))\n\n    def get_task_names(\n        self,\n        models: list[str] | list[ModelMeta] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n    ) -&gt; list[str]:\n        \"\"\"Get all task names in the cache directory.\n\n        Args:\n            models: A list of model names or ModelMeta objects to filter the task names.\n            require_model_meta: If True, only return task names that have a model_meta.json file\n            include_remote: If True, include remote results in the returned task names.\n\n        Returns:\n            A list of task names in the cache directory.\n        \"\"\"\n        cache_paths = self.get_cache_paths(\n            models=models,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n        )\n        tasks = [p.stem for p in cache_paths]\n        return list(set(tasks))\n\n    @staticmethod\n    def _get_model_name_and_revision_from_path(\n        revision_path: Path,\n    ) -&gt; tuple[ModelName, Revision]:\n        model_meta = revision_path / \"model_meta.json\"\n        model_path = revision_path.parent\n\n        if not model_meta.exists():\n            logger.debug(\n                f\"model_meta.json not found in {revision_path}, extracting model_name and revision from the path\"\n            )\n            model_name = model_path.name.replace(\"__\", \"/\")\n            revision = revision_path.name\n            return model_name, revision\n        with model_meta.open(\"r\") as f:\n            model_meta_json = json.load(f)\n            model_name = model_meta_json[\"name\"]\n            revision = model_meta_json[\"revision\"]\n        return model_name, revision\n\n    @staticmethod\n    def _filter_paths_by_model_and_revision(\n        paths: list[Path],\n        models: Sequence[str] | Sequence[ModelMeta] | None = None,\n    ) -&gt; list[Path]:\n        \"\"\"Filter a list of paths by model name and optional revision.\n\n        Returns:\n            A list of paths that match the specified model names and revisions.\n        \"\"\"\n        if not models:\n            return paths\n\n        if isinstance(models[0], ModelMeta):\n            models = cast(list[ModelMeta], models)\n            name_and_revision = {\n                (m.model_name_as_path(), m.revision or \"no_revision_available\")\n                for m in models\n            }\n            return [\n                p\n                for p in paths\n                if (p.parent.parent.name, p.parent.name) in name_and_revision\n            ]\n\n        model_names = {m.replace(\"/\", \"__\").replace(\" \", \"_\") for m in models}\n        return [p for p in paths if p.parent.parent.name in model_names]\n\n    @staticmethod\n    def _filter_paths_by_task(\n        paths: list[Path],\n        tasks: Sequence[str] | Sequence[AbsTask] | None = None,\n    ) -&gt; list[Path]:\n        if tasks is not None:\n            task_names = set()\n\n            for task in tasks:\n                if isinstance(task, AbsTask):\n                    task_names.add(task.metadata.name)\n                else:\n                    task_names.add(task)\n\n            paths = [p for p in paths if p.stem in task_names]\n        return paths\n\n    def load_results(\n        self,\n        models: Sequence[str] | Sequence[ModelMeta] | None = None,\n        tasks: Sequence[str] | Sequence[AbsTask] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n        validate_and_filter: bool = False,\n        only_main_score: bool = False,\n    ) -&gt; BenchmarkResults:\n        \"\"\"Loads the results from the cache directory and returns a BenchmarkResults object.\n\n        Args:\n            models: A list of model names to load the results for. If None it will load the results for all models.\n            tasks: A list of task names to load the results for. If None it will load the results for all tasks.\n            require_model_meta: If True it will ignore results that do not have a model_meta.json file. If false it attempt to\n                extract the model name and revision from the path.\n            include_remote: If True, it will include results from the remote repository.\n            validate_and_filter: If True it will validate that the results object for the task contains the correct splits and filter out\n                splits from the results object that are not default in the task metadata.\n            only_main_score: If True, only the main score will be loaded.\n\n        Returns:\n            A BenchmarkResults object containing the results for the specified models and tasks.\n\n        Examples:\n            &gt;&gt;&gt; from mteb.cache import ResultCache\n            &gt;&gt;&gt; cache = ResultCache()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load results for specific models and tasks\n            &gt;&gt;&gt; results = cache.load_results(\n            ...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n            ...     tasks=[\"STS12\"],\n            ...     require_model_meta=True,\n            ... )\n        \"\"\"\n        paths = self.get_cache_paths(\n            models=models,\n            tasks=tasks,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n        )\n        models_results = defaultdict(list)\n\n        task_names = {}\n        if tasks is not None:\n            for task in tasks:\n                if isinstance(task, AbsTask):\n                    task_names[task.metadata.name] = task\n                else:\n                    task_names[task] = None\n\n        for path in paths:\n            task_result = TaskResult.from_disk(path)\n\n            if only_main_score:\n                task_result = task_result.only_main_score()\n            model_name, revision = self._get_model_name_and_revision_from_path(\n                path.parent\n            )\n\n            if validate_and_filter:\n                task = task_names[task_result.task_name]\n                try:\n                    task_result.validate_and_filter_scores(task=task)\n                except Exception as e:\n                    logger.info(\n                        f\"Validation failed for {task_result.task_name} in {model_name} {revision}: {e}\"\n                    )\n                    continue\n\n            models_results[(model_name, revision)].append(task_result)\n\n        # create BenchmarkResults object\n        models_results = [\n            ModelResult(\n                model_name=model_name,\n                model_revision=revision,\n                task_results=task_results,\n            )\n            for (model_name, revision), task_results in models_results.items()\n        ]\n\n        benchmark_results = BenchmarkResults(\n            model_results=models_results,\n        )\n\n        return benchmark_results\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.default_cache_path","title":"<code>default_cache_path</code>  <code>property</code>","text":"<p>Get the local cache directory for MTEB results.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the local cache directory.</p>"},{"location":"api/results/#mteb.cache.ResultCache.has_remote","title":"<code>has_remote</code>  <code>property</code>","text":"<p>Check if the remote results repository exists in the cache directory.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the remote results repository exists, False otherwise.</p>"},{"location":"api/results/#mteb.cache.ResultCache.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the local cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the local cache directory.\"\"\"\n    if self.cache_path.exists() and self.cache_path.is_dir():\n        shutil.rmtree(self.cache_path)\n        logger.info(f\"Cache directory {self.cache_path} cleared.\")\n    else:\n        logger.warning(f\"Cache directory {self.cache_path} does not exist.\")\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.download_from_remote","title":"<code>download_from_remote(remote='https://github.com/embeddings-benchmark/results', download_latest=True)</code>","text":"<p>Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>remote</code> <code>str</code> <p>The URL of the results repository on GitHub.</p> <code>'https://github.com/embeddings-benchmark/results'</code> <code>download_latest</code> <code>bool</code> <p>If True it will download the latest version of the repository, otherwise it will only update the existing repository.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the local cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def download_from_remote(\n    self,\n    remote: str = \"https://github.com/embeddings-benchmark/results\",\n    download_latest: bool = True,\n) -&gt; Path:\n    \"\"\"Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.\n\n    Args:\n        remote: The URL of the results repository on GitHub.\n        download_latest: If True it will download the latest version of the repository, otherwise it will only update the existing repository.\n\n    Returns:\n        The path to the local cache directory.\n    \"\"\"\n    if not self.cache_path.exists() and not self.cache_path.is_dir():\n        logger.info(\n            f\"Cache directory {self.cache_path} does not exist, creating it\"\n        )\n\n    # if \"results\" folder already exists update it\n    results_directory = self.cache_path / \"remote\"\n\n    if results_directory.exists():\n        # check repository in the directory is the same as the remote\n        remote_url = subprocess.run(\n            [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n            cwd=results_directory,\n            capture_output=True,\n            text=True,\n        ).stdout.strip()\n        if remote_url != remote:\n            msg = (\n                f\"remote repository '{remote}' does not match the one in {results_directory},  which is '{remote_url}'.\"\n                + \" Please remove the directory and try again.\"\n            )\n            raise ValueError(msg)\n\n        if download_latest:\n            logger.info(\n                f\"remote repository already exists in {results_directory}, updating it using git pull\"\n            )\n            subprocess.run([\"git\", \"pull\"], cwd=results_directory)\n        else:\n            logger.debug(\n                f\"Results repository already exists in {results_directory}, skipping update, set download_latest=True to update it\"\n            )\n        return results_directory\n\n    logger.info(\n        f\"No results repository found in {results_directory}, cloning it from {remote}\"\n    )\n\n    subprocess.run([\"git\", \"clone\", remote, \"remote\"], cwd=self.cache_path)\n\n    return results_directory\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_cache_paths","title":"<code>get_cache_paths(models=None, tasks=None, require_model_meta=True, include_remote=True)</code>","text":"<p>Get all paths to result JSON files in the cache directory.</p> <p>These paths can then be used to fetch task results, like: <pre><code>for path in paths:\n    task_result = TaskResult.from_disk(path)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Sequence[str] | Sequence[ModelMeta] | None</code> <p>A list of model names or ModelMeta objects to filter the paths.</p> <code>None</code> <code>tasks</code> <code>Sequence[str] | Sequence[AbsTask] | None</code> <p>A list of task names to filter the paths.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return paths that have a model_meta.json file.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned paths.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>A list of paths in the cache directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mteb.cache import ResultCache\n&gt;&gt;&gt; cache = ResultCache()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths\n&gt;&gt;&gt; paths = cache.get_cache_paths()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific task\n&gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific model\n&gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific model and revision\n&gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n&gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>def get_cache_paths(\n    self,\n    models: Sequence[str] | Sequence[ModelMeta] | None = None,\n    tasks: Sequence[str] | Sequence[AbsTask] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n) -&gt; list[Path]:\n    \"\"\"Get all paths to result JSON files in the cache directory.\n\n    These paths can then be used to fetch task results, like:\n    ```python\n    for path in paths:\n        task_result = TaskResult.from_disk(path)\n    ```\n\n    Args:\n        models: A list of model names or ModelMeta objects to filter the paths.\n        tasks: A list of task names to filter the paths.\n        require_model_meta: If True, only return paths that have a model_meta.json file.\n        include_remote: If True, include remote results in the returned paths.\n\n    Returns:\n        A list of paths in the cache directory.\n\n    Examples:\n        &gt;&gt;&gt; from mteb.cache import ResultCache\n        &gt;&gt;&gt; cache = ResultCache()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths\n        &gt;&gt;&gt; paths = cache.get_cache_paths()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific task\n        &gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific model\n        &gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific model and revision\n        &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n        &gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n    \"\"\"\n    cache_paths = [\n        p\n        for p in (self.cache_path / \"results\").glob(\"**/*.json\")\n        if p.name != \"model_meta.json\"\n    ]\n    if include_remote:\n        cache_paths += [\n            p\n            for p in (self.cache_path / \"remote\" / \"results\").glob(\"**/*.json\")\n            if p.name != \"model_meta.json\"\n        ]\n\n    cache_paths = self._filter_paths_by_model_and_revision(\n        cache_paths,\n        models=models,\n    )\n    cache_paths = self._filter_paths_by_task(cache_paths, tasks=tasks)\n\n    if require_model_meta:\n        cache_paths = [\n            p for p in cache_paths if (p.parent / \"model_meta.json\").exists()\n        ]\n    return cache_paths\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_models","title":"<code>get_models(tasks=None, require_model_meta=True, include_remote=True)</code>","text":"<p>Get all models in the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[str] | None</code> <p>A list of task names to filter the models.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return models that have a model_meta.json file.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned models.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[ModelName, Revision]]</code> <p>A list of tuples containing the model name and revision.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_models(\n    self,\n    tasks: Sequence[str] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n) -&gt; list[tuple[ModelName, Revision]]:\n    \"\"\"Get all models in the cache directory.\n\n    Args:\n        tasks: A list of task names to filter the models.\n        require_model_meta: If True, only return models that have a model_meta.json file.\n        include_remote: If True, include remote results in the returned models.\n\n    Returns:\n        A list of tuples containing the model name and revision.\n    \"\"\"\n    cache_paths = self.get_cache_paths(\n        tasks=tasks,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n    )\n    models = [(p.parent.parent.name, p.parent.name) for p in cache_paths]\n    return list(set(models))\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_task_names","title":"<code>get_task_names(models=None, require_model_meta=True, include_remote=True)</code>","text":"<p>Get all task names in the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[str] | list[ModelMeta] | None</code> <p>A list of model names or ModelMeta objects to filter the task names.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return task names that have a model_meta.json file</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned task names.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names in the cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_task_names(\n    self,\n    models: list[str] | list[ModelMeta] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n) -&gt; list[str]:\n    \"\"\"Get all task names in the cache directory.\n\n    Args:\n        models: A list of model names or ModelMeta objects to filter the task names.\n        require_model_meta: If True, only return task names that have a model_meta.json file\n        include_remote: If True, include remote results in the returned task names.\n\n    Returns:\n        A list of task names in the cache directory.\n    \"\"\"\n    cache_paths = self.get_cache_paths(\n        models=models,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n    )\n    tasks = [p.stem for p in cache_paths]\n    return list(set(tasks))\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_task_result_path","title":"<code>get_task_result_path(task_name, model_name, model_revision=None, remote=False)</code>","text":"<p>Get the path to the results of a specific task for a specific model and revision.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> <code>remote</code> <code>bool</code> <p>If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the results of the task.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_task_result_path(\n    self,\n    task_name: str,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n    remote: bool = False,\n) -&gt; Path:\n    \"\"\"Get the path to the results of a specific task for a specific model and revision.\n\n    Args:\n        task_name: The name of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n        remote: If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.\n\n    Returns:\n        The path to the results of the task.\n    \"\"\"\n    results_folder = \"results\" if not remote else \"remote\"\n\n    if isinstance(model_name, ModelMeta):\n        if model_revision is not None:\n            logger.warning(\n                \"model_revision is ignored when model_name is a ModelMeta object\"\n            )\n        model_revision = model_name.revision\n        model_name = model_name.model_name_as_path()\n    elif isinstance(model_name, str):\n        model_name = model_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n    model_path = self.cache_path / results_folder / model_name\n\n    if model_revision is None:\n        logger.warning(\n            \"model_revision is not specified, attempting to load the latest revision. To disable this behavior, specify model_revision explicitly.\"\n        )\n        # get revs from paths\n        revisions = [p for p in model_path.glob(\"*\") if p.is_dir()]\n        if not revisions:\n            model_revision = \"no_revision_available\"\n        else:\n            if len(revisions) &gt; 1:\n                logger.warning(\n                    f\"Multiple revisions found for model {model_name}: {revisions}. Using the latest one (according to latest edit).\"\n                )\n                # sort folder by latest edit time\n                revisions.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n            model_revision = revisions[0].name\n\n    return model_path / model_revision / f\"{task_name}.json\"\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.load_results","title":"<code>load_results(models=None, tasks=None, require_model_meta=True, include_remote=True, validate_and_filter=False, only_main_score=False)</code>","text":"<p>Loads the results from the cache directory and returns a BenchmarkResults object.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Sequence[str] | Sequence[ModelMeta] | None</code> <p>A list of model names to load the results for. If None it will load the results for all models.</p> <code>None</code> <code>tasks</code> <code>Sequence[str] | Sequence[AbsTask] | None</code> <p>A list of task names to load the results for. If None it will load the results for all tasks.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True it will ignore results that do not have a model_meta.json file. If false it attempt to extract the model name and revision from the path.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, it will include results from the remote repository.</p> <code>True</code> <code>validate_and_filter</code> <code>bool</code> <p>If True it will validate that the results object for the task contains the correct splits and filter out splits from the results object that are not default in the task metadata.</p> <code>False</code> <code>only_main_score</code> <code>bool</code> <p>If True, only the main score will be loaded.</p> <code>False</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A BenchmarkResults object containing the results for the specified models and tasks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from mteb.cache import ResultCache\n&gt;&gt;&gt; cache = ResultCache()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load results for specific models and tasks\n&gt;&gt;&gt; results = cache.load_results(\n...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n...     tasks=[\"STS12\"],\n...     require_model_meta=True,\n... )\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>def load_results(\n    self,\n    models: Sequence[str] | Sequence[ModelMeta] | None = None,\n    tasks: Sequence[str] | Sequence[AbsTask] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n    validate_and_filter: bool = False,\n    only_main_score: bool = False,\n) -&gt; BenchmarkResults:\n    \"\"\"Loads the results from the cache directory and returns a BenchmarkResults object.\n\n    Args:\n        models: A list of model names to load the results for. If None it will load the results for all models.\n        tasks: A list of task names to load the results for. If None it will load the results for all tasks.\n        require_model_meta: If True it will ignore results that do not have a model_meta.json file. If false it attempt to\n            extract the model name and revision from the path.\n        include_remote: If True, it will include results from the remote repository.\n        validate_and_filter: If True it will validate that the results object for the task contains the correct splits and filter out\n            splits from the results object that are not default in the task metadata.\n        only_main_score: If True, only the main score will be loaded.\n\n    Returns:\n        A BenchmarkResults object containing the results for the specified models and tasks.\n\n    Examples:\n        &gt;&gt;&gt; from mteb.cache import ResultCache\n        &gt;&gt;&gt; cache = ResultCache()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load results for specific models and tasks\n        &gt;&gt;&gt; results = cache.load_results(\n        ...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n        ...     tasks=[\"STS12\"],\n        ...     require_model_meta=True,\n        ... )\n    \"\"\"\n    paths = self.get_cache_paths(\n        models=models,\n        tasks=tasks,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n    )\n    models_results = defaultdict(list)\n\n    task_names = {}\n    if tasks is not None:\n        for task in tasks:\n            if isinstance(task, AbsTask):\n                task_names[task.metadata.name] = task\n            else:\n                task_names[task] = None\n\n    for path in paths:\n        task_result = TaskResult.from_disk(path)\n\n        if only_main_score:\n            task_result = task_result.only_main_score()\n        model_name, revision = self._get_model_name_and_revision_from_path(\n            path.parent\n        )\n\n        if validate_and_filter:\n            task = task_names[task_result.task_name]\n            try:\n                task_result.validate_and_filter_scores(task=task)\n            except Exception as e:\n                logger.info(\n                    f\"Validation failed for {task_result.task_name} in {model_name} {revision}: {e}\"\n                )\n                continue\n\n        models_results[(model_name, revision)].append(task_result)\n\n    # create BenchmarkResults object\n    models_results = [\n        ModelResult(\n            model_name=model_name,\n            model_revision=revision,\n            task_results=task_results,\n        )\n        for (model_name, revision), task_results in models_results.items()\n    ]\n\n    benchmark_results = BenchmarkResults(\n        model_results=models_results,\n    )\n\n    return benchmark_results\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.load_task_result","title":"<code>load_task_result(task_name, model_name, model_revision=None, raise_if_not_found=False, prioritize_remote=False)</code>","text":"<p>Load the results from the local cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> <code>raise_if_not_found</code> <code>bool</code> <p>If True, raise an error if the results are not found.</p> <code>False</code> <code>prioritize_remote</code> <code>bool</code> <p>If True, it will first try to load the results from the remote repository, if available.</p> <code>False</code> <p>Returns:</p> Type Description <code>TaskResult | None</code> <p>The results of the task, or None if not found.</p> Source code in <code>mteb/cache.py</code> <pre><code>def load_task_result(\n    self,\n    task_name: str,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n    raise_if_not_found: bool = False,\n    prioritize_remote: bool = False,\n) -&gt; TaskResult | None:\n    \"\"\"Load the results from the local cache directory.\n\n    Args:\n        task_name: The name of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n        raise_if_not_found: If True, raise an error if the results are not found.\n        prioritize_remote: If True, it will first try to load the results from the remote repository, if available.\n\n    Returns:\n        The results of the task, or None if not found.\n    \"\"\"\n    result_path = self.get_task_result_path(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_name=task_name,\n    )\n\n    if self.has_remote:\n        remote_result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_name,\n            remote=True,\n        )\n        if remote_result_path.exists() and prioritize_remote:\n            result_path = remote_result_path\n        elif not result_path.exists():\n            result_path = remote_result_path\n\n    if not result_path.exists():\n        msg = f\"Results for {model_name} on {task_name} not found in {result_path}\"\n        if raise_if_not_found:\n            raise FileNotFoundError(msg)\n        logger.debug(msg)\n        return None\n\n    return TaskResult.from_disk(result_path)\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.save_to_cache","title":"<code>save_to_cache(task_result, model_name, model_revision=None)</code>","text":"<p>Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.</p> <p>Where model_name is a path-normalized model name. In addition we also save a model_meta.json in the revision folder to preserve the model metadata.</p> <p>Parameters:</p> Name Type Description Default <code>task_result</code> <code>TaskResult</code> <p>The results of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> Source code in <code>mteb/cache.py</code> <pre><code>def save_to_cache(\n    self,\n    task_result: TaskResult,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n) -&gt; None:\n    \"\"\"Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.\n\n    Where model_name is a path-normalized model name.\n    In addition we also save a model_meta.json in the revision folder to preserve the model metadata.\n\n    Args:\n        task_result: The results of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n    \"\"\"\n    result_path = self.get_task_result_path(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_name=task_result.task_name,\n    )\n    result_path.parent.mkdir(parents=True, exist_ok=True)\n    task_result.to_disk(result_path)\n\n    model_meta_path = result_path.parent / \"model_meta.json\"\n    if isinstance(model_name, ModelMeta):\n        meta = model_name\n        with model_meta_path.open(\"w\") as f:\n            json.dump(meta.to_dict(), f, default=str)\n</code></pre>"},{"location":"api/results/#result-objects","title":"Result Objects","text":""},{"location":"api/results/#mteb.results.TaskResult","title":"<code>mteb.results.TaskResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class to represent the MTEB result.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>The name of the MTEB task.</p> <code>dataset_revision</code> <code>str</code> <p>The revision dataset for the task on HuggingFace dataset hub.</p> <code>mteb_version</code> <code>str | None</code> <p>The version of the MTEB used to evaluate the model.</p> <code>scores</code> <code>dict[SplitName, list[ScoresDict]]</code> <p>The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, list[Scores]]. Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of the dataset.</p> <code>evaluation_time</code> <code>float | None</code> <p>The time taken to evaluate the model.</p> <code>kg_co2_emissions</code> <code>float | None</code> <p>The kg of CO2 emissions produced by the model during evaluation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scores = {\n...     \"evaluation_time\": 100,\n...     \"train\": {\n...         \"en-de\": {\n...             \"main_score\": 0.5,\n...         },\n...         \"en-fr\": {\n...             \"main_score\": 0.6,\n...         },\n...     },\n... }\n&gt;&gt;&gt; sample_task = ... # some MTEB task\n&gt;&gt;&gt; mteb_results = TaskResult.from_task_results(sample_task, scores)\n&gt;&gt;&gt; mteb_results.get_score()  # get the main score for all languages\n0.55\n&gt;&gt;&gt; mteb_results.get_score(languages=[\"fra\"])  # get the main score for French\n0.6\n&gt;&gt;&gt; mteb_results.to_dict()\n{'dataset_revision': '1.0', 'task_name': 'sample_task', 'mteb_version': '1.0.0', 'evaluation_time': 100, 'scores': {'train':\n    [\n        {'main_score': 0.5, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']},\n        {'main_score': 0.6, 'hf_subset': 'en-fr', 'languages': ['eng-Latn', 'fra-Latn']}\n    ]}\n}\n</code></pre> Source code in <code>mteb/results/task_result.py</code> <pre><code>class TaskResult(BaseModel):\n    \"\"\"A class to represent the MTEB result.\n\n    Attributes:\n        task_name: The name of the MTEB task.\n        dataset_revision: The revision dataset for the task on HuggingFace dataset hub.\n        mteb_version: The version of the MTEB used to evaluate the model.\n        scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, list[Scores]].\n            Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n            the dataset.\n        evaluation_time: The time taken to evaluate the model.\n        kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n\n    Examples:\n        &gt;&gt;&gt; scores = {\n        ...     \"evaluation_time\": 100,\n        ...     \"train\": {\n        ...         \"en-de\": {\n        ...             \"main_score\": 0.5,\n        ...         },\n        ...         \"en-fr\": {\n        ...             \"main_score\": 0.6,\n        ...         },\n        ...     },\n        ... }\n        &gt;&gt;&gt; sample_task = ... # some MTEB task\n        &gt;&gt;&gt; mteb_results = TaskResult.from_task_results(sample_task, scores)\n        &gt;&gt;&gt; mteb_results.get_score()  # get the main score for all languages\n        0.55\n        &gt;&gt;&gt; mteb_results.get_score(languages=[\"fra\"])  # get the main score for French\n        0.6\n        &gt;&gt;&gt; mteb_results.to_dict()\n        {'dataset_revision': '1.0', 'task_name': 'sample_task', 'mteb_version': '1.0.0', 'evaluation_time': 100, 'scores': {'train':\n            [\n                {'main_score': 0.5, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']},\n                {'main_score': 0.6, 'hf_subset': 'en-fr', 'languages': ['eng-Latn', 'fra-Latn']}\n            ]}\n        }\n    \"\"\"\n\n    dataset_revision: str\n    task_name: str\n    mteb_version: str | None\n    scores: dict[SplitName, list[ScoresDict]]\n    evaluation_time: float | None\n    kg_co2_emissions: float | None = None\n\n    @classmethod\n    def from_task_results(\n        cls,\n        task: AbsTask | type[AbsTask],\n        scores: dict[SplitName, dict[HFSubset, ScoresDict]],\n        evaluation_time: float,\n        kg_co2_emissions: float | None = None,\n    ) -&gt; Self:\n        \"\"\"Create a TaskResult from the task and scores.\n\n        Args:\n            task: The task to create the TaskResult from.\n            scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]].\n                Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n                the dataset.\n            evaluation_time: The time taken to evaluate the model.\n            kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n        \"\"\"\n        task_meta = task.metadata\n        subset2langscripts = task_meta.hf_subsets_to_langscripts\n        flat_scores = defaultdict(list)\n        for split, hf_subset_scores in scores.items():\n            for hf_subset, hf_scores in hf_subset_scores.items():\n                eval_langs = subset2langscripts[hf_subset]\n                _scores = {\n                    **hf_scores,\n                    \"hf_subset\": hf_subset,\n                    \"languages\": eval_langs,\n                }\n                flat_scores[split].append(_scores)\n\n        return TaskResult(\n            dataset_revision=task.metadata.revision,\n            task_name=task.metadata.name,\n            mteb_version=version(\"mteb\"),\n            scores=flat_scores,\n            evaluation_time=evaluation_time,\n            kg_co2_emissions=kg_co2_emissions,\n        )\n\n    @field_validator(\"scores\")\n    @classmethod\n    def _validate_scores(\n        cls, v: dict[SplitName, list[ScoresDict]]\n    ) -&gt; dict[SplitName, list[ScoresDict]]:\n        for split, hf_subset_scores in v.items():\n            for hf_subset_score in hf_subset_scores:\n                if not isinstance(hf_subset_score, dict):\n                    raise ValueError(\"Scores should be a dictionary\")\n                cls._validate_scores_dict(hf_subset_score)\n        return v\n\n    @staticmethod\n    def _validate_scores_dict(scores: ScoresDict) -&gt; None:\n        if \"main_score\" not in scores:\n            raise ValueError(\"'main_score' should be in scores\")\n        if \"hf_subset\" not in scores or not isinstance(scores[\"hf_subset\"], str):\n            raise ValueError(\"hf_subset should be in scores and should be a string\")\n        if \"languages\" not in scores or not isinstance(scores[\"languages\"], list):\n            raise ValueError(\"languages should be in scores and should be a list\")\n\n        # check that it is json serializable\n        try:\n            _ = json.dumps(scores)\n        except Exception as e:\n            raise ValueError(f\"Scores are not json serializable: {e}\")\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get the languages present in the scores.\"\"\"\n        langs = []\n        for split, split_res in self.scores.items():\n            for entry in split_res:\n                langs.extend([lang.split(\"-\")[0] for lang in entry[\"languages\"]])\n        return list(set(langs))\n\n    @cached_property\n    def task(self) -&gt; AbsTask:\n        \"\"\"Get the task associated with the result.\"\"\"\n        from mteb.get_tasks import get_task\n\n        return get_task(self.task_name)\n\n    @property\n    def domains(self) -&gt; list[str]:\n        \"\"\"Get the domains of the task.\"\"\"\n        doms = self.task.metadata.domains\n        if doms is None:\n            doms = []\n        return doms  # type: ignore\n\n    @property\n    def task_type(self) -&gt; str:\n        \"\"\"Get the type of the task.\"\"\"\n        return self.task.metadata.type\n\n    @property\n    def is_public(self) -&gt; bool:\n        \"\"\"Check if the task is public.\"\"\"\n        return self.task.metadata.is_public\n\n    @property\n    def hf_subsets(self) -&gt; list[str]:\n        \"\"\"Get the hf_subsets present in the scores.\"\"\"\n        hf_subsets = set()\n        for split, split_res in self.scores.items():\n            for entry in split_res:\n                hf_subsets.add(entry[\"hf_subset\"])\n        return list(hf_subsets)\n\n    @property\n    def eval_splits(self) -&gt; list[str]:\n        \"\"\"Get the eval splits present in the scores.\"\"\"\n        return list(self.scores.keys())\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert the TaskResult to a dictionary.\n\n        Returns:\n            The TaskResult as a dictionary.\n        \"\"\"\n        return self.model_dump()\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; Self:\n        \"\"\"Create a TaskResult from a dictionary.\n\n        Args:\n            data: The dictionary to create the TaskResult from.\n\n        Returns:\n            The created TaskResult object.\n        \"\"\"\n        return cls.model_validate(data)\n\n    def _round_scores(self, scores: dict[SplitName, list[ScoresDict]], n: int) -&gt; None:\n        \"\"\"Recursively round scores to n decimal places\"\"\"\n        for key, value in scores.items():\n            if isinstance(value, dict):\n                self._round_scores(value, n)\n            elif isinstance(value, list):\n                for i, v in enumerate(value):\n                    if isinstance(v, dict):\n                        self._round_scores(v, n)\n                    elif isinstance(v, float):\n                        value[i] = round(v, n)\n\n            elif isinstance(value, float):\n                scores[key] = round(value, n)\n\n    def to_disk(self, path: Path) -&gt; None:\n        \"\"\"Save TaskResult to disk.\n\n        Args:\n            path: The path to the file to save.\n        \"\"\"\n        json_obj = self.model_dump()\n        self._round_scores(json_obj[\"scores\"], 6)\n\n        with path.open(\"w\") as f:\n            json.dump(json_obj, f, indent=2)\n\n    @classmethod\n    def from_disk(cls, path: Path, load_historic_data: bool = True) -&gt; Self:  # type: ignore\n        \"\"\"Load TaskResult from disk.\n\n        Args:\n            path: The path to the file to load.\n            load_historic_data: Whether to attempt to load historic data from before v1.11.0.\n\n        Returns:\n            The loaded TaskResult object.\n        \"\"\"\n        with path.open(\"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        if not load_historic_data:\n            try:\n                return cls.model_validate(data)\n            except Exception as e:\n                raise ValueError(\n                    f\"Error loading TaskResult from disk. You can try to load historic data by setting `load_historic_data=True`. Error: {e}\"\n                )\n\n        pre_1_11_load = (\n            (\n                \"mteb_version\" in data\n                and data[\"mteb_version\"] is not None\n                and Version(data[\"mteb_version\"]) &lt; Version(\"1.11.0\")\n            )\n            or \"mteb_version\" not in data\n        )  # assume it is before 1.11.0 if the version is not present\n\n        try:\n            obj = cls.model_validate(data)\n        except Exception as e:\n            if not pre_1_11_load:\n                raise e\n            logger.debug(\n                f\"Could not load TaskResult from disk, got error: {e}. Attempting to load from disk using format from before v1.11.0\"\n            )\n            obj = cls._convert_from_before_v1_11_0(data)\n\n        pre_v_12_48 = (\n            \"mteb_version\" in data\n            and data[\"mteb_version\"] is not None\n            and Version(data[\"mteb_version\"]) &lt; Version(\"1.12.48\")\n        )\n\n        if pre_v_12_48:\n            cls._fix_pair_classification_scores(obj)\n\n        return obj\n\n    @classmethod\n    def _fix_pair_classification_scores(cls, obj: TaskResult) -&gt; None:\n        from mteb import get_task\n\n        task_name = obj.task_name\n        if task_name in outdated_tasks:\n            task = outdated_tasks[task_name]\n        else:\n            task = get_task(obj.task_name)\n\n        if task.metadata.type == \"PairClassification\":\n            for split, split_scores in obj.scores.items():\n                for hf_subset_scores in split_scores:\n                    # concatenate score e.g. [\"max\"][\"ap\"] -&gt; [\"max_ap\"]\n                    for key in list(hf_subset_scores.keys()):\n                        if isinstance(hf_subset_scores[key], dict):\n                            for k, v in hf_subset_scores[key].items():\n                                hf_subset_scores[f\"{key}_{k}\"] = v\n                            hf_subset_scores.pop(key)\n\n    @classmethod\n    def _convert_from_before_v1_11_0(cls, data: dict) -&gt; Self:\n        from mteb.get_tasks import _TASKS_REGISTRY\n\n        # in case the task name is not found in the registry, try to find a lower case version\n        lower_case_registry = {k.lower(): v for k, v in _TASKS_REGISTRY.items()}\n\n        scores = {**data}\n\n        dataset_revision = scores.pop(\n            \"dataset_revision\", \"dataset revision not available\"\n        )\n        task_name = scores.pop(\"mteb_dataset_name\")\n        mteb_version = scores.pop(\"mteb_version\", \"mteb version not available\")\n\n        # calculate evaluation time across all splits (move to top level)\n        evaluation_time = 0\n        for split, split_score in scores.items():\n            if \"evaluation_time\" in split_score:\n                evaluation_time += split_score.pop(\"evaluation_time\")\n\n        # normalize the scores to always be {split: {hf_subset: scores}}\n        contains_hf_subset = any(\n            isinstance(hf_subset_scores, dict)\n            for split_scores in scores.values()\n            for k, hf_subset_scores in split_scores.items()\n            if k\n            not in {\"v_measures\", \"cos_sim\", \"euclidean\", \"manhattan\", \"dot\", \"max\"}\n        )\n        if not contains_hf_subset:\n            for split, split_score in scores.items():\n                scores[split] = {\"default\": split_score.copy()}\n\n        if task_name in outdated_tasks:\n            logger.debug(\n                f\"Loading {task_name} as a dummy task as it no longer exists within MTEB. To avoid this set `load_historic_data=False`\"\n            )\n            task = outdated_tasks[task_name]\n        else:\n            if task_name in renamed_tasks:\n                task_name = renamed_tasks[task_name]\n            task = _TASKS_REGISTRY.get(\n                task_name, lower_case_registry[task_name.lower()]\n            )\n\n        # make sure that main score exists\n        main_score = task.metadata.main_score\n        for split, split_score in scores.items():\n            for hf_subset, hf_subset_scores in split_score.items():\n                for name, prev_name in [\n                    (ScoringFunction.COSINE.value, \"cos_sim\"),\n                    (ScoringFunction.MANHATTAN.value, \"manhattan\"),\n                    (ScoringFunction.EUCLIDEAN.value, \"euclidean\"),\n                    (ScoringFunction.DOT_PRODUCT.value, \"dot\"),\n                    (\"max\", \"max\"),\n                    (\"similarity\", \"similarity\"),\n                ]:\n                    prev_name_scores = hf_subset_scores.pop(prev_name, None)\n                    if prev_name_scores is not None:\n                        for k, v in prev_name_scores.items():\n                            hf_subset_scores[f\"{name}_{k}\"] = v\n\n                if \"main_score\" not in hf_subset_scores:\n                    if main_score in hf_subset_scores:\n                        hf_subset_scores[\"main_score\"] = hf_subset_scores[main_score]\n                    else:\n                        logger.warning(f\"Main score {main_score} not found in scores\")\n                        hf_subset_scores[\"main_score\"] = None\n\n        # specific fixes:\n        if task_name == \"MLSUMClusteringP2P\" and mteb_version in [\n            \"1.1.2.dev0\",\n            \"1.1.3.dev0\",\n        ]:  # back then it was only the french subsection which was implemented\n            scores[\"test\"][\"fr\"] = scores[\"test\"].pop(\"default\")\n        if task_name == \"MLSUMClusteringS2S\" and mteb_version in [\n            \"1.1.2.dev0\",\n            \"1.1.3.dev0\",\n        ]:\n            scores[\"test\"][\"fr\"] = scores[\"test\"].pop(\"default\")\n        if task_name == \"XPQARetrieval\":  # subset were renamed from \"fr\" to \"fra-fra\"\n            if \"test\" in scores and \"fr\" in scores[\"test\"]:\n                scores[\"test\"][\"fra-fra\"] = scores[\"test\"].pop(\"fr\")\n\n        result: TaskResult = TaskResult.from_task_results(\n            task,  # type: ignore\n            scores,\n            evaluation_time,\n            kg_co2_emissions=None,\n        )\n        result.dataset_revision = dataset_revision\n        result.mteb_version = mteb_version\n        return result\n\n    def get_score(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] = lambda scores: scores[\"main_score\"],\n        aggregation: Callable[[list[Score]], Any] = np.mean,\n    ) -&gt; Any:\n        \"\"\"Get a score for the specified splits, languages, scripts and aggregation function.\n\n        Args:\n            splits: The splits to consider.\n            languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n            scripts: The scripts to consider.\n            getter: A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".\n            aggregation: The aggregation function to use.\n\n        Returns:\n            The result of the aggregation function on the scores.\n        \"\"\"\n        if splits is None:\n            splits = list(self.scores.keys())\n\n        lang_scripts = LanguageScripts.from_languages_and_scripts(languages, scripts)\n\n        values = []\n        for split in splits:\n            if split not in self.scores:\n                raise ValueError(f\"Split {split} not found in scores\")\n\n            for scores in self.scores[split]:\n                eval_langs = scores[\"languages\"]\n                for lang in eval_langs:\n                    if lang_scripts.contains_language(lang):\n                        values.append(getter(scores))\n                        break\n\n        return aggregation(values)\n\n    def _get_score_fast(\n        self,\n        splits: Iterable[str] | None = None,\n        languages: str | None = None,\n        subsets: Iterable[str] | None = None,\n    ) -&gt; float:\n        \"\"\"Sped up version of get_score that will be used if no aggregation, script or getter needs to be specified.\n\n        Args:\n            splits: The splits to consider.\n            languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n            subsets: The hf_subsets to consider.\n\n        Returns:\n            The mean main score for the specified splits, languages and subsets.\n        \"\"\"\n        if splits is None:\n            splits = self.scores.keys()\n        val_sum = 0\n        n_val = 0\n        for split in splits:\n            if split not in self.scores:\n                raise ValueError(f\"Split missing from scores: {split}\")\n\n            for scores in self.scores[split]:\n                langs = scores[\"languages\"]\n                hf_subset = scores[\"hf_subset\"]\n                main_score = scores.get(\"main_score\", None)\n                if main_score is None:\n                    raise ValueError(f\"Missing main score for subset: {hf_subset}\")\n                if subsets and hf_subset not in subsets:\n                    continue\n                elif subsets:\n                    val_sum += main_score\n                    n_val += 1\n                    continue\n\n                if languages is None:\n                    val_sum += main_score\n                    n_val += 1\n                    continue\n                for lang in langs:\n                    if lang.split(\"-\")[0] in languages:\n                        val_sum += main_score\n                        n_val += 1\n                        logger.info(f\"{val_sum=}, {n_val=}\")\n                        break\n        if n_val == 0:\n            raise ValueError(\"No splits had scores for the specified languages.\")\n        return val_sum / n_val\n\n    @classmethod\n    def from_validated(cls, **data) -&gt; Self:\n        \"\"\"Create a TaskResult from validated data.\n\n        Returns:\n            The created TaskResult object.\n        \"\"\"\n        return cls.model_construct(**data)\n\n    def __repr__(self) -&gt; str:\n        return f\"TaskResult(task_name={self.task_name}, scores=...)\"\n\n    def only_main_score(self) -&gt; Self:\n        \"\"\"Return a new TaskResult object with only the main score.\n\n        Returns:\n            A new TaskResult object with only the main score.\n        \"\"\"\n        new_scores = {}\n        for split in self.scores:\n            new_scores[split] = []\n            for subset_scores in self.scores[split]:\n                new_scores[split].append(\n                    {\n                        \"hf_subset\": subset_scores.get(\"hf_subset\", \"default\"),\n                        \"main_score\": subset_scores.get(\"main_score\", np.nan),\n                        \"languages\": subset_scores.get(\"languages\", []),\n                    }\n                )\n        new_res = {**self.to_dict(), \"scores\": new_scores}\n        new_res = TaskResult.from_validated(**new_res)\n        return new_res\n\n    def validate_and_filter_scores(self, task: AbsTask | None = None) -&gt; Self:\n        \"\"\"Validate and filter the scores against the task metadata.\n\n        This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata.\n        Additionally it also ensure that all of the splits required as well as the languages are present in the scores.\n        Returns new TaskResult object.\n\n        Args:\n            task: The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages,\n                the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.\n\n        Returns:\n            A new TaskResult object with the validated and filtered scores.\n        \"\"\"\n        from mteb.get_tasks import get_task\n\n        if task is None:\n            task = get_task(self.task_name)\n\n        splits = task.eval_splits\n        hf_subsets = task.hf_subsets\n        hf_subsets = set(hf_subsets)\n\n        new_scores = {}\n        seen_splits = set()\n        for split in self.scores:\n            if split not in splits:\n                continue\n            new_scores[split] = []\n            seen_subsets = set()\n            for _scores in self.scores[split]:\n                if _scores[\"hf_subset\"] not in hf_subsets:\n                    continue\n                new_scores[split].append(_scores)\n                seen_subsets.add(_scores[\"hf_subset\"])\n            if seen_subsets != hf_subsets:\n                missing_subsets = hf_subsets - seen_subsets\n                if len(missing_subsets) &gt; 2:\n                    subset1, subset2 = list(missing_subsets)[:2]\n                    missing_subsets_str = f\"{{'{subset1}', '{subset2}', ...}}\"\n                else:\n                    missing_subsets_str = str(missing_subsets)\n\n                logger.warning(\n                    f\"{task.metadata.name}: Missing subsets {missing_subsets_str} for split {split}\"\n                )\n            seen_splits.add(split)\n        if seen_splits != set(splits):\n            logger.warning(\n                f\"{task.metadata.name}: Missing splits {set(splits) - seen_splits}\"\n            )\n        new_res = {**self.to_dict(), \"scores\": new_scores}\n        new_res = TaskResult.from_validated(**new_res)\n        return new_res\n\n    def is_mergeable(\n        self,\n        result: TaskResult | AbsTask,\n        criteria: list[str] | list[Criteria] = [\n            \"mteb_version\",\n            \"dataset_revision\",\n        ],\n        raise_error: bool = False,\n    ) -&gt; bool:\n        \"\"\"Checks if the TaskResult object can be merged with another TaskResult or Task.\n\n        Args:\n            result: The TaskResult or Task object to check against.\n            criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n                It will always check that the task name match.\n            raise_error: If True, raises an error if the objects cannot be merged. If False, returns False.\n\n        Returns:\n            True if the TaskResult object can be merged with the other object, False otherwise.\n        \"\"\"\n        criteria = [Criteria.from_str(c) if isinstance(c, str) else c for c in criteria]\n        if isinstance(result, TaskResult):\n            name = result.task_name\n            revision = result.dataset_revision\n            mteb_version = result.mteb_version\n        elif isinstance(result, AbsTask):\n            mteb_version = version(\"mteb\")\n            name = result.metadata.name\n            revision = result.metadata.revision\n        else:\n            return False\n\n        if self.task_name != name:\n            if raise_error:\n                raise ValueError(\n                    f\"Cannot merge TaskResult objects as they are derived from different tasks ({self.task_name} and {name})\"\n                )\n            return False\n\n        if Criteria.MTEB_VERSION in criteria and self.mteb_version != mteb_version:\n            if raise_error:\n                raise ValueError(\n                    f\"Cannot merge TaskResult objects as they are derived from different MTEB versions ({self.mteb_version} and {mteb_version})\"\n                )\n            return False\n\n        if Criteria.DATASET_REVISION in criteria and self.dataset_revision != revision:\n            if raise_error:\n                raise ValueError(\n                    f\"Cannot merge TaskResult objects as they are derived from different dataset revisions ({self.dataset_revision} and {revision})\"\n                )\n            return False\n\n        return True\n\n    def merge(\n        self,\n        new_results: TaskResult,\n        criteria: list[str] | list[Criteria] = [\n            \"mteb_version\",\n            \"dataset_revision\",\n        ],\n    ) -&gt; Self:\n        \"\"\"Merges two TaskResult objects.\n\n        Args:\n            new_results: The new TaskResult object to merge with the current one.\n            criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n                It will always check that the task name match.\n\n        Returns:\n            A new TaskResult object with the merged scores.\n        \"\"\"\n        self.is_mergeable(new_results, criteria=criteria, raise_error=True)\n\n        merged_scores = self.scores.copy()\n\n        for split, scores in new_results.scores.items():\n            if split in merged_scores:\n                merged_scores[split] = self._merge_split_scores(\n                    merged_scores[split], scores\n                )\n            else:\n                merged_scores[split] = scores\n\n        existing_kg_co2_emissions = (\n            self.kg_co2_emissions if self.kg_co2_emissions else 0\n        )\n        new_kg_co2_emissions = (\n            new_results.kg_co2_emissions if new_results.kg_co2_emissions else 0\n        )\n        merged_kg_co2_emissions = None\n        if existing_kg_co2_emissions and new_kg_co2_emissions:\n            merged_kg_co2_emissions = existing_kg_co2_emissions + new_kg_co2_emissions\n\n        merged_evaluation_time = None\n        if self.evaluation_time and new_results.evaluation_time:\n            merged_evaluation_time = self.evaluation_time + new_results.evaluation_time\n        merged_results = TaskResult(\n            dataset_revision=new_results.dataset_revision,\n            task_name=new_results.task_name,\n            mteb_version=new_results.mteb_version,\n            scores=merged_scores,\n            evaluation_time=merged_evaluation_time,\n            kg_co2_emissions=merged_kg_co2_emissions,\n        )\n\n        return merged_results\n\n    @staticmethod\n    def _merge_split_scores(\n        existing_scores: list[ScoresDict], new_scores: list[ScoresDict]\n    ) -&gt; list[ScoresDict]:\n        merged = {score[\"hf_subset\"]: score for score in existing_scores}\n        for score in new_scores:\n            merged[score[\"hf_subset\"]] = score\n        return list(merged.values())\n\n    def get_missing_evaluations(self, task: AbsTask) -&gt; dict[str, list[str]]:\n        \"\"\"Checks which splits and subsets are missing from the results.\n\n        Args:\n            task: The task to check against.\n\n        Returns:\n            A dictionary with the splits as keys and a list of missing subsets as values.\n        \"\"\"\n        missing_splits = {}\n        for splits in task.eval_splits:\n            if splits not in self.scores:  # split it fully missing\n                missing_splits[splits] = task.hf_subsets\n            if splits in self.scores:\n                hf_subsets = {score[\"hf_subset\"] for score in self.scores[splits]}\n                missing_subsets = list(set(task.hf_subsets) - hf_subsets)\n                if missing_subsets:\n                    missing_splits[splits] = missing_subsets\n\n        return missing_splits\n\n    def get_hf_eval_results(self) -&gt; list[EvalResult]:\n        \"\"\"Create HF evaluation results objects from TaskResult objects.\n\n        Returns:\n            List of EvalResult objects for each split and subset.\n        \"\"\"\n        task_metadata = self.task.metadata\n        task_type = task_metadata._hf_task_type()[0]\n        results = []\n        for split, scores in self.scores.items():\n            for subset_results in scores:\n                subset = subset_results.get(\"hf_subset\", \"default\")\n                results.append(\n                    EvalResult(\n                        task_type=task_type,\n                        task_name=task_metadata.type,\n                        dataset_type=task_metadata.dataset[\"path\"],\n                        dataset_name=f\"{task_metadata.name} ({subset})\",\n                        dataset_config=subset,\n                        dataset_split=split,\n                        dataset_revision=task_metadata.dataset[\"revision\"],\n                        metric_type=task_metadata.main_score,\n                        metric_name=task_metadata.main_score,\n                        metric_value=subset_results[\"main_score\"],\n                        source_name=\"MTEB\",\n                        source_url=\"https://github.com/embeddings-benchmark/mteb/\",\n                    )\n                )\n        return results\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get the domains of the task.</p>"},{"location":"api/results/#mteb.results.TaskResult.eval_splits","title":"<code>eval_splits</code>  <code>property</code>","text":"<p>Get the eval splits present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.hf_subsets","title":"<code>hf_subsets</code>  <code>property</code>","text":"<p>Get the hf_subsets present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.is_public","title":"<code>is_public</code>  <code>property</code>","text":"<p>Check if the task is public.</p>"},{"location":"api/results/#mteb.results.TaskResult.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get the languages present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.task","title":"<code>task</code>  <code>cached</code> <code>property</code>","text":"<p>Get the task associated with the result.</p>"},{"location":"api/results/#mteb.results.TaskResult.task_type","title":"<code>task_type</code>  <code>property</code>","text":"<p>Get the type of the task.</p>"},{"location":"api/results/#mteb.results.TaskResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dictionary to create the TaskResult from.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Self:\n    \"\"\"Create a TaskResult from a dictionary.\n\n    Args:\n        data: The dictionary to create the TaskResult from.\n\n    Returns:\n        The created TaskResult object.\n    \"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_disk","title":"<code>from_disk(path, load_historic_data=True)</code>  <code>classmethod</code>","text":"<p>Load TaskResult from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to load.</p> required <code>load_historic_data</code> <code>bool</code> <p>Whether to attempt to load historic data from before v1.11.0.</p> <code>True</code> <p>Returns:</p> Type Description <code>Self</code> <p>The loaded TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path, load_historic_data: bool = True) -&gt; Self:  # type: ignore\n    \"\"\"Load TaskResult from disk.\n\n    Args:\n        path: The path to the file to load.\n        load_historic_data: Whether to attempt to load historic data from before v1.11.0.\n\n    Returns:\n        The loaded TaskResult object.\n    \"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    if not load_historic_data:\n        try:\n            return cls.model_validate(data)\n        except Exception as e:\n            raise ValueError(\n                f\"Error loading TaskResult from disk. You can try to load historic data by setting `load_historic_data=True`. Error: {e}\"\n            )\n\n    pre_1_11_load = (\n        (\n            \"mteb_version\" in data\n            and data[\"mteb_version\"] is not None\n            and Version(data[\"mteb_version\"]) &lt; Version(\"1.11.0\")\n        )\n        or \"mteb_version\" not in data\n    )  # assume it is before 1.11.0 if the version is not present\n\n    try:\n        obj = cls.model_validate(data)\n    except Exception as e:\n        if not pre_1_11_load:\n            raise e\n        logger.debug(\n            f\"Could not load TaskResult from disk, got error: {e}. Attempting to load from disk using format from before v1.11.0\"\n        )\n        obj = cls._convert_from_before_v1_11_0(data)\n\n    pre_v_12_48 = (\n        \"mteb_version\" in data\n        and data[\"mteb_version\"] is not None\n        and Version(data[\"mteb_version\"]) &lt; Version(\"1.12.48\")\n    )\n\n    if pre_v_12_48:\n        cls._fix_pair_classification_scores(obj)\n\n    return obj\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_task_results","title":"<code>from_task_results(task, scores, evaluation_time, kg_co2_emissions=None)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from the task and scores.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask | type[AbsTask]</code> <p>The task to create the TaskResult from.</p> required <code>scores</code> <code>dict[SplitName, dict[HFSubset, ScoresDict]]</code> <p>The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]]. Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of the dataset.</p> required <code>evaluation_time</code> <code>float</code> <p>The time taken to evaluate the model.</p> required <code>kg_co2_emissions</code> <code>float | None</code> <p>The kg of CO2 emissions produced by the model during evaluation.</p> <code>None</code> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_task_results(\n    cls,\n    task: AbsTask | type[AbsTask],\n    scores: dict[SplitName, dict[HFSubset, ScoresDict]],\n    evaluation_time: float,\n    kg_co2_emissions: float | None = None,\n) -&gt; Self:\n    \"\"\"Create a TaskResult from the task and scores.\n\n    Args:\n        task: The task to create the TaskResult from.\n        scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]].\n            Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n            the dataset.\n        evaluation_time: The time taken to evaluate the model.\n        kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n    \"\"\"\n    task_meta = task.metadata\n    subset2langscripts = task_meta.hf_subsets_to_langscripts\n    flat_scores = defaultdict(list)\n    for split, hf_subset_scores in scores.items():\n        for hf_subset, hf_scores in hf_subset_scores.items():\n            eval_langs = subset2langscripts[hf_subset]\n            _scores = {\n                **hf_scores,\n                \"hf_subset\": hf_subset,\n                \"languages\": eval_langs,\n            }\n            flat_scores[split].append(_scores)\n\n    return TaskResult(\n        dataset_revision=task.metadata.revision,\n        task_name=task.metadata.name,\n        mteb_version=version(\"mteb\"),\n        scores=flat_scores,\n        evaluation_time=evaluation_time,\n        kg_co2_emissions=kg_co2_emissions,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from validated data.</p> <p>Returns:</p> Type Description <code>Self</code> <p>The created TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data) -&gt; Self:\n    \"\"\"Create a TaskResult from validated data.\n\n    Returns:\n        The created TaskResult object.\n    \"\"\"\n    return cls.model_construct(**data)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_hf_eval_results","title":"<code>get_hf_eval_results()</code>","text":"<p>Create HF evaluation results objects from TaskResult objects.</p> <p>Returns:</p> Type Description <code>list[EvalResult]</code> <p>List of EvalResult objects for each split and subset.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def get_hf_eval_results(self) -&gt; list[EvalResult]:\n    \"\"\"Create HF evaluation results objects from TaskResult objects.\n\n    Returns:\n        List of EvalResult objects for each split and subset.\n    \"\"\"\n    task_metadata = self.task.metadata\n    task_type = task_metadata._hf_task_type()[0]\n    results = []\n    for split, scores in self.scores.items():\n        for subset_results in scores:\n            subset = subset_results.get(\"hf_subset\", \"default\")\n            results.append(\n                EvalResult(\n                    task_type=task_type,\n                    task_name=task_metadata.type,\n                    dataset_type=task_metadata.dataset[\"path\"],\n                    dataset_name=f\"{task_metadata.name} ({subset})\",\n                    dataset_config=subset,\n                    dataset_split=split,\n                    dataset_revision=task_metadata.dataset[\"revision\"],\n                    metric_type=task_metadata.main_score,\n                    metric_name=task_metadata.main_score,\n                    metric_value=subset_results[\"main_score\"],\n                    source_name=\"MTEB\",\n                    source_url=\"https://github.com/embeddings-benchmark/mteb/\",\n                )\n            )\n    return results\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_missing_evaluations","title":"<code>get_missing_evaluations(task)</code>","text":"<p>Checks which splits and subsets are missing from the results.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask</code> <p>The task to check against.</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dictionary with the splits as keys and a list of missing subsets as values.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def get_missing_evaluations(self, task: AbsTask) -&gt; dict[str, list[str]]:\n    \"\"\"Checks which splits and subsets are missing from the results.\n\n    Args:\n        task: The task to check against.\n\n    Returns:\n        A dictionary with the splits as keys and a list of missing subsets as values.\n    \"\"\"\n    missing_splits = {}\n    for splits in task.eval_splits:\n        if splits not in self.scores:  # split it fully missing\n            missing_splits[splits] = task.hf_subsets\n        if splits in self.scores:\n            hf_subsets = {score[\"hf_subset\"] for score in self.scores[splits]}\n            missing_subsets = list(set(task.hf_subsets) - hf_subsets)\n            if missing_subsets:\n                missing_splits[splits] = missing_subsets\n\n    return missing_splits\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_score","title":"<code>get_score(splits=None, languages=None, scripts=None, getter=lambda scores: scores['main_score'], aggregation=np.mean)</code>","text":"<p>Get a score for the specified splits, languages, scripts and aggregation function.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>list[SplitName] | None</code> <p>The splits to consider.</p> <code>None</code> <code>languages</code> <code>list[ISOLanguage | ISOLanguageScript] | None</code> <p>The languages to consider. Can be ISO language codes or ISO language script codes.</p> <code>None</code> <code>scripts</code> <code>list[ISOLanguageScript] | None</code> <p>The scripts to consider.</p> <code>None</code> <code>getter</code> <code>Callable[[ScoresDict], Score]</code> <p>A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".</p> <code>lambda scores: scores['main_score']</code> <code>aggregation</code> <code>Callable[[list[Score]], Any]</code> <p>The aggregation function to use.</p> <code>mean</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the aggregation function on the scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def get_score(\n    self,\n    splits: list[SplitName] | None = None,\n    languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n    scripts: list[ISOLanguageScript] | None = None,\n    getter: Callable[[ScoresDict], Score] = lambda scores: scores[\"main_score\"],\n    aggregation: Callable[[list[Score]], Any] = np.mean,\n) -&gt; Any:\n    \"\"\"Get a score for the specified splits, languages, scripts and aggregation function.\n\n    Args:\n        splits: The splits to consider.\n        languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n        scripts: The scripts to consider.\n        getter: A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".\n        aggregation: The aggregation function to use.\n\n    Returns:\n        The result of the aggregation function on the scores.\n    \"\"\"\n    if splits is None:\n        splits = list(self.scores.keys())\n\n    lang_scripts = LanguageScripts.from_languages_and_scripts(languages, scripts)\n\n    values = []\n    for split in splits:\n        if split not in self.scores:\n            raise ValueError(f\"Split {split} not found in scores\")\n\n        for scores in self.scores[split]:\n            eval_langs = scores[\"languages\"]\n            for lang in eval_langs:\n                if lang_scripts.contains_language(lang):\n                    values.append(getter(scores))\n                    break\n\n    return aggregation(values)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.is_mergeable","title":"<code>is_mergeable(result, criteria=['mteb_version', 'dataset_revision'], raise_error=False)</code>","text":"<p>Checks if the TaskResult object can be merged with another TaskResult or Task.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TaskResult | AbsTask</code> <p>The TaskResult or Task object to check against.</p> required <code>criteria</code> <code>list[str] | list[Criteria]</code> <p>Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\". It will always check that the task name match.</p> <code>['mteb_version', 'dataset_revision']</code> <code>raise_error</code> <code>bool</code> <p>If True, raises an error if the objects cannot be merged. If False, returns False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the TaskResult object can be merged with the other object, False otherwise.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def is_mergeable(\n    self,\n    result: TaskResult | AbsTask,\n    criteria: list[str] | list[Criteria] = [\n        \"mteb_version\",\n        \"dataset_revision\",\n    ],\n    raise_error: bool = False,\n) -&gt; bool:\n    \"\"\"Checks if the TaskResult object can be merged with another TaskResult or Task.\n\n    Args:\n        result: The TaskResult or Task object to check against.\n        criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n            It will always check that the task name match.\n        raise_error: If True, raises an error if the objects cannot be merged. If False, returns False.\n\n    Returns:\n        True if the TaskResult object can be merged with the other object, False otherwise.\n    \"\"\"\n    criteria = [Criteria.from_str(c) if isinstance(c, str) else c for c in criteria]\n    if isinstance(result, TaskResult):\n        name = result.task_name\n        revision = result.dataset_revision\n        mteb_version = result.mteb_version\n    elif isinstance(result, AbsTask):\n        mteb_version = version(\"mteb\")\n        name = result.metadata.name\n        revision = result.metadata.revision\n    else:\n        return False\n\n    if self.task_name != name:\n        if raise_error:\n            raise ValueError(\n                f\"Cannot merge TaskResult objects as they are derived from different tasks ({self.task_name} and {name})\"\n            )\n        return False\n\n    if Criteria.MTEB_VERSION in criteria and self.mteb_version != mteb_version:\n        if raise_error:\n            raise ValueError(\n                f\"Cannot merge TaskResult objects as they are derived from different MTEB versions ({self.mteb_version} and {mteb_version})\"\n            )\n        return False\n\n    if Criteria.DATASET_REVISION in criteria and self.dataset_revision != revision:\n        if raise_error:\n            raise ValueError(\n                f\"Cannot merge TaskResult objects as they are derived from different dataset revisions ({self.dataset_revision} and {revision})\"\n            )\n        return False\n\n    return True\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.merge","title":"<code>merge(new_results, criteria=['mteb_version', 'dataset_revision'])</code>","text":"<p>Merges two TaskResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>new_results</code> <code>TaskResult</code> <p>The new TaskResult object to merge with the current one.</p> required <code>criteria</code> <code>list[str] | list[Criteria]</code> <p>Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\". It will always check that the task name match.</p> <code>['mteb_version', 'dataset_revision']</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new TaskResult object with the merged scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def merge(\n    self,\n    new_results: TaskResult,\n    criteria: list[str] | list[Criteria] = [\n        \"mteb_version\",\n        \"dataset_revision\",\n    ],\n) -&gt; Self:\n    \"\"\"Merges two TaskResult objects.\n\n    Args:\n        new_results: The new TaskResult object to merge with the current one.\n        criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n            It will always check that the task name match.\n\n    Returns:\n        A new TaskResult object with the merged scores.\n    \"\"\"\n    self.is_mergeable(new_results, criteria=criteria, raise_error=True)\n\n    merged_scores = self.scores.copy()\n\n    for split, scores in new_results.scores.items():\n        if split in merged_scores:\n            merged_scores[split] = self._merge_split_scores(\n                merged_scores[split], scores\n            )\n        else:\n            merged_scores[split] = scores\n\n    existing_kg_co2_emissions = (\n        self.kg_co2_emissions if self.kg_co2_emissions else 0\n    )\n    new_kg_co2_emissions = (\n        new_results.kg_co2_emissions if new_results.kg_co2_emissions else 0\n    )\n    merged_kg_co2_emissions = None\n    if existing_kg_co2_emissions and new_kg_co2_emissions:\n        merged_kg_co2_emissions = existing_kg_co2_emissions + new_kg_co2_emissions\n\n    merged_evaluation_time = None\n    if self.evaluation_time and new_results.evaluation_time:\n        merged_evaluation_time = self.evaluation_time + new_results.evaluation_time\n    merged_results = TaskResult(\n        dataset_revision=new_results.dataset_revision,\n        task_name=new_results.task_name,\n        mteb_version=new_results.mteb_version,\n        scores=merged_scores,\n        evaluation_time=merged_evaluation_time,\n        kg_co2_emissions=merged_kg_co2_emissions,\n    )\n\n    return merged_results\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.only_main_score","title":"<code>only_main_score()</code>","text":"<p>Return a new TaskResult object with only the main score.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A new TaskResult object with only the main score.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def only_main_score(self) -&gt; Self:\n    \"\"\"Return a new TaskResult object with only the main score.\n\n    Returns:\n        A new TaskResult object with only the main score.\n    \"\"\"\n    new_scores = {}\n    for split in self.scores:\n        new_scores[split] = []\n        for subset_scores in self.scores[split]:\n            new_scores[split].append(\n                {\n                    \"hf_subset\": subset_scores.get(\"hf_subset\", \"default\"),\n                    \"main_score\": subset_scores.get(\"main_score\", np.nan),\n                    \"languages\": subset_scores.get(\"languages\", []),\n                }\n            )\n    new_res = {**self.to_dict(), \"scores\": new_scores}\n    new_res = TaskResult.from_validated(**new_res)\n    return new_res\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the TaskResult to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The TaskResult as a dictionary.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert the TaskResult to a dictionary.\n\n    Returns:\n        The TaskResult as a dictionary.\n    \"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save TaskResult to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to save.</p> required Source code in <code>mteb/results/task_result.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n    \"\"\"Save TaskResult to disk.\n\n    Args:\n        path: The path to the file to save.\n    \"\"\"\n    json_obj = self.model_dump()\n    self._round_scores(json_obj[\"scores\"], 6)\n\n    with path.open(\"w\") as f:\n        json.dump(json_obj, f, indent=2)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.validate_and_filter_scores","title":"<code>validate_and_filter_scores(task=None)</code>","text":"<p>Validate and filter the scores against the task metadata.</p> <p>This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata. Additionally it also ensure that all of the splits required as well as the languages are present in the scores. Returns new TaskResult object.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask | None</code> <p>The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages, the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new TaskResult object with the validated and filtered scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def validate_and_filter_scores(self, task: AbsTask | None = None) -&gt; Self:\n    \"\"\"Validate and filter the scores against the task metadata.\n\n    This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata.\n    Additionally it also ensure that all of the splits required as well as the languages are present in the scores.\n    Returns new TaskResult object.\n\n    Args:\n        task: The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages,\n            the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.\n\n    Returns:\n        A new TaskResult object with the validated and filtered scores.\n    \"\"\"\n    from mteb.get_tasks import get_task\n\n    if task is None:\n        task = get_task(self.task_name)\n\n    splits = task.eval_splits\n    hf_subsets = task.hf_subsets\n    hf_subsets = set(hf_subsets)\n\n    new_scores = {}\n    seen_splits = set()\n    for split in self.scores:\n        if split not in splits:\n            continue\n        new_scores[split] = []\n        seen_subsets = set()\n        for _scores in self.scores[split]:\n            if _scores[\"hf_subset\"] not in hf_subsets:\n                continue\n            new_scores[split].append(_scores)\n            seen_subsets.add(_scores[\"hf_subset\"])\n        if seen_subsets != hf_subsets:\n            missing_subsets = hf_subsets - seen_subsets\n            if len(missing_subsets) &gt; 2:\n                subset1, subset2 = list(missing_subsets)[:2]\n                missing_subsets_str = f\"{{'{subset1}', '{subset2}', ...}}\"\n            else:\n                missing_subsets_str = str(missing_subsets)\n\n            logger.warning(\n                f\"{task.metadata.name}: Missing subsets {missing_subsets_str} for split {split}\"\n            )\n        seen_splits.add(split)\n    if seen_splits != set(splits):\n        logger.warning(\n            f\"{task.metadata.name}: Missing splits {set(splits) - seen_splits}\"\n        )\n    new_res = {**self.to_dict(), \"scores\": new_scores}\n    new_res = TaskResult.from_validated(**new_res)\n    return new_res\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult","title":"<code>mteb.results.ModelResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data class to hold the results of a model on a set of tasks.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the model.</p> <code>model_revision</code> <code>str | None</code> <p>Revision of the model.</p> <code>task_results</code> <code>list[TaskResult]</code> <p>List of TaskResult objects.</p> Source code in <code>mteb/results/model_result.py</code> <pre><code>class ModelResult(BaseModel):\n    \"\"\"Data class to hold the results of a model on a set of tasks.\n\n    Attributes:\n        model_name: Name of the model.\n        model_revision: Revision of the model.\n        task_results: List of TaskResult objects.\n    \"\"\"\n\n    model_name: str\n    model_revision: str | None\n    task_results: list[TaskResult]\n    default_modalities: list[Modalities] = Field(\n        default_factory=lambda: [\"text\"], alias=\"modalities\"\n    )\n    model_config = (\n        ConfigDict(  # to free up the name model_* which is otherwise protected\n            protected_namespaces=(),\n        )\n    )\n\n    def __repr__(self) -&gt; str:\n        n_entries = len(self.task_results)\n        return f\"ModelResult(model_name={self.model_name}, model_revision={self.model_revision}, task_results=[...](#{n_entries}))\"\n\n    @classmethod\n    def from_validated(cls, **data: dict[str, Any]) -&gt; Self:\n        \"\"\"Create a ModelResult from validated data.\n\n        Args:\n            data: The validated data.\n        \"\"\"\n        data[\"task_results\"] = [\n            TaskResult.from_validated(**res) for res in data[\"task_results\"]\n        ]\n        return cls.model_construct(**data)\n\n    def _filter_tasks(\n        self,\n        task_names: list[str] | None = None,\n        languages: list[str] | None = None,\n        domains: list[TaskDomain] | None = None,\n        task_types: list[TaskType] | None = None,\n        modalities: list[Modalities] | None = None,\n        is_public: bool | None = None,\n    ) -&gt; Self:\n        new_task_results = []\n        for task_result in self.task_results:\n            if (task_names is not None) and (task_result.task_name not in task_names):\n                continue\n            if languages is not None:\n                task_languages = task_result.languages\n                if not any(lang in task_languages for lang in languages):\n                    continue\n            if domains is not None:\n                task_domains = task_result.domains\n                if not any(domain in task_domains for domain in domains):\n                    continue\n            if (task_types is not None) and (task_result.task_type not in task_types):\n                continue\n            if modalities is not None:\n                task_modalities = getattr(task_result, \"modalities\", [])\n                if not any(modality in task_modalities for modality in modalities):\n                    continue\n            if (is_public is not None) and (task_result.is_public is not is_public):\n                continue\n            new_task_results.append(task_result)\n        return type(self).model_construct(\n            model_name=self.model_name,\n            model_revision=self.model_revision,\n            task_results=new_task_results,\n        )\n\n    def select_tasks(self, tasks: Sequence[AbsTask]) -&gt; Self:\n        \"\"\"Select tasks from the ModelResult based on a list of AbsTask objects.\n\n        Args:\n            tasks: A sequence of AbsTask objects to select from the ModelResult.\n        \"\"\"\n        task_name_to_task = {task.metadata.name: task for task in tasks}\n        new_task_results = [\n            task_res.validate_and_filter_scores(task_name_to_task[task_res.task_name])\n            for task_res in self.task_results\n            if task_res.task_name in task_name_to_task\n        ]\n        return type(self).model_construct(\n            model_name=self.model_name,\n            model_revision=self.model_revision,\n            task_results=new_task_results,\n        )\n\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; dict | list:\n        if (getter is not None) or (aggregation is not None) or (scripts is not None):\n            use_fast = False\n            getter = (\n                getter if getter is not None else lambda scores: scores[\"main_score\"]\n            )\n            aggregation = aggregation if aggregation is not None else np.mean\n        else:\n            use_fast = True\n        if format == \"wide\":\n            scores = {}\n            for res in self.task_results:\n                try:\n                    if use_fast:\n                        scores[res.task_name] = res._get_score_fast(\n                            splits=splits,  # type: ignore\n                            languages=languages,  # type: ignore\n                        )\n                    else:\n                        scores[res.task_name] = res.get_score(\n                            splits=splits,\n                            languages=languages,\n                            aggregation=aggregation,  # type: ignore\n                            getter=getter,  # type: ignore\n                            scripts=scripts,\n                        )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {res.task_name} due to {e}.\"\n                    )\n            return scores\n        if format == \"long\":\n            entries = []\n            for task_res in self.task_results:\n                try:\n                    if use_fast:\n                        score = task_res._get_score_fast(\n                            splits=splits,\n                            languages=languages,  # type: ignore\n                        )\n                    else:\n                        score = task_res.get_score(\n                            splits=splits,\n                            languages=languages,\n                            aggregation=aggregation,  # type: ignore\n                            getter=getter,  # type: ignore\n                            scripts=scripts,\n                        )\n                    entry = dict(\n                        model_name=self.model_name,\n                        model_revision=self.model_revision,\n                        task_name=task_res.task_name,\n                        score=score,\n                        mteb_version=task_res.mteb_version,\n                        dataset_revision=task_res.dataset_revision,\n                        evaluation_time=task_res.evaluation_time,\n                        kg_co2_emissions=task_res.kg_co2_emissions,\n                    )\n                    entries.append(entry)\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {task_res.task_name} due to {e}.\"\n                    )\n            return entries\n\n    def _get_score_for_table(self) -&gt; list[dict[str, str | float]]:\n        scores_data = []\n        model_name = self.model_name\n        for task_result in self.task_results:\n            task_name = task_result.task_name\n            for split, scores_list in task_result.scores.items():\n                for score_item in scores_list:\n                    row = {\n                        \"model_name\": model_name,\n                        \"model_revision\": self.model_revision,\n                        \"task_name\": task_name,\n                        \"split\": split,\n                        \"subset\": score_item.get(\"hf_subset\", \"default\"),\n                        \"score\": score_item.get(\"main_score\", None),\n                    }\n\n                    scores_data.append(row)\n\n        return scores_data\n\n    def to_dataframe(\n        self,\n        aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n        aggregation_fn: Callable[[list[Score]], Any] | None = None,\n        include_model_revision: bool = False,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n        The DataFrame will have the following columns in addition to the metadata columns:\n\n        - model_name: The name of the model.\n        - task_name: The name of the task.\n        - score: The main score of the model on the task.\n\n        In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n        - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n        - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n        Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n        Args:\n            aggregation_level: The aggregation to use. Can be one of:\n                - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n                - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n                - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n            aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n            include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n            format: The format of the DataFrame. Can be one of:\n                - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n                - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n        Returns:\n            A DataFrame with the scores for all models and tasks.\n        \"\"\"\n        scores_data = self._get_score_for_table()\n\n        if not scores_data:\n            logger.warning(\"No scores data available. Returning empty DataFrame.\")\n            return pd.DataFrame()\n\n        # Create DataFrame\n        df = pd.DataFrame(scores_data)\n\n        _columns = [\"model_name\"]\n        if include_model_revision is False:\n            df = df.drop(columns=[\"model_revision\"])\n        else:\n            _columns.append(\"model_revision\")\n\n        return _aggregate_and_pivot(\n            df,\n            columns=_columns,\n            aggregation_level=aggregation_level,\n            format=format,\n            aggregation_fn=aggregation_fn,\n        )\n\n    def __hash__(self) -&gt; int:\n        return id(self)\n\n    def __iter__(self) -&gt; Iterable[TaskResult]:\n        return iter(self.task_results)\n\n    def __getitem__(self, index) -&gt; TaskResult:\n        return self.task_results[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.task_results)\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get all languages in the model results.\n\n        Returns:\n            A list of languages in the model results.\n        \"\"\"\n        langs = []\n        for task_res in self.task_results:\n            langs.extend(task_res.languages)\n        return list(set(langs))\n\n    @property\n    def domains(self) -&gt; list[str]:\n        \"\"\"Get all domains in the model results.\n\n        Returns:\n            A list of domains in the model results.\n\n        \"\"\"\n        ds = []\n        for task_res in self.task_results:\n            ds.extend(task_res.domains)\n        return list(set(ds))\n\n    @property\n    def task_types(self) -&gt; list[str]:\n        \"\"\"Get all task types in the model results.\n\n        Returns:\n            A list of task types in the model results.\n        \"\"\"\n        return list({task_res.task_type for task_res in self.task_results})\n\n    @property\n    def task_names(self) -&gt; list[str]:\n        \"\"\"Get all task names in the model results.\n\n        Returns:\n            A list of task names in the model results.\n        \"\"\"\n        return [task_res.task_name for task_res in self.task_results]\n\n    @property\n    def modalities(self) -&gt; list[str]:\n        \"\"\"Get all modalities in the task results.\n\n        Returns:\n            A list of modalities in the task results.\n        \"\"\"\n        mods = []\n        for task_res in self.task_results:\n            task_modalities = getattr(task_res, \"modalities\", [])\n            mods.extend(task_modalities)\n        if not mods:\n            mods = self.default_modalities\n        return list(set(mods))\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get all domains in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of domains in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get all languages in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of languages in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Get all modalities in the task results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of modalities in the task results.</p>"},{"location":"api/results/#mteb.results.ModelResult.task_names","title":"<code>task_names</code>  <code>property</code>","text":"<p>Get all task names in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.task_types","title":"<code>task_types</code>  <code>property</code>","text":"<p>Get all task types in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task types in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create a ModelResult from validated data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The validated data.</p> <code>{}</code> Source code in <code>mteb/results/model_result.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data: dict[str, Any]) -&gt; Self:\n    \"\"\"Create a ModelResult from validated data.\n\n    Args:\n        data: The validated data.\n    \"\"\"\n    data[\"task_results\"] = [\n        TaskResult.from_validated(**res) for res in data[\"task_results\"]\n    ]\n    return cls.model_construct(**data)\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.select_tasks","title":"<code>select_tasks(tasks)</code>","text":"<p>Select tasks from the ModelResult based on a list of AbsTask objects.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[AbsTask]</code> <p>A sequence of AbsTask objects to select from the ModelResult.</p> required Source code in <code>mteb/results/model_result.py</code> <pre><code>def select_tasks(self, tasks: Sequence[AbsTask]) -&gt; Self:\n    \"\"\"Select tasks from the ModelResult based on a list of AbsTask objects.\n\n    Args:\n        tasks: A sequence of AbsTask objects to select from the ModelResult.\n    \"\"\"\n    task_name_to_task = {task.metadata.name: task for task in tasks}\n    new_task_results = [\n        task_res.validate_and_filter_scores(task_name_to_task[task_res.task_name])\n        for task_res in self.task_results\n        if task_res.task_name in task_name_to_task\n    ]\n    return type(self).model_construct(\n        model_name=self.model_name,\n        model_revision=self.model_revision,\n        task_results=new_task_results,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.to_dataframe","title":"<code>to_dataframe(aggregation_level='task', aggregation_fn=None, include_model_revision=False, format='wide')</code>","text":"<p>Get a DataFrame with the scores for all models and tasks.</p> <p>The DataFrame will have the following columns in addition to the metadata columns:</p> <ul> <li>model_name: The name of the model.</li> <li>task_name: The name of the task.</li> <li>score: The main score of the model on the task.</li> </ul> <p>In addition, the DataFrame can have the following columns depending on the aggregation level:</p> <ul> <li>split: The split of the task. E.g. \"test\", \"train\", \"validation\".</li> <li>subset: The subset of the task. E.g. \"en\", \"fr-en\".</li> </ul> <p>Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_level</code> <code>Literal['subset', 'split', 'task']</code> <p>The aggregation to use. Can be one of: - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset. - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split. - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.</p> <code>'task'</code> <code>aggregation_fn</code> <code>Callable[[list[Score]], Any] | None</code> <p>The function to use for aggregation. If None, the mean will be used.</p> <code>None</code> <code>include_model_revision</code> <code>bool</code> <p>If True, the model revision will be included in the DataFrame. If False, it will be excluded.</p> <code>False</code> <code>format</code> <code>Literal['wide', 'long']</code> <p>The format of the DataFrame. Can be one of: - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells. - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.</p> <code>'wide'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the scores for all models and tasks.</p> Source code in <code>mteb/results/model_result.py</code> <pre><code>def to_dataframe(\n    self,\n    aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n    aggregation_fn: Callable[[list[Score]], Any] | None = None,\n    include_model_revision: bool = False,\n    format: Literal[\"wide\", \"long\"] = \"wide\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n    The DataFrame will have the following columns in addition to the metadata columns:\n\n    - model_name: The name of the model.\n    - task_name: The name of the task.\n    - score: The main score of the model on the task.\n\n    In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n    - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n    - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n    Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n    Args:\n        aggregation_level: The aggregation to use. Can be one of:\n            - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n            - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n            - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n        aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n        include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n        format: The format of the DataFrame. Can be one of:\n            - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n            - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n    Returns:\n        A DataFrame with the scores for all models and tasks.\n    \"\"\"\n    scores_data = self._get_score_for_table()\n\n    if not scores_data:\n        logger.warning(\"No scores data available. Returning empty DataFrame.\")\n        return pd.DataFrame()\n\n    # Create DataFrame\n    df = pd.DataFrame(scores_data)\n\n    _columns = [\"model_name\"]\n    if include_model_revision is False:\n        df = df.drop(columns=[\"model_revision\"])\n    else:\n        _columns.append(\"model_revision\")\n\n    return _aggregate_and_pivot(\n        df,\n        columns=_columns,\n        aggregation_level=aggregation_level,\n        format=format,\n        aggregation_fn=aggregation_fn,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults","title":"<code>mteb.results.BenchmarkResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data class to hold the benchmark results of a model.</p> <p>Attributes:</p> Name Type Description <code>model_results</code> <code>list[ModelResult]</code> <p>List of ModelResult objects.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>class BenchmarkResults(BaseModel):\n    \"\"\"Data class to hold the benchmark results of a model.\n\n    Attributes:\n        model_results: List of ModelResult objects.\n    \"\"\"\n\n    model_results: list[ModelResult]\n    model_config = (\n        ConfigDict(  # to free up the name model_results which is otherwise protected\n            protected_namespaces=(),\n        )\n    )\n\n    def __repr__(self) -&gt; str:\n        n_models = len(self.model_results)\n        return f\"BenchmarkResults(model_results=[...](#{n_models}))\"\n\n    def __hash__(self) -&gt; int:\n        return id(self)\n\n    def _filter_tasks(\n        self,\n        task_names: list[str] | None = None,\n        languages: list[str] | None = None,\n        domains: list[TaskDomain] | None = None,\n        task_types: list[TaskType] | None = None,  # type: ignore\n        modalities: list[Modalities] | None = None,\n        is_public: bool | None = None,\n    ) -&gt; Self:\n        # TODO: Same as filter_models\n        model_results = [\n            res._filter_tasks(\n                task_names=task_names,\n                languages=languages,\n                domains=domains,\n                task_types=task_types,\n                modalities=modalities,\n                is_public=is_public,\n            )\n            for res in self.model_results\n        ]\n        return type(self).model_construct(\n            model_results=[res for res in model_results if res.task_results]\n        )\n\n    def select_tasks(self, tasks: Sequence[AbsTask]) -&gt; Self:\n        \"\"\"Select tasks from the benchmark results.\n\n        Args:\n            tasks: List of tasks to select. Can be a list of AbsTask objects or task names.\n\n        Returns:\n            A new BenchmarkResults object with the selected tasks.\n        \"\"\"\n        new_model_results = [\n            model_res.select_tasks(tasks) for model_res in self.model_results\n        ]\n        return type(self).model_construct(model_results=new_model_results)\n\n    def select_models(\n        self,\n        names: list[str] | list[ModelMeta],\n        revisions: list[str | None] | None = None,\n    ) -&gt; Self:\n        \"\"\"Get models by name and revision.\n\n        Args:\n            names: List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.\n            revisions: List of model revisions to filter by. If None, all revisions are returned.\n\n        Returns:\n            A new BenchmarkResults object with the filtered models.\n        \"\"\"\n        models_res = []\n        _revisions = revisions if revisions is not None else [None] * len(names)\n\n        name_rev = {}\n\n        if len(names) != len(_revisions):\n            raise ValueError(\n                \"The length of names and revisions must be the same or revisions must be None.\"\n            )\n\n        for name, revision in zip(names, _revisions):\n            if isinstance(name, ModelMeta):\n                name_rev[name.name] = name.revision\n            else:\n                name_rev[name] = revision\n\n        for model_res in self.model_results:\n            model_name = model_res.model_name\n            revision = model_res.model_revision\n            if model_name in name_rev:\n                if name_rev[model_name] is None or revision == name_rev[model_name]:\n                    models_res.append(model_res)\n\n        return type(self).model_construct(model_results=models_res)\n\n    def _filter_models(\n        self,\n        model_names: Iterable[str] | None = None,\n        languages: Iterable[str] | None = None,\n        open_weights: bool | None = None,\n        frameworks: Iterable[str] | None = None,\n        n_parameters_range: tuple[int | None, int | None] = (None, None),\n        use_instructions: bool | None = None,\n        zero_shot_on: list[AbsTask] | None = None,\n    ) -&gt; Self:\n        # mostly a utility function for the leaderboard app.\n        # I would probably move the filtering of the models outside of this call. No need to call get_model_metas inside the filter.\n        # interface would then be the same as the get_models function\n\n        model_metas = get_model_metas(\n            model_names=model_names,\n            languages=languages,\n            open_weights=open_weights,\n            frameworks=frameworks,\n            n_parameters_range=n_parameters_range,\n            use_instructions=use_instructions,\n            zero_shot_on=zero_shot_on,\n        )\n        models = {meta.name for meta in model_metas}\n        # model_revision_pairs = {(meta.name, meta.revision) for meta in model_metas}\n        new_model_results = []\n        for model_res in self:\n            if model_res.model_name in models:\n                new_model_results.append(model_res)\n\n        return type(self).model_construct(model_results=new_model_results)\n\n    def join_revisions(self) -&gt; Self:\n        \"\"\"Join revisions of the same model.\n\n        In case of conflicts, the following rules are applied:\n        1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object.\n        2) If there is multiple revisions and some of them are None or na, they are filtered out.\n        3) If there is no main revision, we prefer the one run using the latest mteb version.\n\n        Returns:\n            A new BenchmarkResults object with the revisions joined.\n        \"\"\"\n\n        def parse_version(version_str: str) -&gt; Version | None:\n            try:\n                return Version(version_str)\n            except (InvalidVersion, TypeError):\n                return None\n\n        def keep_best(group: pd.DataFrame) -&gt; pd.DataFrame:\n            # Filtering out task_results where no scores are present\n            group = group[group[\"has_scores\"]]\n            is_main_revision = group[\"revision\"] == group[\"main_revision\"]\n            # If the main revision is present we select that\n            if is_main_revision.sum() &gt; 0:\n                return group[is_main_revision].head(n=1)\n            unique_revisions = group[\"revision\"].unique()\n\n            # ensure None/NA/\"external\" revisions is filtered out\n            group.loc[group[\"revision\"].isna(), \"revision\"] = \"no_revision_available\"\n            group.loc[group[\"revision\"] == \"external\", \"revision\"] = (\n                \"no_revision_available\"\n            )\n\n            # Filtering out no_revision_available if other revisions are present\n            if (len(unique_revisions) &gt; 1) and (\n                \"no_revision_available\" in unique_revisions\n            ):\n                group = group[group[\"revision\"] != \"no_revision_available\"]\n            # If there are any not-NA mteb versions, we select the latest one\n            if group[\"mteb_version\"].notna().any():\n                group = group.dropna(subset=[\"mteb_version\"])\n                group = group.sort_values(\"mteb_version\", ascending=False)\n                return group.head(n=1)\n            return group.head(n=1)\n\n        records = []\n        for model_result in self:\n            for task_result in model_result.task_results:\n                records.append(\n                    dict(\n                        model=model_result.model_name,\n                        revision=model_result.model_revision,\n                        task_name=task_result.task_name,\n                        mteb_version=task_result.mteb_version,\n                        task_result=task_result,\n                        has_scores=bool(task_result.scores),\n                    )\n                )\n        if not records:\n            return BenchmarkResults.model_construct(model_results=[])\n        task_df = pd.DataFrame.from_records(records)\n        model_to_main_revision = {\n            meta.name: meta.revision for meta in get_model_metas()\n        }\n        task_df[\"main_revision\"] = task_df[\"model\"].map(model_to_main_revision)  # type: ignore\n        task_df[\"mteb_version\"] = task_df[\"mteb_version\"].map(parse_version)  # type: ignore\n        task_df = (\n            task_df.groupby([\"model\", \"task_name\"])\n            .apply(keep_best)\n            .reset_index(drop=True)\n        )\n        model_results = []\n        for (model, model_revision), group in task_df.groupby([\"model\", \"revision\"]):\n            model_result = ModelResult.model_construct(\n                model_name=model,\n                model_revision=model_revision,\n                task_results=list(group[\"task_result\"]),\n            )\n            model_results.append(model_result)\n        return BenchmarkResults.model_construct(model_results=model_results)\n\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; list[dict]:\n        entries = []\n        if format == \"wide\":\n            for model_res in self:\n                try:\n                    model_scores = model_res._get_scores(\n                        splits=splits,\n                        languages=languages,\n                        scripts=scripts,\n                        getter=getter,\n                        aggregation=aggregation,\n                        format=\"wide\",\n                    )\n                    entries.append(\n                        {\n                            \"model\": model_res.model_name,\n                            \"revision\": model_res.model_revision,\n                            **model_scores,  # type: ignore\n                        }\n                    )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {model_res.model_name}({model_res.model_revision}), due to: {e}\"\n                    )\n        if format == \"long\":\n            for model_res in self:\n                try:\n                    entries.extend(\n                        model_res._get_scores(\n                            splits=splits,\n                            languages=languages,\n                            scripts=scripts,\n                            getter=getter,\n                            aggregation=aggregation,\n                            format=\"long\",\n                        )\n                    )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {model_res.model_name}({model_res.model_revision}), due to: {e}\"\n                    )\n        return entries\n\n    def to_dataframe(\n        self,\n        aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n        aggregation_fn: Callable[[list[Score]], Any] | None = None,\n        include_model_revision: bool = False,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n        The DataFrame will have the following columns in addition to the metadata columns:\n\n        - model_name: The name of the model.\n        - task_name: The name of the task.\n        - score: The main score of the model on the task.\n\n        In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n        - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n        - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n        Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n        Args:\n            aggregation_level: The aggregation to use. Can be one of:\n                - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n                - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n                - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n            aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n            include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n                If there are multiple revisions for the same model, they will be joined using the `join_revisions` method.\n            format: The format of the DataFrame. Can be one of:\n                - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n                - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n        Returns:\n            A DataFrame with the scores for all models and tasks.\n        \"\"\"\n        bench_results = self\n        if include_model_revision is False:\n            bench_results = bench_results.join_revisions()\n\n        scores_data = []\n        for model_result in bench_results:\n            scores_data.extend(model_result._get_score_for_table())\n\n        if not scores_data:\n            logger.warning(\"No scores data available. Returning empty DataFrame.\")\n            return pd.DataFrame()\n\n        # Create DataFrame\n        df = pd.DataFrame(scores_data)\n\n        _columns = [\"model_name\"]\n        if include_model_revision is False:\n            df = df.drop(columns=[\"model_revision\"])\n        else:\n            _columns.append(\"model_revision\")\n\n        # Aggregation\n        return _aggregate_and_pivot(\n            df,\n            columns=_columns,\n            aggregation_level=aggregation_level,\n            aggregation_fn=aggregation_fn,\n            format=format,\n        )\n\n    def __iter__(self) -&gt; Iterator[ModelResult]:\n        return iter(self.model_results)\n\n    def __getitem__(self, index: int) -&gt; ModelResult:\n        return self.model_results[index]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert BenchmarkResults to a dictionary.\"\"\"\n        return self.model_dump()\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; Self:\n        \"\"\"Create BenchmarkResults from a dictionary.\"\"\"\n        return cls.model_validate(data)\n\n    def to_disk(self, path: Path | str) -&gt; None:\n        \"\"\"Save the BenchmarkResults to a JSON file.\"\"\"\n        path = Path(path)\n        with path.open(\"w\") as out_file:\n            out_file.write(self.model_dump_json(indent=2))\n\n    @classmethod\n    def from_validated(cls, **data) -&gt; Self:\n        \"\"\"Create BenchmarkResults from validated data.\n\n        Args:\n            data: Dictionary containing the data.\n\n        Returns:\n            An instance of BenchmarkResults.\n        \"\"\"\n        model_results = []\n        for model_res in data[\"model_results\"]:\n            model_results.append(ModelResult.from_validated(**model_res))\n        return cls.model_construct(model_results=model_results)\n\n    @classmethod\n    def from_disk(cls, path: Path | str) -&gt; Self:\n        \"\"\"Load the BenchmarkResults from a JSON file.\n\n        Args:\n            path: Path to the JSON file.\n\n        Returns:\n            An instance of BenchmarkResults.\n        \"\"\"\n        path = Path(path)\n        with path.open() as in_file:\n            data = json.loads(in_file.read())\n        return cls.from_dict(data)\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get all languages in the benchmark results.\n\n        Returns:\n            A list of languages in ISO 639-1 format.\n        \"\"\"\n        langs = []\n        for model_res in self.model_results:\n            langs.extend(model_res.languages)\n        return list(set(langs))\n\n    @property\n    def domains(self) -&gt; list[str]:\n        \"\"\"Get all domains in the benchmark results.\n\n        Returns:\n            A list of domains in ISO 639-1 format.\n        \"\"\"\n        ds = []\n        for model_res in self.model_results:\n            ds.extend(model_res.domains)\n        return list(set(ds))\n\n    @property\n    def task_types(self) -&gt; list[str]:\n        \"\"\"Get all task types in the benchmark results.\n\n        Returns:\n            A list of task types.\n        \"\"\"\n        ts = []\n        for model_res in self.model_results:\n            ts.extend(model_res.task_types)\n        return list(set(ts))\n\n    @property\n    def task_names(self) -&gt; list[str]:\n        \"\"\"Get all task names in the benchmark results.\n\n        Returns:\n            A list of task names.\n        \"\"\"\n        names = []\n        for model_res in self.model_results:\n            names.extend(model_res.task_names)\n        return list(set(names))\n\n    @property\n    def modalities(self) -&gt; list[str]:\n        \"\"\"Get all modalities in the benchmark results.\n\n        Returns:\n            A list of modalities.\n        \"\"\"\n        mod = []\n        for model_res in self.model_results:\n            mod.extend(model_res.modalities)\n        return list(set(mod))\n\n    @property\n    def model_names(self) -&gt; list[str]:\n        \"\"\"Get all model names in the benchmark results.\n\n        Returns:\n            A list of model names.\n        \"\"\"\n        return [model_res.model_name for model_res in self.model_results]\n\n    @property\n    def model_revisions(self) -&gt; list[dict[str, str | None]]:\n        \"\"\"Get all model revisions in the benchmark results.\n\n        Returns:\n            A list of dictionaries with model names and revisions.\n        \"\"\"\n        return [\n            {\"model_name\": model_res.model_name, \"revision\": model_res.model_revision}\n            for model_res in self.model_results\n        ]\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get all domains in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of domains in ISO 639-1 format.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get all languages in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of languages in ISO 639-1 format.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Get all modalities in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of modalities.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.model_names","title":"<code>model_names</code>  <code>property</code>","text":"<p>Get all model names in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of model names.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.model_revisions","title":"<code>model_revisions</code>  <code>property</code>","text":"<p>Get all model revisions in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[dict[str, str | None]]</code> <p>A list of dictionaries with model names and revisions.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.task_names","title":"<code>task_names</code>  <code>property</code>","text":"<p>Get all task names in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.task_types","title":"<code>task_types</code>  <code>property</code>","text":"<p>Get all task types in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task types.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create BenchmarkResults from a dictionary.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Self:\n    \"\"\"Create BenchmarkResults from a dictionary.\"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load the BenchmarkResults from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the JSON file.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of BenchmarkResults.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path | str) -&gt; Self:\n    \"\"\"Load the BenchmarkResults from a JSON file.\n\n    Args:\n        path: Path to the JSON file.\n\n    Returns:\n        An instance of BenchmarkResults.\n    \"\"\"\n    path = Path(path)\n    with path.open() as in_file:\n        data = json.loads(in_file.read())\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create BenchmarkResults from validated data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>Dictionary containing the data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Self</code> <p>An instance of BenchmarkResults.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data) -&gt; Self:\n    \"\"\"Create BenchmarkResults from validated data.\n\n    Args:\n        data: Dictionary containing the data.\n\n    Returns:\n        An instance of BenchmarkResults.\n    \"\"\"\n    model_results = []\n    for model_res in data[\"model_results\"]:\n        model_results.append(ModelResult.from_validated(**model_res))\n    return cls.model_construct(model_results=model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.join_revisions","title":"<code>join_revisions()</code>","text":"<p>Join revisions of the same model.</p> <p>In case of conflicts, the following rules are applied: 1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object. 2) If there is multiple revisions and some of them are None or na, they are filtered out. 3) If there is no main revision, we prefer the one run using the latest mteb version.</p> <p>Returns:</p> Type Description <code>Self</code> <p>A new BenchmarkResults object with the revisions joined.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def join_revisions(self) -&gt; Self:\n    \"\"\"Join revisions of the same model.\n\n    In case of conflicts, the following rules are applied:\n    1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object.\n    2) If there is multiple revisions and some of them are None or na, they are filtered out.\n    3) If there is no main revision, we prefer the one run using the latest mteb version.\n\n    Returns:\n        A new BenchmarkResults object with the revisions joined.\n    \"\"\"\n\n    def parse_version(version_str: str) -&gt; Version | None:\n        try:\n            return Version(version_str)\n        except (InvalidVersion, TypeError):\n            return None\n\n    def keep_best(group: pd.DataFrame) -&gt; pd.DataFrame:\n        # Filtering out task_results where no scores are present\n        group = group[group[\"has_scores\"]]\n        is_main_revision = group[\"revision\"] == group[\"main_revision\"]\n        # If the main revision is present we select that\n        if is_main_revision.sum() &gt; 0:\n            return group[is_main_revision].head(n=1)\n        unique_revisions = group[\"revision\"].unique()\n\n        # ensure None/NA/\"external\" revisions is filtered out\n        group.loc[group[\"revision\"].isna(), \"revision\"] = \"no_revision_available\"\n        group.loc[group[\"revision\"] == \"external\", \"revision\"] = (\n            \"no_revision_available\"\n        )\n\n        # Filtering out no_revision_available if other revisions are present\n        if (len(unique_revisions) &gt; 1) and (\n            \"no_revision_available\" in unique_revisions\n        ):\n            group = group[group[\"revision\"] != \"no_revision_available\"]\n        # If there are any not-NA mteb versions, we select the latest one\n        if group[\"mteb_version\"].notna().any():\n            group = group.dropna(subset=[\"mteb_version\"])\n            group = group.sort_values(\"mteb_version\", ascending=False)\n            return group.head(n=1)\n        return group.head(n=1)\n\n    records = []\n    for model_result in self:\n        for task_result in model_result.task_results:\n            records.append(\n                dict(\n                    model=model_result.model_name,\n                    revision=model_result.model_revision,\n                    task_name=task_result.task_name,\n                    mteb_version=task_result.mteb_version,\n                    task_result=task_result,\n                    has_scores=bool(task_result.scores),\n                )\n            )\n    if not records:\n        return BenchmarkResults.model_construct(model_results=[])\n    task_df = pd.DataFrame.from_records(records)\n    model_to_main_revision = {\n        meta.name: meta.revision for meta in get_model_metas()\n    }\n    task_df[\"main_revision\"] = task_df[\"model\"].map(model_to_main_revision)  # type: ignore\n    task_df[\"mteb_version\"] = task_df[\"mteb_version\"].map(parse_version)  # type: ignore\n    task_df = (\n        task_df.groupby([\"model\", \"task_name\"])\n        .apply(keep_best)\n        .reset_index(drop=True)\n    )\n    model_results = []\n    for (model, model_revision), group in task_df.groupby([\"model\", \"revision\"]):\n        model_result = ModelResult.model_construct(\n            model_name=model,\n            model_revision=model_revision,\n            task_results=list(group[\"task_result\"]),\n        )\n        model_results.append(model_result)\n    return BenchmarkResults.model_construct(model_results=model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.select_models","title":"<code>select_models(names, revisions=None)</code>","text":"<p>Get models by name and revision.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str] | list[ModelMeta]</code> <p>List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.</p> required <code>revisions</code> <code>list[str | None] | None</code> <p>List of model revisions to filter by. If None, all revisions are returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>A new BenchmarkResults object with the filtered models.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def select_models(\n    self,\n    names: list[str] | list[ModelMeta],\n    revisions: list[str | None] | None = None,\n) -&gt; Self:\n    \"\"\"Get models by name and revision.\n\n    Args:\n        names: List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.\n        revisions: List of model revisions to filter by. If None, all revisions are returned.\n\n    Returns:\n        A new BenchmarkResults object with the filtered models.\n    \"\"\"\n    models_res = []\n    _revisions = revisions if revisions is not None else [None] * len(names)\n\n    name_rev = {}\n\n    if len(names) != len(_revisions):\n        raise ValueError(\n            \"The length of names and revisions must be the same or revisions must be None.\"\n        )\n\n    for name, revision in zip(names, _revisions):\n        if isinstance(name, ModelMeta):\n            name_rev[name.name] = name.revision\n        else:\n            name_rev[name] = revision\n\n    for model_res in self.model_results:\n        model_name = model_res.model_name\n        revision = model_res.model_revision\n        if model_name in name_rev:\n            if name_rev[model_name] is None or revision == name_rev[model_name]:\n                models_res.append(model_res)\n\n    return type(self).model_construct(model_results=models_res)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.select_tasks","title":"<code>select_tasks(tasks)</code>","text":"<p>Select tasks from the benchmark results.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[AbsTask]</code> <p>List of tasks to select. Can be a list of AbsTask objects or task names.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new BenchmarkResults object with the selected tasks.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def select_tasks(self, tasks: Sequence[AbsTask]) -&gt; Self:\n    \"\"\"Select tasks from the benchmark results.\n\n    Args:\n        tasks: List of tasks to select. Can be a list of AbsTask objects or task names.\n\n    Returns:\n        A new BenchmarkResults object with the selected tasks.\n    \"\"\"\n    new_model_results = [\n        model_res.select_tasks(tasks) for model_res in self.model_results\n    ]\n    return type(self).model_construct(model_results=new_model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_dataframe","title":"<code>to_dataframe(aggregation_level='task', aggregation_fn=None, include_model_revision=False, format='wide')</code>","text":"<p>Get a DataFrame with the scores for all models and tasks.</p> <p>The DataFrame will have the following columns in addition to the metadata columns:</p> <ul> <li>model_name: The name of the model.</li> <li>task_name: The name of the task.</li> <li>score: The main score of the model on the task.</li> </ul> <p>In addition, the DataFrame can have the following columns depending on the aggregation level:</p> <ul> <li>split: The split of the task. E.g. \"test\", \"train\", \"validation\".</li> <li>subset: The subset of the task. E.g. \"en\", \"fr-en\".</li> </ul> <p>Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_level</code> <code>Literal['subset', 'split', 'task']</code> <p>The aggregation to use. Can be one of: - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset. - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split. - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.</p> <code>'task'</code> <code>aggregation_fn</code> <code>Callable[[list[Score]], Any] | None</code> <p>The function to use for aggregation. If None, the mean will be used.</p> <code>None</code> <code>include_model_revision</code> <code>bool</code> <p>If True, the model revision will be included in the DataFrame. If False, it will be excluded. If there are multiple revisions for the same model, they will be joined using the <code>join_revisions</code> method.</p> <code>False</code> <code>format</code> <code>Literal['wide', 'long']</code> <p>The format of the DataFrame. Can be one of: - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells. - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.</p> <code>'wide'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the scores for all models and tasks.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_dataframe(\n    self,\n    aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n    aggregation_fn: Callable[[list[Score]], Any] | None = None,\n    include_model_revision: bool = False,\n    format: Literal[\"wide\", \"long\"] = \"wide\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n    The DataFrame will have the following columns in addition to the metadata columns:\n\n    - model_name: The name of the model.\n    - task_name: The name of the task.\n    - score: The main score of the model on the task.\n\n    In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n    - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n    - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n    Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n    Args:\n        aggregation_level: The aggregation to use. Can be one of:\n            - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n            - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n            - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n        aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n        include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n            If there are multiple revisions for the same model, they will be joined using the `join_revisions` method.\n        format: The format of the DataFrame. Can be one of:\n            - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n            - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n    Returns:\n        A DataFrame with the scores for all models and tasks.\n    \"\"\"\n    bench_results = self\n    if include_model_revision is False:\n        bench_results = bench_results.join_revisions()\n\n    scores_data = []\n    for model_result in bench_results:\n        scores_data.extend(model_result._get_score_for_table())\n\n    if not scores_data:\n        logger.warning(\"No scores data available. Returning empty DataFrame.\")\n        return pd.DataFrame()\n\n    # Create DataFrame\n    df = pd.DataFrame(scores_data)\n\n    _columns = [\"model_name\"]\n    if include_model_revision is False:\n        df = df.drop(columns=[\"model_revision\"])\n    else:\n        _columns.append(\"model_revision\")\n\n    # Aggregation\n    return _aggregate_and_pivot(\n        df,\n        columns=_columns,\n        aggregation_level=aggregation_level,\n        aggregation_fn=aggregation_fn,\n        format=format,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert BenchmarkResults to a dictionary.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert BenchmarkResults to a dictionary.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save the BenchmarkResults to a JSON file.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_disk(self, path: Path | str) -&gt; None:\n    \"\"\"Save the BenchmarkResults to a JSON file.\"\"\"\n    path = Path(path)\n    with path.open(\"w\") as out_file:\n        out_file.write(self.model_dump_json(indent=2))\n</code></pre>"},{"location":"api/task/","title":"Tasks","text":"<p>A task is an implementation of a dataset for evaluation. It could, for instance, be the MIRACL dataset consisting of queries, a corpus of documents ,and the correct documents to retrieve for a given query. In addition to the dataset, a task includes the specifications for how a model should be run on the dataset and how its output should be evaluated. Each task also comes with extensive metadata including the license, who annotated the data, etc.</p> An overview of the tasks within <code>mteb</code>"},{"location":"api/task/#utilities","title":"Utilities","text":""},{"location":"api/task/#mteb.get_tasks","title":"<code>mteb.get_tasks</code>","text":"<p>This script contains functions that are used to get an overview of the MTEB benchmark.</p>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks","title":"<code>MTEBTasks</code>","text":"<p>               Bases: <code>tuple[AbsTask]</code></p> <p>A tuple of tasks with additional methods to get an overview of the tasks.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>class MTEBTasks(tuple[AbsTask]):\n    \"\"\"A tuple of tasks with additional methods to get an overview of the tasks.\"\"\"\n\n    def __repr__(self) -&gt; str:\n        return \"MTEBTasks\" + super().__repr__()\n\n    @staticmethod\n    def _extract_property_from_task(task: AbsTask, property: str):\n        if hasattr(task.metadata, property):\n            return getattr(task.metadata, property)\n        elif hasattr(task, property):\n            return getattr(task, property)\n        else:\n            raise KeyError(\"Property neither in Task attribute or in task metadata.\")\n\n    @property\n    def languages(self) -&gt; set:\n        \"\"\"Return all languages from tasks\"\"\"\n        langs = set()\n        for task in self:\n            for lg in task.languages:\n                langs.add(lg)\n        return langs\n\n    def count_languages(self) -&gt; Counter:\n        \"\"\"Summarize count of all languages from tasks\n\n        Returns:\n            Counter with language as key and count as value.\n        \"\"\"\n        langs = []\n        for task in self:\n            langs.extend(task.languages)\n        return Counter(langs)\n\n    def to_markdown(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n        limit_n_entries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Generate markdown table with tasks summary\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n            limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n                there are more entries.\n\n        Returns:\n            string with a markdown table.\n        \"\"\"\n\n        def _limit_entries_in_cell_inner(cell: Any):\n            if isinstance(cell, list | set):\n                return self._limit_entries_in_cell(cell, limit_n_entries)\n            return cell\n\n        markdown_table = \"| Task\" + \"\".join([f\"| {p}  \" for p in properties]) + \"|\\n\"\n        _head_sep = \"| ---\" * (len(properties) + 1) + \" |\\n\"\n        markdown_table += _head_sep\n        for task in self:\n            markdown_table += f\"| {task.metadata.name} \"\n            markdown_table += \"\".join(\n                [\n                    f\"| {_limit_entries_in_cell_inner(self._extract_property_from_task(task, p))} \"\n                    for p in properties\n                ]\n            )\n            markdown_table += \" |\\n\"\n        return markdown_table\n\n    def to_dataframe(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate pandas DataFrame with tasks summary\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n\n        Returns:\n            pandas DataFrame.\n        \"\"\"\n        data = []\n        for task in self:\n            data.append(\n                {p: self._extract_property_from_task(task, p) for p in properties}\n            )\n        return pd.DataFrame(data)\n\n    @staticmethod\n    def _limit_entries_in_cell(\n        cell: list | set, limit_n_entries: int | None = 3\n    ) -&gt; str:\n        if limit_n_entries and len(cell) &gt; limit_n_entries:\n            ending = \"]\" if isinstance(cell, list) else \"}\"\n            cell = sorted(cell)\n            return str(cell[:limit_n_entries])[:-1] + \", ...\" + ending\n        else:\n            return str(cell)\n\n    def to_latex(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n        group_indices: Sequence[str] | None = (\"type\", \"name\"),\n        include_citation_in_name: bool = True,\n        limit_n_entries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Generate a LaTeX table of the tasks.\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n            group_indices: list of properties to group the table by.\n            include_citation_in_name: Whether to include the citation in the name.\n            limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n                there are more entries.\n\n        Returns:\n            string with a LaTeX table.\n        \"\"\"\n        if include_citation_in_name and \"name\" in properties:\n            properties += [\"intext_citation\"]\n            df = self.to_dataframe(properties)\n            df[\"name\"] = df[\"name\"] + \" \" + df[\"intext_citation\"]\n            df = df.drop(columns=[\"intext_citation\"])\n        else:\n            df = self.to_dataframe(properties)\n\n        if limit_n_entries and df.shape[0]:  # ensure that there are entries\n            for col in df.columns:\n                # check if content is a list or set\n                if isinstance(df[col].iloc[0], list | set):\n                    _col = []\n                    for val in df[col]:\n                        str_col = self._limit_entries_in_cell(val, limit_n_entries)\n\n                        # escape } and { characters\n                        str_col = str_col.replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n                        _col.append(str_col)\n                    df[col] = _col\n\n        if group_indices:\n            df = df.set_index(group_indices)\n\n        return df.to_latex()\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Return all languages from tasks</p>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.count_languages","title":"<code>count_languages()</code>","text":"<p>Summarize count of all languages from tasks</p> <p>Returns:</p> Type Description <code>Counter</code> <p>Counter with language as key and count as value.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def count_languages(self) -&gt; Counter:\n    \"\"\"Summarize count of all languages from tasks\n\n    Returns:\n        Counter with language as key and count as value.\n    \"\"\"\n    langs = []\n    for task in self:\n        langs.extend(task.languages)\n    return Counter(langs)\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_dataframe","title":"<code>to_dataframe(properties=_DEFAULT_PROPRIETIES)</code>","text":"<p>Generate pandas DataFrame with tasks summary</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_dataframe(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate pandas DataFrame with tasks summary\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n\n    Returns:\n        pandas DataFrame.\n    \"\"\"\n    data = []\n    for task in self:\n        data.append(\n            {p: self._extract_property_from_task(task, p) for p in properties}\n        )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_latex","title":"<code>to_latex(properties=_DEFAULT_PROPRIETIES, group_indices=('type', 'name'), include_citation_in_name=True, limit_n_entries=3)</code>","text":"<p>Generate a LaTeX table of the tasks.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <code>group_indices</code> <code>Sequence[str] | None</code> <p>list of properties to group the table by.</p> <code>('type', 'name')</code> <code>include_citation_in_name</code> <code>bool</code> <p>Whether to include the citation in the name.</p> <code>True</code> <code>limit_n_entries</code> <code>int | None</code> <p>Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that there are more entries.</p> <code>3</code> <p>Returns:</p> Type Description <code>str</code> <p>string with a LaTeX table.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_latex(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    group_indices: Sequence[str] | None = (\"type\", \"name\"),\n    include_citation_in_name: bool = True,\n    limit_n_entries: int | None = 3,\n) -&gt; str:\n    \"\"\"Generate a LaTeX table of the tasks.\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n        group_indices: list of properties to group the table by.\n        include_citation_in_name: Whether to include the citation in the name.\n        limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n            there are more entries.\n\n    Returns:\n        string with a LaTeX table.\n    \"\"\"\n    if include_citation_in_name and \"name\" in properties:\n        properties += [\"intext_citation\"]\n        df = self.to_dataframe(properties)\n        df[\"name\"] = df[\"name\"] + \" \" + df[\"intext_citation\"]\n        df = df.drop(columns=[\"intext_citation\"])\n    else:\n        df = self.to_dataframe(properties)\n\n    if limit_n_entries and df.shape[0]:  # ensure that there are entries\n        for col in df.columns:\n            # check if content is a list or set\n            if isinstance(df[col].iloc[0], list | set):\n                _col = []\n                for val in df[col]:\n                    str_col = self._limit_entries_in_cell(val, limit_n_entries)\n\n                    # escape } and { characters\n                    str_col = str_col.replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n                    _col.append(str_col)\n                df[col] = _col\n\n    if group_indices:\n        df = df.set_index(group_indices)\n\n    return df.to_latex()\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_markdown","title":"<code>to_markdown(properties=_DEFAULT_PROPRIETIES, limit_n_entries=3)</code>","text":"<p>Generate markdown table with tasks summary</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <code>limit_n_entries</code> <code>int | None</code> <p>Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that there are more entries.</p> <code>3</code> <p>Returns:</p> Type Description <code>str</code> <p>string with a markdown table.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_markdown(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    limit_n_entries: int | None = 3,\n) -&gt; str:\n    \"\"\"Generate markdown table with tasks summary\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n        limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n            there are more entries.\n\n    Returns:\n        string with a markdown table.\n    \"\"\"\n\n    def _limit_entries_in_cell_inner(cell: Any):\n        if isinstance(cell, list | set):\n            return self._limit_entries_in_cell(cell, limit_n_entries)\n        return cell\n\n    markdown_table = \"| Task\" + \"\".join([f\"| {p}  \" for p in properties]) + \"|\\n\"\n    _head_sep = \"| ---\" * (len(properties) + 1) + \" |\\n\"\n    markdown_table += _head_sep\n    for task in self:\n        markdown_table += f\"| {task.metadata.name} \"\n        markdown_table += \"\".join(\n            [\n                f\"| {_limit_entries_in_cell_inner(self._extract_property_from_task(task, p))} \"\n                for p in properties\n            ]\n        )\n        markdown_table += \" |\\n\"\n    return markdown_table\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.get_task","title":"<code>get_task(task_name, languages=None, script=None, eval_splits=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Get a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task to fetch.</p> required <code>languages</code> <code>list[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>list[str] | None</code> <p>A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts</p> <code>None</code> <code>eval_splits</code> <code>list[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>hf_subsets</code> <code>list[str] | None</code> <p>A list of Huggingface subsets to evaluate on.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>AbsTask</code> <p>An initialized task object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_task(\n    task_name: str,\n    languages: list[str] | None = None,\n    script: list[str] | None = None,\n    eval_splits: list[str] | None = None,\n    hf_subsets: list[str] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; AbsTask:\n    \"\"\"Get a task by name.\n\n    Args:\n        task_name: The name of the task to fetch.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        hf_subsets: A list of Huggingface subsets to evaluate on.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        An initialized task object.\n\n    Examples:\n        &gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n    \"\"\"\n    if task_name in _TASK_RENAMES:\n        _task_name = _TASK_RENAMES[task_name]\n        logger.warning(\n            f\"The task with the given name '{task_name}' has been renamed to '{_task_name}'. To prevent this warning use the new name.\"\n        )\n\n    if task_name not in _TASKS_REGISTRY:\n        close_matches = difflib.get_close_matches(task_name, _TASKS_REGISTRY.keys())\n        if close_matches:\n            suggestion = f\"KeyError: '{task_name}' not found. Did you mean: '{close_matches[0]}'?\"\n        else:\n            suggestion = (\n                f\"KeyError: '{task_name}' not found and no similar keys were found.\"\n            )\n        raise KeyError(suggestion)\n    task = _TASKS_REGISTRY[task_name]()\n    if eval_splits:\n        task.filter_eval_splits(eval_splits=eval_splits)\n    return task.filter_languages(\n        languages,\n        script,\n        hf_subsets=hf_subsets,\n        exclusive_language_filter=exclusive_language_filter,\n    )\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.get_tasks","title":"<code>get_tasks(tasks=None, *, languages=None, script=None, domains=None, task_types=None, categories=None, exclude_superseded=True, eval_splits=None, exclusive_language_filter=False, modalities=None, exclusive_modality_filter=False, exclude_aggregate=False, exclude_private=True)</code>","text":"<p>Get a list of tasks based on the specified filters.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>list[str] | None</code> <p>A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.</p> <code>None</code> <code>languages</code> <code>list[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>list[str] | None</code> <p>A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts that are not in the specified list.</p> <code>None</code> <code>domains</code> <code>list[TaskDomain] | None</code> <p>A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".</p> <code>None</code> <code>task_types</code> <code>list[TaskType] | None</code> <p>A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.</p> <code>None</code> <code>categories</code> <code>list[TaskCategory] | None</code> <p>A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.</p> <code>None</code> <code>exclude_superseded</code> <code>bool</code> <p>A boolean flag to exclude datasets which are superseded by another.</p> <code>True</code> <code>eval_splits</code> <code>list[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <code>modalities</code> <code>list[Modalities] | None</code> <p>A list of modalities to include. If None, all modalities are included.</p> <code>None</code> <code>exclusive_modality_filter</code> <code>bool</code> <p>If True, only keep tasks where all filter modalities are included in the task's modalities and ALL task modalities are in filter modalities (exact match). If False, keep tasks if any of the task's modalities match the filter modalities.</p> <code>False</code> <code>exclude_aggregate</code> <code>bool</code> <p>If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.</p> <code>False</code> <code>exclude_private</code> <code>bool</code> <p>If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.</p> <code>True</code> <p>Returns:</p> Type Description <code>MTEBTasks</code> <p>A list of all initialized tasks objects which pass all of the filters (AND operation).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Classification\"])\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Clustering\"], exclude_superseded=False)\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], tasks=[\"WikipediaRetrievalMultilingual\"], eval_splits=[\"test\"])\n&gt;&gt;&gt; get_tasks(tasks=[\"STS22\"], languages=[\"eng\"], exclusive_language_filter=True) # don't include multilingual subsets containing English\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_tasks(\n    tasks: list[str] | None = None,\n    *,\n    languages: list[str] | None = None,\n    script: list[str] | None = None,\n    domains: list[TaskDomain] | None = None,\n    task_types: list[TaskType] | None = None,  # type: ignore\n    categories: list[TaskCategory] | None = None,\n    exclude_superseded: bool = True,\n    eval_splits: list[str] | None = None,\n    exclusive_language_filter: bool = False,\n    modalities: list[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = True,\n) -&gt; MTEBTasks:\n    \"\"\"Get a list of tasks based on the specified filters.\n\n    Args:\n        tasks: A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts\n            that are not in the specified list.\n        domains: A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".\n        task_types: A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.\n        categories: A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.\n        exclude_superseded: A boolean flag to exclude datasets which are superseded by another.\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n        modalities: A list of modalities to include. If None, all modalities are included.\n        exclusive_modality_filter: If True, only keep tasks where _all_ filter modalities are included in the\n            task's modalities and ALL task modalities are in filter modalities (exact match).\n            If False, keep tasks if _any_ of the task's modalities match the filter modalities.\n        exclude_aggregate: If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.\n        exclude_private: If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.\n\n    Returns:\n        A list of all initialized tasks objects which pass all of the filters (AND operation).\n\n    Examples:\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Classification\"])\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Clustering\"], exclude_superseded=False)\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], tasks=[\"WikipediaRetrievalMultilingual\"], eval_splits=[\"test\"])\n        &gt;&gt;&gt; get_tasks(tasks=[\"STS22\"], languages=[\"eng\"], exclusive_language_filter=True) # don't include multilingual subsets containing English\n    \"\"\"\n    if tasks:\n        if domains or task_types or categories:\n            logger.warning(\n                \"When `tasks` is provided, other filters like domains, task_types, and categories are ignored. \"\n                + \"If you want to filter a list of tasks, please use `mteb.filter_tasks` instead.\"\n            )\n        _tasks = [\n            get_task(\n                task,\n                languages,\n                script,\n                eval_splits=eval_splits,\n                exclusive_language_filter=exclusive_language_filter,\n            )\n            for task in tasks\n        ]\n        return MTEBTasks(_tasks)\n\n    _tasks = filter_tasks(\n        TASK_LIST,\n        languages=languages,\n        script=script,\n        domains=domains,\n        task_types=task_types,\n        categories=categories,\n        modalities=modalities,\n        exclusive_modality_filter=exclusive_modality_filter,\n        exclude_superseded=exclude_superseded,\n        exclude_aggregate=exclude_aggregate,\n        exclude_private=exclude_private,\n    )\n    _tasks = [\n        cls().filter_languages(languages, script).filter_eval_splits(eval_splits)\n        for cls in _tasks\n    ]\n\n    return MTEBTasks(_tasks)\n</code></pre>"},{"location":"api/task/#mteb.get_task","title":"<code>mteb.get_task(task_name, languages=None, script=None, eval_splits=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Get a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task to fetch.</p> required <code>languages</code> <code>list[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>list[str] | None</code> <p>A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts</p> <code>None</code> <code>eval_splits</code> <code>list[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>hf_subsets</code> <code>list[str] | None</code> <p>A list of Huggingface subsets to evaluate on.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>AbsTask</code> <p>An initialized task object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_task(\n    task_name: str,\n    languages: list[str] | None = None,\n    script: list[str] | None = None,\n    eval_splits: list[str] | None = None,\n    hf_subsets: list[str] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; AbsTask:\n    \"\"\"Get a task by name.\n\n    Args:\n        task_name: The name of the task to fetch.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        hf_subsets: A list of Huggingface subsets to evaluate on.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        An initialized task object.\n\n    Examples:\n        &gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n    \"\"\"\n    if task_name in _TASK_RENAMES:\n        _task_name = _TASK_RENAMES[task_name]\n        logger.warning(\n            f\"The task with the given name '{task_name}' has been renamed to '{_task_name}'. To prevent this warning use the new name.\"\n        )\n\n    if task_name not in _TASKS_REGISTRY:\n        close_matches = difflib.get_close_matches(task_name, _TASKS_REGISTRY.keys())\n        if close_matches:\n            suggestion = f\"KeyError: '{task_name}' not found. Did you mean: '{close_matches[0]}'?\"\n        else:\n            suggestion = (\n                f\"KeyError: '{task_name}' not found and no similar keys were found.\"\n            )\n        raise KeyError(suggestion)\n    task = _TASKS_REGISTRY[task_name]()\n    if eval_splits:\n        task.filter_eval_splits(eval_splits=eval_splits)\n    return task.filter_languages(\n        languages,\n        script,\n        hf_subsets=hf_subsets,\n        exclusive_language_filter=exclusive_language_filter,\n    )\n</code></pre>"},{"location":"api/task/#mteb.filter_tasks","title":"<code>mteb.filter_tasks</code>","text":"<p>This script contains functions that are used to get an overview of the MTEB benchmark.</p>"},{"location":"api/task/#mteb.filter_tasks.filter_tasks","title":"<code>filter_tasks(tasks, *, languages=None, script=None, domains=None, task_types=None, categories=None, modalities=None, exclusive_modality_filter=False, exclude_superseded=False, exclude_aggregate=False, exclude_private=False)</code>","text":"<pre><code>filter_tasks(tasks: Sequence[AbsTask], *, languages: list[str] | None = None, script: list[str] | None = None, domains: list[TaskDomain] | None = None, task_types: list[TaskType] | None = None, categories: list[TaskCategory] | None = None, modalities: list[Modalities] | None = None, exclusive_modality_filter: bool = False, exclude_superseded: bool = False, exclude_aggregate: bool = False, exclude_private: bool = False) -&gt; list[AbsTask]\n</code></pre><pre><code>filter_tasks(tasks: Sequence[type[AbsTask]], *, languages: list[str] | None = None, script: list[str] | None = None, domains: list[TaskDomain] | None = None, task_types: list[TaskType] | None = None, categories: list[TaskCategory] | None = None, modalities: list[Modalities] | None = None, exclusive_modality_filter: bool = False, exclude_superseded: bool = False, exclude_aggregate: bool = False, exclude_private: bool = False) -&gt; list[type[AbsTask]]\n</code></pre> <p>Filter tasks based on the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[AbsTask] | Sequence[type[AbsTask]]</code> <p>A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.</p> required <code>languages</code> <code>list[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>list[str] | None</code> <p>A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts that are not in the specified list.</p> <code>None</code> <code>domains</code> <code>list[TaskDomain] | None</code> <p>A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".</p> <code>None</code> <code>task_types</code> <code>list[TaskType] | None</code> <p>A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.</p> <code>None</code> <code>categories</code> <code>list[TaskCategory] | None</code> <p>A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.</p> <code>None</code> <code>exclude_superseded</code> <code>bool</code> <p>A boolean flag to exclude datasets which are superseded by another.</p> <code>False</code> <code>eval_splits</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> required <code>modalities</code> <code>list[Modalities] | None</code> <p>A list of modalities to include. If None, all modalities are included.</p> <code>None</code> <code>exclusive_modality_filter</code> <code>bool</code> <p>If True, only keep tasks where all filter modalities are included in the task's modalities and ALL task modalities are in filter modalities (exact match). If False, keep tasks if any of the task's modalities match the filter modalities.</p> <code>False</code> <code>exclude_aggregate</code> <code>bool</code> <p>If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.</p> <code>False</code> <code>exclude_private</code> <code>bool</code> <p>If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[AbsTask] | list[type[AbsTask]]</code> <p>A list of tasks objects which pass all of the filters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text_classification_tasks = filter_tasks(my_tasks, task_types=[\"Classification\"], modalities=[\"text\"])\n&gt;&gt;&gt; medical_tasks = filter_tasks(my_tasks, domains=[\"Medical\"])\n&gt;&gt;&gt; english_tasks = filter_tasks(my_tasks, languages=[\"eng\"])\n&gt;&gt;&gt; latin_script_tasks = filter_tasks(my_tasks, script=[\"Latn\"])\n&gt;&gt;&gt; text_image_tasks = filter_tasks(my_tasks, modalities=[\"text\", \"image\"], exclusive_modality_filter=True)\n</code></pre> Source code in <code>mteb/filter_tasks.py</code> <pre><code>def filter_tasks(\n    tasks: Sequence[AbsTask] | Sequence[type[AbsTask]],\n    *,\n    languages: list[str] | None = None,\n    script: list[str] | None = None,\n    domains: list[TaskDomain] | None = None,\n    task_types: list[TaskType] | None = None,  # type: ignore\n    categories: list[TaskCategory] | None = None,\n    modalities: list[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_superseded: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = False,\n) -&gt; list[AbsTask] | list[type[AbsTask]]:\n    \"\"\"Filter tasks based on the specified criteria.\n\n    Args:\n        tasks: A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts\n            that are not in the specified list.\n        domains: A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".\n        task_types: A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.\n        categories: A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.\n        exclude_superseded: A boolean flag to exclude datasets which are superseded by another.\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        modalities: A list of modalities to include. If None, all modalities are included.\n        exclusive_modality_filter: If True, only keep tasks where _all_ filter modalities are included in the\n            task's modalities and ALL task modalities are in filter modalities (exact match).\n            If False, keep tasks if _any_ of the task's modalities match the filter modalities.\n        exclude_aggregate: If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.\n        exclude_private: If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.\n\n    Returns:\n        A list of tasks objects which pass all of the filters.\n\n    Examples:\n        &gt;&gt;&gt; text_classification_tasks = filter_tasks(my_tasks, task_types=[\"Classification\"], modalities=[\"text\"])\n        &gt;&gt;&gt; medical_tasks = filter_tasks(my_tasks, domains=[\"Medical\"])\n        &gt;&gt;&gt; english_tasks = filter_tasks(my_tasks, languages=[\"eng\"])\n        &gt;&gt;&gt; latin_script_tasks = filter_tasks(my_tasks, script=[\"Latn\"])\n        &gt;&gt;&gt; text_image_tasks = filter_tasks(my_tasks, modalities=[\"text\", \"image\"], exclusive_modality_filter=True)\n\n    \"\"\"\n    langs_to_keep = None\n    if languages:\n        [_check_is_valid_language(lang) for lang in languages]\n        langs_to_keep = set(languages)\n\n    script_to_keep = None\n    if script:\n        [_check_is_valid_script(s) for s in script]\n        script_to_keep = set(script)\n\n    domains_to_keep = None\n    if domains:\n        domains_to_keep = set(domains)\n\n    def _convert_to_set(domain: list[TaskDomain] | None) -&gt; set:\n        return set(domain) if domain is not None else set()\n\n    task_types_to_keep = None\n    if task_types:\n        task_types_to_keep = set(task_types)\n\n    categories_to_keep = None\n    if categories:\n        categories_to_keep = set(categories)\n\n    modalities_to_keep = None\n    if modalities:\n        modalities_to_keep = set(modalities)\n\n    _tasks = []\n    for t in tasks:\n        # For metadata and superseded_by, we can access them directly\n        metadata = t.metadata\n\n        if langs_to_keep and not langs_to_keep.intersection(metadata.languages):\n            continue\n        if script_to_keep and not script_to_keep.intersection(metadata.scripts):\n            continue\n        if domains_to_keep and not domains_to_keep.intersection(\n            _convert_to_set(metadata.domains)\n        ):\n            continue\n        if task_types_to_keep and metadata.type not in task_types_to_keep:\n            continue\n        if categories_to_keep and metadata.category not in categories_to_keep:\n            continue\n        if modalities_to_keep:\n            if exclusive_modality_filter:\n                if set(metadata.modalities) != modalities_to_keep:\n                    continue\n            else:\n                if not modalities_to_keep.intersection(metadata.modalities):\n                    continue\n        if exclude_superseded and metadata.superseded_by is not None:\n            continue\n        is_aggregate = (\n            issubclass(t, AbsTaskAggregate)\n            if isinstance(t, type)\n            else isinstance(t, AbsTaskAggregate)\n        )\n        if exclude_aggregate and is_aggregate:\n            continue\n        if exclude_private and not metadata.is_public:\n            continue\n\n        _tasks.append(t)\n\n    return _tasks\n</code></pre>"},{"location":"api/task/#metadata","title":"Metadata","text":"<p>Each task also contains extensive metadata. We annotate this using the following object, which allows us to use pydantic to validate the metadata.</p>"},{"location":"api/task/#mteb.TaskMetadata","title":"<code>mteb.TaskMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a task.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>MetadataDatasetDict</code> <p>All arguments to pass to datasets.load_dataset to load the dataset for the task.</p> <code>name</code> <code>str</code> <p>The name of the task.</p> <code>description</code> <code>str</code> <p>A description of the task.</p> <code>type</code> <code>TaskType</code> <p>The type of the task. This includes \"Classification\", \"Summarization\", \"STS\", \"Retrieval\", \"Reranking\", \"Clustering\", \"PairClassification\", \"BitextMining\". The type should match the abstask type.</p> <code>category</code> <code>TaskCategory | None</code> <p>The category of the task. E.g. includes \"t2t\" (text to text), \"t2i\" (text to image).</p> <code>reference</code> <code>StrURL | None</code> <p>A URL to the documentation of the task. E.g. a published paper.</p> <code>eval_splits</code> <code>list[str]</code> <p>The splits of the dataset used for evaluation.</p> <code>eval_langs</code> <code>Languages</code> <p>The languages of the dataset used for evaluation. Languages follows a ETF BCP 47 standard consisting of \"{language}-{script}\" tag (e.g. \"eng-Latn\"). Where language is specified as a list of ISO 639-3 language codes (e.g. \"eng\") followed by ISO 15924 script codes (e.g. \"Latn\"). Can be either a list of languages or a dictionary mapping huggingface subsets to lists of languages (e.g. if a the huggingface dataset contain different languages).</p> <code>main_score</code> <code>str</code> <p>The main score used for evaluation.</p> <code>date</code> <code>tuple[StrDate, StrDate] | None</code> <p>The date when the data was collected. Specified as a tuple of two dates.</p> <code>domains</code> <code>list[TaskDomain] | None</code> <p>The domains of the data. This includes \"Non-fiction\", \"Social\", \"Fiction\", \"News\", \"Academic\", \"Blog\", \"Encyclopaedic\", \"Government\", \"Legal\", \"Medical\", \"Poetry\", \"Religious\", \"Reviews\", \"Web\", \"Spoken\", \"Written\". A dataset can belong to multiple domains.</p> <code>task_subtypes</code> <code>list[TaskSubtype] | None</code> <p>The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". Feel free to update the list as needed.</p> <code>license</code> <code>Licenses | StrURL | None</code> <p>The license of the data specified as lowercase, e.g. \"cc-by-nc-4.0\". If the license is not specified, use \"not specified\". For custom licenses a URL is used.</p> <code>annotations_creators</code> <code>AnnotatorType | None</code> <p>The type of the annotators. Includes \"expert-annotated\" (annotated by experts), \"human-annotated\" (annotated e.g. by mturkers), \"derived\" (derived from structure in the data).</p> <code>dialect</code> <code>list[str] | None</code> <p>The dialect of the data, if applicable. Ideally specified as a BCP-47 language tag. Empty list if no dialects are present.</p> <code>sample_creation</code> <code>SampleCreationMethod | None</code> <p>The method of text creation. Includes \"found\", \"created\", \"machine-translated\", \"machine-translated and verified\", and \"machine-translated and localized\".</p> <code>prompt</code> <code>str | PromptDict | None</code> <p>The prompt used for the task. Can be a string or a dictionary containing the query and passage prompts.</p> <code>bibtex_citation</code> <code>str | None</code> <p>The BibTeX citation for the dataset. Should be an empty string if no citation is available.</p> <code>adapted_from</code> <code>Sequence[str] | None</code> <p>Datasets adapted (translated, sampled from, etc.) from other datasets.</p> <code>is_public</code> <code>bool</code> <p>Whether the dataset is publicly available. If False (closed/private), a HuggingFace token is required to run the datasets.</p> <code>superseded_by</code> <code>str | None</code> <p>Denotes the task that this task is superseded by. Used to issue warning to users of outdated datasets, while maintaining reproducibility of existing benchmarks.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>class TaskMetadata(BaseModel):\n    \"\"\"Metadata for a task.\n\n    Attributes:\n        dataset: All arguments to pass to [datasets.load_dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/loading_methods#datasets.load_dataset) to load the dataset for the task.\n        name: The name of the task.\n        description: A description of the task.\n        type: The type of the task. This includes \"Classification\", \"Summarization\", \"STS\", \"Retrieval\", \"Reranking\", \"Clustering\",\n            \"PairClassification\", \"BitextMining\". The type should match the abstask type.\n        category: The category of the task. E.g. includes \"t2t\" (text to text), \"t2i\" (text to image).\n        reference: A URL to the documentation of the task. E.g. a published paper.\n        eval_splits: The splits of the dataset used for evaluation.\n        eval_langs: The languages of the dataset used for evaluation. Languages follows a ETF BCP 47 standard consisting of \"{language}-{script}\"\n            tag (e.g. \"eng-Latn\"). Where language is specified as a list of ISO 639-3 language codes (e.g. \"eng\") followed by ISO 15924 script codes\n            (e.g. \"Latn\"). Can be either a list of languages or a dictionary mapping huggingface subsets to lists of languages (e.g. if a the\n            huggingface dataset contain different languages).\n        main_score: The main score used for evaluation.\n        date: The date when the data was collected. Specified as a tuple of two dates.\n        domains: The domains of the data. This includes \"Non-fiction\", \"Social\", \"Fiction\", \"News\", \"Academic\", \"Blog\", \"Encyclopaedic\",\n            \"Government\", \"Legal\", \"Medical\", \"Poetry\", \"Religious\", \"Reviews\", \"Web\", \"Spoken\", \"Written\". A dataset can belong to multiple domains.\n        task_subtypes: The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". Feel free to update the list as needed.\n        license: The license of the data specified as lowercase, e.g. \"cc-by-nc-4.0\". If the license is not specified, use \"not specified\". For custom licenses a URL is used.\n        annotations_creators: The type of the annotators. Includes \"expert-annotated\" (annotated by experts), \"human-annotated\" (annotated e.g. by\n            mturkers), \"derived\" (derived from structure in the data).\n        dialect: The dialect of the data, if applicable. Ideally specified as a BCP-47 language tag. Empty list if no dialects are present.\n        sample_creation: The method of text creation. Includes \"found\", \"created\", \"machine-translated\", \"machine-translated and verified\", and\n            \"machine-translated and localized\".\n        prompt: The prompt used for the task. Can be a string or a dictionary containing the query and passage prompts.\n        bibtex_citation: The BibTeX citation for the dataset. Should be an empty string if no citation is available.\n        adapted_from: Datasets adapted (translated, sampled from, etc.) from other datasets.\n        is_public: Whether the dataset is publicly available. If False (closed/private), a HuggingFace token is required to run the datasets.\n        superseded_by: Denotes the task that this task is superseded by. Used to issue warning to users of outdated datasets, while maintaining\n            reproducibility of existing benchmarks.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    dataset: MetadataDatasetDict\n\n    name: str\n    description: str\n    prompt: str | PromptDict | None = None\n    type: TaskType\n    modalities: list[Modalities] = [\"text\"]\n    category: TaskCategory | None = None\n    reference: StrURL | None = None\n\n    eval_splits: list[str] = [\"test\"]\n    eval_langs: Languages\n    main_score: str\n\n    date: tuple[StrDate, StrDate] | None = None\n    domains: list[TaskDomain] | None = None\n    task_subtypes: list[TaskSubtype] | None = None\n    license: Licenses | StrURL | None = None\n\n    annotations_creators: AnnotatorType | None = None\n    dialect: list[str] | None = None\n\n    sample_creation: SampleCreationMethod | None = None\n    bibtex_citation: str | None = None\n    adapted_from: Sequence[str] | None = None\n    is_public: bool = True\n    superseded_by: str | None = None\n\n    def _validate_metadata(self) -&gt; None:\n        self._eval_langs_are_valid(self.eval_langs)\n\n    @field_validator(\"prompt\")\n    @classmethod\n    def _check_prompt_is_valid(\n        cls, prompt: str | PromptDict | None\n    ) -&gt; str | PromptDict | None:\n        if isinstance(prompt, dict):\n            for key in prompt:\n                if key not in [e.value for e in PromptType]:\n                    raise ValueError(\n                        \"The prompt dictionary should only contain the keys 'query' and 'passage'.\"\n                    )\n        return prompt\n\n    def _eval_langs_are_valid(self, eval_langs: Languages) -&gt; None:\n        \"\"\"This method checks that the eval_langs are specified as a list of languages.\"\"\"\n        if isinstance(eval_langs, dict):\n            for langs in eval_langs.values():\n                for code in langs:\n                    check_language_code(code)\n        else:\n            for code in eval_langs:\n                check_language_code(code)\n\n    @property\n    def bcp47_codes(self) -&gt; list[ISOLanguageScript]:\n        \"\"\"Return the languages and script codes of the dataset formatting in accordance with the BCP-47 standard.\"\"\"\n        if isinstance(self.eval_langs, dict):\n            return sorted(\n                {lang for langs in self.eval_langs.values() for lang in langs}\n            )\n        return sorted(set(self.eval_langs))\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Return the languages of the dataset as iso639-3 codes.\"\"\"\n\n        def get_lang(lang: str) -&gt; str:\n            return lang.split(\"-\")[0]\n\n        if isinstance(self.eval_langs, dict):\n            return sorted(\n                {get_lang(lang) for langs in self.eval_langs.values() for lang in langs}\n            )\n        return sorted({get_lang(lang) for lang in self.eval_langs})\n\n    @property\n    def scripts(self) -&gt; set[str]:\n        \"\"\"Return the scripts of the dataset as iso15924 codes.\"\"\"\n\n        def get_script(lang: str) -&gt; str:\n            return lang.split(\"-\")[1]\n\n        if isinstance(self.eval_langs, dict):\n            return {\n                get_script(lang) for langs in self.eval_langs.values() for lang in langs\n            }\n        return {get_script(lang) for lang in self.eval_langs}\n\n    def is_filled(self) -&gt; bool:\n        \"\"\"Check if all the metadata fields are filled.\n\n        Returns:\n            True if all the metadata fields are filled, False otherwise.\n        \"\"\"\n        return all(\n            getattr(self, field_name) is not None\n            for field_name in self.model_fields\n            if field_name not in [\"prompt\", \"adapted_from\", \"superseded_by\"]\n        )\n\n    @property\n    def hf_subsets_to_langscripts(self) -&gt; dict[HFSubset, list[ISOLanguageScript]]:\n        \"\"\"Return a dictionary mapping huggingface subsets to languages.\"\"\"\n        if isinstance(self.eval_langs, dict):\n            return self.eval_langs\n        return {\"default\": self.eval_langs}  # type: ignore\n\n    @property\n    def intext_citation(self, include_cite: bool = True) -&gt; str:\n        \"\"\"Create an in-text citation for the dataset.\"\"\"\n        cite = \"\"\n        if self.bibtex_citation:\n            cite = f\"{self.bibtex_citation.split(',')[0].split('{')[1]}\"\n        if include_cite and cite:\n            # check for whitespace in the citation\n            if \" \" in cite:\n                logger.warning(\n                    \"Citation contains whitespace. Please ensure that the citation is correctly formatted.\"\n                )\n            return f\"\\\\cite{{{cite}}}\"\n        return cite\n\n    @property\n    def descriptive_stats(self) -&gt; dict[str, DescriptiveStatistics] | None:\n        \"\"\"Return the descriptive statistics for the dataset.\"\"\"\n        if self.descriptive_stat_path.exists():\n            with self.descriptive_stat_path.open(\"r\") as f:\n                return json.load(f)\n        return None\n\n    @property\n    def descriptive_stat_path(self) -&gt; Path:\n        \"\"\"Return the path to the descriptive statistics file.\"\"\"\n        descriptive_stat_base_dir = Path(__file__).parent.parent / \"descriptive_stats\"\n        if self.type in MIEB_TASK_TYPE:\n            descriptive_stat_base_dir = descriptive_stat_base_dir / \"Image\"\n        task_type_dir = descriptive_stat_base_dir / self.type\n        if not descriptive_stat_base_dir.exists():\n            descriptive_stat_base_dir.mkdir()\n        if not task_type_dir.exists():\n            task_type_dir.mkdir()\n        return task_type_dir / f\"{self.name}.json\"\n\n    @property\n    def n_samples(self) -&gt; dict[str, int] | None:\n        \"\"\"Returns the number of samples in the dataset\"\"\"\n        stats = self.descriptive_stats\n        if not stats:\n            return None\n\n        n_samples = {}\n        for subset, subset_value in stats.items():\n            if subset == \"hf_subset_descriptive_stats\":\n                continue\n            n_samples[subset] = subset_value[\"num_samples\"]  # type: ignore\n        return n_samples\n\n    @property\n    def hf_subsets(self) -&gt; list[str]:\n        \"\"\"Return the huggingface subsets.\"\"\"\n        return list(self.hf_subsets_to_langscripts.keys())\n\n    @property\n    def is_multilingual(self) -&gt; bool:\n        \"\"\"Check if the task is multilingual.\"\"\"\n        return isinstance(self.eval_langs, dict)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.model_dump_json())\n\n    @property\n    def revision(self) -&gt; str:\n        \"\"\"Return the dataset revision.\"\"\"\n        return self.dataset[\"revision\"]\n\n    def get_modalities(self, prompt_type: PromptType | None = None) -&gt; list[Modalities]:\n        \"\"\"Get the modalities for the task based category if prompt_type provided.\n\n        Args:\n            prompt_type: The prompt type to get the modalities for.\n\n        Returns:\n            A list of modalities for the task.\n\n        Raises:\n            ValueError: If the prompt type is not recognized.\n        \"\"\"\n        if prompt_type is None:\n            return self.modalities\n        query_modalities, doc_modalities = self.category.split(\"2\")\n        category_to_modality: dict[str, Modalities] = {\n            \"t\": \"text\",\n            \"i\": \"image\",\n        }\n        if prompt_type == PromptType.query:\n            return [\n                category_to_modality[query_modality]\n                for query_modality in query_modalities\n            ]\n        if prompt_type == PromptType.document:\n            return [\n                category_to_modality[doc_modality] for doc_modality in doc_modalities\n            ]\n        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n\n    def _create_dataset_card_data(\n        self,\n        existing_dataset_card_data: DatasetCardData | None = None,\n    ) -&gt; tuple[DatasetCardData, dict[str, Any]]:\n        \"\"\"Create a DatasetCardData object from the task metadata.\n\n        Args:\n            existing_dataset_card_data: The existing DatasetCardData object to update. If None, a new object will be created.\n\n        Returns:\n            A DatasetCardData object with the metadata for the task with kwargs to card\n        \"\"\"\n        if existing_dataset_card_data is None:\n            existing_dataset_card_data = DatasetCardData()\n\n        dataset_type = [\n            *self._hf_task_type(),\n            *self._hf_task_category(),\n            *self._hf_subtypes(),\n        ]\n        languages = self._hf_languages()\n\n        multilinguality = \"monolingual\" if len(languages) == 1 else \"multilingual\"\n        if self.sample_creation and \"translated\" in self.sample_creation:\n            multilinguality = \"translated\"\n\n        if self.adapted_from is not None:\n            source_datasets = [\n                task.metadata.dataset[\"path\"]\n                for task in mteb.get_tasks(self.adapted_from)\n            ]\n            source_datasets.append(self.dataset[\"path\"])\n        else:\n            source_datasets = None if not self.dataset else [self.dataset[\"path\"]]\n\n        tags = [\"mteb\"] + self.modalities\n\n        descriptive_stats = self.descriptive_stats\n        if descriptive_stats is not None:\n            for split, split_stat in descriptive_stats.items():\n                if len(split_stat.get(\"hf_subset_descriptive_stats\", {})) &gt; 10:\n                    split_stat.pop(\"hf_subset_descriptive_stats\", {})\n            descriptive_stats = json.dumps(descriptive_stats, indent=4)\n\n        dataset_card_data_params = existing_dataset_card_data.to_dict()\n        # override the existing values\n        dataset_card_data_params.update(\n            dict(\n                language=languages,\n                license=self._hf_license(),\n                annotations_creators=[self.annotations_creators]\n                if self.annotations_creators\n                else None,\n                multilinguality=multilinguality,\n                source_datasets=source_datasets,\n                task_categories=dataset_type,\n                task_ids=self._hf_subtypes(),\n                tags=tags,\n            )\n        )\n\n        return (\n            DatasetCardData(**dataset_card_data_params),\n            # parameters for readme generation\n            dict(\n                citation=self.bibtex_citation,\n                dataset_description=self.description,\n                dataset_reference=self.reference,\n                descriptive_stats=descriptive_stats,\n                dataset_task_name=self.name,\n                category=self.category,\n                domains=\", \".join(self.domains) if self.domains else None,\n            ),\n        )\n\n    def generate_dataset_card(\n        self,\n        existing_dataset_card: DatasetCard | None = None,\n    ) -&gt; DatasetCard:\n        \"\"\"Generates a dataset card for the task.\n\n        Args:\n            existing_dataset_card: The existing dataset card to update. If None, a new dataset card will be created.\n\n        Returns:\n            DatasetCard: The dataset card for the task.\n        \"\"\"\n        path = Path(__file__).parent / \"dataset_card_template.md\"\n        existing_dataset_card_data = (\n            existing_dataset_card.data if existing_dataset_card else None\n        )\n        dataset_card_data, template_kwargs = self._create_dataset_card_data(\n            existing_dataset_card_data\n        )\n        dataset_card = DatasetCard.from_template(\n            card_data=dataset_card_data,\n            template_path=str(path),\n            **template_kwargs,\n        )\n        return dataset_card\n\n    def push_dataset_card_to_hub(self, repo_name: str) -&gt; None:\n        \"\"\"Pushes the dataset card to the huggingface hub.\n\n        Args:\n            repo_name: The name of the repository to push the dataset card to.\n        \"\"\"\n        dataset_card = None\n        if repo_exists(\n            repo_name, repo_type=constants.REPO_TYPE_DATASET\n        ) and file_exists(\n            repo_name, constants.REPOCARD_NAME, repo_type=constants.REPO_TYPE_DATASET\n        ):\n            dataset_card = DatasetCard.load(repo_name)\n        dataset_card = self.generate_dataset_card(dataset_card)\n        dataset_card.push_to_hub(repo_name, commit_message=\"Add dataset card\")\n\n    def _hf_subtypes(self) -&gt; list[str]:\n        # to get full list of available task_ids execute\n        # requests.post(\"https://huggingface.co/api/validate-yaml\", json={\n        #   \"content\": \"---\\ntask_ids: 'test'\\n---\",\n        #   \"repoType\": \"dataset\"\n        # })\n        mteb_to_hf_subtype = {\n            \"Article retrieval\": [\"document-retrieval\"],\n            \"Conversational retrieval\": [\"conversational\", \"utterance-retrieval\"],\n            \"Dialect pairing\": [],\n            \"Dialog Systems\": [\"dialogue-modeling\", \"dialogue-generation\"],\n            \"Discourse coherence\": [],\n            \"Duplicate Image Retrieval\": [],\n            \"Language identification\": [\"language-identification\"],\n            \"Linguistic acceptability\": [\"acceptability-classification\"],\n            \"Political classification\": [],\n            \"Question answering\": [\n                \"multiple-choice-qa\",\n                \"question-answering\",\n            ],\n            \"Sentiment/Hate speech\": [\n                \"sentiment-analysis\",\n                \"sentiment-scoring\",\n                \"sentiment-classification\",\n                \"hate-speech-detection\",\n            ],\n            \"Thematic clustering\": [],\n            \"Scientific Reranking\": [],\n            \"Claim verification\": [\"fact-checking\", \"fact-checking-retrieval\"],\n            \"Topic classification\": [\"topic-classification\"],\n            \"Code retrieval\": [],\n            \"False Friends\": [],\n            \"Cross-Lingual Semantic Discrimination\": [],\n            \"Textual Entailment\": [\"natural-language-inference\"],\n            \"Counterfactual Detection\": [],\n            \"Emotion classification\": [],\n            \"Reasoning as Retrieval\": [],\n            \"Rendered Texts Understanding\": [],\n            \"Image Text Retrieval\": [],\n            \"Object recognition\": [],\n            \"Scene recognition\": [],\n            \"Caption Pairing\": [\"image-captioning\"],\n            \"Emotion recognition\": [],\n            \"Textures recognition\": [],\n            \"Activity recognition\": [],\n            \"Tumor detection\": [],\n            \"Duplicate Detection\": [],\n            \"Rendered semantic textual similarity\": [\n                \"semantic-similarity-scoring\",\n                \"rendered semantic textual similarity\",\n            ],\n            \"Intent classification\": [\n                \"intent-classification\",\n            ],\n        }\n        subtypes = []\n        if self.task_subtypes:\n            for subtype in self.task_subtypes:\n                subtypes.extend(mteb_to_hf_subtype.get(subtype, []))\n        return subtypes\n\n    def _hf_task_type(self) -&gt; list[str]:\n        # to get full list of task_types execute:\n        # requests.post(\"https://huggingface.co/api/validate-yaml\", json={\n        #     \"content\": \"---\\ntask_categories: ['test']\\n---\", \"repoType\": \"dataset\"\n        # }).json()\n        # or look at https://huggingface.co/tasks\n        mteb_task_type_to_datasets = {\n            # Text\n            \"BitextMining\": [\"translation\"],\n            \"Classification\": [\"text-classification\"],\n            \"MultilabelClassification\": [\"text-classification\"],\n            \"Clustering\": [\"text-classification\"],\n            \"PairClassification\": [\"text-classification\"],\n            \"Reranking\": [\"text-ranking\"],\n            \"Retrieval\": [\"text-retrieval\"],\n            \"STS\": [\"sentence-similarity\"],\n            \"Summarization\": [\"summarization\"],\n            \"InstructionRetrieval\": [\"text-retrieval\"],\n            \"InstructionReranking\": [\"text-ranking\"],\n            # Image\n            \"Any2AnyMultiChoice\": [\"visual-question-answering\"],\n            \"Any2AnyRetrieval\": [\"visual-document-retrieval\"],\n            \"Any2AnyMultilingualRetrieval\": [\"visual-document-retrieval\"],\n            \"VisionCentricQA\": [\"visual-question-answering\"],\n            \"ImageClustering\": [\"image-clustering\"],\n            \"ImageClassification\": [\"image-classification\"],\n            \"ImageMultilabelClassification\": [\"image-classification\"],\n            \"DocumentUnderstanding\": [\"visual-document-retrieval\"],\n            \"VisualSTS(eng)\": [\"other\"],\n            \"VisualSTS(multi)\": [\"other\"],\n            \"ZeroShotClassification\": [\"zero-shot-classification\"],\n            \"Compositionality\": [\"other\"],\n        }\n        if self.type == \"ZeroShotClassification\":\n            if self.modalities == [\"image\"]:\n                return [\"zero-shot-image-classification\"]\n            return [\"zero-shot-classification\"]\n\n        return mteb_task_type_to_datasets[self.type]\n\n    def _hf_task_category(self) -&gt; list[str]:\n        dataset_type = []\n        if self.category in [\"i2i\", \"it2i\", \"i2it\", \"it2it\"]:\n            dataset_type.append(\"image-to-image\")\n        if self.category in [\"i2t\", \"t2i\", \"it2t\", \"it2i\", \"t2it\", \"i2it\", \"it2it\"]:\n            dataset_type.extend([\"image-to-text\", \"text-to-image\"])\n        if self.category in [\"it2t\", \"it2i\", \"t2it\", \"i2it\", \"it2it\"]:\n            dataset_type.extend([\"image-text-to-text\"])\n        return dataset_type\n\n    def _hf_languages(self) -&gt; list[str]:\n        languages: list[str] = []\n        if self.is_multilingual:\n            for val in list(self.eval_langs.values()):\n                languages.extend(val)\n        else:\n            languages = self.eval_langs\n        # value \"python\" is not valid. It must be an ISO 639-1, 639-2 or 639-3 code (two/three letters),\n        # or a special value like \"code\", \"multilingual\".\n        readme_langs = []\n        for lang in languages:\n            lang_name, family = lang.split(\"-\")\n            if family == \"Code\":\n                readme_langs.append(\"code\")\n            else:\n                readme_langs.append(lang_name)\n        return sorted(set(readme_langs))\n\n    def _hf_license(self) -&gt; str:\n        dataset_license = self.license\n        if dataset_license:\n            license_mapping = {\n                \"not specified\": \"unknown\",\n                \"msr-la-nc\": \"other\",\n                \"cc-by-nd-2.1-jp\": \"other\",\n            }\n            dataset_license = license_mapping.get(\n                dataset_license,\n                \"other\" if dataset_license.startswith(\"http\") else dataset_license,\n            )\n        return dataset_license\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.bcp47_codes","title":"<code>bcp47_codes</code>  <code>property</code>","text":"<p>Return the languages and script codes of the dataset formatting in accordance with the BCP-47 standard.</p>"},{"location":"api/task/#mteb.TaskMetadata.descriptive_stat_path","title":"<code>descriptive_stat_path</code>  <code>property</code>","text":"<p>Return the path to the descriptive statistics file.</p>"},{"location":"api/task/#mteb.TaskMetadata.descriptive_stats","title":"<code>descriptive_stats</code>  <code>property</code>","text":"<p>Return the descriptive statistics for the dataset.</p>"},{"location":"api/task/#mteb.TaskMetadata.hf_subsets","title":"<code>hf_subsets</code>  <code>property</code>","text":"<p>Return the huggingface subsets.</p>"},{"location":"api/task/#mteb.TaskMetadata.hf_subsets_to_langscripts","title":"<code>hf_subsets_to_langscripts</code>  <code>property</code>","text":"<p>Return a dictionary mapping huggingface subsets to languages.</p>"},{"location":"api/task/#mteb.TaskMetadata.intext_citation","title":"<code>intext_citation</code>  <code>property</code>","text":"<p>Create an in-text citation for the dataset.</p>"},{"location":"api/task/#mteb.TaskMetadata.is_multilingual","title":"<code>is_multilingual</code>  <code>property</code>","text":"<p>Check if the task is multilingual.</p>"},{"location":"api/task/#mteb.TaskMetadata.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Return the languages of the dataset as iso639-3 codes.</p>"},{"location":"api/task/#mteb.TaskMetadata.n_samples","title":"<code>n_samples</code>  <code>property</code>","text":"<p>Returns the number of samples in the dataset</p>"},{"location":"api/task/#mteb.TaskMetadata.revision","title":"<code>revision</code>  <code>property</code>","text":"<p>Return the dataset revision.</p>"},{"location":"api/task/#mteb.TaskMetadata.scripts","title":"<code>scripts</code>  <code>property</code>","text":"<p>Return the scripts of the dataset as iso15924 codes.</p>"},{"location":"api/task/#mteb.TaskMetadata.generate_dataset_card","title":"<code>generate_dataset_card(existing_dataset_card=None)</code>","text":"<p>Generates a dataset card for the task.</p> <p>Parameters:</p> Name Type Description Default <code>existing_dataset_card</code> <code>DatasetCard | None</code> <p>The existing dataset card to update. If None, a new dataset card will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetCard</code> <code>DatasetCard</code> <p>The dataset card for the task.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def generate_dataset_card(\n    self,\n    existing_dataset_card: DatasetCard | None = None,\n) -&gt; DatasetCard:\n    \"\"\"Generates a dataset card for the task.\n\n    Args:\n        existing_dataset_card: The existing dataset card to update. If None, a new dataset card will be created.\n\n    Returns:\n        DatasetCard: The dataset card for the task.\n    \"\"\"\n    path = Path(__file__).parent / \"dataset_card_template.md\"\n    existing_dataset_card_data = (\n        existing_dataset_card.data if existing_dataset_card else None\n    )\n    dataset_card_data, template_kwargs = self._create_dataset_card_data(\n        existing_dataset_card_data\n    )\n    dataset_card = DatasetCard.from_template(\n        card_data=dataset_card_data,\n        template_path=str(path),\n        **template_kwargs,\n    )\n    return dataset_card\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.get_modalities","title":"<code>get_modalities(prompt_type=None)</code>","text":"<p>Get the modalities for the task based category if prompt_type provided.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_type</code> <code>PromptType | None</code> <p>The prompt type to get the modalities for.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Modalities]</code> <p>A list of modalities for the task.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prompt type is not recognized.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def get_modalities(self, prompt_type: PromptType | None = None) -&gt; list[Modalities]:\n    \"\"\"Get the modalities for the task based category if prompt_type provided.\n\n    Args:\n        prompt_type: The prompt type to get the modalities for.\n\n    Returns:\n        A list of modalities for the task.\n\n    Raises:\n        ValueError: If the prompt type is not recognized.\n    \"\"\"\n    if prompt_type is None:\n        return self.modalities\n    query_modalities, doc_modalities = self.category.split(\"2\")\n    category_to_modality: dict[str, Modalities] = {\n        \"t\": \"text\",\n        \"i\": \"image\",\n    }\n    if prompt_type == PromptType.query:\n        return [\n            category_to_modality[query_modality]\n            for query_modality in query_modalities\n        ]\n    if prompt_type == PromptType.document:\n        return [\n            category_to_modality[doc_modality] for doc_modality in doc_modalities\n        ]\n    raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.is_filled","title":"<code>is_filled()</code>","text":"<p>Check if all the metadata fields are filled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if all the metadata fields are filled, False otherwise.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def is_filled(self) -&gt; bool:\n    \"\"\"Check if all the metadata fields are filled.\n\n    Returns:\n        True if all the metadata fields are filled, False otherwise.\n    \"\"\"\n    return all(\n        getattr(self, field_name) is not None\n        for field_name in self.model_fields\n        if field_name not in [\"prompt\", \"adapted_from\", \"superseded_by\"]\n    )\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.push_dataset_card_to_hub","title":"<code>push_dataset_card_to_hub(repo_name)</code>","text":"<p>Pushes the dataset card to the huggingface hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_name</code> <code>str</code> <p>The name of the repository to push the dataset card to.</p> required Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def push_dataset_card_to_hub(self, repo_name: str) -&gt; None:\n    \"\"\"Pushes the dataset card to the huggingface hub.\n\n    Args:\n        repo_name: The name of the repository to push the dataset card to.\n    \"\"\"\n    dataset_card = None\n    if repo_exists(\n        repo_name, repo_type=constants.REPO_TYPE_DATASET\n    ) and file_exists(\n        repo_name, constants.REPOCARD_NAME, repo_type=constants.REPO_TYPE_DATASET\n    ):\n        dataset_card = DatasetCard.load(repo_name)\n    dataset_card = self.generate_dataset_card(dataset_card)\n    dataset_card.push_to_hub(repo_name, commit_message=\"Add dataset card\")\n</code></pre>"},{"location":"api/task/#metadata-types","title":"Metadata Types","text":""},{"location":"api/task/#mteb.abstasks.task_metadata.AnnotatorType","title":"<code>mteb.abstasks.task_metadata.AnnotatorType = Literal['expert-annotated', 'human-annotated', 'derived', 'LM-generated', 'LM-generated and reviewed']</code>  <code>module-attribute</code>","text":"<p>The type of the annotators. Is often important for understanding the quality of a dataset.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.SampleCreationMethod","title":"<code>mteb.abstasks.task_metadata.SampleCreationMethod = Literal['found', 'created', 'human-translated and localized', 'human-translated', 'machine-translated', 'machine-translated and verified', 'machine-translated and localized', 'LM-generated and verified', 'machine-translated and LM verified', 'rendered', 'multiple']</code>  <code>module-attribute</code>","text":"<p>How the text was created. It can be an important factor for understanding the quality of a dataset. E.g. used to filter out machine-translated datasets.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskCategory","title":"<code>mteb.abstasks.task_metadata.TaskCategory = Literal['t2t', 't2c', 'i2i', 'i2c', 'i2t', 't2i', 'it2t', 'it2i', 'i2it', 't2it', 'it2it']</code>  <code>module-attribute</code>","text":"<p>The category of the task.</p> <ol> <li>t2t: text to text</li> <li>t2c: text to category</li> <li>i2i: image to image</li> <li>i2c: image to category</li> <li>i2t: image to text</li> <li>t2i: text to image</li> <li>it2t: image+text to text</li> <li>it2i: image+text to image</li> <li>i2it: image to image+text</li> <li>t2it: text to image+text</li> <li>it2it: image+text to image+text</li> </ol>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskDomain","title":"<code>mteb.abstasks.task_metadata.TaskDomain = Literal['Academic', 'Blog', 'Constructed', 'Encyclopaedic', 'Engineering', 'Fiction', 'Government', 'Legal', 'Medical', 'News', 'Non-fiction', 'Poetry', 'Religious', 'Reviews', 'Scene', 'Social', 'Spoken', 'Subtitles', 'Web', 'Written', 'Programming', 'Chemistry', 'Financial', 'Entertainment']</code>  <code>module-attribute</code>","text":"<p>The domains follow the categories used in the Universal Dependencies project, though  we updated them where deemed appropriate. These do not have to be mutually exclusive.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskType","title":"<code>mteb.abstasks.task_metadata.TaskType = Literal[_TASK_TYPE]</code>  <code>module-attribute</code>","text":"<p>The type of the task. E.g. includes \"Classification\", \"Retrieval\" and \"Clustering\".</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskSubtype","title":"<code>mteb.abstasks.task_metadata.TaskSubtype = Literal['Article retrieval', 'Patent retrieval', 'Conversational retrieval', 'Dialect pairing', 'Dialog Systems', 'Discourse coherence', 'Duplicate Image Retrieval', 'Language identification', 'Linguistic acceptability', 'Political classification', 'Question answering', 'Sentiment/Hate speech', 'Thematic clustering', 'Scientific Reranking', 'Claim verification', 'Topic classification', 'Code retrieval', 'False Friends', 'Cross-Lingual Semantic Discrimination', 'Textual Entailment', 'Counterfactual Detection', 'Emotion classification', 'Reasoning as Retrieval', 'Rendered Texts Understanding', 'Image Text Retrieval', 'Object recognition', 'Scene recognition', 'Caption Pairing', 'Emotion recognition', 'Textures recognition', 'Activity recognition', 'Tumor detection', 'Duplicate Detection', 'Rendered semantic textual similarity', 'Intent classification']</code>  <code>module-attribute</code>","text":"<p>The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". This list can be updated as needed.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.PromptDict","title":"<code>mteb.abstasks.task_metadata.PromptDict = TypedDict('PromptDict', {(prompt_type.value): strfor prompt_type in PromptType}, total=False)</code>  <code>module-attribute</code>","text":"<p>A dictionary containing the prompt used for the task.</p> <p>Attributes:</p> Name Type Description <code>query</code> <p>The prompt used for the queries in the task.</p> <code>document</code> <p>The prompt used for the passages in the task.</p>"},{"location":"api/task/#the-task-object","title":"The Task Object","text":"<p>All tasks in <code>mteb</code> inherits from the following abstract class.</p>"},{"location":"api/task/#mteb.AbsTask","title":"<code>mteb.AbsTask</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class for the tasks</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>TaskMetadata</code> <p>The metadata describing the task</p> <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>The dataset represented as a dictionary on the form {\"hf subset\": {\"split\": Dataset}} where \"split\" is the dataset split (e.g. \"test\") and Dataset is a datasets.Dataset object. \"hf subset\" is the data subset on Huggingface typically used to denote the language e.g. datasets.load_dataset(\"data\", \"en\"). If the dataset does not have a subset this is simply \"default\".</p> <code>seed</code> <p>The random seed used for reproducibility.</p> <code>hf_subsets</code> <code>list[HFSubset]</code> <p>The list of Huggingface subsets to use.</p> <code>data_loaded</code> <code>bool</code> <p>Denotes if the dataset is loaded or not. This is used to avoid loading the dataset multiple times.</p> <code>abstask_prompt</code> <code>str | None</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>fast_loading</code> <code>bool</code> <p>Deprecated. Denotes if the task should be loaded using the fast loading method. This is only possible if the dataset have a \"default\" config. We don't recommend to use this method, and suggest to use different subsets for loading datasets. This was used only for historical reasons and will be removed in the future.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>class AbsTask(ABC):\n    \"\"\"The abstract class for the tasks\n\n    Attributes:\n        metadata: The metadata describing the task\n        dataset: The dataset represented as a dictionary on the form {\"hf subset\": {\"split\": Dataset}} where \"split\" is the dataset split (e.g. \"test\")\n            and Dataset is a datasets.Dataset object. \"hf subset\" is the data subset on Huggingface typically used to denote the language e.g.\n            datasets.load_dataset(\"data\", \"en\"). If the dataset does not have a subset this is simply \"default\".\n        seed: The random seed used for reproducibility.\n        hf_subsets: The list of Huggingface subsets to use.\n        data_loaded: Denotes if the dataset is loaded or not. This is used to avoid loading the dataset multiple times.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        fast_loading: **Deprecated**. Denotes if the task should be loaded using the fast loading method.\n            This is only possible if the dataset have a \"default\" config. We don't recommend to use this method, and suggest to use different subsets for loading datasets.\n            This was used only for historical reasons and will be removed in the future.\n    \"\"\"\n\n    metadata: TaskMetadata\n    abstask_prompt: str | None = None\n    _eval_splits: list[str] | None = None\n    dataset: dict[HFSubset, DatasetDict] | None = None\n    data_loaded: bool = False\n    hf_subsets: list[HFSubset]\n    fast_loading: bool = False\n\n    _support_cross_encoder: bool = False\n    _support_search: bool = False\n\n    def __init__(self, seed: int = 42, **kwargs: Any) -&gt; None:\n        \"\"\"The init function. This is called primarily to set the seed.\n\n        Args:\n            seed: An integer seed.\n            kwargs: arguments passed to subclasses.\n        \"\"\"\n        self.seed = seed\n        self.rng_state, self.np_rng = _set_seed(seed)\n        self.hf_subsets = self.metadata.hf_subsets\n\n    def check_if_dataset_is_superseded(self) -&gt; None:\n        \"\"\"Check if the dataset is superseded by a newer version.\"\"\"\n        if self.superseded_by:\n            logger.warning(\n                f\"Dataset '{self.metadata.name}' is superseded by '{self.superseded_by}', you might consider using the newer version of the dataset.\"\n            )\n\n    def dataset_transform(self):\n        \"\"\"A transform operations applied to the dataset after loading.\n\n        This method is useful when the dataset from Huggingface is not in an `mteb` compatible format.\n        Override this method if your dataset requires additional transformation.\n        \"\"\"\n        pass\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Evaluates an MTEB compatible model on the task.\n\n        Args:\n            model: MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings\n            split: Which split (e.g. *\"test\"*) to be used.\n            subsets_to_run: List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.\n            encode_kwargs: Additional keyword arguments that are passed to the model's `encode` method.\n            prediction_folder: Folder to save model predictions\n            kwargs: Additional keyword arguments that are passed to the _evaluate_subset method.\n\n        Returns:\n            A dictionary with the scores for each subset.\n\n        Raises:\n            TypeError: If the model is a CrossEncoder and the task does not support CrossEncoders.\n            TypeError: If the model is a SearchProtocol and the task does not support Search.\n        \"\"\"\n        if isinstance(model, CrossEncoderProtocol) and not self._support_cross_encoder:\n            raise TypeError(\n                f\"Model {model} is a CrossEncoder, but this task {self.metadata.name} does not support CrossEncoders. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        # encoders might implement search protocols\n        if (\n            isinstance(model, SearchProtocol)\n            and not isinstance(model, EncoderProtocol)\n            and not self._support_search\n        ):\n            raise TypeError(\n                f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        if not self.data_loaded:\n            self.load_data()\n\n        self.dataset = cast(dict[HFSubset, DatasetDict], self.dataset)\n\n        scores = {}\n        if self.hf_subsets is None:\n            hf_subsets = list(self.dataset.keys())\n        else:\n            hf_subsets = copy(self.hf_subsets)\n\n        if subsets_to_run is not None:  # allow overwrites of pre-filtering\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Running task {self.metadata.name} ({split=}, {hf_subset=})...\"\n            )\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                data_split = self.dataset[split]\n            else:\n                data_split = self.dataset[hf_subset][split]\n            scores[hf_subset] = self._evaluate_subset(\n                model,\n                data_split,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                **kwargs,\n            )\n            self._add_main_score(scores[hf_subset])\n        return scores\n\n    @abstractmethod\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        raise NotImplementedError(\n            \"If you are using the default evaluate method, you must implement _evaluate_subset method.\"\n        )\n\n    def _save_task_predictions(\n        self,\n        predictions: dict[str, Any] | list[Any],\n        model: MTEBModels,\n        prediction_folder: Path,\n        hf_split: str,\n        hf_subset: str,\n    ) -&gt; None:\n        \"\"\"Saves the predictions of the model on the task to a json file.\n\n        Args:\n            predictions: Dictionary containing the predictions.\n            model: The model used to generate the predictions.\n            prediction_folder: The folder to save the predictions to.\n            hf_split: The split of the dataset (e.g. \"test\").\n            hf_subset: The subset of the dataset (e.g. \"en\").\n        \"\"\"\n        predictions_path = self._predictions_path(prediction_folder)\n        existing_results = {\n            \"mteb_model_meta\": {\n                \"model_name\": model.mteb_model_meta.name,\n                \"revision\": model.mteb_model_meta.revision,\n            }\n        }\n        if predictions_path.exists():\n            with predictions_path.open(\"r\") as predictions_file:\n                existing_results = json.load(predictions_file)\n\n        if hf_subset not in existing_results:\n            existing_results[hf_subset] = {}\n\n        existing_results[hf_subset][hf_split] = predictions\n        with predictions_path.open(\"w\") as predictions_file:\n            json.dump(existing_results, predictions_file)\n\n    def _predictions_path(\n        self,\n        output_folder: Path | str,\n    ) -&gt; Path:\n        if isinstance(output_folder, str):\n            output_folder = Path(output_folder)\n\n        if not output_folder.exists():\n            output_folder.mkdir(parents=True, exist_ok=True)\n        return output_folder / self.prediction_file_name\n\n    @property\n    def prediction_file_name(self) -&gt; str:\n        \"\"\"The name of the prediction file in format {task_name}_predictions.json\"\"\"\n        return f\"{self.metadata.name}_predictions.json\"\n\n    @staticmethod\n    def stratified_subsampling(\n        dataset_dict: DatasetDict,\n        seed: int,\n        splits: list[str] = [\"test\"],\n        label: str = \"label\",\n        n_samples: int = 2048,\n    ) -&gt; DatasetDict:\n        \"\"\"Subsamples the dataset with stratification by the supplied label.\n\n        Args:\n            dataset_dict: the DatasetDict object.\n            seed: the random seed.\n            splits: the splits of the dataset.\n            label: the label with which the stratified sampling is based on.\n            n_samples: Optional, number of samples to subsample. Default is max_n_samples.\n\n        Returns:\n            A subsampled DatasetDict object.\n        \"\"\"\n        # Can only do this if the label column is of ClassLabel.\n        if not isinstance(dataset_dict[splits[0]].features[label], ClassLabel):\n            try:\n                dataset_dict = dataset_dict.class_encode_column(label)\n            except ValueError as e:\n                if isinstance(dataset_dict[splits[0]][label][0], Sequence):\n                    return _multilabel_subsampling(\n                        dataset_dict, seed, splits, label, n_samples\n                    )\n                else:\n                    raise e\n\n        for split in splits:\n            if n_samples &gt;= len(dataset_dict[split]):\n                logger.debug(\n                    f\"Subsampling not needed for split {split}, as n_samples is equal or greater than the number of samples.\"\n                )\n                continue\n            dataset_dict.update(\n                {\n                    split: dataset_dict[split].train_test_split(\n                        test_size=n_samples, seed=seed, stratify_by_column=label\n                    )[\"test\"]\n                }\n            )  # only take the specified test split.\n        return dataset_dict\n\n    def load_data(self) -&gt; None:\n        \"\"\"Loads dataset from HuggingFace hub\n\n        This is the main loading function for Task. Do not overwrite this, instead we recommend using `dataset_transform`, which is called after the\n        dataset is loaded using `datasets.load_dataset`.\n        \"\"\"\n        if self.data_loaded:\n            return\n        if self.metadata.is_multilingual:\n            if self.fast_loading:\n                self.fast_load()\n            else:\n                self.dataset = {}\n                for hf_subset in self.hf_subsets:\n                    self.dataset[hf_subset] = load_dataset(\n                        name=hf_subset,\n                        **self.metadata.dataset,\n                    )\n        else:\n            # some of monolingual datasets explicitly adding the split name to the dataset name\n            self.dataset = load_dataset(**self.metadata.dataset)  # type: ignore\n        self.dataset_transform()\n        self.data_loaded = True\n\n    def fast_load(self) -&gt; None:\n        \"\"\"**Deprecated**. Load all subsets at once, then group by language. Using fast loading has two requirements:\n\n        - Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair\n        - The datasets must have a 'default' config that loads all the subsets of the dataset (see more [here](https://huggingface.co/docs/datasets/en/repository_structure#configurations))\n        \"\"\"\n        self.dataset = {}\n        merged_dataset = load_dataset(**self.metadata.dataset)  # load \"default\" subset\n        for split in merged_dataset.keys():\n            df_split = merged_dataset[split].to_polars()\n            df_grouped = dict(df_split.group_by([\"lang\"]))\n            for lang in set(df_split[\"lang\"].unique()) &amp; set(self.hf_subsets):\n                self.dataset.setdefault(lang, {})\n                self.dataset[lang][split] = Dataset.from_polars(\n                    df_grouped[(lang,)].drop(\"lang\")\n                )  # Remove lang column and convert back to HF datasets, not strictly necessary but better for compatibility\n        for lang, subset in self.dataset.items():\n            self.dataset[lang] = DatasetDict(subset)\n\n    def calculate_descriptive_statistics(\n        self, overwrite_results: bool = False\n    ) -&gt; dict[str, DescriptiveStatistics]:\n        \"\"\"Calculates descriptive statistics from the dataset.\n\n        Args:\n            overwrite_results: Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.\n\n        Returns:\n            A dictionary containing descriptive statistics for each split.\n        \"\"\"\n        from mteb.abstasks import AbsTaskClassification\n\n        if self.metadata.descriptive_stat_path.exists() and not overwrite_results:\n            logger.info(\"Loading metadata descriptive statistics from cache.\")\n            return self.metadata.descriptive_stats\n\n        if not self.data_loaded:\n            self.load_data()\n\n        descriptive_stats: dict[str, DescriptiveStatistics] = {}\n        hf_subset_stat = \"hf_subset_descriptive_stats\"\n        eval_splits = self.metadata.eval_splits\n        if isinstance(self, AbsTaskClassification):\n            eval_splits.append(self.train_split)\n\n        pbar_split = tqdm(eval_splits, desc=\"Processing Splits...\")\n        for split in pbar_split:\n            pbar_split.set_postfix_str(f\"Split: {split}\")\n            logger.info(f\"Processing metadata for split {split}\")\n            if self.metadata.is_multilingual:\n                descriptive_stats[split] = (\n                    self._calculate_descriptive_statistics_from_split(\n                        split, compute_overall=True\n                    )\n                )\n                descriptive_stats[split][hf_subset_stat] = {}\n\n                pbar_subsets = tqdm(\n                    self.metadata.hf_subsets,\n                    desc=\"Processing Languages...\",\n                )\n                for hf_subset in pbar_subsets:\n                    pbar_subsets.set_postfix_str(f\"Huggingface subset: {hf_subset}\")\n                    logger.info(f\"Processing metadata for subset {hf_subset}\")\n                    split_details = self._calculate_descriptive_statistics_from_split(\n                        split, hf_subset\n                    )\n                    descriptive_stats[split][hf_subset_stat][hf_subset] = split_details\n            else:\n                split_details = self._calculate_descriptive_statistics_from_split(split)\n                descriptive_stats[split] = split_details\n\n        with self.metadata.descriptive_stat_path.open(\"w\") as f:\n            json.dump(descriptive_stats, f, indent=4)\n\n        return descriptive_stats\n\n    def calculate_metadata_metrics(\n        self, overwrite_results: bool = False\n    ) -&gt; dict[str, DescriptiveStatistics]:\n        \"\"\"Old name of `calculate_descriptive_statistics`, kept for backward compatibility.\"\"\"\n        return self.calculate_descriptive_statistics(\n            overwrite_results=overwrite_results\n        )\n\n    @abstractmethod\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; SplitDescriptiveStatistics:\n        raise NotImplementedError\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Returns the languages of the task.\"\"\"\n        if self.hf_subsets:\n            eval_langs = self.metadata.hf_subsets_to_langscripts\n            languages = []\n\n            for lang in self.hf_subsets:\n                for langscript in eval_langs[lang]:\n                    iso_lang, script = langscript.split(\"-\")\n                    languages.append(iso_lang)\n\n            return sorted(set(languages))\n\n        return self.metadata.languages\n\n    def filter_eval_splits(self, eval_splits: list[str] | None) -&gt; Self:\n        \"\"\"Filter the evaluation splits of the task.\n\n        Args:\n            eval_splits: A list of evaluation splits to keep. If None, all splits are kept.\n\n        Returns:\n            The filtered task\n        \"\"\"\n        self._eval_splits = eval_splits\n        return self\n\n    def filter_languages(\n        self,\n        languages: list[str] | None,\n        script: list[str] | None = None,\n        hf_subsets: list[HFSubset] | None = None,\n        exclusive_language_filter: bool = False,\n    ) -&gt; Self:\n        \"\"\"Filter the languages of the task.\n\n        Args:\n            languages: list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script\n                (e.g. \"eng-Latn\")\n            script: A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included.\n                If the language code does not specify the script the intersection of the language and script will be used.\n            hf_subsets: A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language,\n                but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.\n            exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n                exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n                specified will be kept.\n\n        Returns:\n            The filtered task\n        \"\"\"\n        lang_scripts = LanguageScripts.from_languages_and_scripts(languages, script)\n\n        subsets_to_keep = []\n\n        for hf_subset, langs in self.metadata.hf_subsets_to_langscripts.items():\n            if (hf_subsets is not None) and (hf_subset not in hf_subsets):\n                continue\n            if exclusive_language_filter is False:\n                for langscript in langs:\n                    if lang_scripts.contains_language(\n                        langscript\n                    ) or lang_scripts.contains_script(langscript):\n                        subsets_to_keep.append(hf_subset)\n                        break\n\n            if exclusive_language_filter is True and languages:\n                if lang_scripts.contains_languages(langs):\n                    subsets_to_keep.append(hf_subset)\n\n        if len(subsets_to_keep) == 0:\n            raise ValueError(\n                f\"No subsets were found for {self.metadata.name} with filters: language code {languages}, script {script}, hf subsets {hf_subsets}.\"\n            )\n\n        self.hf_subsets = subsets_to_keep\n        return self\n\n    def _add_main_score(self, scores: dict[HFSubset, ScoresDict]) -&gt; None:\n        scores[\"main_score\"] = scores[self.metadata.main_score]\n\n    def _upload_dataset_to_hub(\n        self, repo_name: str, fields: list[str] | dict[str, str]\n    ) -&gt; None:\n        if self.metadata.is_multilingual:\n            for config in self.metadata.eval_langs:\n                logger.info(f\"Converting {config} of {self.metadata.name}\")\n                sentences = {}\n                for split in self.dataset[config]:\n                    if isinstance(fields, dict):\n                        sentences[split] = Dataset.from_dict(\n                            {\n                                mapped_name: self.dataset[config][split][original_name]\n                                for original_name, mapped_name in fields.items()\n                            }\n                        )\n                    else:\n                        sentences[split] = Dataset.from_dict(\n                            {\n                                field: self.dataset[config][split][field]\n                                for field in fields\n                            }\n                        )\n                sentences = DatasetDict(sentences)\n                sentences.push_to_hub(\n                    repo_name, config, commit_message=f\"Add {config} dataset\"\n                )\n        else:\n            sentences = {}\n            for split in self.dataset:\n                if isinstance(fields, dict):\n                    sentences[split] = Dataset.from_dict(\n                        {\n                            mapped_name: self.dataset[split][original_name]\n                            for original_name, mapped_name in fields.items()\n                        }\n                    )\n                else:\n                    sentences[split] = Dataset.from_dict(\n                        {field: self.dataset[split][field] for field in fields}\n                    )\n            sentences = DatasetDict(sentences)\n            sentences.push_to_hub(repo_name, commit_message=\"Add dataset\")\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        raise NotImplementedError\n\n    def push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        \"\"\"Push the dataset to the HuggingFace Hub.\n\n        Args:\n            repo_name: The name of the repository to push the dataset to.\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n            &gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n            &gt;&gt;&gt; # Push the dataset to the Hub\n            &gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n        \"\"\"\n        if not self.data_loaded:\n            self.load_data()\n\n        self._push_dataset_to_hub(repo_name)\n        # dataset repo not creating when pushing card\n        self.metadata.push_dataset_card_to_hub(repo_name)\n\n    @property\n    def is_aggregate(self) -&gt; bool:\n        \"\"\"Whether the task is an aggregate of multiple tasks.\"\"\"\n        return False\n\n    @property\n    def eval_splits(self) -&gt; list[str]:\n        \"\"\"Returns the evaluation splits of the task.\"\"\"\n        if self._eval_splits:\n            return self._eval_splits\n        return self.metadata.eval_splits\n\n    @property\n    def modalities(self) -&gt; list[Modalities]:\n        \"\"\"Returns the modalities of the task.\"\"\"\n        return self.metadata.modalities\n\n    def __repr__(self) -&gt; str:\n        # Format the representation of the task such that it appears as:\n        # TaskObjectName(name='{name}', languages={lang1, lang2, ...})\n\n        langs = self.languages\n        if len(langs) &gt; 3:\n            langs = langs[:3]\n            langs.append(\"...\")\n        return (\n            f\"{self.__class__.__name__}(name='{self.metadata.name}', languages={langs})\"\n        )\n\n    def __hash__(self) -&gt; int:\n        return hash(self.metadata)\n\n    def unload_data(self) -&gt; None:\n        \"\"\"Unloads the dataset from memory\"\"\"\n        if self.data_loaded:\n            self.dataset = None\n            self.data_loaded = False\n            logger.info(f\"Unloaded dataset {self.metadata.name} from memory.\")\n        else:\n            logger.warning(\n                f\"Dataset {self.metadata.name} is not loaded, cannot unload it.\"\n            )\n\n    @property\n    def superseded_by(self) -&gt; str | None:\n        \"\"\"If the dataset is superseded by another dataset, return the name of the new dataset.\"\"\"\n        return self.metadata.superseded_by\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.eval_splits","title":"<code>eval_splits</code>  <code>property</code>","text":"<p>Returns the evaluation splits of the task.</p>"},{"location":"api/task/#mteb.AbsTask.is_aggregate","title":"<code>is_aggregate</code>  <code>property</code>","text":"<p>Whether the task is an aggregate of multiple tasks.</p>"},{"location":"api/task/#mteb.AbsTask.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Returns the languages of the task.</p>"},{"location":"api/task/#mteb.AbsTask.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Returns the modalities of the task.</p>"},{"location":"api/task/#mteb.AbsTask.prediction_file_name","title":"<code>prediction_file_name</code>  <code>property</code>","text":"<p>The name of the prediction file in format {task_name}_predictions.json</p>"},{"location":"api/task/#mteb.AbsTask.superseded_by","title":"<code>superseded_by</code>  <code>property</code>","text":"<p>If the dataset is superseded by another dataset, return the name of the new dataset.</p>"},{"location":"api/task/#mteb.AbsTask.__init__","title":"<code>__init__(seed=42, **kwargs)</code>","text":"<p>The init function. This is called primarily to set the seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>An integer seed.</p> <code>42</code> <code>kwargs</code> <code>Any</code> <p>arguments passed to subclasses.</p> <code>{}</code> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def __init__(self, seed: int = 42, **kwargs: Any) -&gt; None:\n    \"\"\"The init function. This is called primarily to set the seed.\n\n    Args:\n        seed: An integer seed.\n        kwargs: arguments passed to subclasses.\n    \"\"\"\n    self.seed = seed\n    self.rng_state, self.np_rng = _set_seed(seed)\n    self.hf_subsets = self.metadata.hf_subsets\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.calculate_descriptive_statistics","title":"<code>calculate_descriptive_statistics(overwrite_results=False)</code>","text":"<p>Calculates descriptive statistics from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite_results</code> <code>bool</code> <p>Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, DescriptiveStatistics]</code> <p>A dictionary containing descriptive statistics for each split.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def calculate_descriptive_statistics(\n    self, overwrite_results: bool = False\n) -&gt; dict[str, DescriptiveStatistics]:\n    \"\"\"Calculates descriptive statistics from the dataset.\n\n    Args:\n        overwrite_results: Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.\n\n    Returns:\n        A dictionary containing descriptive statistics for each split.\n    \"\"\"\n    from mteb.abstasks import AbsTaskClassification\n\n    if self.metadata.descriptive_stat_path.exists() and not overwrite_results:\n        logger.info(\"Loading metadata descriptive statistics from cache.\")\n        return self.metadata.descriptive_stats\n\n    if not self.data_loaded:\n        self.load_data()\n\n    descriptive_stats: dict[str, DescriptiveStatistics] = {}\n    hf_subset_stat = \"hf_subset_descriptive_stats\"\n    eval_splits = self.metadata.eval_splits\n    if isinstance(self, AbsTaskClassification):\n        eval_splits.append(self.train_split)\n\n    pbar_split = tqdm(eval_splits, desc=\"Processing Splits...\")\n    for split in pbar_split:\n        pbar_split.set_postfix_str(f\"Split: {split}\")\n        logger.info(f\"Processing metadata for split {split}\")\n        if self.metadata.is_multilingual:\n            descriptive_stats[split] = (\n                self._calculate_descriptive_statistics_from_split(\n                    split, compute_overall=True\n                )\n            )\n            descriptive_stats[split][hf_subset_stat] = {}\n\n            pbar_subsets = tqdm(\n                self.metadata.hf_subsets,\n                desc=\"Processing Languages...\",\n            )\n            for hf_subset in pbar_subsets:\n                pbar_subsets.set_postfix_str(f\"Huggingface subset: {hf_subset}\")\n                logger.info(f\"Processing metadata for subset {hf_subset}\")\n                split_details = self._calculate_descriptive_statistics_from_split(\n                    split, hf_subset\n                )\n                descriptive_stats[split][hf_subset_stat][hf_subset] = split_details\n        else:\n            split_details = self._calculate_descriptive_statistics_from_split(split)\n            descriptive_stats[split] = split_details\n\n    with self.metadata.descriptive_stat_path.open(\"w\") as f:\n        json.dump(descriptive_stats, f, indent=4)\n\n    return descriptive_stats\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.calculate_metadata_metrics","title":"<code>calculate_metadata_metrics(overwrite_results=False)</code>","text":"<p>Old name of <code>calculate_descriptive_statistics</code>, kept for backward compatibility.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def calculate_metadata_metrics(\n    self, overwrite_results: bool = False\n) -&gt; dict[str, DescriptiveStatistics]:\n    \"\"\"Old name of `calculate_descriptive_statistics`, kept for backward compatibility.\"\"\"\n    return self.calculate_descriptive_statistics(\n        overwrite_results=overwrite_results\n    )\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.check_if_dataset_is_superseded","title":"<code>check_if_dataset_is_superseded()</code>","text":"<p>Check if the dataset is superseded by a newer version.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def check_if_dataset_is_superseded(self) -&gt; None:\n    \"\"\"Check if the dataset is superseded by a newer version.\"\"\"\n    if self.superseded_by:\n        logger.warning(\n            f\"Dataset '{self.metadata.name}' is superseded by '{self.superseded_by}', you might consider using the newer version of the dataset.\"\n        )\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.dataset_transform","title":"<code>dataset_transform()</code>","text":"<p>A transform operations applied to the dataset after loading.</p> <p>This method is useful when the dataset from Huggingface is not in an <code>mteb</code> compatible format. Override this method if your dataset requires additional transformation.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def dataset_transform(self):\n    \"\"\"A transform operations applied to the dataset after loading.\n\n    This method is useful when the dataset from Huggingface is not in an `mteb` compatible format.\n    Override this method if your dataset requires additional transformation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, **kwargs)</code>","text":"<p>Evaluates an MTEB compatible model on the task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MTEBModels</code> <p>MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings</p> required <code>split</code> <code>str</code> <p>Which split (e.g. \"test\") to be used.</p> <code>'test'</code> <code>subsets_to_run</code> <code>list[HFSubset] | None</code> <p>List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.</p> <code>None</code> <code>encode_kwargs</code> <code>dict[str, Any]</code> <p>Additional keyword arguments that are passed to the model's <code>encode</code> method.</p> required <code>prediction_folder</code> <code>Path | None</code> <p>Folder to save model predictions</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments that are passed to the _evaluate_subset method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[HFSubset, ScoresDict]</code> <p>A dictionary with the scores for each subset.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the model is a CrossEncoder and the task does not support CrossEncoders.</p> <code>TypeError</code> <p>If the model is a SearchProtocol and the task does not support Search.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: dict[str, Any],\n    prediction_folder: Path | None = None,\n    **kwargs: Any,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Evaluates an MTEB compatible model on the task.\n\n    Args:\n        model: MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings\n        split: Which split (e.g. *\"test\"*) to be used.\n        subsets_to_run: List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.\n        encode_kwargs: Additional keyword arguments that are passed to the model's `encode` method.\n        prediction_folder: Folder to save model predictions\n        kwargs: Additional keyword arguments that are passed to the _evaluate_subset method.\n\n    Returns:\n        A dictionary with the scores for each subset.\n\n    Raises:\n        TypeError: If the model is a CrossEncoder and the task does not support CrossEncoders.\n        TypeError: If the model is a SearchProtocol and the task does not support Search.\n    \"\"\"\n    if isinstance(model, CrossEncoderProtocol) and not self._support_cross_encoder:\n        raise TypeError(\n            f\"Model {model} is a CrossEncoder, but this task {self.metadata.name} does not support CrossEncoders. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    # encoders might implement search protocols\n    if (\n        isinstance(model, SearchProtocol)\n        and not isinstance(model, EncoderProtocol)\n        and not self._support_search\n    ):\n        raise TypeError(\n            f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    if not self.data_loaded:\n        self.load_data()\n\n    self.dataset = cast(dict[HFSubset, DatasetDict], self.dataset)\n\n    scores = {}\n    if self.hf_subsets is None:\n        hf_subsets = list(self.dataset.keys())\n    else:\n        hf_subsets = copy(self.hf_subsets)\n\n    if subsets_to_run is not None:  # allow overwrites of pre-filtering\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    for hf_subset in hf_subsets:\n        logger.info(\n            f\"Running task {self.metadata.name} ({split=}, {hf_subset=})...\"\n        )\n        if hf_subset not in self.dataset and hf_subset == \"default\":\n            data_split = self.dataset[split]\n        else:\n            data_split = self.dataset[hf_subset][split]\n        scores[hf_subset] = self._evaluate_subset(\n            model,\n            data_split,\n            hf_split=split,\n            hf_subset=hf_subset,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            **kwargs,\n        )\n        self._add_main_score(scores[hf_subset])\n    return scores\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.fast_load","title":"<code>fast_load()</code>","text":"<p>Deprecated. Load all subsets at once, then group by language. Using fast loading has two requirements:</p> <ul> <li>Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair</li> <li>The datasets must have a 'default' config that loads all the subsets of the dataset (see more here)</li> </ul> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def fast_load(self) -&gt; None:\n    \"\"\"**Deprecated**. Load all subsets at once, then group by language. Using fast loading has two requirements:\n\n    - Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair\n    - The datasets must have a 'default' config that loads all the subsets of the dataset (see more [here](https://huggingface.co/docs/datasets/en/repository_structure#configurations))\n    \"\"\"\n    self.dataset = {}\n    merged_dataset = load_dataset(**self.metadata.dataset)  # load \"default\" subset\n    for split in merged_dataset.keys():\n        df_split = merged_dataset[split].to_polars()\n        df_grouped = dict(df_split.group_by([\"lang\"]))\n        for lang in set(df_split[\"lang\"].unique()) &amp; set(self.hf_subsets):\n            self.dataset.setdefault(lang, {})\n            self.dataset[lang][split] = Dataset.from_polars(\n                df_grouped[(lang,)].drop(\"lang\")\n            )  # Remove lang column and convert back to HF datasets, not strictly necessary but better for compatibility\n    for lang, subset in self.dataset.items():\n        self.dataset[lang] = DatasetDict(subset)\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.filter_eval_splits","title":"<code>filter_eval_splits(eval_splits)</code>","text":"<p>Filter the evaluation splits of the task.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>list[str] | None</code> <p>A list of evaluation splits to keep. If None, all splits are kept.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The filtered task</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def filter_eval_splits(self, eval_splits: list[str] | None) -&gt; Self:\n    \"\"\"Filter the evaluation splits of the task.\n\n    Args:\n        eval_splits: A list of evaluation splits to keep. If None, all splits are kept.\n\n    Returns:\n        The filtered task\n    \"\"\"\n    self._eval_splits = eval_splits\n    return self\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.filter_languages","title":"<code>filter_languages(languages, script=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Filter the languages of the task.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>list[str] | None</code> <p>list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script (e.g. \"eng-Latn\")</p> required <code>script</code> <code>list[str] | None</code> <p>A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included. If the language code does not specify the script the intersection of the language and script will be used.</p> <code>None</code> <code>hf_subsets</code> <code>list[HFSubset] | None</code> <p>A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language, but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The filtered task</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def filter_languages(\n    self,\n    languages: list[str] | None,\n    script: list[str] | None = None,\n    hf_subsets: list[HFSubset] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; Self:\n    \"\"\"Filter the languages of the task.\n\n    Args:\n        languages: list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script\n            (e.g. \"eng-Latn\")\n        script: A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included.\n            If the language code does not specify the script the intersection of the language and script will be used.\n        hf_subsets: A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language,\n            but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        The filtered task\n    \"\"\"\n    lang_scripts = LanguageScripts.from_languages_and_scripts(languages, script)\n\n    subsets_to_keep = []\n\n    for hf_subset, langs in self.metadata.hf_subsets_to_langscripts.items():\n        if (hf_subsets is not None) and (hf_subset not in hf_subsets):\n            continue\n        if exclusive_language_filter is False:\n            for langscript in langs:\n                if lang_scripts.contains_language(\n                    langscript\n                ) or lang_scripts.contains_script(langscript):\n                    subsets_to_keep.append(hf_subset)\n                    break\n\n        if exclusive_language_filter is True and languages:\n            if lang_scripts.contains_languages(langs):\n                subsets_to_keep.append(hf_subset)\n\n    if len(subsets_to_keep) == 0:\n        raise ValueError(\n            f\"No subsets were found for {self.metadata.name} with filters: language code {languages}, script {script}, hf subsets {hf_subsets}.\"\n        )\n\n    self.hf_subsets = subsets_to_keep\n    return self\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.load_data","title":"<code>load_data()</code>","text":"<p>Loads dataset from HuggingFace hub</p> <p>This is the main loading function for Task. Do not overwrite this, instead we recommend using <code>dataset_transform</code>, which is called after the dataset is loaded using <code>datasets.load_dataset</code>.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Loads dataset from HuggingFace hub\n\n    This is the main loading function for Task. Do not overwrite this, instead we recommend using `dataset_transform`, which is called after the\n    dataset is loaded using `datasets.load_dataset`.\n    \"\"\"\n    if self.data_loaded:\n        return\n    if self.metadata.is_multilingual:\n        if self.fast_loading:\n            self.fast_load()\n        else:\n            self.dataset = {}\n            for hf_subset in self.hf_subsets:\n                self.dataset[hf_subset] = load_dataset(\n                    name=hf_subset,\n                    **self.metadata.dataset,\n                )\n    else:\n        # some of monolingual datasets explicitly adding the split name to the dataset name\n        self.dataset = load_dataset(**self.metadata.dataset)  # type: ignore\n    self.dataset_transform()\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.push_dataset_to_hub","title":"<code>push_dataset_to_hub(repo_name)</code>","text":"<p>Push the dataset to the HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_name</code> <code>str</code> <p>The name of the repository to push the dataset to.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n&gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n&gt;&gt;&gt; # Push the dataset to the Hub\n&gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n</code></pre> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def push_dataset_to_hub(self, repo_name: str) -&gt; None:\n    \"\"\"Push the dataset to the HuggingFace Hub.\n\n    Args:\n        repo_name: The name of the repository to push the dataset to.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n        &gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n        &gt;&gt;&gt; # Push the dataset to the Hub\n        &gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n    \"\"\"\n    if not self.data_loaded:\n        self.load_data()\n\n    self._push_dataset_to_hub(repo_name)\n    # dataset repo not creating when pushing card\n    self.metadata.push_dataset_card_to_hub(repo_name)\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.stratified_subsampling","title":"<code>stratified_subsampling(dataset_dict, seed, splits=['test'], label='label', n_samples=2048)</code>  <code>staticmethod</code>","text":"<p>Subsamples the dataset with stratification by the supplied label.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DatasetDict</code> <p>the DatasetDict object.</p> required <code>seed</code> <code>int</code> <p>the random seed.</p> required <code>splits</code> <code>list[str]</code> <p>the splits of the dataset.</p> <code>['test']</code> <code>label</code> <code>str</code> <p>the label with which the stratified sampling is based on.</p> <code>'label'</code> <code>n_samples</code> <code>int</code> <p>Optional, number of samples to subsample. Default is max_n_samples.</p> <code>2048</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A subsampled DatasetDict object.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>@staticmethod\ndef stratified_subsampling(\n    dataset_dict: DatasetDict,\n    seed: int,\n    splits: list[str] = [\"test\"],\n    label: str = \"label\",\n    n_samples: int = 2048,\n) -&gt; DatasetDict:\n    \"\"\"Subsamples the dataset with stratification by the supplied label.\n\n    Args:\n        dataset_dict: the DatasetDict object.\n        seed: the random seed.\n        splits: the splits of the dataset.\n        label: the label with which the stratified sampling is based on.\n        n_samples: Optional, number of samples to subsample. Default is max_n_samples.\n\n    Returns:\n        A subsampled DatasetDict object.\n    \"\"\"\n    # Can only do this if the label column is of ClassLabel.\n    if not isinstance(dataset_dict[splits[0]].features[label], ClassLabel):\n        try:\n            dataset_dict = dataset_dict.class_encode_column(label)\n        except ValueError as e:\n            if isinstance(dataset_dict[splits[0]][label][0], Sequence):\n                return _multilabel_subsampling(\n                    dataset_dict, seed, splits, label, n_samples\n                )\n            else:\n                raise e\n\n    for split in splits:\n        if n_samples &gt;= len(dataset_dict[split]):\n            logger.debug(\n                f\"Subsampling not needed for split {split}, as n_samples is equal or greater than the number of samples.\"\n            )\n            continue\n        dataset_dict.update(\n            {\n                split: dataset_dict[split].train_test_split(\n                    test_size=n_samples, seed=seed, stratify_by_column=label\n                )[\"test\"]\n            }\n        )  # only take the specified test split.\n    return dataset_dict\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.unload_data","title":"<code>unload_data()</code>","text":"<p>Unloads the dataset from memory</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def unload_data(self) -&gt; None:\n    \"\"\"Unloads the dataset from memory\"\"\"\n    if self.data_loaded:\n        self.dataset = None\n        self.data_loaded = False\n        logger.info(f\"Unloaded dataset {self.metadata.name} from memory.\")\n    else:\n        logger.warning(\n            f\"Dataset {self.metadata.name} is not loaded, cannot unload it.\"\n        )\n</code></pre>"},{"location":"api/task/#multimodal-tasks","title":"Multimodal Tasks","text":"<p>Tasks that support any modality (text, image, etc.) inherit from the following abstract class. Retrieval tasks support multimodal input (e.g. image + text queries and image corpus or vice versa).</p>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval","title":"<code>mteb.abstasks.retrieval.AbsTaskRetrieval</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for retrieval experiments.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[str, dict[str, RetrievalSplitData]]</code> <p>A nested dictionary where the first key is the subset (language or \"default\"),      the second key is the split (e.g., \"train\", \"test\"), and the value is a RetrievalSplitData object.</p> <code>ignore_identical_ids</code> <code>bool</code> <p>If True, identical IDs in queries and corpus are ignored during evaluation.</p> <code>k_values</code> <code>Sequence[int]</code> <p>A sequence of integers representing the k values for evaluation metrics.</p> <code>skip_first_result</code> <code>bool</code> <p>If True, the first result is skipped during evaluation</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>class AbsTaskRetrieval(AbsTask):\n    \"\"\"Abstract class for retrieval experiments.\n\n    Attributes:\n        dataset: A nested dictionary where the first key is the subset (language or \"default\"),\n                 the second key is the split (e.g., \"train\", \"test\"), and the value is a RetrievalSplitData object.\n        ignore_identical_ids: If True, identical IDs in queries and corpus are ignored during evaluation.\n        k_values: A sequence of integers representing the k values for evaluation metrics.\n        skip_first_result: If True, the first result is skipped during evaluation\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    ignore_identical_ids: bool = False\n    abstask_prompt = \"Retrieve text based on user query.\"\n    k_values: Sequence[int] = (1, 3, 5, 10, 20, 100, 1000)\n    _top_k: int = max(k_values)\n    dataset: dict[str, dict[str, RetrievalSplitData]]\n    _support_cross_encoder: bool = True\n    _support_search: bool = True\n    _previous_results_model_meta: dict[str, Any] | None = None\n    skip_first_result: bool = False\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        empty_dataset = Dataset.from_dict({})\n        self.dataset = defaultdict(\n            lambda: defaultdict(\n                lambda: RetrievalSplitData(\n                    corpus=empty_dataset,\n                    queries=empty_dataset,\n                    relevant_docs={},\n                    top_ranked=None,\n                )\n            )\n        )\n\n    def convert_v1_dataset_format_to_v2(self):\n        \"\"\"Convert dataset from v1 (from `self.queries`, `self.document`) format to v2 format (`self.dotaset`).\"\"\"\n        # check if dataset is `v1` version\n        if not hasattr(self, \"queries\"):\n            return\n        empty_dataset = Dataset.from_dict({})\n\n        self.dataset = defaultdict(\n            lambda: defaultdict(\n                lambda: RetrievalSplitData(\n                    corpus=empty_dataset,\n                    queries=empty_dataset,\n                    relevant_docs={},\n                    top_ranked=None,\n                )\n            )\n        )\n\n        def _process_split(\n            ds_queries: dict | Dataset, ds_corpus: dict | Dataset\n        ) -&gt; tuple[Dataset, Dataset]:\n            if isinstance(ds_queries, dict):\n                queries = Dataset.from_list(\n                    [{\"id\": k, \"text\": v} for k, v in ds_queries.items()]\n                )\n            elif isinstance(ds_queries, Dataset):\n                queries = ds_queries\n            else:\n                raise ValueError(f\"Can't convert queries of type {type(ds_queries)}\")\n\n            if isinstance(ds_corpus, dict):\n                corpus = Dataset.from_list(\n                    [\n                        {\n                            \"id\": k,\n                            \"text\": v if isinstance(v, str) else v[\"text\"],\n                            \"title\": v.get(\"title\", \"\") if isinstance(v, dict) else \"\",\n                        }\n                        for k, v in ds_corpus.items()\n                    ]\n                )\n            elif isinstance(ds_corpus, Dataset):\n                corpus = ds_corpus\n            else:\n                raise ValueError(f\"Can't convert corpus of type {type(ds_corpus)}\")\n            return queries, corpus\n\n        if self.metadata.is_multilingual:\n            for subset in self.queries:\n                for split in self.queries[subset]:\n                    queries = self.queries[subset][split]\n                    corpus = self.corpus[subset][split]\n\n                    (\n                        self.dataset[subset][split][\"queries\"],\n                        self.dataset[subset][split][\"corpus\"],\n                    ) = _process_split(queries, corpus)\n\n                    self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[\n                        subset\n                    ][split]\n                    if hasattr(self, \"instructions\"):\n                        instructions = self.instructions[subset][split]\n                        self.dataset[subset][split][\"queries\"] = (\n                            _combine_queries_with_instructions_datasets(\n                                self.dataset[subset][split][\"queries\"],\n                                instructions,\n                            )\n                        )\n                    if hasattr(self, \"top_ranked\"):\n                        self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                            subset\n                        ][split]\n        else:\n            subset = \"default\"\n            for split in self.queries:\n                queries = self.queries[split]\n                corpus = self.corpus[split]\n                (\n                    self.dataset[subset][split][\"queries\"],\n                    self.dataset[subset][split][\"corpus\"],\n                ) = _process_split(queries, corpus)\n\n                self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[\n                    split\n                ].copy()\n                if hasattr(self, \"instructions\"):\n                    instructions = self.instructions[split]\n                    self.dataset[subset][split][\"queries\"] = (\n                        _combine_queries_with_instructions_datasets(\n                            self.dataset[subset][split][\"queries\"],\n                            instructions,\n                        )\n                    )\n                if hasattr(self, \"top_ranked\"):\n                    self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                        split\n                    ].copy()\n\n        del self.queries\n        del self.corpus\n        del self.relevant_docs\n        if hasattr(self, \"instructions\"):\n            del self.instructions\n        if hasattr(self, \"top_ranked\"):\n            del self.top_ranked\n\n    def load_data(self) -&gt; None:\n        \"\"\"Load the dataset for the retrieval task.\"\"\"\n        if self.data_loaded:\n            return\n\n        dataset_path = self.metadata.dataset[\"path\"]\n        eval_splits = self.metadata.eval_splits\n        trust_remote_code = self.metadata.dataset.get(\"trust_remote_code\", False)\n        revision = self.metadata.dataset[\"revision\"]\n\n        def _process_data(split: str, hf_subset: str = \"default\"):\n            \"\"\"Helper function to load and process data for a given split and language\"\"\"\n            logger.debug(\n                f\"Loading {split} split for {hf_subset} subset of {self.metadata.name}\"\n            )\n\n            self.dataset[hf_subset][split] = RetrievalDatasetLoader(\n                hf_repo=dataset_path,\n                revision=revision,\n                trust_remote_code=trust_remote_code,\n                split=split,\n                config=hf_subset,\n            ).load()\n\n        if self.metadata.is_multilingual:\n            for lang in self.metadata.eval_langs:\n                for split in eval_splits:\n                    _process_data(split, lang)\n        else:\n            for split in eval_splits:\n                _process_data(split)\n        self.dataset_transform()\n        self.data_loaded = True\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Evaluate the model on the retrieval task.\n\n        Args:\n            model: Model to evaluate. Model should implement the [SearchProtocol][mteb.models.models_protocols.SearchProtocol]\n                or be an [Encoder][mteb.models.models_protocols.EncoderProtocol] or [CrossEncoderProtocol][mteb.models.models_protocols.CrossEncoderProtocol].\n            split: Split to evaluate on\n            subsets_to_run: Optional list of subsets to evaluate on\n            encode_kwargs: Keyword arguments passed to the encoder\n            prediction_folder: Folder to save model predictions\n            **kwargs: Additional keyword arguments passed to the evaluator\n\n\n        Returns:\n            Dictionary mapping subsets to their evaluation scores\n        \"\"\"\n        if not self.data_loaded:\n            self.load_data()\n        # TODO: convert all tasks directly https://github.com/embeddings-benchmark/mteb/issues/2030\n        self.convert_v1_dataset_format_to_v2()\n\n        return super().evaluate(\n            model,\n            split,\n            subsets_to_run,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            **kwargs,\n        )\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: RetrievalSplitData,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; ScoresDict:\n        \"\"\"Evaluate a model on a specific subset of the data.\n\n        Args:\n            model: Model to evaluate\n            data_split: Data split to evaluate on\n            encode_kwargs: Keyword arguments passed to the encoder\n            hf_split: Split to evaluate on\n            hf_subset: Subset to evaluate on\n            prediction_folder: Folder with results prediction\n            **kwargs: Additional keyword arguments passed to the evaluator\n\n        Returns:\n            Dictionary of evaluation scores\n        \"\"\"\n        # ensure queries format (see #3030)\n        data_split[\"relevant_docs\"], data_split[\"queries\"] = (\n            _filter_queries_without_positives(\n                data_split[\"relevant_docs\"], data_split[\"queries\"]\n            )\n        )\n        retriever = RetrievalEvaluator(\n            corpus=data_split[\"corpus\"],\n            queries=data_split[\"queries\"],\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            top_ranked=data_split[\"top_ranked\"],\n            top_k=self._top_k,\n            **kwargs,\n        )\n\n        if isinstance(model, EncoderProtocol) and not isinstance(model, SearchProtocol):\n            search_model = SearchEncoderWrapper(model)\n        elif isinstance(model, CrossEncoderProtocol):\n            search_model = SearchCrossEncoderWrapper(model)\n        elif isinstance(model, SearchProtocol):\n            search_model = model\n        else:\n            raise TypeError(\n                f\"RetrievalEvaluator expects a SearchInterface, Encoder, or CrossEncoder, got {type(model)}\"\n            )\n\n        start_time = time()\n        results = retriever(\n            search_model,\n            encode_kwargs=encode_kwargs,\n        )\n        end_time = time()\n        logger.debug(\n            f\"Running retrieval task - Time taken to retrieve: {end_time - start_time:.2f} seconds\"\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                results,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        logger.info(\"Running retrieval task - Evaluating retrieval scores...\")\n        (\n            all_scores,\n            ndcg,\n            _map,\n            recall,\n            precision,\n            naucs,\n            mrr,\n            naucs_mrr,\n            cv_recall,\n        ) = retriever.evaluate(\n            data_split[\"relevant_docs\"],\n            results,\n            self.k_values,\n            ignore_identical_ids=self.ignore_identical_ids,\n            skip_first_result=self.skip_first_result,\n        )\n        task_specific_scores = self.task_specific_scores(\n            all_scores,\n            data_split[\"relevant_docs\"],\n            results,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n        )\n        logger.info(\"Running retrieval task - Finished.\")\n        return make_score_dict(\n            ndcg,\n            _map,\n            recall,\n            precision,\n            mrr,\n            naucs,\n            naucs_mrr,\n            cv_recall,\n            task_specific_scores,\n            self._previous_results_model_meta,\n        )\n\n    def task_specific_scores(\n        self,\n        scores: dict[str, dict[str, float]],\n        qrels: RelevantDocumentsType,\n        results: dict[str, dict[str, float]],\n        hf_split: str,\n        hf_subset: str,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate task specific scores. Override in subclass if needed.\n\n        Args:\n            scores: Dictionary of scores\n            qrels: Relevant documents\n            results: Retrieval results\n            hf_split: Split to evaluate on\n            hf_subset: Subset to evaluate on\n        \"\"\"\n        return {}\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; RetrievalDescriptiveStatistics:\n        self.convert_v1_dataset_format_to_v2()\n        if hf_subset and hf_subset in self.dataset:\n            split_data = self.dataset[hf_subset][split]\n            queries = split_data[\"queries\"]\n            corpus = split_data[\"corpus\"]\n            relevant_docs = split_data[\"relevant_docs\"]\n            top_ranked = split_data[\"top_ranked\"]\n        elif compute_overall:\n            queries = None\n            corpus = None\n            relevant_docs = {}\n            top_ranked = {}\n            for hf_subset in self.metadata.eval_langs:\n                split_data = self.dataset[hf_subset][split]\n                if queries is None:\n                    queries = split_data[\"queries\"]\n                else:\n                    queries = concatenate_datasets([queries, split_data[\"queries\"]])\n                if corpus is None:\n                    corpus = split_data[\"corpus\"]\n                else:\n                    corpus = concatenate_datasets([corpus, split_data[\"corpus\"]])\n\n                relevant_docs.update(\n                    _process_relevant_docs(\n                        split_data[\"relevant_docs\"], hf_subset, split\n                    )\n                )\n\n                if \"top_ranked\" in split_data and split_data[\"top_ranked\"] is not None:\n                    top_ranked.update(\n                        {\n                            f\"{split}_{hf_subset}_{k}\": v\n                            for k, v in split_data[\"top_ranked\"].items()\n                        }\n                    )\n        else:\n            if \"default\" in self.dataset and split != \"default\":\n                return self._calculate_descriptive_statistics_from_split(\n                    split=split, hf_subset=\"default\"\n                )\n            split_data = self.dataset[\"default\"][split]\n            queries = split_data[\"queries\"]\n            corpus = split_data[\"corpus\"]\n            relevant_docs = split_data[\"relevant_docs\"]\n            top_ranked = split_data[\"top_ranked\"]\n\n        num_documents = len(corpus)\n        num_queries = len(queries)\n\n        if self.metadata.category is None:\n            queries_modalities = \"t\"\n            corpus_modalities = \"t\"\n        else:\n            queries_modalities, corpus_modalities = self.metadata.category.split(\"2\")\n\n        number_of_characters = 0\n\n        documents_text_statistics = None\n        documents_image_statistics = None\n        queries_text_statistics = None\n        queries_image_statistics = None\n\n        if \"t\" in corpus_modalities:\n            corpus_texts = corpus.map(_corpus_to_dict)[\"text\"]\n            documents_text_statistics = calculate_text_statistics(corpus_texts)\n            number_of_characters += documents_text_statistics[\"total_text_length\"]\n\n        if \"i\" in corpus_modalities:\n            documents_image_statistics = calculate_image_statistics(corpus[\"image\"])\n\n        if \"t\" in queries_modalities:\n            queries_ = queries\n            if \"instruction\" in queries_[0]:\n                queries_ = queries_.map(_combine_queries_with_instruction_text)\n\n            if isinstance(queries_[\"text\"][0], dict | list):\n                queries_ = queries_.map(_convert_conv_history_to_query)\n            queries_text_statistics = calculate_text_statistics(queries_[\"text\"])\n\n            number_of_characters += queries_text_statistics[\"total_text_length\"]\n\n        if \"i\" in queries_modalities:\n            queries_image_statistics = calculate_image_statistics(queries[\"image\"])\n\n        relevant_docs_statistics = calculate_relevant_docs_statistics(relevant_docs)\n\n        if top_ranked is not None and num_queries and len(top_ranked) &gt; 0:\n            top_ranked_statistics = calculate_top_ranked_statistics(\n                top_ranked, num_queries\n            )\n        else:\n            top_ranked_statistics = None\n\n        return RetrievalDescriptiveStatistics(\n            num_samples=num_documents + num_queries,\n            number_of_characters=number_of_characters,\n            documents_text_statistics=documents_text_statistics,\n            documents_image_statistics=documents_image_statistics,\n            queries_text_statistics=queries_text_statistics,\n            queries_image_statistics=queries_image_statistics,\n            relevant_docs_statistics=relevant_docs_statistics,\n            top_ranked_statistics=top_ranked_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self.convert_v1_dataset_format_to_v2()\n\n        def _push_section(\n            data: dict[str, RetrievalSplitData],\n            subset_item: Literal[\"corpus\", \"queries\", \"relevant_docs\", \"top_ranked\"],\n            hf_subset_name: str,\n            converter: Callable[[Any, Any], dict[str, Any]] | None = None,\n        ) -&gt; None:\n            \"\"\"Helper function to push dataset\n\n            Args:\n                data: Dataset with all items\n                subset_item: Select which part to take. E. g. corpus, queries etc\n                hf_subset_name: Name of the current item on HF\n                converter: Function to convert dict to datasets format\n            \"\"\"\n            sections = {}\n            for split in data.keys():\n                # skip empty instructions and top ranked\n                if subset_item not in data[split] or data[split][subset_item] is None:\n                    continue\n                if isinstance(data[split][subset_item], Dataset):\n                    sections[split] = data[split][subset_item]\n                elif converter is not None:\n                    sections[split] = Dataset.from_list(\n                        [\n                            converter(idx, item)\n                            for idx, item in data[split][subset_item].items()\n                        ]\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unexpected subset item type {subset_item} without converter\"\n                    )\n            if len(sections) &gt; 0:\n                DatasetDict(sections).push_to_hub(\n                    repo_name,\n                    hf_subset_name,\n                    commit_message=f\"Add {hf_subset_name}-{subset_item}\",\n                )\n\n        for subset in self.dataset:\n            logger.info(f\"Converting {subset} of {self.metadata.name}\")\n            _push_section(\n                self.dataset[subset],\n                \"queries\",\n                f\"{subset}-queries\" if subset != \"default\" else \"queries\",\n            )\n            _push_section(\n                self.dataset[subset],\n                \"corpus\",\n                f\"{subset}-corpus\" if subset != \"default\" else \"corpus\",\n            )\n            # Handle relevant_docs separately since one entry expands to multiple records.\n            relevant_sections = {}\n            for split, values in self.dataset[subset].items():\n                relevant_docs = values[\"relevant_docs\"]\n                entries = []\n                for query_id, docs in relevant_docs.items():\n                    for doc_id, score in docs.items():\n                        entries.append(\n                            {\n                                \"query-id\": query_id,\n                                \"corpus-id\": doc_id,\n                                \"score\": score,\n                            }\n                        )\n                relevant_sections[split] = Dataset.from_list(entries)\n            DatasetDict(relevant_sections).push_to_hub(\n                repo_name,\n                f\"{subset}-qrels\" if subset != \"default\" else \"qrels\",\n                commit_message=f\"Add {subset}-qrels\",\n            )\n\n            _push_section(\n                self.dataset[subset],\n                \"top_ranked\",\n                f\"{subset}-top_ranked\" if subset != \"default\" else \"top_ranked\",\n                lambda idx, docs: {\"query-id\": idx, \"corpus-ids\": docs},\n            )\n\n    def convert_to_reranking(\n        self,\n        top_ranked_path: str | Path,\n        top_k: int = 10,\n    ) -&gt; Self:\n        \"\"\"Converts a reranking task to re-ranking by loading predictions from previous model run where the `prediction_folder` was specified.\n\n        Args:\n            top_ranked_path: Path to file or folder with the top ranked predictions.\n            top_k: Number of results to load.\n\n        Returns:\n            The current task reformulated as a reranking task\n\n        Raises:\n            FileNotFoundError: If the specified path does not exist.\n            ValueError: If the loaded top ranked results are not in the expected format.\n        \"\"\"\n        self._top_k = top_k\n\n        top_ranked_path = Path(top_ranked_path)\n        if top_ranked_path.is_dir():\n            top_ranked_path = self._predictions_path(top_ranked_path)\n\n        if not top_ranked_path.exists():\n            raise FileNotFoundError(\n                f\"Can't find previous results for this task. File {top_ranked_path} does not exist.\"\n            )\n\n        with top_ranked_path.open(\"r\") as previous_results_file:\n            previous_results = json.load(previous_results_file)\n\n        if not self.data_loaded:\n            self.load_data()\n\n        self._previous_results_model_meta = previous_results[\"mteb_model_meta\"]\n\n        for subset in self.dataset:\n            for split in self.dataset[subset]:\n                top_ranked: RetrievalOutputType = previous_results[subset][split]\n                if not isinstance(top_ranked, dict):\n                    raise ValueError(\"Previous top ranked results is not a dictionary.\")\n\n                top_k_sorted = defaultdict(list)\n                for query_id, values in top_ranked.items():\n                    sorted_keys = sorted(values, key=values.get, reverse=True)\n                    top_k_sorted[query_id] = sorted_keys[: self._top_k]\n\n                self.dataset[subset][split][\"top_ranked\"] = top_k_sorted\n        return self\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.convert_to_reranking","title":"<code>convert_to_reranking(top_ranked_path, top_k=10)</code>","text":"<p>Converts a reranking task to re-ranking by loading predictions from previous model run where the <code>prediction_folder</code> was specified.</p> <p>Parameters:</p> Name Type Description Default <code>top_ranked_path</code> <code>str | Path</code> <p>Path to file or folder with the top ranked predictions.</p> required <code>top_k</code> <code>int</code> <p>Number of results to load.</p> <code>10</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current task reformulated as a reranking task</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified path does not exist.</p> <code>ValueError</code> <p>If the loaded top ranked results are not in the expected format.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def convert_to_reranking(\n    self,\n    top_ranked_path: str | Path,\n    top_k: int = 10,\n) -&gt; Self:\n    \"\"\"Converts a reranking task to re-ranking by loading predictions from previous model run where the `prediction_folder` was specified.\n\n    Args:\n        top_ranked_path: Path to file or folder with the top ranked predictions.\n        top_k: Number of results to load.\n\n    Returns:\n        The current task reformulated as a reranking task\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n        ValueError: If the loaded top ranked results are not in the expected format.\n    \"\"\"\n    self._top_k = top_k\n\n    top_ranked_path = Path(top_ranked_path)\n    if top_ranked_path.is_dir():\n        top_ranked_path = self._predictions_path(top_ranked_path)\n\n    if not top_ranked_path.exists():\n        raise FileNotFoundError(\n            f\"Can't find previous results for this task. File {top_ranked_path} does not exist.\"\n        )\n\n    with top_ranked_path.open(\"r\") as previous_results_file:\n        previous_results = json.load(previous_results_file)\n\n    if not self.data_loaded:\n        self.load_data()\n\n    self._previous_results_model_meta = previous_results[\"mteb_model_meta\"]\n\n    for subset in self.dataset:\n        for split in self.dataset[subset]:\n            top_ranked: RetrievalOutputType = previous_results[subset][split]\n            if not isinstance(top_ranked, dict):\n                raise ValueError(\"Previous top ranked results is not a dictionary.\")\n\n            top_k_sorted = defaultdict(list)\n            for query_id, values in top_ranked.items():\n                sorted_keys = sorted(values, key=values.get, reverse=True)\n                top_k_sorted[query_id] = sorted_keys[: self._top_k]\n\n            self.dataset[subset][split][\"top_ranked\"] = top_k_sorted\n    return self\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.convert_v1_dataset_format_to_v2","title":"<code>convert_v1_dataset_format_to_v2()</code>","text":"<p>Convert dataset from v1 (from <code>self.queries</code>, <code>self.document</code>) format to v2 format (<code>self.dotaset</code>).</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def convert_v1_dataset_format_to_v2(self):\n    \"\"\"Convert dataset from v1 (from `self.queries`, `self.document`) format to v2 format (`self.dotaset`).\"\"\"\n    # check if dataset is `v1` version\n    if not hasattr(self, \"queries\"):\n        return\n    empty_dataset = Dataset.from_dict({})\n\n    self.dataset = defaultdict(\n        lambda: defaultdict(\n            lambda: RetrievalSplitData(\n                corpus=empty_dataset,\n                queries=empty_dataset,\n                relevant_docs={},\n                top_ranked=None,\n            )\n        )\n    )\n\n    def _process_split(\n        ds_queries: dict | Dataset, ds_corpus: dict | Dataset\n    ) -&gt; tuple[Dataset, Dataset]:\n        if isinstance(ds_queries, dict):\n            queries = Dataset.from_list(\n                [{\"id\": k, \"text\": v} for k, v in ds_queries.items()]\n            )\n        elif isinstance(ds_queries, Dataset):\n            queries = ds_queries\n        else:\n            raise ValueError(f\"Can't convert queries of type {type(ds_queries)}\")\n\n        if isinstance(ds_corpus, dict):\n            corpus = Dataset.from_list(\n                [\n                    {\n                        \"id\": k,\n                        \"text\": v if isinstance(v, str) else v[\"text\"],\n                        \"title\": v.get(\"title\", \"\") if isinstance(v, dict) else \"\",\n                    }\n                    for k, v in ds_corpus.items()\n                ]\n            )\n        elif isinstance(ds_corpus, Dataset):\n            corpus = ds_corpus\n        else:\n            raise ValueError(f\"Can't convert corpus of type {type(ds_corpus)}\")\n        return queries, corpus\n\n    if self.metadata.is_multilingual:\n        for subset in self.queries:\n            for split in self.queries[subset]:\n                queries = self.queries[subset][split]\n                corpus = self.corpus[subset][split]\n\n                (\n                    self.dataset[subset][split][\"queries\"],\n                    self.dataset[subset][split][\"corpus\"],\n                ) = _process_split(queries, corpus)\n\n                self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[\n                    subset\n                ][split]\n                if hasattr(self, \"instructions\"):\n                    instructions = self.instructions[subset][split]\n                    self.dataset[subset][split][\"queries\"] = (\n                        _combine_queries_with_instructions_datasets(\n                            self.dataset[subset][split][\"queries\"],\n                            instructions,\n                        )\n                    )\n                if hasattr(self, \"top_ranked\"):\n                    self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                        subset\n                    ][split]\n    else:\n        subset = \"default\"\n        for split in self.queries:\n            queries = self.queries[split]\n            corpus = self.corpus[split]\n            (\n                self.dataset[subset][split][\"queries\"],\n                self.dataset[subset][split][\"corpus\"],\n            ) = _process_split(queries, corpus)\n\n            self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[\n                split\n            ].copy()\n            if hasattr(self, \"instructions\"):\n                instructions = self.instructions[split]\n                self.dataset[subset][split][\"queries\"] = (\n                    _combine_queries_with_instructions_datasets(\n                        self.dataset[subset][split][\"queries\"],\n                        instructions,\n                    )\n                )\n            if hasattr(self, \"top_ranked\"):\n                self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                    split\n                ].copy()\n\n    del self.queries\n    del self.corpus\n    del self.relevant_docs\n    if hasattr(self, \"instructions\"):\n        del self.instructions\n    if hasattr(self, \"top_ranked\"):\n        del self.top_ranked\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, **kwargs)</code>","text":"<p>Evaluate the model on the retrieval task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MTEBModels</code> <p>Model to evaluate. Model should implement the SearchProtocol or be an Encoder or CrossEncoderProtocol.</p> required <code>split</code> <code>str</code> <p>Split to evaluate on</p> <code>'test'</code> <code>subsets_to_run</code> <code>list[HFSubset] | None</code> <p>Optional list of subsets to evaluate on</p> <code>None</code> <code>encode_kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments passed to the encoder</p> required <code>prediction_folder</code> <code>Path | None</code> <p>Folder to save model predictions</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the evaluator</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[HFSubset, ScoresDict]</code> <p>Dictionary mapping subsets to their evaluation scores</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: dict[str, Any],\n    prediction_folder: Path | None = None,\n    **kwargs,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Evaluate the model on the retrieval task.\n\n    Args:\n        model: Model to evaluate. Model should implement the [SearchProtocol][mteb.models.models_protocols.SearchProtocol]\n            or be an [Encoder][mteb.models.models_protocols.EncoderProtocol] or [CrossEncoderProtocol][mteb.models.models_protocols.CrossEncoderProtocol].\n        split: Split to evaluate on\n        subsets_to_run: Optional list of subsets to evaluate on\n        encode_kwargs: Keyword arguments passed to the encoder\n        prediction_folder: Folder to save model predictions\n        **kwargs: Additional keyword arguments passed to the evaluator\n\n\n    Returns:\n        Dictionary mapping subsets to their evaluation scores\n    \"\"\"\n    if not self.data_loaded:\n        self.load_data()\n    # TODO: convert all tasks directly https://github.com/embeddings-benchmark/mteb/issues/2030\n    self.convert_v1_dataset_format_to_v2()\n\n    return super().evaluate(\n        model,\n        split,\n        subsets_to_run,\n        encode_kwargs=encode_kwargs,\n        prediction_folder=prediction_folder,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.load_data","title":"<code>load_data()</code>","text":"<p>Load the dataset for the retrieval task.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Load the dataset for the retrieval task.\"\"\"\n    if self.data_loaded:\n        return\n\n    dataset_path = self.metadata.dataset[\"path\"]\n    eval_splits = self.metadata.eval_splits\n    trust_remote_code = self.metadata.dataset.get(\"trust_remote_code\", False)\n    revision = self.metadata.dataset[\"revision\"]\n\n    def _process_data(split: str, hf_subset: str = \"default\"):\n        \"\"\"Helper function to load and process data for a given split and language\"\"\"\n        logger.debug(\n            f\"Loading {split} split for {hf_subset} subset of {self.metadata.name}\"\n        )\n\n        self.dataset[hf_subset][split] = RetrievalDatasetLoader(\n            hf_repo=dataset_path,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            split=split,\n            config=hf_subset,\n        ).load()\n\n    if self.metadata.is_multilingual:\n        for lang in self.metadata.eval_langs:\n            for split in eval_splits:\n                _process_data(split, lang)\n    else:\n        for split in eval_splits:\n            _process_data(split)\n    self.dataset_transform()\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.task_specific_scores","title":"<code>task_specific_scores(scores, qrels, results, hf_split, hf_subset)</code>","text":"<p>Calculate task specific scores. Override in subclass if needed.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>dict[str, dict[str, float]]</code> <p>Dictionary of scores</p> required <code>qrels</code> <code>RelevantDocumentsType</code> <p>Relevant documents</p> required <code>results</code> <code>dict[str, dict[str, float]]</code> <p>Retrieval results</p> required <code>hf_split</code> <code>str</code> <p>Split to evaluate on</p> required <code>hf_subset</code> <code>str</code> <p>Subset to evaluate on</p> required Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def task_specific_scores(\n    self,\n    scores: dict[str, dict[str, float]],\n    qrels: RelevantDocumentsType,\n    results: dict[str, dict[str, float]],\n    hf_split: str,\n    hf_subset: str,\n) -&gt; dict[str, float]:\n    \"\"\"Calculate task specific scores. Override in subclass if needed.\n\n    Args:\n        scores: Dictionary of scores\n        qrels: Relevant documents\n        results: Retrieval results\n        hf_split: Split to evaluate on\n        hf_subset: Subset to evaluate on\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval_dataset_loaders.RetrievalSplitData","title":"<code>mteb.abstasks.retrieval_dataset_loaders.RetrievalSplitData</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary containing the corpus, queries, relevant documents, instructions, and top-ranked documents for a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>corpus</code> <code>CorpusDatasetType</code> <p>The corpus dataset containing documents. Should have columns <code>id</code>, <code>title</code>, <code>text</code> or <code>image</code>.</p> <code>queries</code> <code>QueryDatasetType</code> <p>The queries dataset containing queries. Should have columns <code>id</code>, <code>text</code>, <code>instruction</code> (for instruction retrieval/reranking) or <code>image</code>.</p> <code>relevant_docs</code> <code>RelevantDocumentsType</code> <p>A mapping of query IDs to relevant document IDs and their relevance scores. Should have columns <code>query-id</code>, <code>corpus-id</code>, <code>score</code>.</p> <code>top_ranked</code> <code>TopRankedDocumentsType | None</code> <p>A mapping of query IDs to a list of top-ranked document IDs. Should have columns <code>query-id</code>, <code>corpus-ids</code> (list[str]). This is optional and used for reranking tasks.</p> Source code in <code>mteb/abstasks/retrieval_dataset_loaders.py</code> <pre><code>class RetrievalSplitData(TypedDict):\n    \"\"\"A dictionary containing the corpus, queries, relevant documents, instructions, and top-ranked documents for a retrieval task.\n\n    Attributes:\n        corpus: The corpus dataset containing documents. Should have columns `id`, `title`, `text` or `image`.\n        queries: The queries dataset containing queries. Should have columns `id`, `text`, `instruction` (for instruction retrieval/reranking) or `image`.\n        relevant_docs: A mapping of query IDs to relevant document IDs and their relevance scores. Should have columns `query-id`, `corpus-id`, `score`.\n        top_ranked: A mapping of query IDs to a list of top-ranked document IDs. Should have columns `query-id`, `corpus-ids` (list[str]). This is optional and used for reranking tasks.\n    \"\"\"\n\n    corpus: CorpusDatasetType\n    queries: QueryDatasetType\n    relevant_docs: RelevantDocumentsType\n    top_ranked: TopRankedDocumentsType | None\n</code></pre>"},{"location":"api/task/#mteb.abstasks.classification.AbsTaskClassification","title":"<code>mteb.abstasks.classification.AbsTaskClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for classification tasks</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Hugging Face dataset containing the data for the task. Should have train split (split name can be changed by train_split. Must contain the following columns: text: str (for text) or PIL.Image (for image). Column name can be changed via <code>input_column_name</code> attribute. label: int. Column name can be changed via <code>label_column_name</code> attribute.</p> <code>evaluator_model</code> <code>SklearnModelProtocol</code> <p>The model to use for evaluation. Can be any sklearn compatible model. Default is <code>LogisticRegression</code>. Full details of api in [<code>SklearnModelProtocol</code>][mteb._evaluators.sklearn_evaluator.SklearnModelProtocol].</p> <code>samples_per_label</code> <code>int</code> <p>Number of samples per label to use for training the evaluator model. Default is 8.</p> <code>n_experiments</code> <code>int</code> <p>Number of experiments to run. Default is 10.</p> <code>train_split</code> <code>str</code> <p>Name of the split to use for training the evaluator model. Default is \"train\".</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels. Default is \"label\".</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input data. Default is \"text\".</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/classification.py</code> <pre><code>class AbsTaskClassification(AbsTask):\n    \"\"\"Abstract class for classification tasks\n\n    Attributes:\n        dataset: Hugging Face dataset containing the data for the task. Should have train split (split name can be changed by train_split. Must contain the following columns:\n            text: str (for text) or PIL.Image (for image). Column name can be changed via `input_column_name` attribute.\n            label: int. Column name can be changed via `label_column_name` attribute.\n        evaluator_model: The model to use for evaluation. Can be any sklearn compatible model. Default is `LogisticRegression`.\n            Full details of api in [`SklearnModelProtocol`][mteb._evaluators.sklearn_evaluator.SklearnModelProtocol].\n        samples_per_label: Number of samples per label to use for training the evaluator model. Default is 8.\n        n_experiments: Number of experiments to run. Default is 10.\n        train_split: Name of the split to use for training the evaluator model. Default is \"train\".\n        label_column_name: Name of the column containing the labels. Default is \"label\".\n        input_column_name: Name of the column containing the input data. Default is \"text\".\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    evaluator: type[SklearnEvaluator] = SklearnEvaluator\n    evaluator_model: SklearnModelProtocol = LogisticRegression(\n        n_jobs=-1,\n        max_iter=100,\n    )\n\n    samples_per_label: int = 8\n    n_experiments: int = 10\n    train_split: str = \"train\"\n    label_column_name: str = \"label\"\n    input_column_name: str = \"text\"\n    abstask_prompt = \"Classify user passages.\"\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Evaluate a model on the classification task.\n\n        Differs from other tasks as it requires train split.\n        \"\"\"\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\n                f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        if not self.data_loaded:\n            self.load_data()\n\n        if \"random_state\" in self.evaluator_model.get_params():\n            self.evaluator_model = self.evaluator_model.set_params(\n                random_state=self.seed\n            )\n        scores = {}\n        hf_subsets = self.hf_subsets\n        if subsets_to_run is not None:\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n            )\n\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                ds = self.dataset\n            else:\n                ds = self.dataset[hf_subset]\n\n            if isinstance(ds, Dataset | DatasetDict):\n                ds = ds.select_columns([self.label_column_name, self.input_column_name])\n            scores[hf_subset] = self._evaluate_subset(\n                model,\n                ds,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                **kwargs,\n            )\n            self._add_main_score(scores[hf_subset])\n\n        return scores\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: DatasetDict,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; FullClassificationMetrics:\n        train_split = data_split[self.train_split]\n        eval_split = data_split[hf_split]\n\n        scores = []\n        # we store idxs to make the shuffling reproducible\n        test_cache, idxs = None, None\n\n        all_predictions = []\n        for i in range(self.n_experiments):\n            logger.info(f\"Running experiment ({i}/{self.n_experiments})\")\n            # Bootstrap `self.samples_per_label` samples per label for each split\n            train_dataset, idxs = self._undersample_data(\n                train_split,\n                i,\n                idxs,\n            )\n\n            evaluator = self.evaluator(\n                train_dataset,\n                eval_split,\n                self.input_column_name,\n                self.label_column_name,\n                task_metadata=self.metadata,\n                hf_split=hf_split,\n                hf_subset=hf_subset,\n                evaluator_model=self.evaluator_model,\n            )\n            y_pred, test_cache = evaluator(\n                model, encode_kwargs=encode_kwargs, test_cache=test_cache\n            )\n            if prediction_folder:\n                all_predictions.append(y_pred.tolist())\n            y_test = eval_split[self.label_column_name]\n            scores_exp = self._calculate_scores(y_test, y_pred)\n            scores.append(scores_exp)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_predictions,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        avg_scores: dict[str, Any] = {\n            # ap will be none for non binary classification tasks\n            k: (\n                float(np.mean(values))\n                if (values := [s[k] for s in scores if s[k] is not None])\n                else np.nan\n            )\n            for k in scores[0].keys()\n        }\n        logger.info(f\"Running {self.metadata.name} - Finished.\")\n        return FullClassificationMetrics(\n            scores_per_experiment=scores,\n            **avg_scores,\n        )\n\n    def _calculate_scores(\n        self,\n        y_test: np.ndarray | list[int],\n        y_pred: np.ndarray,\n    ) -&gt; ClassificationMetrics:\n        scores = ClassificationMetrics(\n            accuracy=accuracy_score(y_test, y_pred),\n            f1=f1_score(y_test, y_pred, average=\"macro\"),\n            f1_weighted=f1_score(y_test, y_pred, average=\"weighted\"),\n            precision=precision_score(y_test, y_pred, average=\"macro\"),\n            precision_weighted=precision_score(y_test, y_pred, average=\"weighted\"),\n            recall=recall_score(y_test, y_pred, average=\"macro\"),\n            recall_weighted=recall_score(y_test, y_pred, average=\"weighted\"),\n            ap=None,\n            ap_weighted=None,\n        )\n\n        # if binary classification\n        if len(np.unique(y_test)) == 2:\n            scores[\"ap\"] = average_precision_score(y_test, y_pred, average=\"macro\")\n            scores[\"ap_weighted\"] = average_precision_score(\n                y_test, y_pred, average=\"weighted\"\n            )\n        return scores\n\n    def _undersample_data(\n        self, dataset: Dataset, experiment_num: int, idxs: list[int] | None = None\n    ) -&gt; tuple[Dataset, list[int]]:\n        \"\"\"Undersample data to have `samples_per_label` samples of each label.\n\n        Args:\n            dataset: Hugging Face `datasets.Dataset` containing \"text\" and \"label\".\n            experiment_num: Experiment number, used to set the random seed.\n            idxs: Optional indices to shuffle and sample from.\n\n        Returns:\n            A new Dataset containing undersampled examples.\n            The shuffled indices used for sampling.\n        \"\"\"\n        if idxs is None:\n            idxs = list(range(len(dataset)))\n\n        # using RandomState for backward compatibility with `v1`\n        rng_state = np.random.RandomState(self.seed)\n        rng_state.shuffle(idxs)\n\n        label_counter: dict[str, int] = defaultdict(int)\n        sampled_idxs = []\n\n        for i in idxs:\n            label = dataset[i][self.label_column_name]\n            if label_counter[label] &lt; self.samples_per_label:\n                sampled_idxs.append(i)\n                label_counter[label] += 1\n\n        return dataset.select(sampled_idxs), idxs\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClassificationDescriptiveStatistics:\n        train_text = []\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            label = self.dataset[hf_subset][split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[hf_subset][self.train_split][\n                    self.input_column_name\n                ]\n        elif compute_overall:\n            inputs = []\n            label = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                label.extend(self.dataset[hf_subset][split][self.label_column_name])\n                if split != self.train_split:\n                    train_text.extend(\n                        self.dataset[hf_subset][self.train_split][\n                            self.input_column_name\n                        ]\n                    )\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            label = self.dataset[split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[self.train_split][self.input_column_name]\n\n        image_statistics = None\n        text_statistics = None\n        num_texts_in_train = None\n\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n            num_texts_in_train = (\n                len(set(inputs) &amp; set(train_text))\n                if split != self.train_split\n                else None\n            )\n\n        label_statistics = calculate_label_statistics(label)\n\n        return ClassificationDescriptiveStatistics(\n            num_samples=len(inputs),\n            number_texts_intersect_with_train=num_texts_in_train,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            label_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.classification.AbsTaskClassification.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, **kwargs)</code>","text":"<p>Evaluate a model on the classification task.</p> <p>Differs from other tasks as it requires train split.</p> Source code in <code>mteb/abstasks/classification.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: dict[str, Any],\n    prediction_folder: Path | None = None,\n    **kwargs: Any,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Evaluate a model on the classification task.\n\n    Differs from other tasks as it requires train split.\n    \"\"\"\n    if not isinstance(model, EncoderProtocol):\n        raise TypeError(\n            f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    if not self.data_loaded:\n        self.load_data()\n\n    if \"random_state\" in self.evaluator_model.get_params():\n        self.evaluator_model = self.evaluator_model.set_params(\n            random_state=self.seed\n        )\n    scores = {}\n    hf_subsets = self.hf_subsets\n    if subsets_to_run is not None:\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    for hf_subset in hf_subsets:\n        logger.info(\n            f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n        )\n\n        if hf_subset not in self.dataset and hf_subset == \"default\":\n            ds = self.dataset\n        else:\n            ds = self.dataset[hf_subset]\n\n        if isinstance(ds, Dataset | DatasetDict):\n            ds = ds.select_columns([self.label_column_name, self.input_column_name])\n        scores[hf_subset] = self._evaluate_subset(\n            model,\n            ds,\n            hf_split=split,\n            hf_subset=hf_subset,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            **kwargs,\n        )\n        self._add_main_score(scores[hf_subset])\n\n    return scores\n</code></pre>"},{"location":"api/task/#mteb.abstasks.multilabel_classification.AbsTaskMultilabelClassification","title":"<code>mteb.abstasks.multilabel_classification.AbsTaskMultilabelClassification</code>","text":"<p>               Bases: <code>AbsTaskClassification</code></p> <p>Abstract class for multioutput classification tasks</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Huggingface dataset containing the data for the task. Dataset must contain columns specified by input_column_name and label_column_name. Input column must contain the text or image to be classified, and label column must contain a list of labels for each example.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input text.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels.</p> <code>samples_per_label</code> <code>int</code> <p>Number of samples to use pr. label. These samples are embedded and a classifier is fit using the labels and samples.</p> <code>evaluator</code> <code>SklearnModelProtocol</code> <p>Classifier to use for evaluation. Must implement the SklearnModelProtocol.</p> Source code in <code>mteb/abstasks/multilabel_classification.py</code> <pre><code>class AbsTaskMultilabelClassification(AbsTaskClassification):\n    \"\"\"Abstract class for multioutput classification tasks\n\n    Attributes:\n        dataset: Huggingface dataset containing the data for the task. Dataset must contain columns specified by input_column_name and label_column_name.\n            Input column must contain the text or image to be classified, and label column must contain a list of labels for each example.\n        input_column_name: Name of the column containing the input text.\n        label_column_name: Name of the column containing the labels.\n        samples_per_label: Number of samples to use pr. label. These samples are embedded and a classifier is fit using the labels and samples.\n        evaluator: Classifier to use for evaluation. Must implement the SklearnModelProtocol.\n    \"\"\"\n\n    evaluator: SklearnModelProtocol = KNeighborsClassifier(n_neighbors=5)\n    input_column_name: str = \"text\"\n    label_column_name: str = \"label\"\n\n    @override\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: DatasetDict,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; FullMultilabelClassificationMetrics:\n        if isinstance(data_split, DatasetDict):\n            data_split = data_split.select_columns(\n                [self.input_column_name, self.label_column_name]\n            )\n        train_split = data_split[self.train_split]\n        eval_split = data_split[hf_split]\n\n        logger.info(\n            \"Running multilabel classification task - Sampling training data...\"\n        )\n        scores = []\n        # Bootstrap sample indices from training set for each experiment\n        train_samples = []\n        for _ in range(self.n_experiments):\n            sample_indices, _ = self._undersample_data_indices(\n                train_split[self.label_column_name], self.samples_per_label, None\n            )\n            train_samples.append(sample_indices)\n        # Encode all unique sentences at the indices\n        unique_train_indices = list(set(itertools.chain.from_iterable(train_samples)))\n        unique_train_dataset = train_split.select(unique_train_indices).select_columns(\n            self.input_column_name\n        )\n        dataloader_train = create_dataloader(\n            unique_train_dataset,\n            self.metadata,\n            input_column=self.input_column_name,\n            batch_size=encode_kwargs[\"batch_size\"],\n        )\n\n        logger.info(\"Running multilabel classification - Encoding training set...\")\n        _unique_train_embeddings = model.encode(\n            dataloader_train,\n            task_metadata=self.metadata,\n            hf_split=self.train_split,\n            hf_subset=hf_subset,\n            **encode_kwargs,\n        )\n        unique_train_embeddings = dict(\n            zip(unique_train_indices, _unique_train_embeddings)\n        )\n        # Stratified subsampling of test set to 2000 examples.\n        test_dataset = eval_split\n        try:\n            if len(test_dataset) &gt; 2000:\n                split_dataset = eval_split.train_test_split(\n                    test_size=2000, seed=42, stratify_by_column=\"label\"\n                )\n                test_dataset = split_dataset[\"test\"]\n        except ValueError:\n            logger.warning(\"Couldn't subsample, continuing with the entire test set.\")\n\n        dataloader_test = create_dataloader(\n            test_dataset.select_columns(self.input_column_name),\n            self.metadata,\n            input_column=self.input_column_name,\n            batch_size=encode_kwargs[\"batch_size\"],\n        )\n\n        logger.info(\"Running multilabel classification - Encoding test set...\")\n        X_test = model.encode(\n            dataloader_test,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **encode_kwargs,\n        )\n        binarizer = MultiLabelBinarizer()\n        y_test = binarizer.fit_transform(test_dataset[self.label_column_name])\n\n        logger.info(\"Running multilabel classification - Evaluating classifiers...\")\n        all_predictions = []\n        for i_experiment, sample_indices in enumerate(train_samples):\n            X_train = np.stack([unique_train_embeddings[idx] for idx in sample_indices])\n            y_train = train_split.select(sample_indices)[self.label_column_name]\n            y_train = binarizer.transform(y_train)\n            y_pred, current_classifier = _evaluate_classifier(\n                X_train, y_train, X_test, self.evaluator\n            )\n            if prediction_folder:\n                all_predictions.append(y_pred.tolist())\n\n            scores_exp = self._calculate_scores(\n                y_test, y_pred, X_test, current_classifier\n            )\n            scores.append(scores_exp)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_predictions,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        avg_scores: dict[str, Any] = {\n            k: np.mean([s[k] for s in scores]) for k in scores[0].keys()\n        }\n        logger.info(\"Running multilabel classification - Finished.\")\n        return FullMultilabelClassificationMetrics(\n            scores_per_experiment=scores,\n            **avg_scores,\n        )\n\n    def _calculate_scores(\n        self,\n        y_test: np.ndarray,\n        y_pred: np.ndarray,\n        x_test_embedding: np.ndarray,\n        current_classifier: SklearnModelProtocol,\n    ) -&gt; MultilabelClassificationMetrics:\n        accuracy = current_classifier.score(x_test_embedding, y_test)\n        if isinstance(current_classifier, MultiOutputClassifier):\n            predictions = current_classifier.predict_proba(x_test_embedding)\n            all_probs = [emb[:, 1] for emb in predictions]\n\n            y_score = np.stack(all_probs, axis=1)  # shape: (n_samples, n_labels)\n            lrap = label_ranking_average_precision_score(y_test, y_score)\n        else:\n            lrap = label_ranking_average_precision_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average=\"macro\")\n        return MultilabelClassificationMetrics(\n            accuracy=accuracy,\n            lrap=lrap,\n            f1=f1,\n        )\n\n    def _undersample_data_indices(\n        self, y: list[list[int]], samples_per_label: int, idxs: list[int] | None = None\n    ) -&gt; tuple[list[int], list[int]]:\n        \"\"\"Undersample data to have samples_per_label samples of each label.\n\n        Returns:\n            A tuple containing:\n                - List of sampled indices.\n                - List of all indices after shuffling.\n        \"\"\"\n        sample_indices = []\n        if idxs is None:\n            idxs = np.arange(len(y))\n        self.np_rng.shuffle(idxs)\n        idxs = idxs.tolist()\n        label_counter = defaultdict(int)\n        for i in idxs:\n            if any((label_counter[label] &lt; samples_per_label) for label in y[i]):\n                sample_indices.append(i)\n                for label in y[i]:\n                    label_counter[label] += 1\n        return sample_indices, idxs\n</code></pre>"},{"location":"api/task/#mteb.abstasks.clustering.AbsTaskClustering","title":"<code>mteb.abstasks.clustering.AbsTaskClustering</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for Clustering tasks.</p> <p>This class embeds the corpus sentences then samples N samples from the corpus and clusters them. The similarity then is calculated using the V-measure metric, which is invariant to the permutation of the labels. This approach is then repeated K times.</p> <p>There are two ways to specify how a dataset is downsampled <code>max_document_to_embed</code> and <code>max_fraction_of_documents_to_embed</code>. If both parameters are set to None, no downsampling is done in self._evaluate_subset(). Only one of these two parameters can be not None at the same time.</p> <p>If the clustering is hierarchical, and more than one label is specified in order for each observation, V-measures are calculated in the outlined way on each of the levels separately.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the clustering task. Must contain the following columns <code>sentences</code> that contains inputs (texts or images) and labels columns.</p> <code>max_fraction_of_documents_to_embed</code> <code>float | None</code> <p>Fraction of documents to embed for clustering.</p> <code>max_document_to_embed</code> <code>int | None</code> <p>Maximum number of documents to embed for clustering.</p> <code>max_documents_per_cluster</code> <code>int</code> <p>Number of documents to sample for each clustering experiment.</p> <code>n_clusters</code> <code>int</code> <p>Number of clustering experiments to run.</p> <code>k_mean_batch_size</code> <code>int</code> <p>Batch size to use for k-means clustering.</p> <code>max_depth</code> <p>Maximum depth to evaluate clustering. If None, evaluates all levels.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input sentences or data points.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the true cluster labels.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/clustering.py</code> <pre><code>class AbsTaskClustering(AbsTask):\n    \"\"\"Abstract class for Clustering tasks.\n\n    This class embeds the corpus sentences then samples N samples from the corpus and clusters them.\n    The similarity then is calculated using the V-measure metric, which is invariant to the permutation of the labels.\n    This approach is then repeated K times.\n\n    There are two ways to specify how a dataset is downsampled `max_document_to_embed` and `max_fraction_of_documents_to_embed`.\n    If both parameters are set to None, no downsampling is done in self._evaluate_subset().\n    Only one of these two parameters can be not None at the same time.\n\n    If the clustering is hierarchical, and more than one label is specified in order for each observation,\n    V-measures are calculated in the outlined way on each of the levels separately.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the clustering task. Must contain the following columns `sentences` that contains inputs (texts or images) and labels columns.\n        max_fraction_of_documents_to_embed: Fraction of documents to embed for clustering.\n        max_document_to_embed: Maximum number of documents to embed for clustering.\n        max_documents_per_cluster: Number of documents to sample for each clustering experiment.\n        n_clusters: Number of clustering experiments to run.\n        k_mean_batch_size: Batch size to use for k-means clustering.\n        max_depth: Maximum depth to evaluate clustering. If None, evaluates all levels.\n        input_column_name: Name of the column containing the input sentences or data points.\n        label_column_name: Name of the column containing the true cluster labels.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    max_fraction_of_documents_to_embed: float | None = 0.04\n    max_document_to_embed: int | None = None\n    max_documents_per_cluster: int = 16_384\n    n_clusters: int = 10\n    k_mean_batch_size: int = 512\n    max_depth = None\n    abstask_prompt = \"Identify categories in user passages.\"\n    input_column_name: str = \"sentences\"\n    label_column_name: str = \"labels\"\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        if (\n            self.max_document_to_embed is not None\n            and self.max_fraction_of_documents_to_embed is not None\n        ):\n            raise Exception(\n                \"Both max_document_to_embed and max_fraction_of_documents_to_embed are set. Please only set one.\"\n            )\n\n        logger.info(\"Running clustering - Preparing data...\")\n        if (\n            self.max_document_to_embed is None\n            and self.max_fraction_of_documents_to_embed is None\n        ):\n            downsampled_dataset = data_split\n        else:\n            if self.max_fraction_of_documents_to_embed is not None:\n                max_documents_to_embed = int(\n                    self.max_fraction_of_documents_to_embed * len(data_split)\n                )\n            else:\n                max_documents_to_embed = self.max_document_to_embed\n\n            max_documents_to_embed = min(len(data_split), max_documents_to_embed)  # type: ignore\n            example_indices = self.rng_state.sample(\n                range(len(data_split)), k=max_documents_to_embed\n            )\n            downsampled_dataset = data_split.select(example_indices)  # type: ignore\n\n        downsampled_dataset = downsampled_dataset.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n\n        logger.info(\"Running clustering - Encoding samples...\")\n        embeddings = model.encode(\n            create_dataloader(\n                downsampled_dataset,\n                self.metadata,\n                input_column=self.input_column_name,\n                batch_size=encode_kwargs[\"batch_size\"],\n            ),\n            task_metadata=self.metadata,\n            hf_subset=hf_subset,\n            hf_split=hf_split,\n            **encode_kwargs,\n        )\n\n        logger.info(\"Running clustering - Evaluating clustering...\")\n        labels = []\n        for label in downsampled_dataset[self.label_column_name]:\n            if not isinstance(label, list):\n                label = [label]\n            labels.append(label)\n\n        all_v_scores, all_assignments = _evaluate_clustering_bootstrapped(\n            embeddings,\n            labels,\n            n_clusters=self.n_clusters,\n            cluster_size=self.max_documents_per_cluster,\n            kmean_batch_size=self.k_mean_batch_size,\n            max_depth=self.max_depth,\n            rng_state=self.rng_state,\n            seed=self.seed,\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_assignments,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        v_measures = list(itertools.chain.from_iterable(all_v_scores.values()))\n\n        logger.info(\"Running clustering - Finished.\")\n        mean_v_measure = np.mean(v_measures)\n        v_std = np.std(v_measures)\n        return {\n            \"v_measures\": all_v_scores,\n            \"v_measure\": float(mean_v_measure),\n            \"v_measure_std\": v_std,\n        }\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClusteringFastDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs = []\n            labels = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        if isinstance(inputs[0], list):\n            inputs = [item for sublist in inputs for item in sublist]\n        if isinstance(labels[0], list):\n            labels = [item for sublist in labels for item in sublist]\n\n        text_statistics, image_statistics = None, None\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n\n        return ClusteringFastDescriptiveStatistics(\n            num_samples=len(inputs),\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            labels_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name, [self.input_column_name, self.label_column_name]\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.sts.AbsTaskSTS","title":"<code>mteb.abstasks.sts.AbsTaskSTS</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for STS experiments.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Dataset or dict of Datasets for different subsets (e.g., languages). Dataset must contain columns specified in column_names and a 'score' column. Columns in column_names should contain the text or image data to be compared.</p> <code>column_names</code> <code>tuple[str, str]</code> <p>Tuple containing the names of the two columns to compare.</p> <code>min_score</code> <code>int</code> <p>Minimum possible score in the dataset.</p> <code>max_score</code> <code>int</code> <p>Maximum possible score in the dataset.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/sts.py</code> <pre><code>class AbsTaskSTS(AbsTask):\n    \"\"\"Abstract class for STS experiments.\n\n    Attributes:\n        dataset: Dataset or dict of Datasets for different subsets (e.g., languages). Dataset must contain columns specified in column_names and a 'score' column.\n            Columns in column_names should contain the text or image data to be compared.\n        column_names: Tuple containing the names of the two columns to compare.\n        min_score: Minimum possible score in the dataset.\n        max_score: Maximum possible score in the dataset.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    abstask_prompt = \"Retrieve semantically similar text.\"\n    column_names: tuple[str, str] = (\"sentence1\", \"sentence2\")\n    min_score: int = 0\n    max_score: int = 5\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; STSMetrics:\n        normalized_scores = list(map(self._normalize, data_split[\"score\"]))\n        data_split = data_split.select_columns(list(self.column_names))\n\n        evaluator = AnySTSEvaluator(\n            data_split,\n            self.column_names,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        scores = evaluator(model, encode_kwargs=encode_kwargs)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._calculate_scores(scores, normalized_scores)\n\n    def _calculate_scores(\n        self, scores: STSEvaluatorScores, normalized_scores: list[float]\n    ) -&gt; STSMetrics:\n        def compute_corr(x: list[float], y: list[float]) -&gt; tuple[float, float]:\n            \"\"\"Return (pearson, spearman) correlations between x and y.\"\"\"\n            return pearsonr(x, y)[0], spearmanr(x, y)[0]\n\n        cosine_pearson, cosine_spearman = compute_corr(\n            normalized_scores, scores[\"cosine_scores\"]\n        )\n        manhattan_pearson, manhattan_spearman = compute_corr(\n            normalized_scores, scores[\"manhattan_distances\"]\n        )\n        euclidean_pearson, euclidean_spearman = compute_corr(\n            normalized_scores, scores[\"euclidean_distances\"]\n        )\n\n        if scores[\"similarity_scores\"] is not None:\n            pearson, spearman = compute_corr(\n                normalized_scores, scores[\"similarity_scores\"]\n            )\n        else:\n            # if model does not have a similarity function, assume cosine similarity\n            pearson, spearman = cosine_pearson, cosine_spearman\n\n        return STSMetrics(\n            # using the models own similarity score\n            pearson=pearson,\n            spearman=spearman,\n            # generic similarity scores\n            cosine_pearson=cosine_pearson,\n            cosine_spearman=cosine_spearman,\n            manhattan_pearson=manhattan_pearson,\n            manhattan_spearman=manhattan_spearman,\n            euclidean_pearson=euclidean_pearson,\n            euclidean_spearman=euclidean_spearman,\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; AnySTSDescriptiveStatistics:\n        first_column, second_column = self.column_names\n        self.dataset = cast(dict[str, dict[str, Dataset]], self.dataset)\n\n        if hf_subset:\n            sentence1 = self.dataset[hf_subset][split][first_column]\n            sentence2 = self.dataset[hf_subset][split][second_column]\n            score = self.dataset[hf_subset][split][\"score\"]\n        elif compute_overall:\n            sentence1 = []\n            sentence2 = []\n            score = []\n            for hf_subset in self.metadata.eval_langs:\n                sentence1.extend(self.dataset[hf_subset][split][first_column])\n                sentence2.extend(self.dataset[hf_subset][split][second_column])\n                score.extend(self.dataset[hf_subset][split][\"score\"])\n        else:\n            sentence1 = self.dataset[split][first_column]\n            sentence2 = self.dataset[split][second_column]\n            score = self.dataset[split][\"score\"]\n\n        if \"text\" in self.metadata.modalities:\n            text1_statistics = calculate_text_statistics(sentence1)\n            text2_statistics = calculate_text_statistics(sentence2)\n\n            unique_pairs = len(set(zip(sentence1, sentence2)))\n        else:\n            text1_statistics = None\n            text2_statistics = None\n            unique_pairs = None\n\n        if \"image\" in self.metadata.modalities:\n            image1_statistics = calculate_image_statistics(sentence1)\n            image2_statistics = calculate_image_statistics(sentence2)\n        else:\n            image1_statistics = None\n            image2_statistics = None\n\n        labels_statistics = calculate_score_statistics(score)\n\n        return AnySTSDescriptiveStatistics(\n            num_samples=len(sentence1),\n            number_of_characters=(\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n                if text1_statistics\n                else None\n            ),\n            unique_pairs=unique_pairs,\n            text1_statistics=text1_statistics,\n            text2_statistics=text2_statistics,\n            image1_statistics=image1_statistics,\n            image2_statistics=image2_statistics,\n            label_statistics=labels_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name, [self.column_names[0], self.column_names[1], \"score\"]\n        )\n\n    def _normalize(self, x: float) -&gt; float:\n        return (x - self.min_score) / (self.max_score - self.min_score)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification","title":"<code>mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for ZeroShot Classification tasks for any modality.</p> <p>The similarity between an input (can be image or text) and candidate text prompts, such as this is a dog/this is a cat.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Huggingface dataset containing the data for the task. Dataset must contain columns specified by self.input_column_name and self.label_column_name.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the inputs (image or text).</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels (str).</p> Source code in <code>mteb/abstasks/zeroshot_classification.py</code> <pre><code>class AbsTaskZeroShotClassification(AbsTask):\n    \"\"\"Abstract class for ZeroShot Classification tasks for any modality.\n\n    The similarity between an input (can be image or text) and candidate text prompts, such as this is a dog/this is a cat.\n\n    Attributes:\n        dataset: Huggingface dataset containing the data for the task. Dataset must contain columns specified by self.input_column_name and self.label_column_name.\n        input_column_name: Name of the column containing the inputs (image or text).\n        label_column_name: Name of the column containing the labels (str).\n    \"\"\"\n\n    input_column_name: str = \"image\"\n    label_column_name: str = \"label\"\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ZeroShotClassificationDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs, labels = [], []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        num_samples = len(inputs)\n\n        image_statistics = None\n        text_statistics = None\n\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n        if self.metadata.modalities == [\"text\"]:\n            text_statistics = calculate_text_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n        candidate_lens = calculate_text_statistics(self.get_candidate_labels())\n\n        return ZeroShotClassificationDescriptiveStatistics(\n            num_samples=num_samples,\n            number_of_characters=None,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            label_statistics=label_statistics,\n            candidates_labels_text_statistics=candidate_lens,\n        )\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; ZeroShotClassificationMetrics:\n        candidate_labels = self.get_candidate_labels()\n        data_split = data_split.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n        evaluator = ZeroShotClassificationEvaluator(\n            data_split,\n            self.input_column_name,\n            candidate_labels,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        probs = evaluator(model, encode_kwargs=encode_kwargs)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                probs.tolist(),\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._calculate_scores(\n            data_split[self.label_column_name],\n            torch.tensor(probs).argmax(dim=1).tolist(),\n        )\n\n    def _calculate_scores(\n        self,\n        labels: list[int],\n        predictions: list[float],\n    ) -&gt; ZeroShotClassificationMetrics:\n        return ZeroShotClassificationMetrics(\n            accuracy=metrics.accuracy_score(labels, predictions),\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n        )\n        labels_dataset = Dataset.from_dict({\"labels\": self.get_candidate_labels()})\n        labels_dataset.push_to_hub(repo_name, config_name=\"labels\")\n\n    def get_candidate_labels(self) -&gt; list[str]:\n        \"\"\"Return the text candidates for zeroshot classification\"\"\"\n        raise NotImplementedError(\"This method should be overridden by subclasses\")\n</code></pre>"},{"location":"api/task/#mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification.get_candidate_labels","title":"<code>get_candidate_labels()</code>","text":"<p>Return the text candidates for zeroshot classification</p> Source code in <code>mteb/abstasks/zeroshot_classification.py</code> <pre><code>def get_candidate_labels(self) -&gt; list[str]:\n    \"\"\"Return the text candidates for zeroshot classification\"\"\"\n    raise NotImplementedError(\"This method should be overridden by subclasses\")\n</code></pre>"},{"location":"api/task/#mteb.abstasks.regression.AbsTaskRegression","title":"<code>mteb.abstasks.regression.AbsTaskRegression</code>","text":"<p>               Bases: <code>AbsTaskClassification</code></p> <p>Abstract class for regression tasks</p> <p>self.load_data() must generate a huggingface dataset with a split matching self.metadata.eval_splits, and assign it to self.dataset. It must contain the following columns:     text: str     value: float</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the regression task. It must contain the following columns: input_column_name and label_column_name. Input can be any text or images, and label must be a continuous value.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the text inputs.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the continuous values.</p> <code>train_split</code> <code>str</code> <p>Name of the training split in the dataset.</p> <code>n_experiments</code> <code>int</code> <p>Number of experiments to run with different random seeds.</p> <code>n_samples</code> <code>int</code> <p>Number of samples to use for training the regression model. If the dataset has fewer samples than n_samples, all samples are used.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>evaluator_model</code> <code>SklearnModelProtocol</code> <p>The model to use for evaluation. Can be any sklearn compatible model. Default is <code>LinearRegression</code>. Full details of api in [<code>SklearnModelProtocol</code>][mteb._evaluators.sklearn_evaluator.SklearnModelProtocol].</p> Source code in <code>mteb/abstasks/regression.py</code> <pre><code>class AbsTaskRegression(AbsTaskClassification):\n    \"\"\"Abstract class for regression tasks\n\n    self.load_data() must generate a huggingface dataset with a split matching self.metadata.eval_splits, and assign it to self.dataset. It\n    must contain the following columns:\n        text: str\n        value: float\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the regression task. It must contain the following columns: input_column_name and label_column_name.\n            Input can be any text or images, and label must be a continuous value.\n        input_column_name: Name of the column containing the text inputs.\n        label_column_name: Name of the column containing the continuous values.\n        train_split: Name of the training split in the dataset.\n        n_experiments: Number of experiments to run with different random seeds.\n        n_samples: Number of samples to use for training the regression model. If the dataset has fewer samples than n_samples, all samples are used.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        evaluator_model: The model to use for evaluation. Can be any sklearn compatible model. Default is `LinearRegression`.\n            Full details of api in [`SklearnModelProtocol`][mteb._evaluators.sklearn_evaluator.SklearnModelProtocol].\n    \"\"\"\n\n    evaluator: type[SklearnModelProtocol] = SklearnEvaluator\n    evaluator_model: SklearnModelProtocol = LinearRegression(n_jobs=-1)\n\n    train_split: str = \"train\"\n    label_column_name: str = \"value\"\n    input_column_name: str = \"text\"\n    abstask_prompt = \"Predict the value of the user passage.\"\n\n    n_experiments: int = 10\n    n_samples: int = 2048\n\n    def _undersample_data(\n        self, dataset: Dataset, experiment_num: int, idxs: list[int] | None = None\n    ) -&gt; tuple[Dataset, list[int]]:\n        if self.n_samples &gt;= len(dataset):\n            train_split_sampled = dataset\n        else:\n            train_split_sampled = self.stratified_subsampling(\n                datasets.DatasetDict({\"train\": dataset}),\n                seed=self.seed + experiment_num,\n                splits=[\"train\"],\n                label=self.label_column_name,\n                n_samples=self.n_samples,\n            )[\"train\"]\n        return train_split_sampled, []\n\n    def _calculate_scores(\n        self,\n        y_test: np.ndarray | list[int],\n        y_pred: np.ndarray,\n    ) -&gt; RegressionMetrics:\n        mse = mean_squared_error(y_test, y_pred)\n        return RegressionMetrics(\n            mse=mse,\n            mae=mean_absolute_error(y_test, y_pred),\n            r2=r2_score(y_test, y_pred),\n            kendalltau=kendalltau(y_test, y_pred).statistic,\n            rmse=np.sqrt(mse),\n        )\n\n    @staticmethod\n    def stratified_subsampling(\n        dataset_dict: datasets.DatasetDict,\n        seed: int,\n        splits: list[str] = [\"test\"],\n        label: str = \"value\",\n        n_samples: int = 2048,\n        n_bins: int = 10,\n    ) -&gt; datasets.DatasetDict:\n        \"\"\"Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.\n\n        The continuous values are bucketized into `n_bins` bins based on quantiles.\n\n        Args:\n            dataset_dict: the DatasetDict object.\n            seed: the random seed.\n            splits: the splits of the dataset.\n            label: the label with which the stratified sampling is based on.\n            n_samples: Optional, number of samples to subsample.\n            n_bins: Optional, number of bins to bucketize the continuous label.\n\n        Returns:\n            A subsampled DatasetDict object.\n        \"\"\"\n        stratify_col_name = f\"{label}_binned_for_stratification\"\n\n        for split in splits:\n            if n_samples &gt;= len(dataset_dict[split]):\n                logger.debug(\n                    \"Subsampling not needed for split %s, as n_samples is equal or greater than the number of samples.\",\n                    split,\n                )\n                continue\n\n            dataset = dataset_dict[split]\n            labels = dataset[label]\n\n            binned_labels = pd.qcut(labels, q=n_bins, labels=False, duplicates=\"drop\")\n            dataset_with_bins: datasets.Dataset = dataset.add_column(\n                name=stratify_col_name,\n                column=binned_labels.tolist(),\n            )\n            dataset_with_bins = dataset_with_bins.cast_column(\n                stratify_col_name,\n                datasets.ClassLabel(names=np.unique(binned_labels).tolist()),\n            )\n\n            subsampled_dataset = dataset_with_bins.train_test_split(\n                test_size=n_samples, seed=seed, stratify_by_column=stratify_col_name\n            )[\"test\"]\n\n            subsampled_dataset = subsampled_dataset.remove_columns([stratify_col_name])\n            dataset_dict[split] = subsampled_dataset\n\n        return dataset_dict\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; RegressionDescriptiveStatistics:\n        train_text = []\n        if hf_subset:\n            texts = self.dataset[hf_subset][split][self.input_column_name]\n            values = self.dataset[hf_subset][split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[hf_subset][self.train_split][\n                    self.input_column_name\n                ]\n        elif compute_overall:\n            texts = []\n            values = []\n            for lang_subset in self.metadata.eval_langs:\n                texts.extend(self.dataset[lang_subset][split][self.input_column_name])\n                values.extend(self.dataset[lang_subset][split][self.label_column_name])\n                if split != \"train\":\n                    train_text.extend(\n                        self.dataset[lang_subset][self.train_split][\n                            self.input_column_name\n                        ]\n                    )\n        else:\n            texts = self.dataset[split][self.input_column_name]\n            values = self.dataset[split][self.label_column_name]\n            if split != \"train\":\n                train_text = self.dataset[self.train_split][self.input_column_name]\n\n        text_statistics = None\n        image_statistics = None\n        num_texts_in_train = None\n        if self.metadata.modalities == [\"text\"]:\n            text_statistics = calculate_text_statistics(texts)\n            num_texts_in_train = (\n                len(set(texts) &amp; set(train_text)) if split != self.train_split else None\n            )\n        elif self.metadata.modalities == [\"image\"]:\n            image_statistics = calculate_image_statistics(texts)\n\n        return RegressionDescriptiveStatistics(\n            num_samples=len(texts),\n            num_texts_in_train=num_texts_in_train,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            values_statistics=calculate_score_statistics(values),\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.regression.AbsTaskRegression.stratified_subsampling","title":"<code>stratified_subsampling(dataset_dict, seed, splits=['test'], label='value', n_samples=2048, n_bins=10)</code>  <code>staticmethod</code>","text":"<p>Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.</p> <p>The continuous values are bucketized into <code>n_bins</code> bins based on quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DatasetDict</code> <p>the DatasetDict object.</p> required <code>seed</code> <code>int</code> <p>the random seed.</p> required <code>splits</code> <code>list[str]</code> <p>the splits of the dataset.</p> <code>['test']</code> <code>label</code> <code>str</code> <p>the label with which the stratified sampling is based on.</p> <code>'value'</code> <code>n_samples</code> <code>int</code> <p>Optional, number of samples to subsample.</p> <code>2048</code> <code>n_bins</code> <code>int</code> <p>Optional, number of bins to bucketize the continuous label.</p> <code>10</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A subsampled DatasetDict object.</p> Source code in <code>mteb/abstasks/regression.py</code> <pre><code>@staticmethod\ndef stratified_subsampling(\n    dataset_dict: datasets.DatasetDict,\n    seed: int,\n    splits: list[str] = [\"test\"],\n    label: str = \"value\",\n    n_samples: int = 2048,\n    n_bins: int = 10,\n) -&gt; datasets.DatasetDict:\n    \"\"\"Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.\n\n    The continuous values are bucketized into `n_bins` bins based on quantiles.\n\n    Args:\n        dataset_dict: the DatasetDict object.\n        seed: the random seed.\n        splits: the splits of the dataset.\n        label: the label with which the stratified sampling is based on.\n        n_samples: Optional, number of samples to subsample.\n        n_bins: Optional, number of bins to bucketize the continuous label.\n\n    Returns:\n        A subsampled DatasetDict object.\n    \"\"\"\n    stratify_col_name = f\"{label}_binned_for_stratification\"\n\n    for split in splits:\n        if n_samples &gt;= len(dataset_dict[split]):\n            logger.debug(\n                \"Subsampling not needed for split %s, as n_samples is equal or greater than the number of samples.\",\n                split,\n            )\n            continue\n\n        dataset = dataset_dict[split]\n        labels = dataset[label]\n\n        binned_labels = pd.qcut(labels, q=n_bins, labels=False, duplicates=\"drop\")\n        dataset_with_bins: datasets.Dataset = dataset.add_column(\n            name=stratify_col_name,\n            column=binned_labels.tolist(),\n        )\n        dataset_with_bins = dataset_with_bins.cast_column(\n            stratify_col_name,\n            datasets.ClassLabel(names=np.unique(binned_labels).tolist()),\n        )\n\n        subsampled_dataset = dataset_with_bins.train_test_split(\n            test_size=n_samples, seed=seed, stratify_by_column=stratify_col_name\n        )[\"test\"]\n\n        subsampled_dataset = subsampled_dataset.remove_columns([stratify_col_name])\n        dataset_dict[split] = subsampled_dataset\n\n    return dataset_dict\n</code></pre>"},{"location":"api/task/#mteb.abstasks.clustering_legacy.AbsTaskClusteringLegacy","title":"<code>mteb.abstasks.clustering_legacy.AbsTaskClusteringLegacy</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Legacy abstract task for clustering. For new tasks, we recommend using AbsTaskClustering because it is faster, more sample-efficient, and produces more robust statistical estimates.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the clustering task. It must contain the following columns: sentences: List of inputs to be clustered. Can be text, images, etc. Name can be changed via <code>input_column_name</code>. labels: List of integer labels representing the true cluster assignments. Name can be changed via <code>label_column_name</code>.</p> <code>input_column_name</code> <code>str</code> <p>The name of the column in the dataset that contains the input sentences or data points.</p> <code>label_column_name</code> <code>str</code> <p>The name of the column in the dataset that contains the true cluster labels.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/clustering_legacy.py</code> <pre><code>class AbsTaskClusteringLegacy(AbsTask):\n    \"\"\"Legacy abstract task for clustering. For new tasks, we recommend using AbsTaskClustering because it is faster, more sample-efficient, and produces more robust statistical estimates.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the clustering task. It must contain the following columns:\n            sentences: List of inputs to be clustered. Can be text, images, etc. Name can be changed via `input_column_name`.\n            labels: List of integer labels representing the true cluster assignments. Name can be changed via `label_column_name`.\n        input_column_name: The name of the column in the dataset that contains the input sentences or data points.\n        label_column_name: The name of the column in the dataset that contains the true cluster labels.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    abstask_prompt = \"Identify categories in user passages.\"\n    evaluator: type[ClusteringEvaluator] = ClusteringEvaluator\n    input_column_name: str = \"sentences\"\n    label_column_name: str = \"labels\"\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        # MTEB text clustering requires renaming and eval per subset.\n        if self.metadata.modalities == [\"text\"]:\n            all_metrics = []\n            clusters = []\n            for i, cluster_set in enumerate(data_split):\n                logger.info(\n                    f\"Running clustering on cluster ({i + 1}/{len(data_split)})\"\n                )\n                clustering_dataset = Dataset.from_dict(cluster_set).select_columns(\n                    [self.input_column_name, self.label_column_name]\n                )\n                evaluator = self.evaluator(\n                    clustering_dataset,\n                    input_column_name=self.input_column_name,\n                    label_column_name=self.label_column_name,\n                    task_metadata=self.metadata,\n                    hf_split=hf_split,\n                    hf_subset=hf_subset,\n                    **kwargs,\n                )\n                clusters_assignment = evaluator(model, encode_kwargs=encode_kwargs)\n                clusters.append(clusters_assignment)\n                set_metrics = self._compute_metrics(\n                    clustering_dataset[self.label_column_name],\n                    clusters_assignment,\n                    v_measure_only=True,\n                )\n                all_metrics.append(set_metrics)\n\n            if prediction_folder:\n                self._save_task_predictions(\n                    clusters,\n                    model,\n                    prediction_folder,\n                    hf_subset=hf_subset,\n                    hf_split=hf_split,\n                )\n            v_measures = [m[\"v_measure\"] for m in all_metrics]\n            v_mean = np.mean(v_measures)\n            v_std = np.std(v_measures)\n            scores = {\n                \"v_measure\": v_mean,\n                \"v_measure_std\": v_std,\n                \"v_measures\": v_measures,\n            }\n            return scores\n\n        data_split = data_split.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n        evaluator = self.evaluator(\n            data_split,\n            input_column_name=self.input_column_name,\n            label_column_name=self.label_column_name,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        clusters = evaluator(model, encode_kwargs=encode_kwargs)\n        if prediction_folder:\n            self._save_task_predictions(\n                clusters,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._compute_metrics(\n            data_split[self.label_column_name],\n            clusters,\n        )\n\n    def _compute_metrics(\n        self,\n        labels: list[int],\n        cluster_assignment: list[int],\n        v_measure_only: bool = False,\n    ) -&gt; ClusteringMetrics:\n        logger.info(\"Running clustering - Evaluating clustering...\")\n        v_measure = metrics.cluster.v_measure_score(labels, cluster_assignment)\n        if v_measure_only:\n            return ClusteringMetrics(\n                v_measure=v_measure,\n            )\n        nmi = metrics.cluster.normalized_mutual_info_score(labels, cluster_assignment)\n        ari = metrics.cluster.adjusted_rand_score(labels, cluster_assignment)\n\n        matrix = metrics.confusion_matrix(labels, cluster_assignment)\n        # get linear sum assignment\n        row_ind, col_ind = linear_sum_assignment(matrix, maximize=True)\n        total_correct = matrix[row_ind, col_ind].sum()\n        clustering_accuracy = total_correct / len(labels)\n        return ClusteringMetrics(\n            v_measure=float(v_measure),\n            nmi=float(nmi),\n            ari=float(ari),\n            cluster_accuracy=float(clustering_accuracy),\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClusteringDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs = []\n            labels = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        if isinstance(inputs[0], list):\n            inputs = [item for sublist in inputs for item in sublist]\n        if isinstance(labels[0], list):\n            labels = [item for sublist in labels for item in sublist]\n\n        text_statistics, image_statistics = None, None\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n\n        return ClusteringDescriptiveStatistics(\n            num_samples=len(inputs),\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            label_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n        )\n</code></pre>"},{"location":"api/task/#text-tasks","title":"Text Tasks","text":""},{"location":"api/task/#mteb.abstasks.text.bitext_mining.AbsTaskBitextMining","title":"<code>mteb.abstasks.text.bitext_mining.AbsTaskBitextMining</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for BitextMining tasks</p> <p>The similarity is computed between pairs and the results are ranked.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace dataset containing the data for the task. It must contain the following columns sentence1 and sentence2 for the two texts to be compared.</p> <code>parallel_subsets</code> <p>If true task language pairs should be in one split as column names, otherwise each language pair should be a subset.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/text/bitext_mining.py</code> <pre><code>class AbsTaskBitextMining(AbsTask):\n    \"\"\"Abstract class for BitextMining tasks\n\n    The similarity is computed between pairs and the results are ranked.\n\n    Attributes:\n        dataset: A HuggingFace dataset containing the data for the task. It must contain the following columns sentence1 and sentence2 for the two texts to be compared.\n        parallel_subsets: If true task language pairs should be in one split as column names, otherwise each language pair should be a subset.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    parallel_subsets = False\n    abstask_prompt = \"Retrieve parallel sentences.\"\n    _DEFAULT_PAIR: ClassVar[list[tuple[str, str]]] = [(\"sentence1\", \"sentence2\")]\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Added load for \"parallel\" datasets\"\"\"\n        if not self.data_loaded:\n            self.load_data()\n\n        hf_subsets = self.hf_subsets\n\n        # If subsets_to_run is specified, filter the hf_subsets accordingly\n        if subsets_to_run is not None:\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        scores = {}\n        if self.parallel_subsets:\n            scores = self._evaluate_subset(\n                model,\n                self.dataset[split],  # type: ignore\n                parallel=True,\n                hf_split=split,\n                hf_subset=\"parallel\",\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                **kwargs,\n            )\n        else:\n            for hf_subset in hf_subsets:\n                logger.info(\n                    f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n                )\n\n                if hf_subset not in self.dataset and hf_subset == \"default\":\n                    data_split = self.dataset[split]\n                else:\n                    data_split = self.dataset[hf_subset][split]\n                scores[hf_subset] = self._evaluate_subset(\n                    model,\n                    data_split,\n                    hf_split=split,\n                    hf_subset=hf_subset,\n                    encode_kwargs=encode_kwargs,\n                    prediction_folder=prediction_folder,\n                    **kwargs,\n                )\n\n        return scores\n\n    def _get_pairs(self, parallel: bool) -&gt; list[tuple[str, str]]:\n        pairs = self._DEFAULT_PAIR\n        if parallel:\n            pairs = [langpair.split(\"-\") for langpair in self.hf_subsets]\n        return pairs\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        parallel: bool = False,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; ScoresDict:\n        pairs = self._get_pairs(parallel)\n\n        evaluator = BitextMiningEvaluator(\n            data_split,\n            task_metadata=self.metadata,\n            pair_columns=pairs,  # type: ignore\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        # NOTE: used only by BUCC\n        gold = (\n            list(zip(range(len(data_split)), range(len(data_split))))\n            if \"gold\" not in data_split\n            else data_split[\"gold\"]\n        )\n\n        neighbours = evaluator(model, encode_kwargs=encode_kwargs)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                neighbours,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        if parallel:\n            metrics = {}\n            for keys, nearest_neighbors in neighbours.items():\n                metrics[keys] = self._compute_metrics(nearest_neighbors, gold)\n\n            for v in metrics.values():\n                self._add_main_score(v)\n        else:\n            def_pair_str = \"-\".join(self._DEFAULT_PAIR[0])\n            metrics = self._compute_metrics(neighbours[def_pair_str], gold)\n            self._add_main_score(metrics)\n        return metrics\n\n    def _compute_metrics(\n        self,\n        nearest_neighbors: list[dict[str, float]],\n        gold: list[tuple[int, int]],\n    ) -&gt; BitextMiningMetrics:\n        logger.info(\"Computing metrics...\")\n        labels = []\n        predictions = []\n        for i, x in enumerate(nearest_neighbors):\n            j = x[\"corpus_id\"]\n            predictions.append(j)\n            labels.append(gold[i][1])\n\n        return BitextMiningMetrics(\n            precision=precision_score(\n                labels, predictions, zero_division=0, average=\"weighted\"\n            ),\n            recall=recall_score(\n                labels, predictions, zero_division=0, average=\"weighted\"\n            ),\n            f1=f1_score(labels, predictions, zero_division=0, average=\"weighted\"),\n            accuracy=accuracy_score(labels, predictions),\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; BitextDescriptiveStatistics:\n        pairs_cols = self._get_pairs(self.parallel_subsets)\n        if hf_subset:\n            if self.parallel_subsets:\n                sent_1, sent_2 = hf_subset.split(\"-\")\n                sentence1 = self.dataset[split][sent_1]\n                sentence2 = self.dataset[split][sent_2]\n            else:\n                sent_1, sent_2 = pairs_cols[0]\n                sentence1 = self.dataset[hf_subset][split][sent_1]\n                sentence2 = self.dataset[hf_subset][split][sent_2]\n        elif compute_overall:\n            sentence1, sentence2 = [], []\n            if self.parallel_subsets:\n                for hf_subset in self.metadata.eval_langs:\n                    sent_1, sent_2 = hf_subset.split(\"-\")\n                    sentence1.extend(self.dataset[split][sent_1])\n                    sentence2.extend(self.dataset[split][sent_2])\n            else:\n                sent_1, sent_2 = pairs_cols[0]\n                for hf_subset in self.metadata.eval_langs:\n                    sentence1.extend(self.dataset[hf_subset][split][sent_1])\n                    sentence2.extend(self.dataset[hf_subset][split][sent_2])\n        else:\n            sent_1, sent_2 = pairs_cols[0]\n            sentence1 = self.dataset[split][sent_1]\n            sentence2 = self.dataset[split][sent_2]\n\n        text1_statistics = calculate_text_statistics(sentence1)\n        text2_statistics = calculate_text_statistics(sentence2)\n        unique_pairs = len(set(zip(sentence1, sentence2)))\n\n        return BitextDescriptiveStatistics(\n            num_samples=len(sentence1),\n            number_of_characters=(\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n            ),\n            unique_pairs=unique_pairs,\n            sentence1_statistics=text1_statistics,\n            sentence2_statistics=text2_statistics,\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        if self.metadata.is_multilingual:\n            dataset = defaultdict(dict)\n            for config in self.metadata.eval_langs:\n                logger.info(f\"Converting {config} of {self.metadata.name}\")\n\n                if self.parallel_subsets:\n                    for split in self.dataset:\n                        sent_1, sent_2 = config.split(\"-\")\n                        dataset[split][sent_1] = self.dataset[split][sent_1]\n                        dataset[split][sent_2] = self.dataset[split][sent_2]\n                else:\n                    sent_1, sent_2 = self._get_pairs(self.parallel_subsets)[0]\n                    lang_1, lang_2 = config.split(\"-\")\n                    for split in self.dataset[config]:\n                        dataset[split][lang_1] = self.dataset[config][split][sent_1]\n                        dataset[split][lang_2] = self.dataset[config][split][sent_2]\n            for split in dataset:\n                dataset[split] = Dataset.from_dict(dataset[split])\n            dataset = DatasetDict(dataset)\n            dataset.push_to_hub(repo_name)\n        else:\n            sentences = {}\n            for split in self.dataset:\n                sent_1, sent_2 = self._get_pairs(self.parallel_subsets)[0]\n                sentences[split] = Dataset.from_dict(\n                    {\n                        \"sentence1\": self.dataset[split][sent_1],\n                        \"sentence2\": self.dataset[split][sent_2],\n                    }\n                )\n            sentences = DatasetDict(sentences)\n            sentences.push_to_hub(repo_name)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.bitext_mining.AbsTaskBitextMining.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, **kwargs)</code>","text":"<p>Added load for \"parallel\" datasets</p> Source code in <code>mteb/abstasks/text/bitext_mining.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: dict[str, Any],\n    prediction_folder: Path | None = None,\n    **kwargs: Any,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Added load for \"parallel\" datasets\"\"\"\n    if not self.data_loaded:\n        self.load_data()\n\n    hf_subsets = self.hf_subsets\n\n    # If subsets_to_run is specified, filter the hf_subsets accordingly\n    if subsets_to_run is not None:\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    scores = {}\n    if self.parallel_subsets:\n        scores = self._evaluate_subset(\n            model,\n            self.dataset[split],  # type: ignore\n            parallel=True,\n            hf_split=split,\n            hf_subset=\"parallel\",\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            **kwargs,\n        )\n    else:\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n            )\n\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                data_split = self.dataset[split]\n            else:\n                data_split = self.dataset[hf_subset][split]\n            scores[hf_subset] = self._evaluate_subset(\n                model,\n                data_split,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                **kwargs,\n            )\n\n    return scores\n</code></pre>"},{"location":"api/task/#mteb.abstasks.pair_classification.AbsTaskPairClassification","title":"<code>mteb.abstasks.pair_classification.AbsTaskPairClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for PairClassificationTasks</p> <p>The similarity is computed between pairs and the results are ranked. Average precision is computed to measure how well the methods can be used for pairwise pair classification.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace dataset containing the data for the task. Should contain the following columns: sentence1, sentence2, labels.</p> <code>input1_column_name</code> <code>str</code> <p>The name of the column containing the first sentence in the pair.</p> <code>input2_column_name</code> <code>str</code> <p>The name of the column containing the second sentence in the pair.</p> <code>label_column_name</code> <code>str</code> <p>The name of the column containing the labels for the pairs. Labels should be 0 or 1.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/pair_classification.py</code> <pre><code>class AbsTaskPairClassification(AbsTask):\n    \"\"\"Abstract class for PairClassificationTasks\n\n    The similarity is computed between pairs and the results are ranked. Average precision\n    is computed to measure how well the methods can be used for pairwise pair classification.\n\n    Attributes:\n        dataset: A HuggingFace dataset containing the data for the task. Should contain the following columns: sentence1, sentence2, labels.\n        input1_column_name: The name of the column containing the first sentence in the pair.\n        input2_column_name: The name of the column containing the second sentence in the pair.\n        label_column_name: The name of the column containing the labels for the pairs. Labels should be 0 or 1.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    abstask_prompt = \"Retrieve text that are semantically similar to the given text.\"\n    input1_column_name: str = \"sentence1\"\n    input2_column_name: str = \"sentence2\"\n    label_column_name: str = \"labels\"\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, str],\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; dict[str, float]:\n        if self.metadata.modalities == [\"text\"]:\n            # for compatibility with v1 version where datasets were stored in a single row\n            data_split = data_split[0] if len(data_split) == 1 else data_split\n        evaluator = PairClassificationEvaluator(\n            data_split,\n            self.input1_column_name,\n            self.input2_column_name,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        similarity_scores = evaluator(model, encode_kwargs=encode_kwargs)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                similarity_scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n        return self._compute_metrics(\n            similarity_scores, data_split[self.label_column_name]\n        )\n\n    def _compute_metrics(\n        self, similarity_scores: PairClassificationDistances, labels: list[int]\n    ) -&gt; dict[str, float]:\n        logger.info(\"Computing metrics...\")\n        labels = np.asarray(labels)\n        output_scores = {}\n        max_scores = defaultdict(list)\n        for short_name, scores, reverse in [\n            [\n                \"similarity\",\n                similarity_scores[\"similarity_scores\"],\n                True,\n            ],\n            [ScoringFunction.COSINE.value, similarity_scores[\"cosine_scores\"], True],\n            [\n                ScoringFunction.MANHATTAN.value,\n                similarity_scores[\"manhattan_distances\"],\n                False,\n            ],\n            [\n                ScoringFunction.EUCLIDEAN.value,\n                similarity_scores[\"euclidean_distances\"],\n                False,\n            ],\n            [ScoringFunction.DOT_PRODUCT.value, similarity_scores[\"dot_scores\"], True],\n        ]:\n            metrics = self._compute_metrics_values(scores, labels, reverse)\n            for metric_name, metric_value in metrics.items():\n                output_scores[f\"{short_name}_{metric_name}\"] = metric_value\n                max_scores[metric_name].append(metric_value)\n\n        for metric in max_scores:\n            if metric in [\"f1\", \"ap\", \"precision\", \"recall\", \"accuracy\"]:\n                output_scores[f\"max_{metric}\"] = max(max_scores[metric])\n        return output_scores\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; PairClassificationDescriptiveStatistics:\n        if hf_subset:\n            dataset = self.dataset[hf_subset][split]\n        elif compute_overall:\n            dataset = defaultdict(list)\n            for hf_subset in self.metadata.eval_langs:\n                cur_dataset = self.dataset[hf_subset][split]\n                # for compatibility with v1 version where datasets were stored in a single row\n                if isinstance(cur_dataset, list) or len(cur_dataset) == 1:\n                    cur_dataset = cur_dataset[0]\n                if isinstance(cur_dataset, Dataset):\n                    for row in cur_dataset:\n                        for k, v in row.items():\n                            dataset[k].append(v)\n                else:\n                    for key, value in cur_dataset.items():\n                        dataset[key].extend(value[0] if len(value) == 1 else value)\n        else:\n            dataset = self.dataset[split]\n\n        if isinstance(dataset, list):\n            dataset = dataset[0]\n\n        input1 = (\n            dataset[self.input1_column_name][0]\n            if len(dataset[self.input1_column_name]) == 1\n            else dataset[self.input1_column_name]\n        )\n        input2 = (\n            dataset[self.input2_column_name][0]\n            if len(dataset[self.input2_column_name]) == 1\n            else dataset[self.input2_column_name]\n        )\n        labels = (\n            dataset[self.label_column_name][0]\n            if len(dataset[self.label_column_name]) == 1\n            else dataset[self.label_column_name]\n        )\n\n        text1_statistics = None\n        text2_statistics = None\n        image1_statistics = None\n        image2_statistics = None\n        number_of_characters = None\n        unique_pairs = None\n        if self.metadata.modalities == [\"text\"]:\n            text1_statistics = calculate_text_statistics(input1)\n            text2_statistics = calculate_text_statistics(input2)\n            number_of_characters = (\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n            )\n            unique_pairs = len(set(zip(input1, input2)))\n\n        elif self.metadata.modalities == [\"image\"]:\n            image1_statistics = calculate_image_statistics(input1)\n            image2_statistics = calculate_image_statistics(input2)\n\n            def _compute_image_hash(inputs: list) -&gt; list[str]:\n                hashes = set()\n                for img in inputs:\n                    img_bytes = img.tobytes()\n                    img_hash = hashlib.md5(img_bytes).hexdigest()\n                    hashes.add(img_hash)\n                return list(hashes)\n\n            image_1_hashes = _compute_image_hash(input1)\n            image_2_hashes = _compute_image_hash(input2)\n            unique_pairs = len(set(zip(image_1_hashes, image_2_hashes)))\n\n        return PairClassificationDescriptiveStatistics(\n            num_samples=len(input1),\n            unique_pairs=unique_pairs,\n            number_of_characters=number_of_characters,\n            text1_statistics=text1_statistics,\n            image1_statistics=image1_statistics,\n            text2_statistics=text2_statistics,\n            image2_statistics=image2_statistics,\n            labels_statistics=calculate_label_statistics(labels),\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        # previously pair classification datasets were stored in a single row\n        if self.metadata.is_multilingual:\n            for subset in self.dataset:\n                for split in self.dataset[subset]:\n                    if len(self.dataset[subset][split]) == 1:\n                        self.dataset[subset][split] = self.dataset[subset][split][0]\n        else:\n            for split in self.dataset:\n                if len(self.dataset[split]) == 1:\n                    self.dataset[split] = self.dataset[split][0]\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input1_column_name,\n                self.input2_column_name,\n                self.label_column_name,\n            ],\n        )\n\n    def _compute_metrics_values(\n        self, scores: list[float], labels: np.ndarray, high_score_more_similar: bool\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics for the given scores and labels.\n\n        Args:\n            scores: The similarity/dissimilarity scores for the pairs, specified as an array of shape (n_pairs, ).\n            labels: The labels for the pairs, specified as an array of shape (n_pairs, ).\n            high_score_more_similar: If true, then the higher the score, the more similar the pairs are.\n\n        Returns:\n            The metrics for the given scores and labels.\n        \"\"\"\n        acc, acc_threshold = self._find_best_acc_and_threshold(\n            scores, labels, high_score_more_similar\n        )\n        (\n            f1,\n            precision,\n            recall,\n            f1_threshold,\n        ) = self._find_best_f1_and_threshold(scores, labels, high_score_more_similar)\n        ap = average_precision_score(\n            labels, np.array(scores) * (1 if high_score_more_similar else -1)\n        )\n\n        return dict(\n            accuracy=float(acc),\n            f1=float(f1),\n            precision=float(precision),\n            recall=float(recall),\n            ap=float(ap),\n        )\n\n    def _find_best_acc_and_threshold(\n        self, scores: np.ndarray, labels: np.ndarray, high_score_more_similar: bool\n    ) -&gt; tuple[float, float]:\n        rows = list(zip(scores, labels))\n        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n\n        max_acc = 0\n        best_threshold = -1\n        positive_so_far = 0\n        remaining_negatives = sum(np.array(labels) == 0)\n\n        for i in range(len(rows) - 1):\n            score, label = rows[i]\n            if label == 1:\n                positive_so_far += 1\n            else:\n                remaining_negatives -= 1\n\n            acc = (positive_so_far + remaining_negatives) / len(labels)\n            if acc &gt; max_acc:\n                max_acc = acc\n                best_threshold = (rows[i][0] + rows[i + 1][0]) / 2\n        return max_acc, best_threshold\n\n    def _find_best_f1_and_threshold(\n        self, scores, labels, high_score_more_similar: bool\n    ) -&gt; tuple[float, float, float, float]:\n        scores = np.asarray(scores)\n        labels = np.asarray(labels)\n\n        rows = list(zip(scores, labels))\n\n        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n\n        best_f1 = best_precision = best_recall = 0\n        threshold = 0\n        nextract = 0\n        ncorrect = 0\n        total_num_duplicates = sum(labels)\n\n        for i in range(len(rows) - 1):\n            score, label = rows[i]\n            nextract += 1\n\n            if label == 1:\n                ncorrect += 1\n\n            if ncorrect &gt; 0:\n                precision = ncorrect / nextract\n                recall = ncorrect / total_num_duplicates\n                f1 = 2 * precision * recall / (precision + recall)\n                if f1 &gt; best_f1:\n                    best_f1 = f1\n                    best_precision = precision\n                    best_recall = recall\n                    threshold = (rows[i][0] + rows[i + 1][0]) / 2\n\n        return best_f1, best_precision, best_recall, threshold\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.summarization.AbsTaskSummarization","title":"<code>mteb.abstasks.text.summarization.AbsTaskSummarization</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for summarization experiments.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>HuggingFace dataset containing the data for the task. Should have columns: - text: The original text to be summarized. - human_summaries: A list of human-written summaries for the text. - machine_summaries: A list of machine-generated summaries for the text. - relevance: A list of relevance scores (integers) corresponding to each machine summary, indicating how relevant each summary is to the original text.</p> <code>min_score</code> <code>int</code> <p>Minimum possible relevance score (inclusive).</p> <code>max_score</code> <code>int</code> <p>Maximum possible relevance score (inclusive).</p> <code>human_summaries_column_name</code> <code>str</code> <p>Name of the column containing human summaries.</p> <code>machine_summaries_column_name</code> <code>str</code> <p>Name of the column containing machine summaries.</p> <code>text_column_name</code> <code>str</code> <p>Name of the column containing the original text.</p> <code>relevancy_column_name</code> <code>str</code> <p>Name of the column containing relevance scores.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/text/summarization.py</code> <pre><code>class AbsTaskSummarization(AbsTask):\n    \"\"\"Abstract class for summarization experiments.\n\n    Attributes:\n        dataset: HuggingFace dataset containing the data for the task. Should have columns:\n            - text: The original text to be summarized.\n            - human_summaries: A list of human-written summaries for the text.\n            - machine_summaries: A list of machine-generated summaries for the text.\n            - relevance: A list of relevance scores (integers) corresponding to each machine summary, indicating how relevant each summary is to the original text.\n        min_score: Minimum possible relevance score (inclusive).\n        max_score: Maximum possible relevance score (inclusive).\n        human_summaries_column_name: Name of the column containing human summaries.\n        machine_summaries_column_name: Name of the column containing machine summaries.\n        text_column_name: Name of the column containing the original text.\n        relevancy_column_name: Name of the column containing relevance scores.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    min_score: int\n    max_score: int\n\n    abstask_prompt = (\n        \"Given a news summary, retrieve other semantically similar summaries.\"\n    )\n    # SummEval has DeprecatedSummarizationEvaluator\n    evaluator = SummarizationEvaluator\n    text_column_name: str = \"text\"\n    human_summaries_column_name: str = \"human_summaries\"\n    machine_summaries_column_name: str = \"machine_summaries\"\n    relevancy_column_name: str = \"relevance\"\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, Any],\n        prediction_folder: Path | None = None,\n        **kwargs,\n    ) -&gt; SummarizationMetrics:\n        normalized_scores = [\n            (np.array(x) - self.min_score) / (self.max_score - self.min_score)\n            for x in data_split[self.relevancy_column_name]\n        ]\n        evaluator = self.evaluator(\n            machine_summaries=data_split[self.machine_summaries_column_name],\n            human_summaries=data_split[self.human_summaries_column_name],\n            texts=data_split[self.text_column_name],\n            gold_scores=normalized_scores,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        scores = evaluator(model, encode_kwargs=encode_kwargs)\n        if prediction_folder:\n            self._save_task_predictions(\n                scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n        return evaluator._calculate_metrics(scores)\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; SummarizationDescriptiveStatistics:\n        if hf_subset:\n            text = self.dataset[hf_subset][split][self.text_column_name]\n            human_summaries = self.dataset[hf_subset][split][\n                self.human_summaries_column_name\n            ]\n            machine_summaries = self.dataset[hf_subset][split][\n                self.machine_summaries_column_name\n            ]\n            relevance = self.dataset[hf_subset][split][self.relevancy_column_name]\n        elif compute_overall:\n            text = []\n            human_summaries = []\n            machine_summaries = []\n            relevance = []\n\n            for hf_subset in self.metadata.eval_langs:\n                text.extend(self.dataset[hf_subset][split][self.text_column_name])\n                human_summaries.extend(\n                    self.dataset[hf_subset][split][self.human_summaries_column_name]\n                )\n                machine_summaries.extend(\n                    self.dataset[hf_subset][split][self.machine_summaries_column_name]\n                )\n                relevance.extend(\n                    self.dataset[hf_subset][split][self.relevancy_column_name]\n                )\n        else:\n            text = self.dataset[split][self.text_column_name]\n            human_summaries = self.dataset[split][self.human_summaries_column_name]\n            machine_summaries = self.dataset[split][self.machine_summaries_column_name]\n            relevance = self.dataset[split][self.relevancy_column_name]\n\n        all_human_summaries = []\n        for s in human_summaries:\n            all_human_summaries.extend(s)\n\n        all_machine_summaries = []\n        for s in machine_summaries:\n            all_machine_summaries.extend(s)\n\n        text_statistics = calculate_text_statistics(text)\n        human_summaries_statistics = calculate_text_statistics(all_human_summaries)\n        machine_summaries_statistics = calculate_text_statistics(all_machine_summaries)\n\n        relevance = [item for sublist in relevance for item in sublist]\n\n        return SummarizationDescriptiveStatistics(\n            num_samples=len(text),\n            number_of_characters=(\n                text_statistics[\"total_text_length\"]\n                + human_summaries_statistics[\"total_text_length\"]\n                + machine_summaries_statistics[\"total_text_length\"]\n            ),\n            text_statistics=text_statistics,\n            human_summaries_statistics=human_summaries_statistics,\n            machine_summaries_statistics=machine_summaries_statistics,\n            score_statistics=calculate_score_statistics(relevance),\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking","title":"<code>mteb.abstasks.text.reranking.AbsTaskReranking</code>","text":"<p>               Bases: <code>AbsTaskRetrieval</code></p> <p>Reranking task class.</p> Deprecated <p>This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead. You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format. You can reupload it using <code>task.push_dataset_to_hub('your/repository')</code> after loading the data. For dataformat and other information, see AbsTaskRetrieval.</p> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>@deprecated(\n    \"This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead. \"\n    \"You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format. \"\n    \"You can reupload it using `task.push_dataset_to_hub('your/repository')` after loading the data.\"\n)\nclass AbsTaskReranking(AbsTaskRetrieval):\n    \"\"\"Reranking task class.\n\n    Warning: Deprecated\n        This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead.\n        You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format.\n        You can reupload it using `task.push_dataset_to_hub('your/repository')` after loading the data.\n        For dataformat and other information, see [AbsTaskRetrieval][mteb.abstasks.retrieval.AbsTaskRetrieval].\n    \"\"\"\n\n    def load_data(self) -&gt; None:\n        \"\"\"Load the dataset.\"\"\"\n        if self.data_loaded:\n            return\n\n        if self.metadata.name in OLD_FORMAT_RERANKING_TASKS:\n            self.transform_old_dataset_format()\n        else:\n            # use AbsTaskRetrieval default to load the data\n            return super().load_data()\n\n    def _process_example(self, example: dict, split: str, query_idx: int) -&gt; dict:\n        \"\"\"Process a single example from the dataset.\n\n        Args:\n            example: A single example from the dataset containing 'query', 'positive', and 'negative' fields.\n            split: The dataset split (e.g., 'train', 'validation', 'test').\n            query_idx: The index of the query in the dataset split.\n\n        Returns:\n            A dictionary containing the processed example with query_id, query text, document ids, document texts, and relevance scores.\n        \"\"\"\n        query = example[\"query\"]\n        positive_docs = example[\"positive\"]\n        negative_docs = example[\"negative\"]\n\n        query_id = f\"{split}_query{query_idx}\"\n\n        # Initialize the structures for this example\n        example_data = {\n            \"query_id\": query_id,\n            \"query\": query,\n            \"doc_ids\": [],\n            \"doc_texts\": [],\n            \"relevance_scores\": [],\n        }\n\n        for i, pos_doc in enumerate(positive_docs):\n            # format i as a five digit number\n            formatted_i = str(i).zfill(5)\n            # have \"a\" in front so that positives are first, then negatives\n            #   this shouldn't matter except for ties, and the previous reranking results\n            #   had the positives first\n            doc_id = f\"apositive_{query_id}_{formatted_i}\"\n            example_data[\"doc_ids\"].append(doc_id)\n            example_data[\"doc_texts\"].append(pos_doc)\n            example_data[\"relevance_scores\"].append(1)\n\n        for i, neg_doc in enumerate(negative_docs):\n            formatted_i = str(i).zfill(5)\n            doc_id = f\"negative_{query_id}_{formatted_i}\"\n            example_data[\"doc_ids\"].append(doc_id)\n            example_data[\"doc_texts\"].append(neg_doc)\n            example_data[\"relevance_scores\"].append(0)\n\n        return example_data\n\n    def transform_old_dataset_format(self, given_dataset: Dataset | None = None):\n        \"\"\"Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.\n\n        Args:\n            given_dataset (Dataset, optional): The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.\n        \"\"\"\n        if self.metadata.name not in OLD_FORMAT_RERANKING_TASKS:\n            return\n\n        logging.info(\n            f\"Transforming old format to standard format for {self.metadata.name}\"\n        )\n\n        given_dataset = copy(given_dataset)\n        self.dataset = defaultdict(lambda: defaultdict(dict))\n\n        hf_subsets = self.hf_subsets\n\n        for hf_subset in hf_subsets:\n            if given_dataset:\n                cur_dataset = given_dataset\n                if hf_subset in cur_dataset:\n                    cur_dataset = cur_dataset[hf_subset]\n            elif \"name\" in self.metadata.dataset:\n                cur_dataset = datasets.load_dataset(**self.metadata.dataset)  # type: ignore\n                assert hf_subset == \"default\", (\n                    f\"Only default subset is supported for {self.metadata.name} since `name` is given in the metadata.\"\n                )\n            else:\n                cur_dataset = datasets.load_dataset(\n                    **self.metadata.dataset, name=hf_subset\n                )  # type: ignore\n\n            for split in cur_dataset:\n                corpus = []\n                queries = []\n                relevant_docs = defaultdict(dict)\n                top_ranked = defaultdict(list)\n\n                # Create an enumerated dataset to pass indices\n                enumerated_dataset = Dataset.from_dict(\n                    {\n                        \"index\": range(len(cur_dataset[split])),\n                        \"query\": cur_dataset[split][\"query\"],\n                        \"positive\": cur_dataset[split][\"positive\"],\n                        \"negative\": cur_dataset[split][\"negative\"],\n                    }\n                )\n\n                # first, filter out the ones that have no positive or no negatives\n                enumerated_dataset = enumerated_dataset.filter(\n                    lambda example: len(example[\"positive\"]) &gt; 0\n                    and len(example[\"negative\"]) &gt; 0\n                )\n\n                logger.info(\n                    f\"Filtered out {len(cur_dataset[split]) - len(enumerated_dataset)} examples with no positive or no negative examples. {len(enumerated_dataset)} examples remaining.\"\n                )\n\n                # Map the transformation function over the dataset\n                processed_dataset = enumerated_dataset.map(\n                    lambda example, idx: self._process_example(example, split, idx),\n                    with_indices=True,\n                    remove_columns=enumerated_dataset.column_names,\n                )\n\n                # Populate the data structures\n                for item in processed_dataset:\n                    query_id = item[\"query_id\"]\n                    queries.append({\"id\": query_id, \"text\": item[\"query\"]})\n\n                    # Add documents and relevance information\n                    for doc_id, doc_text, relevance in zip(\n                        item[\"doc_ids\"], item[\"doc_texts\"], item[\"relevance_scores\"]\n                    ):\n                        corpus.append(\n                            {\n                                \"title\": \"\",\n                                \"text\": doc_text,\n                                \"id\": doc_id,\n                            }\n                        )\n                        top_ranked[query_id].append(doc_id)\n                        relevant_docs[query_id][doc_id] = relevance\n\n                self.dataset[hf_subset][split] = RetrievalSplitData(\n                    corpus=Dataset.from_list(corpus),\n                    queries=Dataset.from_list(queries),\n                    relevant_docs=relevant_docs,\n                    top_ranked=top_ranked,\n                )\n        self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking.load_data","title":"<code>load_data()</code>","text":"<p>Load the dataset.</p> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Load the dataset.\"\"\"\n    if self.data_loaded:\n        return\n\n    if self.metadata.name in OLD_FORMAT_RERANKING_TASKS:\n        self.transform_old_dataset_format()\n    else:\n        # use AbsTaskRetrieval default to load the data\n        return super().load_data()\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking.transform_old_dataset_format","title":"<code>transform_old_dataset_format(given_dataset=None)</code>","text":"<p>Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.</p> <p>Parameters:</p> Name Type Description Default <code>given_dataset</code> <code>Dataset</code> <p>The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.</p> <code>None</code> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>def transform_old_dataset_format(self, given_dataset: Dataset | None = None):\n    \"\"\"Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.\n\n    Args:\n        given_dataset (Dataset, optional): The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.\n    \"\"\"\n    if self.metadata.name not in OLD_FORMAT_RERANKING_TASKS:\n        return\n\n    logging.info(\n        f\"Transforming old format to standard format for {self.metadata.name}\"\n    )\n\n    given_dataset = copy(given_dataset)\n    self.dataset = defaultdict(lambda: defaultdict(dict))\n\n    hf_subsets = self.hf_subsets\n\n    for hf_subset in hf_subsets:\n        if given_dataset:\n            cur_dataset = given_dataset\n            if hf_subset in cur_dataset:\n                cur_dataset = cur_dataset[hf_subset]\n        elif \"name\" in self.metadata.dataset:\n            cur_dataset = datasets.load_dataset(**self.metadata.dataset)  # type: ignore\n            assert hf_subset == \"default\", (\n                f\"Only default subset is supported for {self.metadata.name} since `name` is given in the metadata.\"\n            )\n        else:\n            cur_dataset = datasets.load_dataset(\n                **self.metadata.dataset, name=hf_subset\n            )  # type: ignore\n\n        for split in cur_dataset:\n            corpus = []\n            queries = []\n            relevant_docs = defaultdict(dict)\n            top_ranked = defaultdict(list)\n\n            # Create an enumerated dataset to pass indices\n            enumerated_dataset = Dataset.from_dict(\n                {\n                    \"index\": range(len(cur_dataset[split])),\n                    \"query\": cur_dataset[split][\"query\"],\n                    \"positive\": cur_dataset[split][\"positive\"],\n                    \"negative\": cur_dataset[split][\"negative\"],\n                }\n            )\n\n            # first, filter out the ones that have no positive or no negatives\n            enumerated_dataset = enumerated_dataset.filter(\n                lambda example: len(example[\"positive\"]) &gt; 0\n                and len(example[\"negative\"]) &gt; 0\n            )\n\n            logger.info(\n                f\"Filtered out {len(cur_dataset[split]) - len(enumerated_dataset)} examples with no positive or no negative examples. {len(enumerated_dataset)} examples remaining.\"\n            )\n\n            # Map the transformation function over the dataset\n            processed_dataset = enumerated_dataset.map(\n                lambda example, idx: self._process_example(example, split, idx),\n                with_indices=True,\n                remove_columns=enumerated_dataset.column_names,\n            )\n\n            # Populate the data structures\n            for item in processed_dataset:\n                query_id = item[\"query_id\"]\n                queries.append({\"id\": query_id, \"text\": item[\"query\"]})\n\n                # Add documents and relevance information\n                for doc_id, doc_text, relevance in zip(\n                    item[\"doc_ids\"], item[\"doc_texts\"], item[\"relevance_scores\"]\n                ):\n                    corpus.append(\n                        {\n                            \"title\": \"\",\n                            \"text\": doc_text,\n                            \"id\": doc_id,\n                        }\n                    )\n                    top_ranked[query_id].append(doc_id)\n                    relevant_docs[query_id][doc_id] = relevance\n\n            self.dataset[hf_subset][split] = RetrievalSplitData(\n                corpus=Dataset.from_list(corpus),\n                queries=Dataset.from_list(queries),\n                relevant_docs=relevant_docs,\n                top_ranked=top_ranked,\n            )\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#image-tasks","title":"Image Tasks","text":""},{"location":"api/task/#mteb.abstasks.image.image_text_pair_classification.AbsTaskImageTextPairClassification","title":"<code>mteb.abstasks.image.image_text_pair_classification.AbsTaskImageTextPairClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for Image Text Pair Classification tasks (Compositionality evaluation).</p> <p>The similarity is computed between pairs and the results are ranked. Note that the number of images and the number of captions can be different.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the ImageTextPairClassification task. Should have columns: - images: List of images. - captions: List of captions.</p> <code>images_column_names</code> <code>str | Sequence[str]</code> <p>Name of the column(s) containing the images.</p> <code>texts_column_names</code> <code>str | Sequence[str]</code> <p>Name of the column(s) containing the captions.</p> <code>abstask_prompt</code> <code>str | None</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/image/image_text_pair_classification.py</code> <pre><code>class AbsTaskImageTextPairClassification(AbsTask):\n    \"\"\"Abstract class for Image Text Pair Classification tasks (Compositionality evaluation).\n\n    The similarity is computed between pairs and the results are ranked.\n    Note that the number of images and the number of captions can be different.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the ImageTextPairClassification task. Should have columns:\n            - images: List of images.\n            - captions: List of captions.\n        images_column_names: Name of the column(s) containing the images.\n        texts_column_names: Name of the column(s) containing the captions.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    # it can be [\"image_0\", \"image_1\"]; [\"text_0\", \"text_1\"] for datasets like WinoGround\n    images_column_names: str | Sequence[str] = \"image\"\n    texts_column_names: str | Sequence[str] = \"caption\"\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ImageTextPairClassificationDescriptiveStatistics:\n        if compute_overall:\n            dataset = concatenate_datasets(\n                [\n                    self.dataset[hf_subset][split]\n                    for hf_subset in self.metadata.eval_langs\n                ]\n            )\n        else:\n            dataset = (\n                self.dataset[split]\n                if hf_subset is None\n                else self.dataset[hf_subset][split]\n            )\n        num_samples = len(dataset)\n\n        images = None\n        texts = None\n\n        if isinstance(self.images_column_names, str):\n            images = list(dataset[self.images_column_names])\n        elif isinstance(self.images_column_names, Sequence):\n            images = [\n                img\n                for img_column in self.images_column_names\n                for img in dataset[img_column]\n            ]\n\n        if isinstance(self.texts_column_names, str):\n            texts = list(dataset[self.texts_column_names])\n        elif isinstance(self.texts_column_names, Sequence):\n            texts = [\n                text\n                for text_column in self.texts_column_names\n                for text in dataset[text_column]\n            ]\n\n        return ImageTextPairClassificationDescriptiveStatistics(\n            num_samples=num_samples,\n            text_statistics=calculate_text_statistics(texts),\n            image_statistics=calculate_image_statistics(images),\n        )\n\n    def _evaluate_subset(\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        encode_kwargs: dict[str, Any],\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        **kwargs: Any,\n    ) -&gt; ImageTextPairClassificationMetrics:\n        select_columns = []\n        for columns in (self.images_column_names, self.texts_column_names):\n            if isinstance(columns, str):\n                select_columns.append(columns)\n            else:\n                select_columns.extend(columns)\n\n        data_split = data_split.select_columns(select_columns)\n        num_images_per_sample = (\n            1\n            if isinstance(self.images_column_names, str)\n            else len(self.images_column_names)\n        )\n        num_texts_per_sample = (\n            1\n            if isinstance(self.texts_column_names, str)\n            else len(self.texts_column_names)\n        )\n        evaluator = ImageTextPairClassificationEvaluator(\n            data_split,\n            images_column_names=self.images_column_names,\n            texts_column_names=self.texts_column_names,\n            task_metadata=self.metadata,\n            num_texts_per_sample=num_texts_per_sample,\n            num_images_per_sample=num_images_per_sample,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        scores = evaluator(model, encode_kwargs=encode_kwargs)\n        if prediction_folder:\n            self._save_task_predictions(\n                [score.tolist() for score in scores],\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._compute_metrics(\n            scores,\n            num_images_per_sample,\n            num_texts_per_sample,\n        )\n\n    def _compute_metrics(\n        self,\n        scores: list[torch.Tensor],\n        num_images_per_sample: int,\n        num_texts_per_sample: int,\n    ) -&gt; ImageTextPairClassificationMetrics:\n        image_score = []\n        text_score = []\n        all_correct_scores = []\n        img_ground_truths = torch.arange(num_images_per_sample)\n        caption_ground_truths = torch.arange(num_texts_per_sample)\n\n        for score in scores:\n            image_closest_text = score.argmax(dim=1)  # shape = (num_images_per_sample)\n            text_closest_image = score.argmax(dim=0)  # shape = (num_texts_per_sample)\n            pred_text_is_correct = (\n                (image_closest_text == img_ground_truths).all().item()\n            )\n            pred_image_is_correct = (\n                (text_closest_image == caption_ground_truths).all().item()\n            )\n            all_correct = pred_text_is_correct and pred_image_is_correct\n            image_score.append(pred_image_is_correct)\n            text_score.append(pred_text_is_correct)\n            all_correct_scores.append(all_correct)\n\n        return ImageTextPairClassificationMetrics(\n            image_acc=torch.Tensor(image_score).float().mean().item(),\n            text_acc=torch.Tensor(text_score).float().mean().item(),\n            accuracy=torch.Tensor(all_correct_scores).float().mean().item(),\n        )\n\n    def _push_dataset_to_hub(self, repo_name: str) -&gt; None:\n        text_columns = (\n            [self.texts_column_names]\n            if isinstance(self.texts_column_names, str)\n            else self.texts_column_names\n        )\n        image_columns = (\n            [self.images_column_names]\n            if isinstance(self.images_column_names, str)\n            else self.images_column_names\n        )\n\n        self._upload_dataset_to_hub(\n            repo_name,\n            [*text_columns, *image_columns],\n        )\n</code></pre>"},{"location":"api/types/","title":"Additional Types","text":"<p>MTEB implements a variety of utility types to allow us and you to better know what a model returns. This page documents some of these types.</p>"},{"location":"api/types/#mteb.types._encoder_io","title":"Encoder Input/Output types","text":""},{"location":"api/types/#mteb.types._encoder_io.Array","title":"<code>Array = np.ndarray | torch.Tensor</code>  <code>module-attribute</code>","text":"<p>General array type, can be a numpy array or a torch tensor.</p>"},{"location":"api/types/#mteb.types._encoder_io.Conversation","title":"<code>Conversation = list[ConversationTurn]</code>  <code>module-attribute</code>","text":"<p>A conversation, consisting of a list of messages.</p>"},{"location":"api/types/#mteb.types._encoder_io.BatchedInput","title":"<code>BatchedInput = TextInput | CorpusInput | QueryInput | ImageInput | AudioInput | MultimodalInput</code>  <code>module-attribute</code>","text":"<p>Represents the input format accepted by the encoder for a batch of data.</p> <p>The encoder can process several input types depending on the task or modality. Each type is defined as a separate structured input with its own fields.</p>"},{"location":"api/types/#mteb.types._encoder_io.BatchedInput--supported-input-types","title":"Supported input types","text":"<ol> <li><code>TextInput</code>    For pure text inputs.</li> </ol> <p><pre><code>{\"text\": [\"This is a sample text.\", \"Another text.\"]}\n</code></pre> 2. <code>CorpusInput</code>    For corpus-style inputs with titles and bodies.</p> <p><pre><code>{\"text\": [\"Title 1 Body 1\", \"Title 2 Body 2\"], \"title\": [\"Title 1\", \"Title 2\"], \"body\": [\"Body 1\", \"Body 2\"]}\n</code></pre> 3. <code>QueryInput</code>    For query\u2013instruction pairs, typically used in retrieval or question answering tasks. Queries and instructions are combined with the model's instruction template.</p> <p><pre><code>{\n    \"text\": [\"Instruction: Your task is to find document for this query. Query: What is AI?\", \"Instruction: Your task is to find term for definition. Query: Define machine learning.\"],\n    \"query\": [\"What is AI?\", \"Define machine learning.\"],\n    \"instruction\": [\"Your task is find document for this query.\", \"Your task is to find term for definition.\"]\n}\n</code></pre> 4. <code>ImageInput</code>    For visual inputs consisting of images.</p> <p><pre><code>{\"image\": [PIL.Image1, PIL.Image2]}\n</code></pre> 5. <code>MultimodalInput</code>    For combined text\u2013image (multimodal) inputs.</p> <pre><code>{\"text\": [\"This is a sample text.\"], \"image\": [PIL.Image1]}\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.TextBatchedInput","title":"<code>TextBatchedInput = TextInput | CorpusInput | QueryInput</code>  <code>module-attribute</code>","text":"<p>The input to the encoder for a batch of text data.</p>"},{"location":"api/types/#mteb.types._encoder_io.QueryDatasetType","title":"<code>QueryDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval query dataset, containing queries. Should have columns <code>id</code>, <code>text</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.CorpusDatasetType","title":"<code>CorpusDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval corpus dataset, containing documents. Should have columns <code>id</code>, <code>title</code>, <code>body</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.InstructionDatasetType","title":"<code>InstructionDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval instruction dataset, containing instructions. Should have columns <code>query-id</code>, <code>instruction</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.RelevantDocumentsType","title":"<code>RelevantDocumentsType = Mapping[str, Mapping[str, int]]</code>  <code>module-attribute</code>","text":"<p>Relevant documents for each query, mapping query IDs to a mapping of document IDs and their relevance scores. Should have columns <code>query-id</code>, <code>corpus-id</code>, <code>score</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.TopRankedDocumentsType","title":"<code>TopRankedDocumentsType = Mapping[str, list[str]]</code>  <code>module-attribute</code>","text":"<p>Top-ranked documents for each query, mapping query IDs to a list of document IDs. Should have columns <code>query-id</code>, <code>corpus-ids</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.RetrievalOutputType","title":"<code>RetrievalOutputType = dict[str, dict[str, float]]</code>  <code>module-attribute</code>","text":"<p>Retrieval output, containing the scores for each query-document pair.</p>"},{"location":"api/types/#mteb.types._encoder_io.PromptType","title":"<code>PromptType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The type of prompt used in the input for retrieval models. Used to differentiate between queries and documents.</p> <p>Attributes:</p> Name Type Description <code>query</code> <p>A prompt that is a query.</p> <code>document</code> <p>A prompt that is a document.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class PromptType(str, Enum):\n    \"\"\"The type of prompt used in the input for retrieval models. Used to differentiate between queries and documents.\n\n    Attributes:\n        query: A prompt that is a query.\n        document: A prompt that is a document.\n    \"\"\"\n\n    query = \"query\"\n    document = \"document\"\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.ConversationTurn","title":"<code>ConversationTurn</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A conversation, consisting of a list of messages.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message sender.</p> <code>content</code> <code>str</code> <p>The content of the message.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class ConversationTurn(TypedDict):\n    \"\"\"A conversation, consisting of a list of messages.\n\n    Attributes:\n        role: The role of the message sender.\n        content: The content of the message.\n    \"\"\"\n\n    role: str\n    content: str\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.TextInput","title":"<code>TextInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for text.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>list[str]</code> <p>The text to encode. Can be a list of texts or a list of lists of texts.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class TextInput(TypedDict):\n    \"\"\"The input to the encoder for text.\n\n    Attributes:\n        text: The text to encode. Can be a list of texts or a list of lists of texts.\n    \"\"\"\n\n    text: list[str]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.CorpusInput","title":"<code>CorpusInput</code>","text":"<p>               Bases: <code>TextInput</code></p> <p>The input to the encoder for retrieval corpus.</p> <p>Attributes:</p> Name Type Description <code>title</code> <code>list[str]</code> <p>The title of the text to encode. Can be a list of titles or a list of lists of titles.</p> <code>body</code> <code>list[str]</code> <p>The body of the text to encode. Can be a list of bodies or a list of lists of bodies.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class CorpusInput(TextInput):\n    \"\"\"The input to the encoder for retrieval corpus.\n\n    Attributes:\n        title: The title of the text to encode. Can be a list of titles or a\n            list of lists of titles.\n        body: The body of the text to encode. Can be a list of bodies or a\n            list of lists of bodies.\n    \"\"\"\n\n    title: list[str]\n    body: list[str]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.QueryInput","title":"<code>QueryInput</code>","text":"<p>               Bases: <code>TextInput</code></p> <p>The input to the encoder for queries.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>list[str]</code> <p>The query to encode. Can be a list of queries or a list of lists of queries.</p> <code>conversation</code> <code>NotRequired[list[Conversation]]</code> <p>Optional. A list of conversations, each conversation is a list of messages.</p> <code>instruction</code> <code>NotRequired[list[str]]</code> <p>Optional. A list of instructions to encode.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class QueryInput(TextInput):\n    \"\"\"The input to the encoder for queries.\n\n    Attributes:\n        query: The query to encode. Can be a list of queries or a list of lists of queries.\n        conversation: Optional. A list of conversations, each conversation is a list of messages.\n        instruction: Optional. A list of instructions to encode.\n    \"\"\"\n\n    query: list[str]\n    conversation: NotRequired[list[Conversation]]\n    instruction: NotRequired[list[str]]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.ImageInput","title":"<code>ImageInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for images.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>list[Image]</code> <p>The image to encode. Can be a list of images or a list of lists of images.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class ImageInput(TypedDict):\n    \"\"\"The input to the encoder for images.\n\n    Attributes:\n        image: The image to encode. Can be a list of images or a list of lists of images.\n    \"\"\"\n\n    image: list[Image.Image]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.AudioInput","title":"<code>AudioInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for audio.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>list[list[bytes]]</code> <p>The audio to encode. Can be a list of audio files or a list of lists of audio files.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class AudioInput(TypedDict):\n    \"\"\"The input to the encoder for audio.\n\n    Attributes:\n        audio: The audio to encode. Can be a list of audio files or a list of lists of audio files.\n    \"\"\"\n\n    audio: list[list[bytes]]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.MultimodalInput","title":"<code>MultimodalInput</code>","text":"<p>               Bases: <code>TextInput</code>, <code>CorpusInput</code>, <code>QueryInput</code>, <code>ImageInput</code>, <code>AudioInput</code></p> <p>The input to the encoder for multimodal data.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class MultimodalInput(TextInput, CorpusInput, QueryInput, ImageInput, AudioInput):  # type: ignore[misc]\n    \"\"\"The input to the encoder for multimodal data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/types/#mteb.types._metadata","title":"Metadata types","text":""},{"location":"api/types/#mteb.types._metadata.ISOLanguageScript","title":"<code>ISOLanguageScript = str</code>  <code>module-attribute</code>","text":"<p>A string representing the language and script. Language is denoted as a 3-letter ISO 639-3 language code and the script is denoted by a 4-letter ISO 15924 script code (e.g. \"eng-Latn\").</p>"},{"location":"api/types/#mteb.types._metadata.ISOLanguage","title":"<code>ISOLanguage = str</code>  <code>module-attribute</code>","text":"<p>A string representing the language. Language is denoted as a 3-letter ISO 639-3 language code (e.g. \"eng\").</p>"},{"location":"api/types/#mteb.types._metadata.ISOScript","title":"<code>ISOScript = str</code>  <code>module-attribute</code>","text":"<p>A string representing the script. The script is denoted by a 4-letter ISO 15924 script code (e.g. \"Latn\").</p>"},{"location":"api/types/#mteb.types._metadata.Languages","title":"<code>Languages = list[ISOLanguageScript] | Mapping[HFSubset, list[ISOLanguageScript]]</code>  <code>module-attribute</code>","text":"<p>A list of languages or a mapping from HFSubset to a list of languages. E.g. [\"eng-Latn\", \"deu-Latn\"] or {\"en-de\": [\"eng-Latn\", \"deu-Latn\"], \"fr-it\": [\"fra-Latn\", \"ita-Latn\"]}.</p>"},{"location":"api/types/#mteb.types._metadata.Licenses","title":"<code>Licenses = Literal['not specified', 'mit', 'cc-by-2.0', 'cc-by-3.0', 'cc-by-4.0', 'cc-by-sa-3.0', 'cc-by-sa-4.0', 'cc-by-nc-3.0', 'cc-by-nc-4.0', 'cc-by-nc-sa-3.0', 'cc-by-nc-sa-4.0', 'cc-by-nc-nd-4.0', 'cc-by-nd-4.0', 'openrail', 'openrail++', 'odc-by', 'afl-3.0', 'apache-2.0', 'cc-by-nd-2.1-jp', 'cc0-1.0', 'bsd-3-clause', 'gpl-3.0', 'lgpl', 'lgpl-3.0', 'cdla-sharing-1.0', 'mpl-2.0', 'msr-la-nc', 'multiple', 'gemma']</code>  <code>module-attribute</code>","text":"<p>The different licenses that a dataset or model can have. This list can be extended as needed.</p>"},{"location":"api/types/#mteb.types._metadata.ModelName","title":"<code>ModelName = str</code>  <code>module-attribute</code>","text":"<p>The name of a model, typically as found on HuggingFace e.g. <code>sentence-transformers/all-MiniLM-L6-v2</code>.</p>"},{"location":"api/types/#mteb.types._metadata.Revision","title":"<code>Revision = str</code>  <code>module-attribute</code>","text":"<p>The revision of a model, typically a git commit hash. For APIs this can be a version string e.g. <code>1</code>.</p>"},{"location":"api/types/#mteb.types._metadata.Modalities","title":"<code>Modalities = Literal['text', 'image']</code>  <code>module-attribute</code>","text":"<p>The different modalities that a model can support.</p>"},{"location":"api/types/#mteb.types._result","title":"Results types","text":""},{"location":"api/types/#mteb.types._result.HFSubset","title":"<code>HFSubset = str</code>  <code>module-attribute</code>","text":"<p>The name of a HuggingFace dataset subset, e.g. 'en-de', 'en', 'default' (default is used when there is no subset).</p>"},{"location":"api/types/#mteb.types._result.SplitName","title":"<code>SplitName = str</code>  <code>module-attribute</code>","text":"<p>The name of a data split, e.g. 'test', 'validation', 'train'.</p>"},{"location":"api/types/#mteb.types._result.Score","title":"<code>Score = Any</code>  <code>module-attribute</code>","text":"<p>A score value, could e.g. be accuracy. Normally it is a float or int, but it can take on any value. Should be json serializable.</p>"},{"location":"api/types/#mteb.types._result.ScoresDict","title":"<code>ScoresDict = dict[str, Score]</code>  <code>module-attribute</code>","text":"<p>A dictionary of scores, typically also include metadata, e.g {'main_score': 0.5, 'accuracy': 0.5, 'f1': 0.6, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']}</p>"},{"location":"api/types/#mteb.types._result.RetrievalEvaluationResult","title":"<code>RetrievalEvaluationResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Holds the results of retrieval evaluation metrics.</p> Source code in <code>mteb/types/_result.py</code> <pre><code>class RetrievalEvaluationResult(NamedTuple):\n    \"\"\"Holds the results of retrieval evaluation metrics.\"\"\"\n\n    all_scores: dict[str, dict[str, float]]\n    ndcg: dict[str, float]\n    map: dict[str, float]\n    recall: dict[str, float]\n    precision: dict[str, float]\n    naucs: dict[str, float]\n    mrr: dict[str, float]\n    naucs_mrr: dict[str, float]\n    cv_recall: dict[str, float]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics","title":"Statistics types","text":""},{"location":"api/types/#mteb.types.statistics.SplitDescriptiveStatistics","title":"<code>SplitDescriptiveStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Base class for descriptive statistics for the subset.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class SplitDescriptiveStatistics(TypedDict):\n    \"\"\"Base class for descriptive statistics for the subset.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.DescriptiveStatistics","title":"<code>DescriptiveStatistics</code>","text":"<p>               Bases: <code>TypedDict</code>, <code>SplitDescriptiveStatistics</code></p> <p>Class for descriptive statistics for the full task.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class DescriptiveStatistics(TypedDict, SplitDescriptiveStatistics):\n    \"\"\"Class for descriptive statistics for the full task.\"\"\"\n\n    hf_subset_descriptive_stats: NotRequired[dict[HFSubset, SplitDescriptiveStatistics]]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.TextStatistics","title":"<code>TextStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>total_text_length</code> <code>int</code> <p>Total length of all texts</p> <code>min_text_length</code> <code>int</code> <p>Minimum length of text</p> <code>average_text_length</code> <code>float</code> <p>Average length of text</p> <code>max_text_length</code> <code>int</code> <p>Maximum length of text</p> <code>unique_texts</code> <code>int</code> <p>Number of unique texts</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class TextStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        total_text_length: Total length of all texts\n        min_text_length: Minimum length of text\n        average_text_length: Average length of text\n        max_text_length: Maximum length of text\n        unique_texts: Number of unique texts\n    \"\"\"\n\n    total_text_length: int\n    min_text_length: int\n    average_text_length: float\n    max_text_length: int\n    unique_texts: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.ImageStatistics","title":"<code>ImageStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for images.</p> <p>Attributes:</p> Name Type Description <code>min_image_width</code> <code>float</code> <p>Minimum width of images</p> <code>average_image_width</code> <code>float</code> <p>Average width of images</p> <code>max_image_width</code> <code>float</code> <p>Maximum width of images</p> <code>min_image_height</code> <code>float</code> <p>Minimum height of images</p> <code>average_image_height</code> <code>float</code> <p>Average height of images</p> <code>max_image_height</code> <code>float</code> <p>Maximum height of images</p> <code>unique_images</code> <code>int</code> <p>Number of unique images</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class ImageStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for images.\n\n    Attributes:\n        min_image_width: Minimum width of images\n        average_image_width: Average width of images\n        max_image_width: Maximum width of images\n\n        min_image_height: Minimum height of images\n        average_image_height: Average height of images\n        max_image_height: Maximum height of images\n\n        unique_images: Number of unique images\n    \"\"\"\n\n    min_image_width: float\n    average_image_width: float\n    max_image_width: float\n\n    min_image_height: float\n    average_image_height: float\n    max_image_height: float\n\n    unique_images: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.LabelStatistics","title":"<code>LabelStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>min_labels_per_text</code> <code>int</code> <p>Minimum number of labels per text</p> <code>average_label_per_text</code> <code>float</code> <p>Average number of labels per text</p> <code>max_labels_per_text</code> <code>int</code> <p>Maximum number of labels per text</p> <code>unique_labels</code> <code>int</code> <p>Number of unique labels</p> <code>labels</code> <code>dict[str, dict[str, int]]</code> <p>dict of label frequencies</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class LabelStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        min_labels_per_text: Minimum number of labels per text\n        average_label_per_text: Average number of labels per text\n        max_labels_per_text: Maximum number of labels per text\n\n        unique_labels: Number of unique labels\n        labels: dict of label frequencies\n    \"\"\"\n\n    min_labels_per_text: int\n    average_label_per_text: float\n    max_labels_per_text: int\n\n    unique_labels: int\n    labels: dict[str, dict[str, int]]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.ScoreStatistics","title":"<code>ScoreStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>min_score</code> <code>int</code> <p>Minimum score</p> <code>avg_score</code> <code>float</code> <p>Average score</p> <code>max_score</code> <code>int</code> <p>Maximum score</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class ScoreStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        min_score: Minimum score\n        avg_score: Average score\n        max_score: Maximum score\n    \"\"\"\n\n    min_score: int\n    avg_score: float\n    max_score: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.TopRankedStatistics","title":"<code>TopRankedStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Statistics for top ranked documents in a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>num_top_ranked</code> <code>int</code> <p>Total number of top ranked documents across all queries.</p> <code>min_top_ranked_per_query</code> <code>int</code> <p>Minimum number of top ranked documents for any query.</p> <code>average_top_ranked_per_query</code> <code>float</code> <p>Average number of top ranked documents per query.</p> <code>max_top_ranked_per_query</code> <code>int</code> <p>Maximum number of top ranked documents for any query.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class TopRankedStatistics(TypedDict):\n    \"\"\"Statistics for top ranked documents in a retrieval task.\n\n    Attributes:\n        num_top_ranked: Total number of top ranked documents across all queries.\n        min_top_ranked_per_query: Minimum number of top ranked documents for any query.\n        average_top_ranked_per_query: Average number of top ranked documents per query.\n        max_top_ranked_per_query: Maximum number of top ranked documents for any query.\n    \"\"\"\n\n    num_top_ranked: int\n    min_top_ranked_per_query: int\n    average_top_ranked_per_query: float\n    max_top_ranked_per_query: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.RelevantDocsStatistics","title":"<code>RelevantDocsStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Statistics for relevant documents in a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>num_relevant_docs</code> <code>int</code> <p>Total number of relevant documents across all queries.</p> <code>min_relevant_docs_per_query</code> <code>int</code> <p>Minimum number of relevant documents for any query.</p> <code>average_relevant_docs_per_query</code> <code>float</code> <p>Average number of relevant documents per query.</p> <code>max_relevant_docs_per_query</code> <code>float</code> <p>Maximum number of relevant documents for any query.</p> <code>unique_relevant_docs</code> <code>int</code> <p>Number of unique relevant documents across all queries.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class RelevantDocsStatistics(TypedDict):\n    \"\"\"Statistics for relevant documents in a retrieval task.\n\n    Attributes:\n        num_relevant_docs: Total number of relevant documents across all queries.\n        min_relevant_docs_per_query: Minimum number of relevant documents for any query.\n        average_relevant_docs_per_query: Average number of relevant documents per query.\n        max_relevant_docs_per_query: Maximum number of relevant documents for any query.\n        unique_relevant_docs: Number of unique relevant documents across all queries.\n    \"\"\"\n\n    num_relevant_docs: int\n    min_relevant_docs_per_query: int\n    average_relevant_docs_per_query: float\n    max_relevant_docs_per_query: float\n    unique_relevant_docs: int\n</code></pre>"},{"location":"contributing/adding_a_benchmark/","title":"Adding a Benchmark","text":""},{"location":"contributing/adding_a_benchmark/#adding-a-benchmark","title":"Adding a benchmark","text":"<p>The MTEB Leaderboard is available here and we encourage additions of new benchmarks.</p> <p>To add a new benchmark:</p> <ol> <li>Add your benchmark to benchmarks.py as a <code>Benchmark</code> object, and select the MTEB tasks that will be in the benchmark. If some of the tasks do not exist in MTEB, follow the \"add a dataset\" instructions to add them.</li> <li>Open a PR at results repository with results of models on your benchmark.</li> <li>[optional]\u00a0When your PR with benchmarks results is merged, you can add your benchmark to the most fitting section in benchmark_selector.py to be shown on the leaderboard.</li> <li>When PRs are merged, your benchmark will be added to the leaderboard automatically after the next workflow trigger (every day at midnight Pacific Time (8 AM UTC)).</li> </ol>"},{"location":"contributing/adding_a_dataset/","title":"Adding a Dataset","text":"<p>To add a new dataset to MTEB, you need to do three things:</p> <p>1) Implement a task with the desired dataset, by subclassing an abstract task 2) Add metadata to the task 3) Calculate statistics of the task (run <code>task.calculate_descriptive_statistics()</code>) 4) Submit the edits to the MTEB repository</p> <p>If you have any questions regarding this process feel free to open a discussion thread.</p> <p>Note</p> <p>When we mention adding a dataset we refer to a subclass of one of the abstasks.</p>"},{"location":"contributing/adding_a_dataset/#creating-a-new-subclass","title":"Creating a new subclass","text":""},{"location":"contributing/adding_a_dataset/#a-simple-example","title":"A Simple Example","text":"<p>To add a new task, you need to implement a new class that inherits from the <code>AbsTask</code> associated with the task type (e.g. <code>AbsTaskRetrieval</code> for retrieval tasks). You can find the supported task types in here.</p> SciDocs Reranking Task <pre><code>from mteb.abstasks.retrieval import AbsTaskRetrieval\nfrom mteb.abstasks.task_metadata import TaskMetadata\n\nclass SciDocsReranking(AbsTaskRetrieval):\n    metadata = TaskMetadata(\n        name=\"SciDocsRR\",\n        description=\"Ranking of related scientific papers based on their title.\",\n        reference=\"https://allenai.org/data/scidocs\",\n        type=\"Reranking\",\n        category=\"t2t\",\n        modalities=[\"text\"],\n        eval_splits=[\"test\"],\n        eval_langs=[\"eng-Latn\"],\n        main_score=\"map\",\n        dataset={\n            \"path\": \"mteb/scidocs-reranking\",\n            \"revision\": \"d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\",\n        },\n        date=(\"2000-01-01\", \"2020-12-31\"), # best guess\n        domains=[\"Academic\", \"Non-fiction\", \"Domains\"],\n        task_subtypes=[\"Scientific Reranking\"],\n        license=\"cc-by-4.0\",\n        annotations_creators=\"derived\",\n        dialect=[],\n        sample_creation=\"found\",\n        bibtex_citation=\"\"\"\n@inproceedings{cohan-etal-2020-specter,\n    title = \"{SPECTER}: Document-level Representation Learning using Citation-informed Transformers\",\n    author = \"Cohan, Arman  and\n      Feldman, Sergey  and\n      Beltagy, Iz  and\n      Downey, Doug  and\n      Weld, Daniel\",\n    editor = \"Jurafsky, Dan  and\n      Chai, Joyce  and\n      Schluter, Natalie  and\n      Tetreault, Joel\",\n    booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",\n    month = jul,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2020.acl-main.207\",\n    doi = \"10.18653/v1/2020.acl-main.207\",\n    pages = \"2270--2282\",\n}\n\"\"\",\n)\n\n# testing the task with a model:\nmodel = mteb.get_model(\"intfloat/multilingual-e5-small\")\nresults = mteb.evaluate(model, tasks=[SciDocsReranking()])\n</code></pre> <p>Note</p> <p>For multilingual/crosslingual tasks, make sure you've specified <code>eval_langs</code> as a dictionary, as shown in this example.</p>"},{"location":"contributing/adding_a_dataset/#a-detailed-example","title":"A Detailed Example","text":"<p>Often the dataset from HuggingFace is not in the format expected by MTEB. To resolve this you can either change the format on Hugging Face or add a <code>dataset_transform</code> method to your dataset to transform it into the right format on the fly. Here is an example along with some design considerations:</p> DBpediaClassificationV2 Task <pre><code>from mteb.abstasks.task_metadata import TaskMetadata\nfrom mteb.abstasks.classification import AbsTaskClassification\n\nclass DBpediaClassificationV2(AbsTaskClassification):\n    metadata = TaskMetadata(\n        ... # fill in metadata as shown in the simple example above\n    )\n\n    def load_dataset(self):\n        self.dataset = load_dataset(\n            **self.metadata.dataset,\n        )\n        ... # some processing\n        self.data_loaded = True\n\n    # dataset transform will be called if `load_dataset` is not overridden\n    def dataset_transform(self):\n        self.dataset = self.stratified_subsampling(\n            self.dataset, seed=self.seed, splits=[\"train\", \"test\"]\n        )\n</code></pre>"},{"location":"contributing/adding_a_dataset/#creating-the-metadata-object","title":"Creating the metadata object","text":"<p>Along with the task MTEB requires metadata regarding the task. If the metadata isn't available please provide your best guess or leave the field as <code>None</code>.</p> <p>To get an overview of the fields in the metadata object, you can look at the TaskMetadata class.</p> <p>Note</p> <p>That these fields can be left blank if the information is not available and can be extended if necessary. We do not include any machine-translated (without verification) datasets in the benchmark.</p>"},{"location":"contributing/adding_a_dataset/#submit-a-pr","title":"Submit a PR","text":"<p>Once you are finished create a PR to the MTEB repository. If you haven't created a PR before please refer to the GitHub documentation</p> <p>The PR will be reviewed by one of the organizers or contributors who might ask you to change things. Once the PR is approved the dataset will be added into the main repository.</p> <p>Before you commit, here is a checklist you should complete before submitting:</p> <pre><code>- [ ] I have outlined why this dataset is filling an existing gap in `mteb`\n- [ ] I have tested that the dataset runs with the `mteb` package.\n- [ ] I have run the following models on the task (adding the results to the pr). These can be run using the `mteb run -m {model_name} -t {task_name}` command.\n  - [ ] `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`\n  - [ ] `intfloat/multilingual-e5-small`\n- [ ] I have checked that the performance is neither trivial (both models gain close to perfect scores) nor random (both models gain close to random scores).\n- [ ] I have considered the size of the dataset and reduced it if it is too big (2048 examples is typically large enough for most tasks)\n</code></pre> <p>An easy way to test it is using:</p> PythonCLI <pre><code>import mteb\n# sample model:\nmodel = mteb.get_model(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\ntask = mteb.get_task(\"{name of your task}\")\n\nresults = mteb.evaluate(model, task)\n</code></pre> <pre><code>mteb run -m sentence-transformers/paraphrase-multilingual-MiniLM -t {name of your task}\n</code></pre>"},{"location":"contributing/adding_a_model/","title":"Adding a Model","text":""},{"location":"contributing/adding_a_model/#adding-a-model-to-the-leaderboard","title":"Adding a model to the Leaderboard","text":"<p>The MTEB Leaderboard is available here. To submit to it:</p> <ol> <li>Add the model meta to <code>mteb</code></li> <li>Evaluate the desired model using <code>mteb</code> on the benchmarks</li> <li>Push the results to the results repository via a PR. Once merged they will appear on the leaderboard after a day.</li> </ol>"},{"location":"contributing/adding_a_model/#adding-a-model-implementation","title":"Adding a model implementation","text":"<p>Adding a model implementation to <code>mteb</code> is quite straightforward. Typically it only requires that you fill in metadata about the model and add it to the model directory:</p> Adding a ModelMeta object <pre><code>from mteb.models import ModelMeta, sentence_transformers_loader\n\nmy_model = ModelMeta(\n    name=\"model_name\",\n    loader=sentence_transformers_loader,\n    languages=[\"eng-Latn\"], # follows ISO 639-3 and BCP-47\n    open_weights=True,\n    revision=\"5617a9f61b028005a4858fdac845db406aefb181\",\n    release_date=\"2025-01-01\",\n    n_parameters=568_000_000,\n    memory_usage_mb=2167,\n    embed_dim=4096,\n    license=\"mit\",\n    max_tokens=8194,\n    reference=\"https://huggingface.co/user-or-org/model-name\",\n    similarity_fn_name=\"cosine\",\n    framework=[\"Sentence Transformers\", \"PyTorch\"],\n    use_instructions=False,\n    public_training_code=\"https://github.com/user-or-org/my-training-code\",\n    public_training_data=\"https://huggingface.co/datasets/user-or-org/full-dataset\",\n    training_datasets={\"MSMARCO\"}, # if you trained on the MSMARCO training set\n)\n</code></pre> <p>This works for all Sentence Transformers compatible models. Once filled out, you can submit your model to <code>mteb</code> by submitting a PR.</p>"},{"location":"contributing/adding_a_model/#calculating-the-memory-usage","title":"Calculating the Memory Usage","text":"<p>To calculate <code>memory_usage_mb</code>, run:</p> <pre><code>model_meta = mteb.get_model_meta(\"model_name\")\nmodel_meta.calculate_memory_usage_mb()\n</code></pre>"},{"location":"contributing/adding_a_model/#adding-instruction-models","title":"Adding instruction models","text":"<p>Some models, such as the [E5 models]<sup>1</sup>, use instructions or prefixes. You can directly add the prompts when saving and uploading your model to the Hub. Refer to this configuration file as an example.</p> <p>However, you can also add these directly to the model configuration:</p> <pre><code>model = ModelMeta(\n    loader=sentence_transformers_loader\n    loader_kwargs=dict(\n        model_prompts={\n           \"query\": \"query: \",\n           \"passage\": \"passage: \",\n        },\n    ),\n    ...\n)\n</code></pre>"},{"location":"contributing/adding_a_model/#using-a-custom-implementation","title":"Using a custom Implementation","text":"<p>If you need to use a custom implementation, you can specify the <code>loader</code> parameter in the <code>ModelMeta</code> class. It should implement one of the following protocols: Encoder, CrossEncoder, or Search.</p> Custom Model Implementation <pre><code>from mteb.types import PromptType\nimport numpy as np\n\nclass CustomModel:\n    def __init__(self, model_name: str, revision: str, **kwargs):\n        pass # your initialization of model here\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n\n        arrays = []\n        for batch in inputs:\n            documents = batch[\"text\"]\n            # embed documents:\n            embed_dim = 100\n            embedding = np.zeros((len(documents), embed_dim))\n\n        embeddings = np.concat(arrays)\n        return embeddings\n</code></pre> <p>Then you can specify the <code>loader</code> parameter in the <code>ModelMeta</code> class:</p> <pre><code>your_model = ModelMeta(\n    loader=CustomModel,\n    loader_kwargs={...},\n    ...\n)\n</code></pre>"},{"location":"contributing/adding_a_model/#adding-model-dependencies","title":"Adding model dependencies","text":"<p>If you are adding a model that requires additional dependencies, you can add them to the <code>pyproject.toml</code> file, under optional dependencies:</p> <pre><code>voyageai = [\"voyageai&gt;=1.0.0,&lt;2.0.0\"]\n</code></pre> <p>This ensures that the implementation does not break if a package is updated.</p> <p>As it is an optional dependency, you can't use top-level dependencies, but will instead have to use import inside the wrapper scope:</p> Adding optional dependencies <pre><code>from mteb._requires_package import requires_package\n\nclass VoyageAIModel:\n    def __init__(self, model_name: str, revision: str, **kwargs) -&gt; None:\n        requires_package(self, \"voyageai\", model_name, \"pip install 'mteb[voyageai]'\")\n        import voyageai\n        ...\n</code></pre>"},{"location":"contributing/adding_a_model/#submitting-your-model-as-a-pr","title":"Submitting your model as a PR","text":"<p>When submitting you models as a PR, please copy and paste the following checklist into pull request message:</p> <pre><code>- [ ] I have filled out the ModelMeta object to the extent possible\n- [ ] I have ensured that my model can be loaded using\n  - [ ] `mteb.get_model(model_name, revision)` and\n  - [ ] `mteb.get_model_meta(model_name, revision)`\n- [ ] I have tested the implementation works on a representative set of tasks.\n- [ ] The model is public, i.e. is available either as an API or the weight are publicly available to download\n</code></pre> <ol> <li> <p>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: a technical report. arXiv preprint arXiv:2402.05672, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"overview/","title":"Overview","text":"<p>This section provides an overview of available tasks, models and benchmarks in MTEB.</p>"},{"location":"overview/available_benchmarks/","title":"Available Benchmarks","text":""},{"location":"overview/available_benchmarks/#beir","title":"BEIR","text":"<p>BEIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark.</p> <p>Learn more \u2192</p> Tasks name type modalities languages TRECCOVID Retrieval text eng NFCorpus Retrieval text eng NQ Retrieval text eng HotpotQA Retrieval text eng FiQA2018 Retrieval text eng ArguAna Retrieval text eng Touche2020 Retrieval text eng CQADupstackRetrieval Retrieval text eng QuoraRetrieval Retrieval text eng DBPedia Retrieval text eng SCIDOCS Retrieval text eng FEVER Retrieval text eng ClimateFEVER Retrieval text eng SciFact Retrieval text eng MSMARCO Retrieval text eng Citation <pre><code>@article{thakur2021beir,\n  author = {Thakur, Nandan and Reimers, Nils and R{\\\"u}ckl{\\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},\n  journal = {arXiv preprint arXiv:2104.08663},\n  title = {Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#beir-nl","title":"BEIR-NL","text":"<p>BEIR-NL is a Dutch adaptation of the publicly available BEIR benchmark, created through automated translation.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ArguAna-NL Retrieval text nld CQADupstack-NL Retrieval text nld FEVER-NL Retrieval text nld NQ-NL Retrieval text nld Touche2020-NL Retrieval text nld FiQA2018-NL Retrieval text nld Quora-NL Retrieval text nld HotpotQA-NL Retrieval text nld SCIDOCS-NL Retrieval text nld ClimateFEVER-NL Retrieval text nld mMARCO-NL Retrieval text nld SciFact-NL Retrieval text nld DBPedia-NL Retrieval text nld NFCorpus-NL Retrieval text nld TRECCOVID-NL Retrieval text nld Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#bright","title":"BRIGHT","text":"<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval.     BRIGHT is the first text retrieval     benchmark that requires intensive reasoning to retrieve relevant documents with     a dataset consisting of 1,384 real-world queries spanning diverse domains, such as     economics, psychology, mathematics, and coding. These queries are drawn from     naturally occurring and carefully curated human data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BrightRetrieval Retrieval text eng Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#bright-long","title":"BRIGHT (long)","text":"<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents with a dataset consisting of 1,384 real-world queries spanning diverse domains, such as economics, psychology, mathematics, and coding. These queries are drawn from naturally occurring and carefully curated human data.</p> <p>This is the long version of the benchmark, which only filter longer documents.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BrightLongRetrieval Retrieval text eng Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#builtbencheng","title":"BuiltBench(eng)","text":"<p>\"Built-Bench\" is an ongoing effort aimed at evaluating text embedding models in the context of built asset management, spanning over various disciplines such as architecture, engineering, construction, and operations management of the built environment.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BuiltBenchClusteringP2P Clustering text eng BuiltBenchClusteringS2S Clustering text eng BuiltBenchRetrieval Retrieval text eng BuiltBenchReranking Reranking text eng Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#chemteb","title":"ChemTEB","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PubChemSMILESBitextMining BitextMining text eng SDSEyeProtectionClassification Classification text eng SDSGlovesClassification Classification text eng WikipediaBioMetChemClassification Classification text eng WikipediaGreenhouseEnantiopureClassification Classification text eng WikipediaSolidStateColloidalClassification Classification text eng WikipediaOrganicInorganicClassification Classification text eng WikipediaCryobiologySeparationClassification Classification text eng WikipediaChemistryTopicsClassification Classification text eng WikipediaTheoreticalAppliedClassification Classification text eng WikipediaChemFieldsClassification Classification text eng WikipediaLuminescenceClassification Classification text eng WikipediaIsotopesFissionClassification Classification text eng WikipediaSaltsSemiconductorsClassification Classification text eng WikipediaBiolumNeurochemClassification Classification text eng WikipediaCrystallographyAnalyticalClassification Classification text eng WikipediaCompChemSpectroscopyClassification Classification text eng WikipediaChemEngSpecialtiesClassification Classification text eng WikipediaChemistryTopicsClustering Clustering text eng WikipediaSpecialtiesInChemistryClustering Clustering text eng PubChemAISentenceParaphrasePC PairClassification text eng PubChemSMILESPC PairClassification text eng PubChemSynonymPC PairClassification text eng PubChemWikiParagraphsPC PairClassification text eng PubChemWikiPairClassification PairClassification text ces, deu, eng, fra, hin, ... (13) ChemNQRetrieval Retrieval text eng ChemHotpotQARetrieval Retrieval text eng Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\\\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#coir","title":"CoIR","text":"<p>CoIR: A Comprehensive Benchmark for Code Information Retrieval Models</p> <p>Learn more \u2192</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python CodeFeedbackMT Retrieval text eng CodeFeedbackST Retrieval text eng CodeSearchNetCCRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeTransOceanContest Retrieval text c++, python CodeTransOceanDL Retrieval text python CosQA Retrieval text eng, python COIRCodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) StackOverflowQA Retrieval text eng SyntheticText2SQL Retrieval text eng, sql Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#coderag","title":"CodeRAG","text":"<p>A benchmark for evaluating code retrieval augmented generation, testing models' ability to retrieve relevant programming solutions, tutorials and documentation.</p> <p>Learn more \u2192</p> Tasks name type modalities languages CodeRAGLibraryDocumentationSolutions Reranking text python CodeRAGOnlineTutorials Reranking text python CodeRAGProgrammingSolutions Reranking text python CodeRAGStackoverflowPosts Reranking text python Citation <pre><code>@misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#encodechka","title":"Encodechka","text":"<p>A benchmark for evaluating text embedding models on Russian data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages RUParaPhraserSTS STS text rus SentiRuEval2016 Classification text rus RuToxicOKMLCUPClassification Classification text rus InappropriatenessClassificationv2 Classification text rus RuNLUIntentClassification Classification text rus XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) RuSTSBenchmarkSTS STS text rus Citation <pre><code>@misc{dale_encodechka,\n  author = {Dale, David},\n  editor = {habr.com},\n  month = {June},\n  note = {[Online; posted 12-June-2022]},\n  title = {Russian rating of sentence encoders},\n  url = {https://habr.com/ru/articles/669674/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#followir","title":"FollowIR","text":"<p>Retrieval w/Instructions is the task of finding relevant documents for a query that has detailed instructions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Robust04InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Core17InstructionRetrieval InstructionReranking text eng Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#humev1","title":"HUME(v1)","text":"<p>The HUME benchmark is designed to evaluate the performance of text embedding models and humans on a comparable set of tasks. This captures areas where models perform better than human annotators and the reverse. In the paper, we go further into the analysis and what conclusions can be drawn.</p> <p>Learn more \u2192</p> Tasks name type modalities languages HUMEEmotionClassification Classification text eng HUMEToxicConversationsClassification Classification text eng HUMETweetSentimentExtractionClassification Classification text eng HUMEMultilingualSentimentClassification Classification text ara, eng, nob, rus HUMEArxivClusteringP2P Clustering text eng HUMERedditClusteringP2P Clustering text eng HUMEWikiCitiesClustering Clustering text eng HUMESIB200ClusteringS2S Clustering text ara, dan, eng, fra, rus HUMECore17InstructionReranking Reranking text eng HUMENews21InstructionReranking Reranking text eng HUMERobust04InstructionReranking Reranking text eng HUMEWikipediaRerankingMultilingual Reranking text dan, eng, nob HUMESICK-R STS text eng HUMESTS12 STS text eng HUMESTSBenchmark STS text eng HUMESTS22 STS text ara, eng, fra, rus"},{"location":"overview/available_benchmarks/#jinavdr","title":"JinaVDR","text":"<p>Multilingual, domain-diverse and layout-rich document retrieval benchmark.</p> <p>Learn more \u2192</p> Tasks name type modalities languages JinaVDRMedicalPrescriptionsRetrieval DocumentUnderstanding text, image eng JinaVDRStanfordSlideRetrieval DocumentUnderstanding text, image eng JinaVDRDonutVQAISynHMPRetrieval DocumentUnderstanding text, image eng JinaVDRTableVQARetrieval DocumentUnderstanding text, image eng JinaVDRChartQARetrieval DocumentUnderstanding text, image eng JinaVDRTQARetrieval DocumentUnderstanding text, image eng JinaVDROpenAINewsRetrieval DocumentUnderstanding text, image eng JinaVDREuropeanaDeNewsRetrieval DocumentUnderstanding text, image deu JinaVDREuropeanaEsNewsRetrieval DocumentUnderstanding text, image spa JinaVDREuropeanaItScansRetrieval DocumentUnderstanding text, image ita JinaVDREuropeanaNlLegalRetrieval DocumentUnderstanding text, image nld JinaVDRHindiGovVQARetrieval DocumentUnderstanding text, image hin JinaVDRAutomobileCatelogRetrieval DocumentUnderstanding text, image jpn JinaVDRBeveragesCatalogueRetrieval DocumentUnderstanding text, image rus JinaVDRRamensBenchmarkRetrieval DocumentUnderstanding text, image jpn JinaVDRJDocQARetrieval DocumentUnderstanding text, image jpn JinaVDRHungarianDocQARetrieval DocumentUnderstanding text, image hun JinaVDRArabicChartQARetrieval DocumentUnderstanding text, image ara JinaVDRArabicInfographicsVQARetrieval DocumentUnderstanding text, image ara JinaVDROWIDChartsRetrieval DocumentUnderstanding text, image eng JinaVDRMPMQARetrieval DocumentUnderstanding text, image eng JinaVDRJina2024YearlyBookRetrieval DocumentUnderstanding text, image eng JinaVDRWikimediaCommonsMapsRetrieval DocumentUnderstanding text, image eng JinaVDRPlotQARetrieval DocumentUnderstanding text, image eng JinaVDRMMTabRetrieval DocumentUnderstanding text, image eng JinaVDRCharXivOCRRetrieval DocumentUnderstanding text, image eng JinaVDRStudentEnrollmentSyntheticRetrieval DocumentUnderstanding text, image eng JinaVDRGitHubReadmeRetrieval DocumentUnderstanding text, image ara, ben, deu, eng, fra, ... (17) JinaVDRTweetStockSyntheticsRetrieval DocumentUnderstanding text, image ara, deu, eng, fra, hin, ... (10) JinaVDRAirbnbSyntheticRetrieval DocumentUnderstanding text, image ara, deu, eng, fra, hin, ... (10) JinaVDRShanghaiMasterPlanRetrieval DocumentUnderstanding text, image zho JinaVDRWikimediaCommonsDocumentsRetrieval DocumentUnderstanding text, image ara, ben, deu, eng, fra, ... (20) JinaVDREuropeanaFrNewsRetrieval DocumentUnderstanding text, image fra JinaVDRDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng JinaVDRDocQAAI DocumentUnderstanding text, image eng JinaVDRShiftProjectRetrieval DocumentUnderstanding text, image eng JinaVDRTatQARetrieval DocumentUnderstanding text, image eng JinaVDRInfovqaRetrieval DocumentUnderstanding text, image eng JinaVDRDocVQARetrieval DocumentUnderstanding text, image eng JinaVDRDocQAGovReportRetrieval DocumentUnderstanding text, image eng JinaVDRTabFQuadRetrieval DocumentUnderstanding text, image eng JinaVDRDocQAEnergyRetrieval DocumentUnderstanding text, image eng JinaVDRArxivQARetrieval DocumentUnderstanding text, image eng Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#longembed","title":"LongEmbed","text":"<p>LongEmbed is a benchmark oriented at exploring models' performance on long-context retrieval.     The benchmark comprises two synthetic tasks and four carefully chosen real-world tasks,     featuring documents of varying length and dispersed target information.</p> <p>Learn more \u2192</p> Tasks name type modalities languages LEMBNarrativeQARetrieval Retrieval text eng LEMBNeedleRetrieval Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng LEMBQMSumRetrieval Retrieval text eng LEMBSummScreenFDRetrieval Retrieval text eng LEMBWikimQARetrieval Retrieval text eng Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#miebimg","title":"MIEB(Img)","text":"<p>A image-only version of MIEB(Multilingual) that consists of 49 tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages CUB200I2IRetrieval Any2AnyRetrieval image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng METI2IRetrieval Any2AnyRetrieval image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng VOC2007 ImageClassification image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng STS17MultilingualVisualSTS VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) STSBenchmarkMultilingualVisualSTS VisualSTS(multi) image cmn, deu, eng, fra, ita, ... (10) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#miebmultilingual","title":"MIEB(Multilingual)","text":"<p>MIEB(Multilingual) is a comprehensive image embeddings benchmark, spanning 10 task types, covering 130 tasks and a total of 39 languages.     In addition to image classification (zero shot and linear probing), clustering, retrieval, MIEB includes tasks in compositionality evaluation,     document understanding, visual STS, and CV-centric tasks. This benchmark consists of MIEB(eng) + 3 multilingual retrieval     datasets + the multilingual parts of VisualSTS-b and VisualSTS-16.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng VOC2007 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng BirdsnapZeroShot ZeroShotClassification image, text eng Caltech101ZeroShot ZeroShotClassification text, image eng CIFAR10ZeroShot ZeroShotClassification text, image eng CIFAR100ZeroShot ZeroShotClassification text, image eng CLEVRZeroShot ZeroShotClassification text, image eng CLEVRCountZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng DTDZeroShot ZeroShotClassification image, text eng EuroSATZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng GTSRBZeroShot ZeroShotClassification image eng Imagenet1kZeroShot ZeroShotClassification image, text eng MNISTZeroShot ZeroShotClassification image, text eng OxfordPetsZeroShot ZeroShotClassification text, image eng PatchCamelyonZeroShot ZeroShotClassification image, text eng RenderedSST2 ZeroShotClassification text, image eng RESISC45ZeroShot ZeroShotClassification image, text eng StanfordCarsZeroShot ZeroShotClassification image, text eng STL10ZeroShot ZeroShotClassification image, text eng SUN397ZeroShot ZeroShotClassification image, text eng UCF101ZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng BLINKIT2TMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng SugarCrepe Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng BLINKIT2IRetrieval Any2AnyRetrieval text, image eng BLINKIT2TRetrieval Any2AnyRetrieval text, image eng CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng EDIST2ITRetrieval Any2AnyRetrieval text, image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng Fashion200kT2IRetrieval Any2AnyRetrieval text, image eng FashionIQIT2IRetrieval Any2AnyRetrieval text, image eng Flickr30kI2TRetrieval Any2AnyRetrieval text, image eng Flickr30kT2IRetrieval Any2AnyRetrieval text, image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng GLDv2I2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesT2IRetrieval Any2AnyRetrieval text, image eng ImageCoDeT2IRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2ITRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng MemotionI2TRetrieval Any2AnyRetrieval text, image eng MemotionT2IRetrieval Any2AnyRetrieval text, image eng METI2IRetrieval Any2AnyRetrieval image eng MSCOCOI2TRetrieval Any2AnyRetrieval text, image eng MSCOCOT2IRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2ITRetrieval Any2AnyRetrieval image, text eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SciMMIRI2TRetrieval Any2AnyRetrieval text, image eng SciMMIRT2IRetrieval Any2AnyRetrieval text, image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng TUBerlinT2IRetrieval Any2AnyRetrieval text, image eng VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VisualNewsT2IRetrieval Any2AnyRetrieval image, text eng VizWizIT2TRetrieval Any2AnyRetrieval text, image eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WebQAT2TRetrieval Any2AnyRetrieval text eng WITT2IRetrieval Any2AnyMultilingualRetrieval text, image ara, bul, dan, ell, eng, ... (11) XFlickr30kCoT2IRetrieval Any2AnyMultilingualRetrieval text, image deu, eng, ind, jpn, rus, ... (8) XM3600T2IRetrieval Any2AnyMultilingualRetrieval text, image ara, ben, ces, dan, deu, ... (38) VisualSTS17Eng VisualSTS(eng) image eng VisualSTS-b-Eng VisualSTS(eng) image eng VisualSTS17Multilingual VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) VisualSTS-b-Multilingual VisualSTS(multi) image cmn, deu, fra, ita, nld, ... (9) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#miebeng","title":"MIEB(eng)","text":"<p>MIEB(eng) is a comprehensive image embeddings benchmark, spanning 8 task types, covering 125 tasks.     In addition to image classification (zero shot and linear probing), clustering, retrieval, MIEB includes tasks in compositionality evaluation,     document understanding, visual STS, and CV-centric tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng VOC2007 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng BirdsnapZeroShot ZeroShotClassification image, text eng Caltech101ZeroShot ZeroShotClassification text, image eng CIFAR10ZeroShot ZeroShotClassification text, image eng CIFAR100ZeroShot ZeroShotClassification text, image eng CLEVRZeroShot ZeroShotClassification text, image eng CLEVRCountZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng DTDZeroShot ZeroShotClassification image, text eng EuroSATZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng GTSRBZeroShot ZeroShotClassification image eng Imagenet1kZeroShot ZeroShotClassification image, text eng MNISTZeroShot ZeroShotClassification image, text eng OxfordPetsZeroShot ZeroShotClassification text, image eng PatchCamelyonZeroShot ZeroShotClassification image, text eng RenderedSST2 ZeroShotClassification text, image eng RESISC45ZeroShot ZeroShotClassification image, text eng StanfordCarsZeroShot ZeroShotClassification image, text eng STL10ZeroShot ZeroShotClassification image, text eng SUN397ZeroShot ZeroShotClassification image, text eng UCF101ZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng BLINKIT2TMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng SugarCrepe Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng BLINKIT2IRetrieval Any2AnyRetrieval text, image eng BLINKIT2TRetrieval Any2AnyRetrieval text, image eng CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng EDIST2ITRetrieval Any2AnyRetrieval text, image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng Fashion200kT2IRetrieval Any2AnyRetrieval text, image eng FashionIQIT2IRetrieval Any2AnyRetrieval text, image eng Flickr30kI2TRetrieval Any2AnyRetrieval text, image eng Flickr30kT2IRetrieval Any2AnyRetrieval text, image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng GLDv2I2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesT2IRetrieval Any2AnyRetrieval text, image eng ImageCoDeT2IRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2ITRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng MemotionI2TRetrieval Any2AnyRetrieval text, image eng MemotionT2IRetrieval Any2AnyRetrieval text, image eng METI2IRetrieval Any2AnyRetrieval image eng MSCOCOI2TRetrieval Any2AnyRetrieval text, image eng MSCOCOT2IRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2ITRetrieval Any2AnyRetrieval image, text eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SciMMIRI2TRetrieval Any2AnyRetrieval text, image eng SciMMIRT2IRetrieval Any2AnyRetrieval text, image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng TUBerlinT2IRetrieval Any2AnyRetrieval text, image eng VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VisualNewsT2IRetrieval Any2AnyRetrieval image, text eng VizWizIT2TRetrieval Any2AnyRetrieval text, image eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WebQAT2TRetrieval Any2AnyRetrieval text eng VisualSTS17Eng VisualSTS(eng) image eng VisualSTS-b-Eng VisualSTS(eng) image eng Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mieblite","title":"MIEB(lite)","text":"<p>MIEB(lite) is a comprehensive image embeddings benchmark, spanning 10 task types, covering 51 tasks.     This is a lite version of MIEB(Multilingual), designed to be run at a fraction of the cost while maintaining     relative rank of models.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng GTSRB ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng SUN397 ImageClassification image eng ImageNetDog15Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng CIFAR100ZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng OxfordPetsZeroShot ZeroShotClassification text, image eng StanfordCarsZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS13VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng VisualSTS17Multilingual VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) VisualSTS-b-Multilingual VisualSTS(multi) image cmn, deu, fra, ita, nld, ... (9) CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng RP2kI2IRetrieval Any2AnyRetrieval image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WITT2IRetrieval Any2AnyMultilingualRetrieval text, image ara, bul, dan, ell, eng, ... (11) XM3600T2IRetrieval Any2AnyMultilingualRetrieval text, image ara, ben, ces, dan, deu, ... (38) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#minersbitextmining","title":"MINERSBitextMining","text":"<p>Bitext Mining texts from the MINERS benchmark, a benchmark designed to evaluate the     ability of multilingual LMs in semantic retrieval tasks,     including bitext mining and classification via retrieval-augmented contexts.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BUCC BitextMining text cmn, deu, eng, fra, rus LinceMTBitextMining BitextMining text eng, hin NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) PhincBitextMining BitextMining text eng, hin Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) Citation <pre><code>@article{winata2024miners,\n  author = {Winata, Genta Indra and Zhang, Ruochen and Adelani, David Ifeoluwa},\n  journal = {arXiv preprint arXiv:2406.07424},\n  title = {MINERS: Multilingual Language Models as Semantic Retrievers},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebcode-v1","title":"MTEB(Code, v1)","text":"<p>A massive code embedding benchmark covering retrieval tasks in a miriad of popular programming languages.</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python CodeEditSearchRetrieval Retrieval text c, c++, go, java, javascript, ... (13) CodeFeedbackMT Retrieval text eng CodeFeedbackST Retrieval text eng CodeSearchNetCCRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeTransOceanContest Retrieval text c++, python CodeTransOceanDL Retrieval text python CosQA Retrieval text eng, python COIRCodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) StackOverflowQA Retrieval text eng SyntheticText2SQL Retrieval text eng, sql Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeurope-v1","title":"MTEB(Europe, v1)","text":"<p>A regional geopolitical text embedding benchmark targeting embedding performance on European languages.</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicChatClassification Classification text eng ToxicConversationsClassification Classification text eng EstonianValenceClassification Classification text est ItaCaseholdClassification Classification text ita AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita TweetSentimentClassification Classification text ara, deu, eng, fra, hin, ... (8) CBD Classification text pol PolEmo2.0-OUT Classification text pol CSFDSKMovieReviewSentimentClassification Classification text slk DalajClassification Classification text swe WikiCitiesClustering Clustering text eng RomaniBibleClustering Clustering text rom BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan LegalQuAD Retrieval text deu ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng WinoGrande Retrieval text eng AlloprofRetrieval Retrieval text fra BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PSC PairClassification text pol WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SICK-R STS text eng STS12 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FinParaSTS STS text fin STS17 STS text ara, deu, eng, fra, ita, ... (9) SICK-R-PL STS text pol STSES STS text spa Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebindic-v1","title":"MTEB(Indic, v1)","text":"<p>A regional geopolitical text embedding benchmark targeting embedding performance on Indic languages.</p> Tasks name type modalities languages IN22ConvBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) BengaliSentimentAnalysis Classification text ben GujaratiNewsClassification Classification text guj HindiDiscourseClassification Classification text hin SentimentAnalysisHindi Classification text hin MalayalamNewsClassification Classification text mal MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) TweetSentimentClassification Classification text ara, deu, eng, fra, hin, ... (8) NepaliNewsClassification Classification text nep PunjabiNewsClassification Classification text pan SanskritShlokasClassification Classification text san UrduRomanSentimentClassification Classification text urd XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) XQuADRetrieval Retrieval text arb, deu, ell, eng, hin, ... (12) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mteblaw-v1","title":"MTEB(Law, v1)","text":"<p>A benchmark of retrieval tasks in the legal domain.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng GerDaLIRSmall Retrieval text deu LeCaRDv2 Retrieval text zho LegalBenchConsumerContractsQA Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LegalQuAD Retrieval text deu"},{"location":"overview/available_benchmarks/#mtebmedical-v1","title":"MTEB(Medical, v1)","text":"<p>A curated set of MTEB tasks designed to evaluate systems in the context of medical information retrieval.</p> Tasks name type modalities languages CUREv1 Retrieval text eng, fra, spa NFCorpus Retrieval text eng TRECCOVID Retrieval text eng TRECCOVID-PL Retrieval text pol SciFact Retrieval text eng SciFact-PL Retrieval text pol MedicalQARetrieval Retrieval text eng PublicHealthQA Retrieval text ara, eng, fra, kor, rus, ... (8) MedrxivClusteringP2P.v2 Clustering text eng MedrxivClusteringS2S.v2 Clustering text eng CmedqaRetrieval Retrieval text cmn CMedQAv2-reranking Reranking text cmn"},{"location":"overview/available_benchmarks/#mtebmultilingual-v1","title":"MTEB(Multilingual, v1)","text":"<p>A large-scale multilingual expansion of MTEB, driven mainly by highly-curated community contributions covering 250+ languages. This benhcmark has been replaced by MTEB(Multilingual, v2) as one of the datasets (SNLHierarchicalClustering) included in v1 was removed from the Hugging Face Hub.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IndicGenBenchFloresBitextMining BitextMining text asm, awa, ben, bgc, bho, ... (30) NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicConversationsClassification Classification text eng TweetTopicSingleClassification Classification text eng EstonianValenceClassification Classification text est FilipinoShopeeReviewsClassification Classification text fil GujaratiNewsClassification Classification text guj SentimentAnalysisHindi Classification text hin IndonesianIdClickbaitClassification Classification text ind ItaCaseholdClassification Classification text ita KorSarcasmClassification Classification text kor KurdishSentimentClassification Classification text kur MacedonianTweetSentimentClassification Classification text mkd AfriSentiClassification Classification text amh, arq, ary, hau, ibo, ... (12) AmazonCounterfactualClassification Classification text deu, eng, jpn CataloniaTweetClassification Classification text cat, spa CyrillicTurkicLangClassification Classification text bak, chv, kaz, kir, krc, ... (9) IndicLangClassification Classification text asm, ben, brx, doi, gom, ... (22) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NusaParagraphEmotionClassification Classification text bbc, bew, bug, jav, mad, ... (10) NusaX-senti Classification text ace, ban, bbc, bjn, bug, ... (12) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita NepaliNewsClassification Classification text nep OdiaNewsClassification Classification text ory PunjabiNewsClassification Classification text pan PolEmo2.0-OUT Classification text pol PAC Classification text pol SinhalaNewsClassification Classification text sin CSFDSKMovieReviewSentimentClassification Classification text slk SiswatiNewsClassification Classification text ssw SlovakMovieReviewSentimentClassification Classification text slk SwahiliNewsClassification Classification text swa DalajClassification Classification text swe TswanaNewsClassification Classification text tsn IsiZuluNewsClassification Classification text zul WikiCitiesClustering Clustering text eng MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) RomaniBibleClustering Clustering text rom ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng MedrxivClusteringP2P.v2 Clustering text eng StackExchangeClustering.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) PlscClusteringP2P.v2 Clustering text pol SwednClusteringP2P Clustering text swe CLSClusteringP2P.v2 Clustering text cmn StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan AILAStatutes Retrieval text eng ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TRECCOVID Retrieval text eng WinoGrande Retrieval text eng BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) MLQARetrieval Retrieval text ara, deu, eng, hin, spa, ... (7) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) CovidRetrieval Retrieval text cmn Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng KorHateSpeechMLClassification MultilabelClassification text kor MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) BrazilianToxicTweetsClassification MultilabelClassification text por CEDRClassification MultilabelClassification text rus CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng TwitterURLCorpus PairClassification text eng ArmenianParaphrasePC PairClassification text hye indonli PairClassification text ind OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PpcPC PairClassification text pol TERRa PairClassification text rus WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra VoyageMMarcoReranking Reranking text jpn WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) RuBQReranking Reranking text rus T2Reranking Reranking text cmn GermanSTSBenchmark STS text deu SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FaroeseSTS STS text fao FinParaSTS STS text fin JSICK STS text jpn IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) SemRel24STS STS text afr, amh, arb, arq, ary, ... (12) STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) STSES STS text spa STSB STS text cmn MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) SNLHierarchicalClusteringP2P Clustering text nob Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebmultilingual-v2","title":"MTEB(Multilingual, v2)","text":"<p>A large-scale multilingual expansion of MTEB, driven mainly by highly-curated community contributions covering 250+ languages. </p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IndicGenBenchFloresBitextMining BitextMining text asm, awa, ben, bgc, bho, ... (30) NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicConversationsClassification Classification text eng TweetTopicSingleClassification Classification text eng EstonianValenceClassification Classification text est FilipinoShopeeReviewsClassification Classification text fil GujaratiNewsClassification Classification text guj SentimentAnalysisHindi Classification text hin IndonesianIdClickbaitClassification Classification text ind ItaCaseholdClassification Classification text ita KorSarcasmClassification Classification text kor KurdishSentimentClassification Classification text kur MacedonianTweetSentimentClassification Classification text mkd AfriSentiClassification Classification text amh, arq, ary, hau, ibo, ... (12) AmazonCounterfactualClassification Classification text deu, eng, jpn CataloniaTweetClassification Classification text cat, spa CyrillicTurkicLangClassification Classification text bak, chv, kaz, kir, krc, ... (9) IndicLangClassification Classification text asm, ben, brx, doi, gom, ... (22) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NusaParagraphEmotionClassification Classification text bbc, bew, bug, jav, mad, ... (10) NusaX-senti Classification text ace, ban, bbc, bjn, bug, ... (12) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita NepaliNewsClassification Classification text nep OdiaNewsClassification Classification text ory PunjabiNewsClassification Classification text pan PolEmo2.0-OUT Classification text pol PAC Classification text pol SinhalaNewsClassification Classification text sin CSFDSKMovieReviewSentimentClassification Classification text slk SiswatiNewsClassification Classification text ssw SlovakMovieReviewSentimentClassification Classification text slk SwahiliNewsClassification Classification text swa DalajClassification Classification text swe TswanaNewsClassification Classification text tsn IsiZuluNewsClassification Classification text zul WikiCitiesClustering Clustering text eng MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) RomaniBibleClustering Clustering text rom ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng MedrxivClusteringP2P.v2 Clustering text eng StackExchangeClustering.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) PlscClusteringP2P.v2 Clustering text pol SwednClusteringP2P Clustering text swe CLSClusteringP2P.v2 Clustering text cmn StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan AILAStatutes Retrieval text eng ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TRECCOVID Retrieval text eng WinoGrande Retrieval text eng BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) MLQARetrieval Retrieval text ara, deu, eng, hin, spa, ... (7) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) CovidRetrieval Retrieval text cmn Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng KorHateSpeechMLClassification MultilabelClassification text kor MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) BrazilianToxicTweetsClassification MultilabelClassification text por CEDRClassification MultilabelClassification text rus CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng TwitterURLCorpus PairClassification text eng ArmenianParaphrasePC PairClassification text hye indonli PairClassification text ind OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PpcPC PairClassification text pol TERRa PairClassification text rus WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra VoyageMMarcoReranking Reranking text jpn WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) RuBQReranking Reranking text rus T2Reranking Reranking text cmn GermanSTSBenchmark STS text deu SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FaroeseSTS STS text fao FinParaSTS STS text fin JSICK STS text jpn IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) SemRel24STS STS text afr, amh, arb, arq, ary, ... (12) STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) STSES STS text spa STSB STS text cmn MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebscandinavian-v1","title":"MTEB(Scandinavian, v1)","text":"<p>A curated selection of tasks coverering the Scandinavian languages; Danish, Swedish and Norwegian, including Bokm\u00e5l and Nynorsk.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan NorwegianCourtsBitextMining BitextMining text nno, nob AngryTweetsClassification Classification text dan DanishPoliticalCommentsClassification Classification text dan DalajClassification Classification text swe DKHateClassification Classification text dan LccSentimentClassification Classification text dan MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NoRecClassification Classification text nob NorwegianParliamentClassification Classification text nob ScalaClassification Classification text dan, nno, nob, swe SwedishSentimentClassification Classification text swe SweRecClassification Classification text swe DanFeverRetrieval Retrieval text dan NorQuadRetrieval Retrieval text nob SNLRetrieval Retrieval text nob SwednRetrieval Retrieval text swe SweFaqRetrieval Retrieval text swe TV2Nordretrieval Retrieval text dan TwitterHjerneRetrieval Retrieval text dan SNLHierarchicalClusteringS2S Clustering text nob SNLHierarchicalClusteringP2P Clustering text nob SwednClusteringP2P Clustering text swe SwednClusteringS2S Clustering text swe VGHierarchicalClusteringS2S Clustering text nob VGHierarchicalClusteringP2P Clustering text nob Citation <pre><code>@inproceedings{enevoldsen2024scandinavian,\n  author = {Enevoldsen, Kenneth and Kardos, M{\\'a}rton and Muennighoff, Niklas and Nielbo, Kristoffer},\n  booktitle = {Advances in Neural Information Processing Systems},\n  title = {The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding},\n  url = {https://nips.cc/virtual/2024/poster/97869},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebcmn-v1","title":"MTEB(cmn, v1)","text":"<p>The Chinese Massive Text Embedding Benchmark (C-MTEB) is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages T2Retrieval Retrieval text cmn MMarcoRetrieval Retrieval text cmn DuRetrieval Retrieval text cmn CovidRetrieval Retrieval text cmn CmedqaRetrieval Retrieval text cmn EcomRetrieval Retrieval text cmn MedicalRetrieval Retrieval text cmn VideoRetrieval Retrieval text cmn T2Reranking Reranking text cmn MMarcoReranking Reranking text cmn CMedQAv1-reranking Reranking text cmn CMedQAv2-reranking Reranking text cmn Ocnli PairClassification text cmn Cmnli PairClassification text cmn CLSClusteringS2S Clustering text cmn CLSClusteringP2P Clustering text cmn ThuNewsClusteringS2S Clustering text cmn ThuNewsClusteringP2P Clustering text cmn LCQMC STS text cmn PAWSX STS text cmn AFQMC STS text cmn QBQTC STS text cmn TNews Classification text cmn IFlyTek Classification text cmn Waimai Classification text cmn OnlineShopping Classification text cmn JDReview Classification text cmn MultilingualSentiment Classification text cmn ATEC STS text cmn BQ STS text cmn STSB STS text cmn MultilingualSentiment Classification text cmn Citation <pre><code>@misc{c-pack,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebdeu-v1","title":"MTEB(deu, v1)","text":"<p>A benchmark for text-embedding performance in German.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AmazonCounterfactualClassification Classification text deu, eng, jpn AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) BlurbsClusteringP2P Clustering text deu BlurbsClusteringS2S Clustering text deu TenKGnadClusteringP2P Clustering text deu TenKGnadClusteringS2S Clustering text deu FalseFriendsGermanEnglish PairClassification text deu PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) GermanQuAD-Retrieval Retrieval text deu GermanDPR Retrieval text deu XMarket Retrieval text deu, eng, spa GerDaLIR Retrieval text deu GermanSTSBenchmark STS text deu STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@misc{wehrli2024germantextembeddingclustering,\n  archiveprefix = {arXiv},\n  author = {Silvan Wehrli and Bert Arnrich and Christopher Irrgang},\n  eprint = {2401.02709},\n  primaryclass = {cs.CL},\n  title = {German Text Embedding Clustering Benchmark},\n  url = {https://arxiv.org/abs/2401.02709},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeng-v1","title":"MTEB(eng, v1)","text":"<p>The original English benchmark by Muennighoff et al., (2023). This page is an adaptation of the old MTEB leaderboard. We recommend that you use MTEB(eng, v2) instead, as it uses updated versions of the task, making it notably faster to run and resolving a known bug in existing tasks. This benchmark also removes datasets common for fine-tuning, such as MSMARCO, which makes model performance scores more comparable. However, generally, both benchmarks provide similar estimates.</p> Tasks name type modalities languages AmazonPolarityClassification Classification text eng AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) ArguAna Retrieval text eng ArxivClusteringP2P Clustering text eng ArxivClusteringS2S Clustering text eng AskUbuntuDupQuestions Reranking text eng BIOSSES STS text eng Banking77Classification Classification text eng BiorxivClusteringP2P Clustering text eng BiorxivClusteringS2S Clustering text eng CQADupstackRetrieval Retrieval text eng ClimateFEVER Retrieval text eng DBPedia Retrieval text eng EmotionClassification Classification text eng FEVER Retrieval text eng FiQA2018 Retrieval text eng HotpotQA Retrieval text eng ImdbClassification Classification text eng MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MedrxivClusteringP2P Clustering text eng MedrxivClusteringS2S Clustering text eng MindSmallReranking Reranking text eng NFCorpus Retrieval text eng NQ Retrieval text eng QuoraRetrieval Retrieval text eng RedditClustering Clustering text eng RedditClusteringP2P Clustering text eng SCIDOCS Retrieval text eng SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STS16 STS text eng STSBenchmark STS text eng SciDocsRR Reranking text eng SciFact Retrieval text eng SprintDuplicateQuestions PairClassification text eng StackExchangeClustering Clustering text eng StackExchangeClusteringP2P Clustering text eng StackOverflowDupQuestions Reranking text eng SummEval Summarization text eng TRECCOVID Retrieval text eng Touche2020 Retrieval text eng ToxicConversationsClassification Classification text eng TweetSentimentExtractionClassification Classification text eng TwentyNewsgroupsClustering Clustering text eng TwitterSemEval2015 PairClassification text eng TwitterURLCorpus PairClassification text eng MSMARCO Retrieval text eng AmazonCounterfactualClassification Classification text deu, eng, jpn STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo\u00efc and Reimers, Nils},\n  doi = {10.48550/ARXIV.2210.07316},\n  journal = {arXiv preprint arXiv:2210.07316},\n  publisher = {arXiv},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2210.07316},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeng-v2","title":"MTEB(eng, v2)","text":"<p>The new English Massive Text Embedding Benchmark. This benchmark was created to account for the fact that many models have now been finetuned to tasks in the original MTEB, and contains tasks that are not as frequently used for model training. This way the new benchmark and leaderboard can give our users a more realistic expectation of models' generalization performance.</p> <p>The original MTEB leaderboard is available under the MTEB(eng, v1) tab.</p> Tasks name type modalities languages ArguAna Retrieval text eng ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng AskUbuntuDupQuestions Reranking text eng BIOSSES STS text eng Banking77Classification Classification text eng BiorxivClusteringP2P.v2 Clustering text eng CQADupstackGamingRetrieval Retrieval text eng CQADupstackUnixRetrieval Retrieval text eng ClimateFEVERHardNegatives Retrieval text eng FEVERHardNegatives Retrieval text eng FiQA2018 Retrieval text eng HotpotQAHardNegatives Retrieval text eng ImdbClassification Classification text eng MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MedrxivClusteringP2P.v2 Clustering text eng MedrxivClusteringS2S.v2 Clustering text eng MindSmallReranking Reranking text eng SCIDOCS Retrieval text eng SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng SprintDuplicateQuestions PairClassification text eng StackExchangeClustering.v2 Clustering text eng StackExchangeClusteringP2P.v2 Clustering text eng TRECCOVID Retrieval text eng Touche2020Retrieval.v3 Retrieval text eng ToxicConversationsClassification Classification text eng TweetSentimentExtractionClassification Classification text eng TwentyNewsgroupsClustering.v2 Clustering text eng TwitterSemEval2015 PairClassification text eng TwitterURLCorpus PairClassification text eng SummEvalSummarization.v2 Summarization text eng AmazonCounterfactualClassification Classification text deu, eng, jpn STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfas-v1","title":"MTEB(fas, v1)","text":"<p>The Persian Massive Text Embedding Benchmark (FaMTEB) is a comprehensive benchmark for Persian text embeddings covering 7 tasks and 60+ datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PersianFoodSentimentClassification Classification text fas SynPerChatbotConvSAClassification Classification text fas SynPerChatbotConvSAToneChatbotClassification Classification text fas SynPerChatbotConvSAToneUserClassification Classification text fas SynPerChatbotSatisfactionLevelClassification Classification text fas SynPerChatbotRAGToneChatbotClassification Classification text fas SynPerChatbotRAGToneUserClassification Classification text fas SynPerChatbotToneChatbotClassification Classification text fas SynPerChatbotToneUserClassification Classification text fas SynPerTextToneClassification Classification text fas SIDClassification Classification text fas DeepSentiPers Classification text fas PersianTextEmotion Classification text fas SentimentDKSF Classification text fas NLPTwitterAnalysisClassification Classification text fas DigikalamagClassification Classification text fas MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) BeytooteClustering Clustering text fas DigikalamagClustering Clustering text fas HamshahriClustring Clustering text fas NLPTwitterAnalysisClustering Clustering text fas SIDClustring Clustering text fas FarsTail PairClassification text fas CExaPPC PairClassification text fas SynPerChatbotRAGFAQPC PairClassification text fas FarsiParaphraseDetection PairClassification text fas SynPerTextKeywordsPC PairClassification text fas SynPerQAPC PairClassification text fas ParsinluEntail PairClassification text fas ParsinluQueryParaphPC PairClassification text fas MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SynPerQARetrieval Retrieval text fas SynPerChatbotTopicsRetrieval Retrieval text fas SynPerChatbotRAGTopicsRetrieval Retrieval text fas SynPerChatbotRAGFAQRetrieval Retrieval text fas PersianWebDocumentRetrieval Retrieval text fas WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) ClimateFEVER-Fa Retrieval text fas DBPedia-Fa Retrieval text fas HotpotQA-Fa Retrieval text fas MSMARCO-Fa Retrieval text fas NQ-Fa Retrieval text fas ArguAna-Fa Retrieval text fas CQADupstackRetrieval-Fa Retrieval text fas FiQA2018-Fa Retrieval text fas NFCorpus-Fa Retrieval text fas QuoraRetrieval-Fa Retrieval text fas SCIDOCS-Fa Retrieval text fas SciFact-Fa Retrieval text fas TRECCOVID-Fa Retrieval text fas Touche2020-Fa Retrieval text fas Farsick STS text fas SynPerSTS STS text fas Query2Query STS text fas SAMSumFa BitextMining text fas SynPerChatbotSumSRetrieval BitextMining text fas SynPerChatbotRAGSumSRetrieval BitextMining text fas Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfas-v2","title":"MTEB(fas, v2)","text":"<p>The Persian Massive Text Embedding Benchmark (FaMTEB) is a comprehensive benchmark for Persian text embeddings covering 7 tasks and 50+ datasets. In version 2, we have optimized large datasets to make them more manageable and accessible, removed low-quality datasets, and added higher-quality data to improve the overall benchmark. For more details on the improvements, see the main PR comment: main PR.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PersianFoodSentimentClassification Classification text fas SynPerChatbotConvSAClassification Classification text fas SynPerChatbotConvSAToneChatbotClassification Classification text fas SynPerChatbotConvSAToneUserClassification Classification text fas SynPerChatbotSatisfactionLevelClassification Classification text fas SynPerTextToneClassification.v3 Classification text fas SIDClassification.v2 Classification text fas DeepSentiPers.v2 Classification text fas PersianTextEmotion.v2 Classification text fas NLPTwitterAnalysisClassification.v2 Classification text fas DigikalamagClassification Classification text fas MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) StyleClassification Classification text fas PerShopDomainClassification Classification text fas PerShopIntentClassification Classification text fas BeytooteClustering Clustering text fas DigikalamagClustering Clustering text fas HamshahriClustring Clustering text fas NLPTwitterAnalysisClustering Clustering text fas SIDClustring Clustering text fas FarsTail PairClassification text fas SynPerChatbotRAGFAQPC PairClassification text fas FarsiParaphraseDetection PairClassification text fas SynPerTextKeywordsPC PairClassification text fas SynPerQAPC PairClassification text fas ParsinluEntail PairClassification text fas ParsinluQueryParaphPC PairClassification text fas MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SynPerQARetrieval Retrieval text fas SynPerChatbotRAGFAQRetrieval Retrieval text fas PersianWebDocumentRetrieval Retrieval text fas WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) HotpotQA-FaHardNegatives Retrieval text fas MSMARCO-FaHardNegatives Retrieval text fas NQ-FaHardNegatives Retrieval text fas ArguAna-Fa.v2 Retrieval text fas FiQA2018-Fa.v2 Retrieval text fas QuoraRetrieval-Fa.v2 Retrieval text fas SCIDOCS-Fa.v2 Retrieval text fas SciFact-Fa.v2 Retrieval text fas TRECCOVID-Fa.v2 Retrieval text fas FEVER-FaHardNegatives Retrieval text fas NeuCLIR2023RetrievalHardNegatives Retrieval text fas, rus, zho WebFAQRetrieval Retrieval text ara, aze, ben, bul, cat, ... (51) Farsick STS text fas SynPerSTS STS text fas SAMSumFa BitextMining text fas SynPerChatbotSumSRetrieval BitextMining text fas SynPerChatbotRAGSumSRetrieval BitextMining text fas Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfra-v1","title":"MTEB(fra, v1)","text":"<p>MTEB-French, a French expansion of the original benchmark with high-quality native French datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) AlloProfClusteringP2P Clustering text fra AlloProfClusteringS2S Clustering text fra HALClusteringS2S Clustering text fra MasakhaNEWSClusteringP2P Clustering text amh, eng, fra, hau, ibo, ... (16) MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) MLSUMClusteringP2P Clustering text deu, fra, rus, spa MLSUMClusteringS2S Clustering text deu, fra, rus, spa PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) AlloprofReranking Reranking text fra SyntecReranking Reranking text fra AlloprofRetrieval Retrieval text fra BSARDRetrieval Retrieval text fra MintakaRetrieval Retrieval text ara, deu, fra, hin, ita, ... (8) SyntecRetrieval Retrieval text fra XPQARetrieval Retrieval text ara, cmn, deu, eng, fra, ... (13) SICKFr STS text fra STSBenchmarkMultilingualSTS STS text cmn, deu, eng, fra, ita, ... (10) SummEvalFr Summarization text fra STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@misc{ciancone2024mtebfrenchresourcesfrenchsentence,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {MTEB-French: Resources for French Sentence Embedding Evaluation and Analysis},\n  url = {https://arxiv.org/abs/2405.20468},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebjpn-v1","title":"MTEB(jpn, v1)","text":"<p>JMTEB is a benchmark for evaluating Japanese text embedding models.</p> <p>Learn more \u2192</p> Tasks name type modalities languages LivedoorNewsClustering.v2 Clustering text jpn MewsC16JaClustering Clustering text jpn AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) JSTS STS text jpn JSICK STS text jpn PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) JaqketRetrieval Retrieval text jpn MrTidyRetrieval Retrieval text ara, ben, eng, fin, ind, ... (11) JaGovFaqsRetrieval Retrieval text jpn NLPJournalTitleAbsRetrieval Retrieval text jpn NLPJournalAbsIntroRetrieval Retrieval text jpn NLPJournalTitleIntroRetrieval Retrieval text jpn ESCIReranking Reranking text eng, jpn, spa"},{"location":"overview/available_benchmarks/#mtebkor-v1","title":"MTEB(kor, v1)","text":"<p>A benchmark and leaderboard for evaluation of text embedding in Korean.</p> Tasks name type modalities languages KLUE-TC Classification text kor MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) Ko-StrategyQA Retrieval text kor KLUE-STS STS text kor KorSTS STS text kor"},{"location":"overview/available_benchmarks/#mtebnld-v1","title":"MTEB(nld, v1)","text":"<p>MTEB-NL</p> <p>Learn more \u2192</p> Tasks name type modalities languages DutchBookReviewSentimentClassification Classification text nld MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) SIB200Classification Classification text ace, acm, acq, aeb, afr, ... (197) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) VaccinChatNLClassification Classification text nld DutchColaClassification Classification text nld DutchGovernmentBiasClassification Classification text nld DutchSarcasticHeadlinesClassification Classification text nld DutchNewsArticlesClassification Classification text nld OpenTenderClassification Classification text nld IconclassClassification Classification text nld SICKNLPairClassification PairClassification text nld XLWICNLPairClassification PairClassification text nld CovidDisinformationNLMultiLabelClassification MultilabelClassification text nld MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) VABBMultiLabelClassification MultilabelClassification text nld DutchNewsArticlesClusteringS2S Clustering text nld DutchNewsArticlesClusteringP2P Clustering text nld SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) VABBClusteringS2S Clustering text nld VABBClusteringP2P Clustering text nld OpenTenderClusteringS2S Clustering text nld OpenTenderClusteringP2P Clustering text nld IconclassClusteringS2S Clustering text nld WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) ArguAna-NL Retrieval text nld SCIDOCS-NL Retrieval text nld SciFact-NL Retrieval text nld NFCorpus-NL Retrieval text nld BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) WebFAQRetrieval Retrieval text ara, aze, ben, bul, cat, ... (51) DutchNewsArticlesRetrieval Retrieval text nld bBSARDNLRetrieval Retrieval text nld LegalQANLRetrieval Retrieval text nld OpenTenderRetrieval Retrieval text nld VABBRetrieval Retrieval text nld WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) SICK-NL-STS STS text nld STSBenchmarkMultilingualSTS STS text cmn, deu, eng, fra, ita, ... (10) Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {22509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebpol-v1","title":"MTEB(pol, v1)","text":"<p>Polish Massive Text Embedding Benchmark (PL-MTEB), a comprehensive benchmark for text embeddings in Polish. The PL-MTEB consists of 28 diverse NLP tasks from 5 task types. With tasks adapted based on previously used datasets by the Polish NLP community. In addition, a new PLSC (Polish Library of Science Corpus) dataset was created consisting of titles and abstracts of scientific publications in Polish, which was used as the basis for two novel clustering tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AllegroReviews Classification text pol CBD Classification text pol MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) PolEmo2.0-IN Classification text pol PolEmo2.0-OUT Classification text pol PAC Classification text pol EightTagsClustering Clustering text pol PlscClusteringS2S Clustering text pol PlscClusteringP2P Clustering text pol CDSC-E PairClassification text pol PpcPC PairClassification text pol PSC PairClassification text pol SICK-E-PL PairClassification text pol CDSC-R STS text pol SICK-R-PL STS text pol STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{poswiata2024plmteb,\n  author = {Rafa\u0142 Po\u015bwiata and S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz},\n  journal = {arXiv preprint arXiv:2405.10138},\n  title = {PL-MTEB: Polish Massive Text Embedding Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebrus-v1","title":"MTEB(rus, v1)","text":"<p>A Russian version of the Massive Text Embedding Benchmark with a number of novel Russian tasks in all task categories of the original MTEB.</p> <p>Learn more \u2192</p> Tasks name type modalities languages GeoreviewClassification Classification text rus HeadlineClassification Classification text rus InappropriatenessClassification Classification text rus KinopoiskClassification Classification text rus MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) RuReviewsClassification Classification text rus RuSciBenchGRNTIClassification Classification text rus RuSciBenchOECDClassification Classification text rus GeoreviewClusteringP2P Clustering text rus RuSciBenchGRNTIClusteringP2P Clustering text rus RuSciBenchOECDClusteringP2P Clustering text rus CEDRClassification MultilabelClassification text rus SensitiveTopicsClassification MultilabelClassification text rus TERRa PairClassification text rus MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) RuBQReranking Reranking text rus MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) RiaNewsRetrieval Retrieval text rus RuBQRetrieval Retrieval text rus RUParaPhraserSTS STS text rus STS22 STS text ara, cmn, deu, eng, fra, ... (10) RuSTSBenchmarkSTS STS text rus Citation <pre><code>@misc{snegirev2024russianfocusedembeddersexplorationrumteb,\n  archiveprefix = {arXiv},\n  author = {Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},\n  eprint = {2408.12503},\n  primaryclass = {cs.CL},\n  title = {The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},\n  url = {https://arxiv.org/abs/2408.12503},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#nanobeir","title":"NanoBEIR","text":"<p>A benchmark to evaluate with subsets of BEIR datasets to use less computational power</p> <p>Learn more \u2192</p> Tasks name type modalities languages NanoArguAnaRetrieval Retrieval text eng NanoClimateFeverRetrieval Retrieval text eng NanoDBPediaRetrieval Retrieval text eng NanoFEVERRetrieval Retrieval text eng NanoFiQA2018Retrieval Retrieval text eng NanoHotpotQARetrieval Retrieval text eng NanoMSMARCORetrieval Retrieval text eng NanoNFCorpusRetrieval Retrieval text eng NanoNQRetrieval Retrieval text eng NanoQuoraRetrieval Retrieval text eng NanoSCIDOCSRetrieval Retrieval text eng NanoSciFactRetrieval Retrieval text eng NanoTouche2020Retrieval Retrieval text eng"},{"location":"overview/available_benchmarks/#r2med","title":"R2MED","text":"<p>R2MED: First Reasoning-Driven Medical Retrieval Benchmark.     R2MED is a high-quality, high-resolution information retrieval (IR) dataset designed for medical scenarios.     It contains 876 queries with three retrieval tasks, five medical scenarios, and twelve body systems.</p> <p>Learn more \u2192</p> Tasks name type modalities languages R2MEDBiologyRetrieval Retrieval text eng R2MEDBioinformaticsRetrieval Retrieval text eng R2MEDMedicalSciencesRetrieval Retrieval text eng R2MEDMedXpertQAExamRetrieval Retrieval text eng R2MEDMedQADiagRetrieval Retrieval text eng R2MEDPMCTreatmentRetrieval Retrieval text eng R2MEDPMCClinicalRetrieval Retrieval text eng R2MEDIIYiClinicalRetrieval Retrieval text eng Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rar-b","title":"RAR-b","text":"<p>A benchmark to evaluate reasoning capabilities of retrievers.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ARCChallenge Retrieval text eng AlphaNLI Retrieval text eng HellaSwag Retrieval text eng WinoGrande Retrieval text eng PIQA Retrieval text eng SIQA Retrieval text eng Quail Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TempReasonL2Pure Retrieval text eng TempReasonL2Fact Retrieval text eng TempReasonL2Context Retrieval text eng TempReasonL3Pure Retrieval text eng TempReasonL3Fact Retrieval text eng TempReasonL3Context Retrieval text eng RARbCode Retrieval text eng RARbMath Retrieval text eng Citation <pre><code>@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Al Moubayed, Noura},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebcode-beta","title":"RTEB(Code, beta)","text":"<p>RTEB Code is a subset of RTEB containing retrieval tasks specifically focused on programming and code domains including algorithmic problems, data science tasks, code evaluation, SQL retrieval, and multilingual code retrieval. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python Code1Retrieval Retrieval text eng JapaneseCode1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebhealth-beta","title":"RTEB(Health, beta)","text":"<p>RTEB Healthcare is a subset of RTEB containing retrieval tasks specifically focused on healthcare and medical domains including medical Q&amp;A, healthcare information retrieval, cross-lingual medical retrieval, and multilingual medical consultation. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa EnglishHealthcare1Retrieval Retrieval text eng GermanHealthcare1Retrieval Retrieval text deu Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rteblaw-beta","title":"RTEB(Law, beta)","text":"<p>RTEB Legal is a subset of RTEB containing retrieval tasks specifically focused on legal domain including case documents, statutes, legal summarization, and multilingual legal Q&amp;A. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng LegalQuAD Retrieval text deu FrenchLegal1Retrieval Retrieval text fra GermanLegal1Retrieval Retrieval text deu JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebbeta","title":"RTEB(beta)","text":"<p>RTEB (ReTrieval Embedding Benchmark) is a comprehensive benchmark for evaluating text retrieval models across multiple specialized domains including legal, finance, code, and healthcare. It contains diverse retrieval tasks designed to test models' ability to understand domain-specific terminology and retrieve relevant documents in specialized contexts across multiple languages. The dataset includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng LegalQuAD Retrieval text deu FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) Code1Retrieval Retrieval text eng JapaneseCode1Retrieval Retrieval text jpn EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng EnglishHealthcare1Retrieval Retrieval text eng French1Retrieval Retrieval text fra FrenchLegal1Retrieval Retrieval text fra German1Retrieval Retrieval text deu GermanHealthcare1Retrieval Retrieval text deu GermanLegal1Retrieval Retrieval text deu JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebdeu-beta","title":"RTEB(deu, beta)","text":"<p>RTEB German is a subset of RTEB containing retrieval tasks in German across legal, healthcare, and business domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages LegalQuAD Retrieval text deu German1Retrieval Retrieval text deu GermanHealthcare1Retrieval Retrieval text deu GermanLegal1Retrieval Retrieval text deu Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebeng-beta","title":"RTEB(eng, beta)","text":"<p>RTEB English is a subset of RTEB containing retrieval tasks in English across legal, finance, code, and healthcare domains. Includes diverse tasks covering specialized domains such as healthcare and finance. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa Code1Retrieval Retrieval text eng EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng EnglishHealthcare1Retrieval Retrieval text eng Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebfin-beta","title":"RTEB(fin, beta)","text":"<p>RTEB Finance is a subset of RTEB  containing retrieval tasks specifically focused on financial domain including finance benchmarks, Q&amp;A, financial document retrieval, and corporate governance. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebfra-beta","title":"RTEB(fra, beta)","text":"<p>RTEB French is a subset of RTEB containing retrieval tasks in French across legal and general knowledge domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages CUREv1 Retrieval text eng, fra, spa French1Retrieval Retrieval text fra FrenchLegal1Retrieval Retrieval text fra Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebjpn-beta","title":"RTEB(jpn, beta)","text":"<p>RTEB Japanese is a subset of RTEB  containing retrieval tasks in Japanese across legal and code domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> Tasks name type modalities languages JapaneseCode1Retrieval Retrieval text jpn JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#ruscibench","title":"RuSciBench","text":"<p>RuSciBench is a benchmark designed for evaluating sentence encoders and language models on scientific texts in both Russian and English. The data is sourced from eLibrary (www.elibrary.ru), Russia's largest electronic library of scientific publications. This benchmark facilitates the evaluation and comparison of models on various research-related tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages RuSciBenchBitextMining BitextMining text eng, rus RuSciBenchCoreRiscClassification Classification text eng, rus RuSciBenchGRNTIClassification.v2 Classification text eng, rus RuSciBenchOECDClassification.v2 Classification text eng, rus RuSciBenchPubTypeClassification Classification text eng, rus RuSciBenchCiteRetrieval Retrieval text eng, rus RuSciBenchCociteRetrieval Retrieval text eng, rus RuSciBenchCitedCountRegression Regression text eng, rus RuSciBenchYearPublRegression Regression text eng, rus Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vn-mteb-vie-v1","title":"VN-MTEB (vie, v1)","text":"<p>A benchmark for text-embedding performance in Vietnamese.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ArguAna-VN Retrieval text vie SciFact-VN Retrieval text vie ClimateFEVER-VN Retrieval text vie FEVER-VN Retrieval text vie DBPedia-VN Retrieval text vie NQ-VN Retrieval text vie HotpotQA-VN Retrieval text vie MSMARCO-VN Retrieval text vie TRECCOVID-VN Retrieval text vie FiQA2018-VN Retrieval text vie NFCorpus-VN Retrieval text vie SCIDOCS-VN Retrieval text vie Touche2020-VN Retrieval text vie Quora-VN Retrieval text vie CQADupstackAndroid-VN Retrieval text vie CQADupstackGis-VN Retrieval text vie CQADupstackMathematica-VN Retrieval text vie CQADupstackPhysics-VN Retrieval text vie CQADupstackProgrammers-VN Retrieval text vie CQADupstackStats-VN Retrieval text vie CQADupstackTex-VN Retrieval text vie CQADupstackUnix-VN Retrieval text vie CQADupstackWebmasters-VN Retrieval text vie CQADupstackWordpress-VN Retrieval text vie Banking77VNClassification Classification text vie EmotionVNClassification Classification text vie AmazonCounterfactualVNClassification Classification text vie MTOPDomainVNClassification Classification text vie TweetSentimentExtractionVNClassification Classification text vie ToxicConversationsVNClassification Classification text vie ImdbVNClassification Classification text vie MTOPIntentVNClassification Classification text vie MassiveScenarioVNClassification Classification text vie MassiveIntentVNClassification Classification text vie AmazonReviewsVNClassification Classification text vie AmazonPolarityVNClassification Classification text vie SprintDuplicateQuestions-VN PairClassification text vie TwitterSemEval2015-VN PairClassification text vie TwitterURLCorpus-VN PairClassification text vie TwentyNewsgroupsClustering-VN Clustering text vie RedditClusteringP2P-VN Clustering text vie StackExchangeClusteringP2P-VN Clustering text vie StackExchangeClustering-VN Clustering text vie RedditClustering-VN Clustering text vie SciDocsRR-VN Reranking text vie AskUbuntuDupQuestions-VN Reranking text vie StackOverflowDupQuestions-VN Reranking text vie BIOSSES-VN STS text vie SICK-R-VN STS text vie STSBenchmark-VN STS text vie Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev1","title":"ViDoRe(v1)","text":"<p>Retrieve associated pages according to questions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev2","title":"ViDoRe(v2)","text":"<p>Retrieve associated pages according to questions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Vidore2ESGReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2EconomicsReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2BioMedicalLecturesRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2ESGReportsHLRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#visualdocumentretrieval","title":"VisualDocumentRetrieval","text":"<p>A benchmark for evaluating visual document retrieval, combining ViDoRe v1 and v2.</p> <p>Learn more \u2192</p> Tasks name type modalities languages VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng Vidore2ESGReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2EconomicsReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2BioMedicalLecturesRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2ESGReportsHLRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_models/image/","title":"Image Model","text":"<ul> <li>Number of models: 21</li> </ul>"},{"location":"overview/available_models/image/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/image/#facebookdinov2-base","title":"<code>facebook/dinov2-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 86.6M 330.0 MB 2023-07-18 eng-Latn"},{"location":"overview/available_models/image/#facebookdinov2-giant","title":"<code>facebook/dinov2-giant</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 4.2 GB 2023-07-18 eng-Latn"},{"location":"overview/available_models/image/#facebookdinov2-large","title":"<code>facebook/dinov2-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.0M 1.1 GB 2023-07-18 eng-Latn"},{"location":"overview/available_models/image/#facebookdinov2-small","title":"<code>facebook/dinov2-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 384 22.1M 84.0 MB 2023-07-18 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino1b-full2b-224","title":"<code>facebook/webssl-dino1b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 4.2 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-full2b-224","title":"<code>facebook/webssl-dino2b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.1B 7.8 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-heavy2b-224","title":"<code>facebook/webssl-dino2b-heavy2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.0B 7.8 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-light2b-224","title":"<code>facebook/webssl-dino2b-light2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.0B 7.8 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino300m-full2b-224","title":"<code>facebook/webssl-dino300m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.0M 1.1 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-full2b-224","title":"<code>facebook/webssl-dino3b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 3.0B 11.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-heavy2b-224","title":"<code>facebook/webssl-dino3b-heavy2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 3.0B 11.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-light2b-224","title":"<code>facebook/webssl-dino3b-light2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 3.0B 11.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino5b-full2b-224","title":"<code>facebook/webssl-dino5b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3584 5.0B 18.4 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-224","title":"<code>facebook/webssl-dino7b-full8b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 7.0B 24.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-378","title":"<code>facebook/webssl-dino7b-full8b-378</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 7.0B 24.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-518","title":"<code>facebook/webssl-dino7b-full8b-518</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 7.0B 24.0 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-mae1b-full2b-224","title":"<code>facebook/webssl-mae1b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.0B 4.2 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-mae300m-full2b-224","title":"<code>facebook/webssl-mae300m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.0M 1.1 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#facebookwebssl-mae700m-full2b-224","title":"<code>facebook/webssl-mae700m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 700.0M 2.4 GB 2025-04-24 eng-Latn"},{"location":"overview/available_models/image/#nyu-visionxmoco-v3-vit-b","title":"<code>nyu-visionx/moco-v3-vit-b</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 86.6M 330.0 MB 2024-06-03 eng-Latn"},{"location":"overview/available_models/image/#nyu-visionxmoco-v3-vit-l","title":"<code>nyu-visionx/moco-v3-vit-l</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.0M 1.1 GB 2024-06-03 eng-Latn"},{"location":"overview/available_models/image_text/","title":"Image-text Model","text":"<ul> <li>Number of models: 46</li> </ul>"},{"location":"overview/available_models/image_text/#instruction-model","title":"Instruction Model","text":""},{"location":"overview/available_models/image_text/#alibaba-nlpgme-qwen2-vl-2b-instruct","title":"<code>Alibaba-NLP/gme-Qwen2-VL-2B-Instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 2.2B 8.2 GB 2024-12-24 cmn-Hans, eng-Latn Citation <pre><code>@misc{zhang2024gme,\n      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs},\n      author={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},\n      year={2024},\n      eprint={2412.16855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={http://arxiv.org/abs/2412.16855}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#alibaba-nlpgme-qwen2-vl-7b-instruct","title":"<code>Alibaba-NLP/gme-Qwen2-VL-7B-Instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 8.3B 30.9 GB 2024-12-24 cmn-Hans, eng-Latn Citation <pre><code>@misc{zhang2024gme,\n      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs},\n      author={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},\n      year={2024},\n      eprint={2412.16855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={http://arxiv.org/abs/2412.16855}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tiger-labvlm2vec-full","title":"<code>TIGER-Lab/VLM2Vec-Full</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 3072 4.2B 7.7 GB 2024-10-08 eng-Latn Citation <pre><code>@article{jiang2024vlm2vec,\n  title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks},\n  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},\n  journal={arXiv preprint arXiv:2410.05160},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tiger-labvlm2vec-lora","title":"<code>TIGER-Lab/VLM2Vec-LoRA</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 3072 not specified not specified 2024-10-08 eng-Latn Citation <pre><code>@article{jiang2024vlm2vec,\n  title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks},\n  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},\n  journal={arXiv preprint arXiv:2410.05160},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#ibm-granitegranite-vision-33-2b-embedding","title":"<code>ibm-granite/granite-vision-3.3-2b-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 11.1 GB 2025-06-11 eng-Latn Citation <pre><code>@article{karlinsky2025granitevision,\n  title={Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence},\n  author={Granite Vision Team and Karlinsky, Leonid and Arbelle, Assaf and Daniels, Abraham and Nassar, Ahmed and Alfassi, Amit and Wu, Bo and Schwartz, Eli and Joshi, Dhiraj and Kondic, Jovana and Shabtay, Nimrod and Li, Pengyuan and Herzig, Roei and Abedin, Shafiq and Perek, Shaked and Harary, Sivan and Barzelay, Udi and Raz Goldfarb, Adi and Oliva, Aude and Wieles, Ben and Bhattacharjee, Bishwaranjan and Huang, Brandon and Auer, Christoph and Gutfreund, Dan and Beymer, David and Wood, David and Kuehne, Hilde and Hansen, Jacob and Shtok, Joseph and Wong, Ken and Bathen, Luis Angel and Mishra, Mayank and Lysak, Maksym and Dolfi, Michele and Yurochkin, Mikhail and Livathinos, Nikolaos and Harel, Nimrod and Azulai, Ophir and Naparstek, Oshri and de Lima, Rafael Teixeira and Panda, Rameswar and Doveh, Sivan and Gupta, Shubham and Das, Subhro and Zawad, Syed and Kim, Yusik and He, Zexue and Brooks, Alexander and Goodhart, Gabe and Govindjee, Anita and Leist, Derek and Ibrahim, Ibrahim and Soffer, Aya and Cox, David and Soule, Kate and Lastras, Luis and Desai, Nirmit and Ofek-koifman, Shila and Raghavan, Sriram and Syeda-Mahmood, Tanveer and Staar, Peter and Drory, Tal and Feris, Rogerio},\n  journal={arXiv preprint arXiv:2502.09927},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#intfloatmme5-mllama-11b-instruct","title":"<code>intfloat/mmE5-mllama-11b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 4096 10.6B 19.8 GB 2025-02-12 eng-Latn Citation <pre><code>@article{chen2025mmE5,\n  title={mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data},\n  author={Chen, Haonan and Wang, Liang and Yang, Nan and Zhu, Yutao and Zhao, Ziliang and Wei, Furu and Dou, Zhicheng},\n  journal={arXiv preprint arXiv:2502.08468},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#jinaaijina-clip-v1","title":"<code>jinaai/jina-clip-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 223.0M 849.0 MB 2024-05-30 eng-Latn Citation <pre><code>@article{koukounas2024jinaclip,\n  title={Jina CLIP: Your CLIP Model Is Also Your Text Retriever},\n  author={Koukounas, Andreas and Mastrapas, Georgios and G\u00fcnther, Michael and Wang, Bo and Martens, Scott and Mohr, Isabelle and Sturua, Saba and Akram, Mohammad Kalim and Mart\u00ednez, Joan Fontanals and Ognawala, Saahil and Guzman, Susana and Werk, Maximilian and Wang, Nan and Xiao, Han},\n  journal={arXiv preprint arXiv:2405.20204},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#jinaaijina-embeddings-v4","title":"<code>jinaai/jina-embeddings-v4</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 3.8B 7.3 GB 2025-06-24 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n      title={jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n      author={Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Sedigheh Eslami and Scott Martens and Bo Wang and Nan Wang and Han Xiao},\n      year={2025},\n      eprint={2506.18902},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2506.18902},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-b-16","title":"<code>microsoft/LLM2CLIP-Openai-B-16</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 361.0M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-l-14-224","title":"<code>microsoft/LLM2CLIP-Openai-L-14-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 578.0M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-l-14-336","title":"<code>microsoft/LLM2CLIP-Openai-L-14-336</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 579.0M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-aicolnomic-embed-multimodal-3b","title":"<code>nomic-ai/colnomic-embed-multimodal-3b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 7.0 GB 2025-03-31 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{nomicembedmultimodal2025,\n  title={Nomic Embed Multimodal: Interleaved Text, Image, and Screenshots for Visual Document Retrieval},\n  author={Nomic Team},\n  year={2025},\n  publisher={Nomic AI},\n  url={https://nomic.ai/blog/posts/nomic-embed-multimodal}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-aicolnomic-embed-multimodal-7b","title":"<code>nomic-ai/colnomic-embed-multimodal-7b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 7.0B 14.1 GB 2025-03-31 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{nomicembedmultimodal2025,\n  title={Nomic Embed Multimodal: Interleaved Text, Image, and Screenshots for Visual Document Retrieval},\n  author={Nomic Team},\n  year={2025},\n  publisher={Nomic AI},\n  url={https://nomic.ai/blog/posts/nomic-embed-multimodal}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-ainomic-embed-vision-v15","title":"<code>nomic-ai/nomic-embed-vision-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 92.9M 355.0 MB 2024-06-08 eng-Latn Citation <pre><code>@article{nussbaum2024nomicembedvision,\n      title={Nomic Embed Vision: Expanding the Latent Space},\n      author={Nussbaum, Zach and Duderstadt, Brandon and Mulyar, Andriy},\n      journal={arXiv preprint arXiv:2406.18587},\n      year={2024},\n      eprint={2406.18587},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2406.18587}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidiallama-nemoretriever-colembed-1b-v1","title":"<code>nvidia/llama-nemoretriever-colembed-1b-v1</code>","text":"<p>License: https://huggingface.co/nvidia/llama-nemoretriever-colembed-1b-v1/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.4B 9.0 GB 2025-06-27 eng-Latn Citation <pre><code>@misc{xu2025llamanemoretrievercolembedtopperforming,\n      title={Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model},\n      author={Mengyao Xu and Gabriel Moreira and Ronay Ak and Radek Osmulski and Yauhen Babakhin and Zhiding Yu and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2507.05513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.05513}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidiallama-nemoretriever-colembed-3b-v1","title":"<code>nvidia/llama-nemoretriever-colembed-3b-v1</code>","text":"<p>License: https://huggingface.co/nvidia/llama-nemoretriever-colembed-1b-v1/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3072 4.4B 16.4 GB 2025-06-27 eng-Latn Citation <pre><code>@misc{xu2025llamanemoretrievercolembedtopperforming,\n      title={Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model},\n      author={Mengyao Xu and Gabriel Moreira and Ronay Ak and Radek Osmulski and Yauhen Babakhin and Zhiding Yu and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2507.05513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.05513}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#royokonge5-v","title":"<code>royokong/e5-v</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.4B 15.6 GB 2024-07-17 eng-Latn Citation <pre><code>@article{jiang2024e5v,\n      title={E5-V: Universal Embeddings with Multimodal Large Language Models},\n      author={Jiang, Ting and Song, Minghui and Zhang, Zihan and Huang, Haizhen and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},\n      journal={arXiv preprint arXiv:2407.12580},\n      year={2024},\n      eprint={2407.12580},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.12580}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolsmol-256m","title":"<code>vidore/colSmol-256M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 128 256.0M 800.0 MB 2025-01-22 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolsmol-500m","title":"<code>vidore/colSmol-500M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 128 500.0M 1.2 GB 2025-01-22 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v11","title":"<code>vidore/colpali-v1.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-08-21 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v12","title":"<code>vidore/colpali-v1.2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-08-26 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v13","title":"<code>vidore/colpali-v1.3</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-11-01 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolqwen2-v10","title":"<code>vidore/colqwen2-v1.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 128 2.2B 7.0 GB 2025-11-03 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolqwen25-v02","title":"<code>vidore/colqwen2.5-v0.2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 7.0 GB 2025-01-31 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/image_text/#baaibge-visualized-base","title":"<code>BAAI/bge-visualized-base</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 196.0M 1.6 GB 2024-06-06 eng-Latn Citation <pre><code>@article{zhou2024vista,\n  title={VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval},\n  author={Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping},\n  journal={arXiv preprint arXiv:2406.04292},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#baaibge-visualized-m3","title":"<code>BAAI/bge-visualized-m3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 872.9M 4.2 GB 2024-06-06 eng-Latn Citation <pre><code>@article{zhou2024vista,\n  title={VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval},\n  author={Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping},\n  journal={arXiv preprint arXiv:2406.04292},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40","title":"<code>Cohere/Cohere-embed-v4.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40-output_dtypebinary","title":"<code>Cohere/Cohere-embed-v4.0 (output_dtype=binary)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40-output_dtypeint8","title":"<code>Cohere/Cohere-embed-v4.0 (output_dtype=int8)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#quansuneva02-clip-b-16","title":"<code>QuanSun/EVA02-CLIP-B-16</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 149.0M 568.0 MB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-l-14","title":"<code>QuanSun/EVA02-CLIP-L-14</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 428.0M 1.6 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-bige-14","title":"<code>QuanSun/EVA02-CLIP-bigE-14</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 4.7B 17.5 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-bige-14-plus","title":"<code>QuanSun/EVA02-CLIP-bigE-14-plus</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 5.0B 18.6 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-image-captioning-base","title":"<code>Salesforce/blip-image-captioning-base</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 247.0M 942.0 MB 2023-08-01 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-image-captioning-large","title":"<code>Salesforce/blip-image-captioning-large</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.0M 1.8 GB 2023-12-07 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-itm-base-coco","title":"<code>Salesforce/blip-itm-base-coco</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 247.0M 942.0 MB 2023-08-01 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-itm-base-flickr","title":"<code>Salesforce/blip-itm-base-flickr</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 247.0M 942.0 MB 2023-08-01 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-itm-large-coco","title":"<code>Salesforce/blip-itm-large-coco</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.0M 1.8 GB 2023-08-01 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-itm-large-flickr","title":"<code>Salesforce/blip-itm-large-flickr</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.0M 1.8 GB 2023-08-01 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-vqa-base","title":"<code>Salesforce/blip-vqa-base</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 247.0M 1.4 GB 2023-12-07 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip-vqa-capfilt-large","title":"<code>Salesforce/blip-vqa-capfilt-large</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 247.0M 942.0 MB 2023-01-22 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip2-opt-27b","title":"<code>Salesforce/blip2-opt-2.7b</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 3.7B 14.0 GB 2024-03-22 eng-Latn"},{"location":"overview/available_models/image_text/#salesforceblip2-opt-67b-coco","title":"<code>Salesforce/blip2-opt-6.7b-coco</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 7.8B 28.9 GB 2024-03-31 eng-Latn"},{"location":"overview/available_models/image_text/#baselinerandom-cross-encoder-baseline","title":"baseline/random-cross-encoder-baseline","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 32 0 0.0 MB not specified not specified"},{"location":"overview/available_models/image_text/#baselinerandom-encoder-baseline","title":"baseline/random-encoder-baseline","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 32 0 0.0 MB not specified not specified"},{"location":"overview/available_models/image_text/#cohereembed-english-v30","title":"<code>cohere/embed-english-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 not specified not specified 2024-10-24 eng-Latn"},{"location":"overview/available_models/image_text/#cohereembed-multilingual-v30","title":"<code>cohere/embed-multilingual-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 not specified not specified 2024-10-24 not specified"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-224","title":"<code>google/siglip-base-patch16-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.0M 775.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-256","title":"<code>google/siglip-base-patch16-256</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.0M 775.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-256-multilingual","title":"<code>google/siglip-base-patch16-256-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 371.0M 1.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-384","title":"<code>google/siglip-base-patch16-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.0M 776.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-512","title":"<code>google/siglip-base-patch16-512</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 204.0M 777.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-large-patch16-256","title":"<code>google/siglip-large-patch16-256</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1024 652.0M 2.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-large-patch16-384","title":"<code>google/siglip-large-patch16-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1024 652.0M 2.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch14-224","title":"<code>google/siglip-so400m-patch14-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16 1152 877.0M 3.3 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch14-384","title":"<code>google/siglip-so400m-patch14-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1152 878.0M 3.3 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch16-256-i18n","title":"<code>google/siglip-so400m-patch16-256-i18n</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1152 1.1B 4.2 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#kakaobrainalign-base","title":"<code>kakaobrain/align-base</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 176.0M 671.0 MB 2023-02-24 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-b-16-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 150.0M 572.0 MB 2023-04-26 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-b-32-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.0M 576.0 MB 2023-04-26 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-b-32-laion2b-s34b-b79k","title":"<code>laion/CLIP-ViT-B-32-laion2B-s34B-b79K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.0M 577.0 MB 2022-09-15 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-h-14-laion2b-s32b-b79k","title":"<code>laion/CLIP-ViT-H-14-laion2B-s32B-b79K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 986.0M 3.7 GB 2022-09-15 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-l-14-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 428.0M 1.6 GB 2023-04-26 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-l-14-laion2b-s32b-b82k","title":"<code>laion/CLIP-ViT-L-14-laion2B-s32B-b82K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 428.0M 1.6 GB 2022-09-15 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-bigg-14-laion2b-39b-b160k","title":"<code>laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1280 2.5B 9.5 GB 2023-01-23 eng-Latn"},{"location":"overview/available_models/image_text/#laionclip-vit-g-14-laion2b-s34b-b88k","title":"<code>laion/CLIP-ViT-g-14-laion2B-s34B-b88K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 1.4B 5.1 GB 2023-03-06 eng-Latn"},{"location":"overview/available_models/image_text/#openaiclip-vit-base-patch16","title":"<code>openai/clip-vit-base-patch16</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.0M 576.0 MB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#openaiclip-vit-base-patch32","title":"<code>openai/clip-vit-base-patch32</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.0M 576.0 MB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#openaiclip-vit-large-patch14","title":"<code>openai/clip-vit-large-patch14</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 428.0M 1.6 GB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#voyageaivoyage-multimodal-3","title":"<code>voyageai/voyage-multimodal-3</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 not specified not specified 2024-11-10 not specified"},{"location":"overview/available_models/text/","title":"Text Model","text":"<ul> <li>Number of models: 206</li> </ul>"},{"location":"overview/available_models/text/#instruction-model","title":"Instruction Model","text":""},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen15-7b-instruct","title":"<code>Alibaba-NLP/gte-Qwen1.5-7B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.7B 28.8 GB 2024-04-20 eng-Latn"},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen2-15b-instruct","title":"<code>Alibaba-NLP/gte-Qwen2-1.5B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 8960 1.8B 6.6 GB 2024-07-29 eng-Latn"},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen2-7b-instruct","title":"<code>Alibaba-NLP/gte-Qwen2-7B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.6B 28.4 GB 2024-06-15 not specified Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-en","title":"<code>BAAI/bge-base-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-08-05 eng-Latn"},{"location":"overview/available_models/text/#baaibge-base-en-v15","title":"<code>BAAI/bge-base-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-09-11 eng-Latn Citation <pre><code>@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding},\n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-zh","title":"<code>BAAI/bge-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-08-05 zho-Hans"},{"location":"overview/available_models/text/#baaibge-base-zh-v15","title":"<code>BAAI/bge-base-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 416.0 MB 2023-09-11 zho-Hans"},{"location":"overview/available_models/text/#baaibge-large-en","title":"<code>BAAI/bge-large-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-08-05 eng-Latn"},{"location":"overview/available_models/text/#baaibge-large-en-v15","title":"<code>BAAI/bge-large-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-09-12 eng-Latn Citation <pre><code>@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding},\n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-large-zh","title":"<code>BAAI/bge-large-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-08-02 zho-Hans"},{"location":"overview/available_models/text/#baaibge-large-zh-v15","title":"<code>BAAI/bge-large-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-09-12 zho-Hans"},{"location":"overview/available_models/text/#baaibge-small-en","title":"<code>BAAI/bge-small-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-08-05 eng-Latn"},{"location":"overview/available_models/text/#baaibge-small-en-v15","title":"<code>BAAI/bge-small-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-09-12 eng-Latn Citation <pre><code>@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding},\n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-small-zh","title":"<code>BAAI/bge-small-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-08-05 zho-Hans"},{"location":"overview/available_models/text/#baaibge-small-zh-v15","title":"<code>BAAI/bge-small-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 91.0 MB 2023-09-12 zho-Hans"},{"location":"overview/available_models/text/#bmretrieverbmretriever-1b","title":"<code>BMRetriever/BMRetriever-1B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 908.8M 3.4 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-2b","title":"<code>BMRetriever/BMRetriever-2B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.5B 9.3 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-410m","title":"<code>BMRetriever/BMRetriever-410M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 353.8M 1.3 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-7b","title":"<code>BMRetriever/BMRetriever-7B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#beastyze5-r-mistral-7b","title":"<code>BeastyZ/e5-R-mistral-7b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 27.0 GB 2024-06-28 eng-Latn"},{"location":"overview/available_models/text/#bytedance-seedseed15-embedding","title":"<code>ByteDance-Seed/Seed1.5-Embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 not specified not specified 2025-04-25 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#bytedanceseed16-embedding","title":"<code>Bytedance/Seed1.6-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 not specified not specified 2025-06-18 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#coherecohere-embed-english-light-v30","title":"<code>Cohere/Cohere-embed-english-light-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#coherecohere-embed-english-v30","title":"<code>Cohere/Cohere-embed-english-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#coherecohere-embed-multilingual-light-v30","title":"<code>Cohere/Cohere-embed-multilingual-light-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#coherecohere-embed-multilingual-v30","title":"<code>Cohere/Cohere-embed-multilingual-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 512 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#geogpt-research-projectgeoembedding","title":"<code>GeoGPT-Research-Project/GeoEmbedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 27.0 GB 2025-04-22 eng-Latn"},{"location":"overview/available_models/text/#gritlmgritlm-7b","title":"<code>GritLM/GritLM-7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 13.5 GB 2024-02-15 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{muennighoff2024generative,\n      title={Generative Representational Instruction Tuning},\n      author={Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},\n      year={2024},\n      eprint={2402.09906},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#gritlmgritlm-8x7b","title":"<code>GritLM/GritLM-8x7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 32768 57.9B 87.0 GB 2024-02-15 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{muennighoff2024generative,\n      title={Generative Representational Instruction Tuning},\n      author={Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},\n      year={2024},\n      eprint={2402.09906},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v1","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-10-23 eng-Latn, zho-Hans Citation <pre><code>@article{hu2025kalm,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others},\n  journal={arXiv preprint arXiv:2501.01028},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v15","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-12-26 eng-Latn, zho-Hans Citation <pre><code>@article{hu2025kalm,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others},\n  journal={arXiv preprint arXiv:2501.01028},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v2","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 942.0 MB 2025-06-25 eng-Latn, zho-Hans Citation <pre><code>@article{hu2025kalm,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others},\n  journal={arXiv preprint arXiv:2501.01028},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25","title":"<code>KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2025-09-30 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#kingsoft-llmqzhou-embedding","title":"<code>Kingsoft-LLM/QZhou-Embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3584 7.1B 28.4 GB 2025-08-24 eng-Latn, zho-Hans Citation <pre><code>@misc{yu2025qzhouembeddingtechnicalreport,\n      title={QZhou-Embedding Technical Report},\n      author={Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu},\n      year={2025},\n      eprint={2508.21632},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.21632},\n}\n</code></pre>"},{"location":"overview/available_models/text/#kingsoft-llmqzhou-embedding-zh","title":"<code>Kingsoft-LLM/QZhou-Embedding-Zh</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1792 7.6B 28.7 GB 2025-09-28 zho-Hans Citation <pre><code>@misc{yu2025qzhouembeddingtechnicalreport,\n      title={QZhou-Embedding Technical Report},\n      author={Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu},\n      year={2025},\n      eprint={2508.21632},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.21632},\n}\n</code></pre>"},{"location":"overview/available_models/text/#linq-ai-researchlinq-embed-mistral","title":"<code>Linq-AI-Research/Linq-Embed-Mistral</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-05-29 eng-Latn Citation <pre><code>@misc{LinqAIResearch2024,\n  title={Linq-Embed-Mistral:Elevating Text Retrieval with Improved GPT Data Through Task-Specific Control and Quality Refinement},\n  author={Junseong Kim and Seolhwa Lee and Jihoon Kwon and Sangmo Gu and Yejin Kim and Minkyung Cho and Jy-yong Sohn and Chanyeol Choi},\n  howpublished={Linq AI Research Blog},\n  year={2024},\n  url={https://getlinq.com/blog/linq-embed-mistral/}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 7.5B 28.0 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 7.5B 28.0 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-sheared-llama-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mongodbmdbr-leaf-ir","title":"<code>MongoDB/mdbr-leaf-ir</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 22.9M 86.0 MB 2025-08-27 eng-Latn Citation <pre><code>@misc{mdbr_leaf,\n  title={LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations},\n  author={Robin Vujanic and Thomas Rueckstiess},\n  year={2025},\n  eprint={2509.12539},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR},\n  url={https://arxiv.org/abs/2509.12539}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mongodbmdbr-leaf-mt","title":"<code>MongoDB/mdbr-leaf-mt</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 23.0M 86.0 MB 2025-08-27 eng-Latn Citation <pre><code>@misc{mdbr_leaf,\n  title={LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations},\n  author={Robin Vujanic and Thomas Rueckstiess},\n  year={2025},\n  eprint={2509.12539},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR},\n  url={https://arxiv.org/abs/2509.12539}\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchjasper_en_vision_language_v1","title":"<code>NovaSearch/jasper_en_vision_language_v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 8960 2.0B 3.7 GB 2024-12-11 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchstella_en_15b_v5","title":"<code>NovaSearch/stella_en_1.5B_v5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 8960 1.5B 5.7 GB 2024-07-12 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchstella_en_400m_v5","title":"<code>NovaSearch/stella_en_400M_v5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 435.0M 1.6 GB 2024-07-12 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-06b","title":"<code>Qwen/Qwen3-Embedding-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 595.8M 2.2 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-4b","title":"<code>Qwen/Qwen3-Embedding-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.0B 15.0 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-8b","title":"<code>Qwen/Qwen3-Embedding-8B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.6B 28.2 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#reasonirreasonir-8b","title":"<code>ReasonIR/ReasonIR-8B</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 4096 7.5B not specified 2025-04-29 eng-Latn Citation <pre><code>@article{shao2025reasonir,\n      title={ReasonIR: Training Retrievers for Reasoning Tasks},\n      author={Rulin Shao and Rui Qiao and Varsha Kishore and Niklas Muennighoff and Xi Victoria Lin and Daniela Rus and Bryan Kian Hsiang Low and Sewon Min and Wen-tau Yih and Pang Wei Koh and Luke Zettlemoyer},\n      year={2025},\n      journal={arXiv preprint arXiv:2504.20595},\n      url={https://arxiv.org/abs/2504.20595},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sailesh97hinvec","title":"<code>Sailesh97/Hinvec</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 939.6M 3.6 GB 2025-06-19 eng-Latn, hin-Deva"},{"location":"overview/available_models/text/#salesforcesfr-embedding-2_r","title":"<code>Salesforce/SFR-Embedding-2_R</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-06-14 eng-Latn Citation <pre><code>@misc{SFR-embedding-2,\n      title={SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training},\n      author={Rui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},\n      year={2024},\n      url={https://huggingface.co/Salesforce/SFR-Embedding-2_R}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#salesforcesfr-embedding-code-2b_r","title":"<code>Salesforce/SFR-Embedding-Code-2B_R</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2304 2.6B 4.9 GB 2025-01-17 eng-Latn Citation <pre><code>@article{liu2024codexembed,\n  title={CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval},\n  author={Liu, Ye and Meng, Rui and Jot, Shafiq and Savarese, Silvio and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih},\n  journal={arXiv preprint arXiv:2411.12644},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#salesforcesfr-embedding-mistral","title":"<code>Salesforce/SFR-Embedding-Mistral</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-01-24 eng-Latn Citation <pre><code>    @misc{SFRAIResearch2024,\n  title={SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning},\n  author={Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},\n  howpublished={Salesforce AI Research Blog},\n  year={2024},\n  url={https://www.salesforce.com/blog/sfr-embedding/}\n}\n</code></pre>"},{"location":"overview/available_models/text/#samilpwc-axnode-genaipwc-embedding_expr","title":"<code>SamilPwC-AXNode-GenAI/PwC-Embedding_expr</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 560.0M 2.1 GB 2025-08-12 kor-Hang"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-l","title":"<code>Snowflake/snowflake-arctic-embed-l</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-l-v20","title":"<code>Snowflake/snowflake-arctic-embed-l-v2.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-12-04 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74) Citation <pre><code>@article{yu2024arctic,\n      title={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\n      author={Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel},\n      journal={arXiv preprint arXiv:2412.04506},\n      year={2024},\n      eprint={2412.04506},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2412.04506}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m","title":"<code>Snowflake/snowflake-arctic-embed-m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 415.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-long","title":"<code>Snowflake/snowflake-arctic-embed-m-long</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 137.0M 522.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-v15","title":"<code>Snowflake/snowflake-arctic-embed-m-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 415.0 MB 2024-07-08 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-v20","title":"<code>Snowflake/snowflake-arctic-embed-m-v2.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 305.0M 1.1 GB 2024-12-04 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74) Citation <pre><code>@article{yu2024arctic,\n      title={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\n      author={Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel},\n      journal={arXiv preprint arXiv:2412.04506},\n      year={2024},\n      eprint={2412.04506},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2412.04506}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-s","title":"<code>Snowflake/snowflake-arctic-embed-s</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 32.2M 127.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-xs","title":"<code>Snowflake/snowflake-arctic-embed-xs</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.6M 86.0 MB 2024-07-08 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#tencentbacconan-embedding-v2","title":"<code>TencentBAC/Conan-embedding-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 not specified not specified 2025-04-10 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#vplabssearchmap_preview","title":"<code>VPLabs/SearchMap_Preview</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 435.0M 1.6 GB 2025-03-05 eng-Latn Citation <pre><code>@misc{vectorpath2025searchmap,\n  title={SearchMap: Conversational E-commerce Search Embedding Model},\n  author={VectorPath Research Team},\n  year={2025},\n  publisher={Hugging Face},\n  journal={HuggingFace Model Hub},\n}\n</code></pre>"},{"location":"overview/available_models/text/#whereisaiuae-large-v1","title":"<code>WhereIsAI/UAE-Large-V1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-12-04 eng-Latn Citation <pre><code>    @article{li2023angle,\n      title={AnglE-optimized Text Embeddings},\n      author={Li, Xianming and Li, Jing},\n      journal={arXiv preprint arXiv:2309.12871},\n      year={2023}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#ai-foreverfrida","title":"<code>ai-forever/FRIDA</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 823.0M 3.1 GB 2024-12-29 rus-Cyrl"},{"location":"overview/available_models/text/#ai-foreverru-en-rosberta","title":"<code>ai-forever/ru-en-RoSBERTa</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 404.0M 1.5 GB 2024-07-29 rus-Cyrl Citation <pre><code>@misc{snegirev2024russianfocusedembeddersexplorationrumteb,\n      title={The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},\n      author={Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},\n      year={2024},\n      eprint={2408.12503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2408.12503},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#ai-sagegiga-embeddings-instruct","title":"<code>ai-sage/Giga-Embeddings-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 2048 3.2B 12.6 GB 2025-09-23 eng-Latn, rus-Cyrl"},{"location":"overview/available_models/text/#annamodelslgai-embedding-preview","title":"<code>annamodels/LGAI-Embedding-Preview</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2025-06-11 eng-Latn Citation <pre><code>@misc{choi2025lgaiembeddingpreviewtechnicalreport,\n      title={LGAI-EMBEDDING-Preview Technical Report},\n      author={Jooyoung Choi and Hyun Kim and Hansol Jang and Changwook Jun and Kyunghoon Bae and Hyewon Choi and Stanley Jungkyu Choi and Honglak Lee and Chulmin Yun},\n      year={2025},\n      eprint={2506.07438},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2506.07438},\n}\n</code></pre>"},{"location":"overview/available_models/text/#bedrockcohere-embed-english-v3","title":"<code>bedrock/cohere-embed-english-v3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#bedrockcohere-embed-multilingual-v3","title":"<code>bedrock/cohere-embed-multilingual-v3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#castorinirepllama-v1-7b-lora-passage","title":"<code>castorini/repllama-v1-7b-lora-passage</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0M 27.0 MB 2023-10-11 eng-Latn Citation <pre><code>@article{rankllama,\n      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval},\n      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},\n      year={2023},\n      journal={arXiv:2310.08319},\n}\n</code></pre>"},{"location":"overview/available_models/text/#codefuse-aif2llm-06b","title":"<code>codefuse-ai/F2LLM-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 595.8M 1.1 GB 2025-09-18 eng-Latn"},{"location":"overview/available_models/text/#codefuse-aif2llm-17b","title":"<code>codefuse-ai/F2LLM-1.7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2560 1.7B 3.2 GB 2025-09-18 eng-Latn"},{"location":"overview/available_models/text/#codefuse-aif2llm-4b","title":"<code>codefuse-ai/F2LLM-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2560 4.0B 7.5 GB 2025-09-18 eng-Latn"},{"location":"overview/available_models/text/#deepvkuser-base","title":"<code>deepvk/USER-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 427.0M 473.0 MB 2024-06-10 rus-Cyrl Citation <pre><code>@misc{deepvk2024user,\n        title={USER: Universal Sentence Encoder for Russian},\n        author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},\n        url={https://huggingface.co/datasets/deepvk/USER-base},\n        publisher={Hugging Face}\n        year={2024},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deepvkuser2-base","title":"<code>deepvk/USER2-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2025-04-19 rus-Cyrl"},{"location":"overview/available_models/text/#deepvkuser2-small","title":"<code>deepvk/USER2-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 384 34.4M 131.0 MB 2025-04-19 rus-Cyrl"},{"location":"overview/available_models/text/#fyaronskiyenglish_code_retriever","title":"<code>fyaronskiy/english_code_retriever</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2025-07-10 eng-Latn"},{"location":"overview/available_models/text/#googleembeddinggemma-300m","title":"<code>google/embeddinggemma-300m</code>","text":"<p>License: gemma</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 307.6M 578.0 MB 2025-09-04 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)"},{"location":"overview/available_models/text/#googlegemini-embedding-001","title":"<code>google/gemini-embedding-001</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 3072 not specified not specified 2025-03-07 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)"},{"location":"overview/available_models/text/#googletext-embedding-004","title":"<code>google/text-embedding-004</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-05-14 eng-Latn"},{"location":"overview/available_models/text/#googletext-embedding-005","title":"<code>google/text-embedding-005</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-11-18 eng-Latn"},{"location":"overview/available_models/text/#googletext-multilingual-embedding-002","title":"<code>google/text-multilingual-embedding-002</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-05-14 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)"},{"location":"overview/available_models/text/#inflyinf-retriever-v1","title":"<code>infly/inf-retriever-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.1B 13.2 GB 2024-12-24 eng-Latn, zho-Hans Citation <pre><code>@misc{infly-ai_2025,\n  author       = {Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi},\n  title        = {inf-retriever-v1 (Revision 5f469d7)},\n  year         = 2025,\n  url          = {https://huggingface.co/infly/inf-retriever-v1},\n  doi          = {10.57967/hf/4262},\n  publisher    = {Hugging Face}\n}\n</code></pre>"},{"location":"overview/available_models/text/#inflyinf-retriever-v1-15b","title":"<code>infly/inf-retriever-v1-1.5b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 1.5B 2.9 GB 2025-02-08 eng-Latn, zho-Hans Citation <pre><code>@misc{infly-ai_2025,\n  author       = {Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi},\n  title        = {inf-retriever-v1 (Revision 5f469d7)},\n  year         = 2025,\n  url          = {https://huggingface.co/infly/inf-retriever-v1},\n  doi          = {10.57967/hf/4262},\n  publisher    = {Hugging Face}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-base","title":"<code>intfloat/e5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2022-12-26 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-base-v2","title":"<code>intfloat/e5-base-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-large","title":"<code>intfloat/e5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2022-12-26 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-large-v2","title":"<code>intfloat/e5-large-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 335.0M 1.2 GB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-mistral-7b-instruct","title":"<code>intfloat/e5-mistral-7b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-02-08 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>    @article{wang2023improving,\n      title={Improving Text Embeddings with Large Language Models},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2401.00368},\n      year={2023}\n    }\n\n    @article{wang2022text,\n      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2212.03533},\n      year={2022}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-small","title":"<code>intfloat/e5-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.0M 127.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-small-v2","title":"<code>intfloat/e5-small-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.0M 127.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-base","title":"<code>intfloat/multilingual-e5-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-large","title":"<code>intfloat/multilingual-e5-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 560.0M 2.1 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-large-instruct","title":"<code>intfloat/multilingual-e5-large-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 560.0M 1.0 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n      title={Multilingual E5 Text Embeddings: A Technical Report},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2402.05672},\n      year={2024}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-small","title":"<code>intfloat/multilingual-e5-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 118.0M 449.0 MB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v3","title":"<code>jinaai/jina-embeddings-v3</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 572.0M 1.1 GB 2024-09-18 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>    @misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA},\n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#jxmcde-small-v1","title":"<code>jxm/cde-small-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 281.0M 1.0 GB 2024-09-24 eng-Latn"},{"location":"overview/available_models/text/#jxmcde-small-v2","title":"<code>jxm/cde-small-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 306.0M 1.1 GB 2025-01-13 eng-Latn"},{"location":"overview/available_models/text/#llamaindexvdr-2b-multi-v1","title":"<code>llamaindex/vdr-2b-multi-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 2.0B 4.1 GB 2024-01-08 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn"},{"location":"overview/available_models/text/#manveertambercadet-embed-base-v1","title":"<code>manveertamber/cadet-embed-base-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2025-05-11 eng-Latn"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-2d-large-v1","title":"<code>mixedbread-ai/mxbai-embed-2d-large-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.0M not specified 2024-03-04 eng-Latn"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-large-v1","title":"<code>mixedbread-ai/mxbai-embed-large-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 639.0 MB 2024-03-07 eng-Latn Citation <pre><code>    @online{emb2024mxbai,\n      title={Open Source Strikes Bread - New Fluffy Embeddings Model},\n      author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\n      year={2024},\n      url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n    }\n\n    @article{li2023angle,\n      title={AnglE-optimized Text Embeddings},\n      author={Li, Xianming and Li, Jing},\n      journal={arXiv preprint arXiv:2309.12871},\n      year={2023}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-xsmall-v1","title":"<code>mixedbread-ai/mxbai-embed-xsmall-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 24.1M not specified 2024-08-13 eng-Latn"},{"location":"overview/available_models/text/#nomic-aimodernbert-embed-base","title":"<code>nomic-ai/modernbert-embed-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2024-12-29 eng-Latn"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1","title":"<code>nomic-ai/nomic-embed-text-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 not specified 522.0 MB 2024-01-31 eng-Latn Citation <pre><code>@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder},\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1-ablated","title":"<code>nomic-ai/nomic-embed-text-v1-ablated</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 not specified not specified 2024-01-15 eng-Latn"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1-unsupervised","title":"<code>nomic-ai/nomic-embed-text-v1-unsupervised</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 not specified not specified 2024-01-15 eng-Latn"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v15","title":"<code>nomic-ai/nomic-embed-text-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 137.0M 522.0 MB 2024-02-10 eng-Latn Citation <pre><code>@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder},\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidianv-embed-v1","title":"<code>nvidia/NV-Embed-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.8B 29.2 GB 2024-09-13 eng-Latn Citation <pre><code>@misc{moreira2025nvretrieverimprovingtextembedding,\n      title={NV-Retriever: Improving text embedding models with effective hard-negative mining},\n      author={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2407.15831},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.15831}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidianv-embed-v2","title":"<code>nvidia/NV-Embed-v2</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.8B 14.6 GB 2024-09-09 eng-Latn Citation <pre><code>@misc{moreira2025nvretrieverimprovingtextembedding,\n      title={NV-Retriever: Improving text embedding models with effective hard-negative mining},\n      author={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2407.15831},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.15831}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidiallama-embed-nemotron-8b","title":"<code>nvidia/llama-embed-nemotron-8b</code>","text":"<p>License: https://huggingface.co/nvidia/llama-embed-nemotron-8b/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.5B 28.0 GB 2025-10-23 afr-Latn, amh-Ethi, ara-Arab, arq-Arab, ary-Arab, ... (66) Citation <pre><code>@misc{moreira2025nvretrieverimprovingtextembedding,\n      title={NV-Retriever: Improving text embedding models with effective hard-negative mining},\n      author={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2407.15831},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.15831}\n}\n</code></pre>"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v1","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 133.0M 507.0 MB 2024-03-07 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 67.0M 267.0 MB 2024-07-17 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 22.7M 86.0 MB 2024-07-18 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 67.0M 267.0 MB 2025-03-28 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 30522 137.4M 549.0 MB 2025-06-18 eng-Latn"},{"location":"overview/available_models/text/#samaya-airepllama-reproduced","title":"<code>samaya-ai/RepLLaMA-reproduced</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0M 27.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{rankllama,\n      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval},\n      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},\n      year={2023},\n      journal={arXiv:2310.08319},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama2-7b-v1","title":"<code>samaya-ai/promptriever-llama2-7b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 27.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama31-8b-instruct-v1","title":"<code>samaya-ai/promptriever-llama3.1-8b-instruct-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.0B 31.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama31-8b-v1","title":"<code>samaya-ai/promptriever-llama3.1-8b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.0B 31.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-mistral-v01-7b-v1","title":"<code>samaya-ai/promptriever-mistral-v0.1-7b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 27.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sergeyzhberta","title":"<code>sergeyzh/BERTA</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 128.0M 489.0 MB 2025-03-10 rus-Cyrl"},{"location":"overview/available_models/text/#sergeyzhrubert-mini-frida","title":"<code>sergeyzh/rubert-mini-frida</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 32.3M 123.0 MB 2025-03-02 rus-Cyrl"},{"location":"overview/available_models/text/#tencentyoutu-embedding","title":"<code>tencent/Youtu-Embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.7B not specified 2025-09-28 zho-Hans Citation <pre><code>@misc{zhang2025codiemb,\n  title={CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity},\n  author={Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing},\n  year={2025},\n  eprint={2508.11442},\n  archivePrefix={arXiv},\n  url={https://arxiv.org/abs/2508.11442},\n}\n</code></pre>"},{"location":"overview/available_models/text/#voyageaivoyage-2","title":"<code>voyageai/voyage-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.0K 1024 not specified not specified 2023-10-29 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3","title":"<code>voyageai/voyage-3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-09-18 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-large","title":"<code>voyageai/voyage-3-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-07 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-lite","title":"<code>voyageai/voyage-3-lite</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 512 not specified not specified 2024-09-18 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-m-exp","title":"<code>voyageai/voyage-3-m-exp</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 2048 6.9B not specified 2025-01-08 eng-Latn"},{"location":"overview/available_models/text/#voyageaivoyage-35","title":"<code>voyageai/voyage-3.5</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-35-output_dtypebinary","title":"<code>voyageai/voyage-3.5 (output_dtype=binary)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-35-output_dtypeint8","title":"<code>voyageai/voyage-3.5 (output_dtype=int8)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-code-2","title":"<code>voyageai/voyage-code-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1536 not specified not specified 2024-01-23 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-code-3","title":"<code>voyageai/voyage-code-3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-12-04 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-finance-2","title":"<code>voyageai/voyage-finance-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-05-30 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-large-2","title":"<code>voyageai/voyage-large-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1536 not specified not specified 2023-10-29 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-large-2-instruct","title":"<code>voyageai/voyage-large-2-instruct</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1024 not specified not specified 2024-05-05 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-law-2","title":"<code>voyageai/voyage-law-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1024 not specified not specified 2024-04-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-multilingual-2","title":"<code>voyageai/voyage-multilingual-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-06-10 not specified"},{"location":"overview/available_models/text/#yibinleilens-d4000","title":"<code>yibinlei/LENS-d4000</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4000 7.1B 26.5 GB 2025-01-17 not specified Citation <pre><code>@article{lei2025lens,\n  title={Enhancing Lexicon-Based Text Embeddings with Large Language Models},\n  author={Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew},\n  journal={arXiv preprint arXiv:2501.09749},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#yibinleilens-d8000","title":"<code>yibinlei/LENS-d8000</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 8000 7.1B 26.5 GB 2025-01-17 not specified Citation <pre><code>@article{lei2025lens,\n  title={Enhancing Lexicon-Based Text Embeddings with Large Language Models},\n  author={Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew},\n  journal={arXiv preprint arXiv:2501.09749},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#zeta-alpha-aizeta-alpha-e5-mistral","title":"<code>zeta-alpha-ai/Zeta-Alpha-E5-Mistral</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-08-30 eng-Latn"},{"location":"overview/available_models/text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/text/#aiteamvnvietnamese_embedding","title":"<code>AITeamVN/Vietnamese_Embedding</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-03-17 vie-Latn"},{"location":"overview/available_models/text/#alibaba-nlpgte-base-en-v15","title":"<code>Alibaba-NLP/gte-base-en-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 137.0M not specified 2024-06-20 eng-Latn"},{"location":"overview/available_models/text/#alibaba-nlpgte-modernbert-base","title":"<code>Alibaba-NLP/gte-modernbert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 284.0 MB 2025-01-21 eng-Latn"},{"location":"overview/available_models/text/#alibaba-nlpgte-multilingual-base","title":"<code>Alibaba-NLP/gte-multilingual-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 305.0M 582.0 MB 2024-07-20 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)"},{"location":"overview/available_models/text/#baaibge-en-icl","title":"<code>BAAI/bge-en-icl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-07-25 eng-Latn Citation <pre><code>    @misc{li2024makingtextembeddersfewshot,\n      title={Making Text Embedders Few-Shot Learners},\n      author={Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2409.15700},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.15700},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-m3","title":"<code>BAAI/bge-m3</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-06-28 afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)"},{"location":"overview/available_models/text/#baaibge-m3-unsupervised","title":"<code>BAAI/bge-m3-unsupervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-01-30 afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)"},{"location":"overview/available_models/text/#baaibge-multilingual-gemma2","title":"<code>BAAI/bge-multilingual-gemma2</code>","text":"<p>License: https://ai.google.dev/gemma/terms</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3584 9.2B 34.4 GB 2024-07-25 eng-Latn, fra-Latn, jpn-Jpan, jpn-Latn, kor-Hang, ... (7)"},{"location":"overview/available_models/text/#baaibge-reranker-v2-m3","title":"BAAI/bge-reranker-v2-m3","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 2.1 GB 2024-06-24 ara-Arab, ben-Beng, dan-Latn, deu-Latn, eng-Latn, ... (32) Citation <pre><code>    @misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval},\n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n    }\n    @misc{chen2024bge,\n          title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n          author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n          year={2024},\n          eprint={2402.03216},\n          archivePrefix={arXiv},\n          primaryClass={cs.CL}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#bytedancelistconranker","title":"<code>ByteDance/ListConRanker</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 401.0M 1.2 GB 2024-12-11 zho-Hans Citation <pre><code>@article{liu2025listconranker,\n  title={ListConRanker: A Contrastive Text Reranker with Listwise Encoding},\n  author={Liu, Junlong and Ma, Yue and Zhao, Ruihui and Zheng, Junhao and Ma, Qianli and Kang, Yangyang},\n  journal={arXiv preprint arXiv:2501.07111},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#classicalyinka","title":"<code>Classical/Yinka</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 326.0M 1.2 GB 2024-01-09 zho-Hans"},{"location":"overview/available_models/text/#dmetasouldmeta-embedding-zh-small","title":"<code>DMetaSoul/Dmeta-embedding-zh-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 1.0K 768 74.2M 283.0 MB 2024-03-25 zho-Hans"},{"location":"overview/available_models/text/#dmetasoulsbert-chinese-general-v1","title":"<code>DMetaSoul/sbert-chinese-general-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 128 not specified not specified 2022-03-25 zho-Hans"},{"location":"overview/available_models/text/#deeppavlovdistilrubert-small-cased-conversational","title":"<code>DeepPavlov/distilrubert-small-cased-conversational</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 107.0M 408.0 MB 2022-06-28 rus-Cyrl Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.02340,\n      doi = {10.48550/ARXIV.2205.02340},\n      url = {https://arxiv.org/abs/2205.02340},\n      author = {Kolesnikova, Alina and Kuratov, Yuri and Konovalov, Vasily and Burtsev, Mikhail},\n      keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Knowledge Distillation of Russian Language Models with Reduction of Vocabulary},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {arXiv.org perpetual, non-exclusive license}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deeppavlovrubert-base-cased","title":"<code>DeepPavlov/rubert-base-cased</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 1.3B 4.8 GB 2020-03-04 rus-Cyrl Citation <pre><code>@misc{kuratov2019adaptationdeepbidirectionalmultilingual,\n      title={Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language},\n      author={Yuri Kuratov and Mikhail Arkhipov},\n      year={2019},\n      eprint={1905.07213},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/1905.07213},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deeppavlovrubert-base-cased-sentence","title":"<code>DeepPavlov/rubert-base-cased-sentence</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 107.0M 408.0 MB 2020-03-04 rus-Cyrl"},{"location":"overview/available_models/text/#gameselosts-multilingual-mpnet-base-v2","title":"<code>Gameselo/STS-multilingual-mpnet-base-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-07 not specified"},{"location":"overview/available_models/text/#greennodegreennode-embedding-large-vn-mixed-v1","title":"<code>GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-04-11 vie-Latn"},{"location":"overview/available_models/text/#greennodegreennode-embedding-large-vn-v1","title":"<code>GreenNode/GreenNode-Embedding-Large-VN-V1</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-04-11 vie-Latn"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-v1","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-08-27 eng-Latn, zho-Hans Citation <pre><code>@article{hu2025kalm,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others},\n  journal={arXiv preprint arXiv:2501.01028},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#haon-chenspeed-embedding-7b-instruct","title":"<code>Haon-Chen/speed-embedding-7b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K not specified 7.1B 13.2 GB 2024-10-31 eng-Latn"},{"location":"overview/available_models/text/#hooshvarelabbert-base-parsbert-uncased","title":"<code>HooshvareLab/bert-base-parsbert-uncased</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2021-05-19 fas-Arab Citation <pre><code>    @article{ParsBERT,\n    title={ParsBERT: Transformer-based Model for Persian Language Understanding},\n    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},\n    journal={ArXiv},\n    year={2020},\n    volume={abs/2005.12515}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hum-workslodestone-base-4096-v1","title":"<code>Hum-Works/lodestone-base-4096-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 not specified not specified 2023-08-25 eng-Latn"},{"location":"overview/available_models/text/#human","title":"Human","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified not specified ara-Arab, dan-Latn, eng-Latn, nob-Latn, rus-Cyrl"},{"location":"overview/available_models/text/#jaumegemma-2b-embeddings","title":"<code>Jaume/gemma-2b-embeddings</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.5B 9.3 GB 2024-06-29 not specified"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-base","title":"<code>Lajavaness/bilingual-embedding-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-26 not specified"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-large","title":"<code>Lajavaness/bilingual-embedding-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2024-06-24 eng-Latn, fra-Latn"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-small","title":"<code>Lajavaness/bilingual-embedding-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2024-07-17 eng-Latn, fra-Latn"},{"location":"overview/available_models/text/#mcinexthakim","title":"<code>MCINext/Hakim</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcinexthakim-small","title":"<code>MCINext/Hakim-small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 38.7M 148.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcinexthakim-unsup","title":"<code>MCINext/Hakim-unsup</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mihaiiibulbasaur","title":"<code>Mihaiii/Bulbasaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-27 not specified"},{"location":"overview/available_models/text/#mihaiiiivysaur","title":"<code>Mihaiii/Ivysaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-04-27 not specified"},{"location":"overview/available_models/text/#mihaiiisquirtle","title":"<code>Mihaiii/Squirtle</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 15.6M 60.0 MB 2024-04-30 not specified"},{"location":"overview/available_models/text/#mihaiiivenusaur","title":"<code>Mihaiii/Venusaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 15.6M 60.0 MB 2024-04-29 not specified"},{"location":"overview/available_models/text/#mihaiiiwartortle","title":"<code>Mihaiii/Wartortle</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-30 not specified"},{"location":"overview/available_models/text/#mihaiiigte-micro","title":"<code>Mihaiii/gte-micro</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-21 not specified"},{"location":"overview/available_models/text/#mihaiiigte-micro-v4","title":"<code>Mihaiii/gte-micro-v4</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 19.2M 73.0 MB 2024-04-22 not specified"},{"location":"overview/available_models/text/#nbailabnb-sbert-base","title":"<code>NbAiLab/nb-sbert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 75 4096 1.8B 197.0 MB 2022-11-23 dan-Latn, nno-Latn, nob-Latn, swe-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-100k","title":"<code>NeuML/pubmedbert-base-embeddings-100K</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 100.0K 0.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-1m","title":"<code>NeuML/pubmedbert-base-embeddings-1M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 1.0M 2.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-2m","title":"<code>NeuML/pubmedbert-base-embeddings-2M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 1.9M 7.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-500k","title":"<code>NeuML/pubmedbert-base-embeddings-500K</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 500.0K 2.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-8m","title":"<code>NeuML/pubmedbert-base-embeddings-8M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.8M 30.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 135.2M 516.0 MB 2024-06-16 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet","title":"<code>Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2024-06-25 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-triplet-matryoshka-v2","title":"<code>Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 768 768 135.0M 516.0 MB 2024-07-28 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-14 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-labse-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabic-labse-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.9M 1.8 GB 2024-06-16 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet","title":"<code>Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 109.5M 418.0 MB 2024-06-15 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2024-06-17 ara-Arab"},{"location":"overview/available_models/text/#opensearch-aiops-moa-conan-embedding-v1","title":"<code>OpenSearch-AI/Ops-MoA-Conan-embedding-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 343.0M 2.0 GB 2025-03-26 zho-Hans"},{"location":"overview/available_models/text/#opensearch-aiops-moa-yuan-embedding-10","title":"<code>OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 343.0M 2.0 GB 2025-03-26 zho-Hans"},{"location":"overview/available_models/text/#ordalietechsolon-embeddings-large-01","title":"<code>OrdalieTech/Solon-embeddings-large-0.1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2023-12-09 fra-Latn"},{"location":"overview/available_models/text/#ordalietechsolon-embeddings-mini-beta-11","title":"<code>OrdalieTech/Solon-embeddings-mini-beta-1.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 210.0M 808.0 MB 2025-01-01 fra-Latn"},{"location":"overview/available_models/text/#orlikbkartonbert-use-base-v1","title":"<code>OrlikB/KartonBERT-USE-base-v1</code>","text":"<p>License: gpl-3.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 103.7M 396.0 MB 2024-09-30 pol-Latn"},{"location":"overview/available_models/text/#orlikbst-polish-kartonberta-base-alpha-v1","title":"<code>OrlikB/st-polish-kartonberta-base-alpha-v1</code>","text":"<p>License: lgpl</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 not specified not specified 2023-11-12 pol-Latn"},{"location":"overview/available_models/text/#partaitooka-sbert","title":"<code>PartAI/Tooka-SBERT</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 353.0M 1.3 GB 2024-12-07 fas-Arab"},{"location":"overview/available_models/text/#partaitooka-sbert-v2-large","title":"<code>PartAI/Tooka-SBERT-V2-Large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 353.0M 1.3 GB 2025-05-01 fas-Arab"},{"location":"overview/available_models/text/#partaitooka-sbert-v2-small","title":"<code>PartAI/Tooka-SBERT-V2-Small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 122.9M 496.0 MB 2025-05-01 fas-Arab"},{"location":"overview/available_models/text/#partaitookabert-base","title":"<code>PartAI/TookaBERT-Base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 122.9M 469.0 MB 2024-12-08 fas-Arab"},{"location":"overview/available_models/text/#qodoqodo-embed-1-15b","title":"<code>Qodo/Qodo-Embed-1-1.5B</code>","text":"<p>License: https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 1.8B 6.6 GB 2025-02-19 c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)"},{"location":"overview/available_models/text/#qodoqodo-embed-1-7b","title":"<code>Qodo/Qodo-Embed-1-7B</code>","text":"<p>License: https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.6B 28.4 GB 2025-02-24 c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)"},{"location":"overview/available_models/text/#shuu12121codesearch-modernbert-crow-plus","title":"<code>Shuu12121/CodeSearch-ModernBERT-Crow-Plus</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 1.0K 768 151.7M 607.0 MB 2025-04-21 eng-Latn"},{"location":"overview/available_models/text/#tencentbacconan-embedding-v1","title":"<code>TencentBAC/Conan-embedding-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 326.0M 1.2 GB 2024-08-22 zho-Hans"},{"location":"overview/available_models/text/#vovanphucsup-simcse-vietnamese-phobert-base","title":"<code>VoVanPhuc/sup-SimCSE-VietNamese-phobert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 768 135.0M 517.0 MB 2021-05-26 vie-Latn"},{"location":"overview/available_models/text/#aari1995german_semantic_sts_v2","title":"<code>aari1995/German_Semantic_STS_V2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.7M 1.3 GB 2022-11-17 deu-Latn"},{"location":"overview/available_models/text/#abhinandmedembed-small-v01","title":"<code>abhinand/MedEmbed-small-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-10-20 eng-Latn"},{"location":"overview/available_models/text/#ai-foreversbert_large_mt_nlu_ru","title":"<code>ai-forever/sbert_large_mt_nlu_ru</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 427.0M 1.6 GB 2021-05-18 rus-Cyrl"},{"location":"overview/available_models/text/#ai-foreversbert_large_nlu_ru","title":"<code>ai-forever/sbert_large_nlu_ru</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 427.0M 1.6 GB 2020-11-20 rus-Cyrl"},{"location":"overview/available_models/text/#amazontitan-text-embeddings-v2","title":"<code>amazon/Titan-text-embeddings-v2</code>","text":"<p>License: https://aws.amazon.com/service-terms/</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2024-04-30 eng-Latn"},{"location":"overview/available_models/text/#avsolatoriogist-embedding-v0","title":"<code>avsolatorio/GIST-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 418.0 MB 2024-01-31 eng-Latn"},{"location":"overview/available_models/text/#avsolatoriogist-all-minilm-l6-v2","title":"<code>avsolatorio/GIST-all-MiniLM-L6-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-02-03 eng-Latn"},{"location":"overview/available_models/text/#avsolatoriogist-large-embedding-v0","title":"<code>avsolatorio/GIST-large-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 1.2 GB 2024-02-14 eng-Latn"},{"location":"overview/available_models/text/#avsolatoriogist-small-embedding-v0","title":"<code>avsolatorio/GIST-small-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-02-03 eng-Latn"},{"location":"overview/available_models/text/#avsolatorionoinstruct-small-embedding-v0","title":"<code>avsolatorio/NoInstruct-small-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-05-01 eng-Latn"},{"location":"overview/available_models/text/#bedrockamazon-titan-embed-text-v1","title":"<code>bedrock/amazon-titan-embed-text-v1</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2023-09-27 not specified"},{"location":"overview/available_models/text/#bedrockamazon-titan-embed-text-v2","title":"<code>bedrock/amazon-titan-embed-text-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 not specified not specified 2024-04-30 not specified"},{"location":"overview/available_models/text/#bigsciencesgpt-bloom-7b1-msmarco","title":"<code>bigscience/sgpt-bloom-7b1-msmarco</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 not specified not specified 2022-08-26 not specified"},{"location":"overview/available_models/text/#bkai-foundation-modelsvietnamese-bi-encoder","title":"<code>bkai-foundation-models/vietnamese-bi-encoder</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 768 135.0M 515.0 MB 2023-09-09 vie-Latn"},{"location":"overview/available_models/text/#bm25s","title":"<code>bm25s</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2024-07-10 eng-Latn Citation <pre><code>@misc{bm25s,\n      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring},\n      author={Xing Han L\u00f9},\n      year={2024},\n      eprint={2407.03618},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.03618},\n}\n</code></pre>"},{"location":"overview/available_models/text/#brahmairesearchslx-v01","title":"<code>brahmairesearch/slx-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-08-13 eng-Latn"},{"location":"overview/available_models/text/#castorinimonobert-large-msmarco","title":"castorini/monobert-large-msmarco","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2020-05-28 eng-Latn"},{"location":"overview/available_models/text/#castorinimonot5-3b-msmarco-10k","title":"castorini/monot5-3b-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-base-msmarco-10k","title":"castorini/monot5-base-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-large-msmarco-10k","title":"castorini/monot5-large-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-small-msmarco-10k","title":"castorini/monot5-small-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#codesagecodesage-base-v2","title":"<code>codesage/codesage-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 356.0M 1.3 GB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)"},{"location":"overview/available_models/text/#codesagecodesage-large-v2","title":"<code>codesage/codesage-large-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 4.8 GB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)"},{"location":"overview/available_models/text/#codesagecodesage-small-v2","title":"<code>codesage/codesage-small-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 130.0M 496.0 MB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)"},{"location":"overview/available_models/text/#cointegratedlabse-en-ru","title":"<code>cointegrated/LaBSE-en-ru</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 129.0M 492.0 MB 2021-06-10 rus-Cyrl"},{"location":"overview/available_models/text/#cointegratedrubert-tiny","title":"<code>cointegrated/rubert-tiny</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 312 11.9M 45.0 MB 2021-05-24 rus-Cyrl"},{"location":"overview/available_models/text/#cointegratedrubert-tiny2","title":"<code>cointegrated/rubert-tiny2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 29.4M 112.0 MB 2021-10-28 rus-Cyrl"},{"location":"overview/available_models/text/#colbert-ircolbertv20","title":"<code>colbert-ir/colbertv2.0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 180 not specified 110.0M 418.0 MB 2024-09-21 eng-Latn"},{"location":"overview/available_models/text/#consciousaicai-lunaris-text-embeddings","title":"<code>consciousAI/cai-lunaris-text-embeddings</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-06-22 not specified"},{"location":"overview/available_models/text/#consciousaicai-stellaris-text-embeddings","title":"<code>consciousAI/cai-stellaris-text-embeddings</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 not specified not specified 2023-06-23 not specified"},{"location":"overview/available_models/text/#deepfileembedder-100p","title":"<code>deepfile/embedder-100p</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 not specified 1.0 GB 2023-07-24 not specified"},{"location":"overview/available_models/text/#deepvkuser-bge-m3","title":"<code>deepvk/USER-bge-m3</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 359.0M 1.3 GB 2024-07-05 rus-Cyrl"},{"location":"overview/available_models/text/#deepvkdeberta-v1-base","title":"<code>deepvk/deberta-v1-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.0M 473.0 MB 2023-02-07 rus-Cyrl"},{"location":"overview/available_models/text/#dunzhangstella-large-zh-v3-1792d","title":"<code>dunzhang/stella-large-zh-v3-1792d</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 not specified not specified 2024-02-17 zho-Hans"},{"location":"overview/available_models/text/#dunzhangstella-mrl-large-zh-v35-1792d","title":"<code>dunzhang/stella-mrl-large-zh-v3.5-1792d</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2024-02-27 zho-Hans"},{"location":"overview/available_models/text/#dwzhue5-base-4k","title":"<code>dwzhu/e5-base-4k</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K not specified not specified not specified 2024-03-28 eng-Latn"},{"location":"overview/available_models/text/#facebooksonar","title":"<code>facebook/SONAR</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2021-05-21 ace-Arab, ace-Latn, acm-Arab, acq-Arab, aeb-Arab, ... (204) Citation <pre><code>@misc{Duquenne:2023:sonar_arxiv,\n  author = {Paul-Ambroise Duquenne and Holger Schwenk and Benoit Sagot},\n  title = {{SONAR:} Sentence-Level Multimodal and Language-Agnostic Representations},\n  publisher = {arXiv},\n  year = {2023},\n  url = {https://arxiv.org/abs/2308.11466},\n}\n</code></pre>"},{"location":"overview/available_models/text/#facebookcontriever-msmarco","title":"<code>facebook/contriever-msmarco</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 150.0M 572.0 MB 2022-06-25 eng-Latn Citation <pre><code>    @misc{izacard2021contriever,\n      title={Unsupervised Dense Information Retrieval with Contrastive Learning},\n      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},\n      year={2021},\n      url = {https://arxiv.org/abs/2112.09118},\n      doi = {10.48550/ARXIV.2112.09118},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#fangxqxyz-embedding","title":"<code>fangxq/XYZ-embedding</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 326.0M 1.2 GB 2024-09-13 zho-Hans"},{"location":"overview/available_models/text/#googleflan-t5-base","title":"google/flan-t5-base","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 944.0 MB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-large","title":"google/flan-t5-large","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 2.9 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-xl","title":"google/flan-t5-xl","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 10.6 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-xxl","title":"google/flan-t5-xxl","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 42.0 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#hiieuhalong_embedding","title":"<code>hiieu/halong_embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-07-06 vie-Latn"},{"location":"overview/available_models/text/#iampandazpoint_large_embedding_zh","title":"<code>iampanda/zpoint_large_embedding_zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2024-06-04 zho-Hans"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-107m-multilingual","title":"<code>ibm-granite/granite-embedding-107m-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 107.0M 204.0 MB 2024-12-18 ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13) Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-125m-english","title":"<code>ibm-granite/granite-embedding-125m-english</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 125.0M 238.0 MB 2024-12-18 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-278m-multilingual","title":"<code>ibm-granite/granite-embedding-278m-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 530.0 MB 2024-12-18 ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13) Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-30m-english","title":"<code>ibm-granite/granite-embedding-30m-english</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 30.0M 58.0 MB 2024-12-18 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-english-r2","title":"<code>ibm-granite/granite-embedding-english-r2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 284.0 MB 2025-08-15 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-small-english-r2","title":"<code>ibm-granite/granite-embedding-small-english-r2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 384 47.0M 91.0 MB 2025-08-15 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#infgradstella-base-en-v2","title":"<code>infgrad/stella-base-en-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 not specified not specified not specified 2023-10-19 eng-Latn"},{"location":"overview/available_models/text/#infgradstella-base-zh-v3-1792d","title":"<code>infgrad/stella-base-zh-v3-1792d</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 not specified not specified 2024-02-17 zho-Hans"},{"location":"overview/available_models/text/#izhxudever-bloom-1b1","title":"<code>izhx/udever-bloom-1b1</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)"},{"location":"overview/available_models/text/#izhxudever-bloom-3b","title":"<code>izhx/udever-bloom-3b</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)"},{"location":"overview/available_models/text/#izhxudever-bloom-560m","title":"<code>izhx/udever-bloom-560m</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)"},{"location":"overview/available_models/text/#izhxudever-bloom-7b1","title":"<code>izhx/udever-bloom-7b1</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)"},{"location":"overview/available_models/text/#jhu-clspfollowir-7b","title":"jhu-clsp/FollowIR-7B","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 13.5 GB 2024-04-29 eng-Latn Citation <pre><code>    @misc{weller2024followir,\n      title={FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n      author={Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n      year={2024},\n      eprint={2403.15246},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-colbert-v2","title":"<code>jinaai/jina-colbert-v2</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K not specified 559.0M 1.0 GB 2024-08-16 ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (22)"},{"location":"overview/available_models/text/#jinaaijina-embedding-b-en-v1","title":"<code>jinaai/jina-embedding-b-en-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 110.0M 420.0 MB 2023-07-07 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models},\n      author={Michael G\u00fcnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao},\n      year={2023},\n      eprint={2307.11224},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embedding-s-en-v1","title":"<code>jinaai/jina-embedding-s-en-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 35.0M 134.0 MB 2023-07-07 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models},\n      author={Michael G\u00fcnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao},\n      year={2023},\n      eprint={2307.11224},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v2-base-en","title":"<code>jinaai/jina-embeddings-v2-base-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 137.0M 262.0 MB 2023-09-27 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents},\n      author={Michael G\u00fcnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},\n      year={2023},\n      eprint={2310.19923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v2-small-en","title":"<code>jinaai/jina-embeddings-v2-small-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 32.7M 62.0 MB 2023-09-27 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents},\n      author={Michael G\u00fcnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},\n      year={2023},\n      eprint={2310.19923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-reranker-v2-base-multilingual","title":"jinaai/jina-reranker-v2-base-multilingual","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 531.0 MB 2024-09-26 eng-Latn"},{"location":"overview/available_models/text/#keeeeenwmicrollama-text-embedding","title":"<code>keeeeenw/MicroLlama-text-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 272.0M 1.0 GB 2024-11-10 eng-Latn"},{"location":"overview/available_models/text/#lier007xiaobu-embedding","title":"<code>lier007/xiaobu-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 326.0M 1.2 GB 2024-01-09 zho-Hans"},{"location":"overview/available_models/text/#lier007xiaobu-embedding-v2","title":"<code>lier007/xiaobu-embedding-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 326.0M 1.2 GB 2024-06-30 zho-Hans"},{"location":"overview/available_models/text/#lightonaigte-moderncolbert-v1","title":"<code>lightonai/GTE-ModernColBERT-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K not specified 149.0M not specified 2025-04-30 eng-Latn"},{"location":"overview/available_models/text/#llmrailsember-v1","title":"<code>llmrails/ember-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-10-10 eng-Latn"},{"location":"overview/available_models/text/#m3hrdadfibert-zwnj-wnli-mean-tokens","title":"<code>m3hrdadfi/bert-zwnj-wnli-mean-tokens</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 118.3M 451.0 MB 2021-06-28 fas-Arab"},{"location":"overview/available_models/text/#m3hrdadfiroberta-zwnj-wnli-mean-tokens","title":"<code>m3hrdadfi/roberta-zwnj-wnli-mean-tokens</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 118.3M 451.0 MB 2021-06-28 fas-Arab"},{"location":"overview/available_models/text/#malenia1ternary-weight-embedding","title":"<code>malenia1/ternary-weight-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 98.7M 158.0 MB 2024-10-23 not specified"},{"location":"overview/available_models/text/#manubge-m3-custom-fr","title":"<code>manu/bge-m3-custom-fr</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-04-11 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v02","title":"<code>manu/sentence_croissant_alpha_v0.2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-03-15 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v03","title":"<code>manu/sentence_croissant_alpha_v0.3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-04-26 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v04","title":"<code>manu/sentence_croissant_alpha_v0.4</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-04-27 eng-Latn, fra-Latn"},{"location":"overview/available_models/text/#meta-llamallama-2-7b-chat-hf","title":"meta-llama/Llama-2-7b-chat-hf","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-07-18 eng-Latn Citation <pre><code>@misc{touvron2023llama2openfoundation,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2307.09288},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#meta-llamallama-2-7b-hf","title":"meta-llama/Llama-2-7b-hf","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-07-18 eng-Latn Citation <pre><code>@misc{touvron2023llama2openfoundation,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2307.09288},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_glove","title":"<code>minishlab/M2V_base_glove</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 102.0M 391.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_glove_subword","title":"<code>minishlab/M2V_base_glove_subword</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 103.0M 391.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_output","title":"<code>minishlab/M2V_base_output</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.6M 29.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_multilingual_output","title":"<code>minishlab/M2V_multilingual_output</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 128.0M 489.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-2m","title":"<code>minishlab/potion-base-2M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 2.0M 7.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-4m","title":"<code>minishlab/potion-base-4M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 128 3.8M 14.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-8m","title":"<code>minishlab/potion-base-8M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.6M 29.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-multilingual-128m","title":"<code>minishlab/potion-multilingual-128M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 128.0M 489.0 MB 2025-05-23 afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (101) Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mistralaimistral-7b-instruct-v02","title":"mistralai/Mistral-7B-Instruct-v0.2","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-12-11 eng-Latn Citation <pre><code>@misc{jiang2023mistral7b,\n      title={Mistral 7B},\n      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L\u00e9lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed},\n      year={2023},\n      eprint={2310.06825},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2310.06825},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-base","title":"<code>moka-ai/m3e-base</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 102.0M 390.0 MB 2023-06-06 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-large","title":"<code>moka-ai/m3e-large</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 not specified not specified 2023-06-21 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-small","title":"<code>moka-ai/m3e-small</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 not specified not specified 2023-06-02 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#myrkursentence-transformer-parsbert-fa","title":"<code>myrkur/sentence-transformer-parsbert-fa</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2024-12-10 fas-Arab"},{"location":"overview/available_models/text/#omarelshehyarabic-english-sts-matryoshka","title":"<code>omarelshehy/arabic-english-sts-matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2024-10-13 ara-Arab, eng-Latn"},{"location":"overview/available_models/text/#openaitext-embedding-3-large","title":"<code>openai/text-embedding-3-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3072 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-large-embed_dim512","title":"<code>openai/text-embedding-3-large (embed_dim=512)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-small","title":"<code>openai/text-embedding-3-small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-small-embed_dim512","title":"<code>openai/text-embedding-3-small (embed_dim=512)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-ada-002","title":"<code>openai/text-embedding-ada-002</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2022-12-15 not specified"},{"location":"overview/available_models/text/#openbmbminicpm-embedding","title":"<code>openbmb/MiniCPM-Embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 2304 2.7B 5.1 GB 2024-09-04 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#panalexeuxlm-roberta-ua-distilled","title":"<code>panalexeu/xlm-roberta-ua-distilled</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2025-04-15 eng-Latn, ukr-Cyrl"},{"location":"overview/available_models/text/#prdevmini-gte","title":"<code>prdev/mini-gte</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 66.3M 253.0 MB 2025-01-28 eng-Latn"},{"location":"overview/available_models/text/#richinfoairitrieve_zh_v1","title":"<code>richinfoai/ritrieve_zh_v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2025-03-25 zho-Hans"},{"location":"overview/available_models/text/#sbunlpfabert","title":"<code>sbunlp/fabert</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2024-10-07 fas-Arab"},{"location":"overview/available_models/text/#sdadasmmlw-e5-base","title":"<code>sdadas/mmlw-e5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2023-11-17 pol-Latn"},{"location":"overview/available_models/text/#sdadasmmlw-e5-large","title":"<code>sdadas/mmlw-e5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2023-11-17 pol-Latn"},{"location":"overview/available_models/text/#sdadasmmlw-e5-small","title":"<code>sdadas/mmlw-e5-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2023-11-17 pol-Latn"},{"location":"overview/available_models/text/#sdadasmmlw-roberta-base","title":"<code>sdadas/mmlw-roberta-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 124.4M 475.0 MB 2023-11-17 pol-Latn"},{"location":"overview/available_models/text/#sdadasmmlw-roberta-large","title":"<code>sdadas/mmlw-roberta-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 435.0M 1.6 GB 2023-11-17 pol-Latn"},{"location":"overview/available_models/text/#sensenovapiccolo-base-zh","title":"<code>sensenova/piccolo-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 not specified not specified 2023-09-04 zho-Hans"},{"location":"overview/available_models/text/#sensenovapiccolo-large-zh-v2","title":"<code>sensenova/piccolo-large-zh-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2024-04-22 zho-Hans"},{"location":"overview/available_models/text/#sentence-transformerslabse","title":"<code>sentence-transformers/LaBSE</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 471.0M 1.8 GB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@misc{feng2022languageagnosticbertsentenceembedding,\n      title={Language-agnostic BERT Sentence Embedding},\n      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},\n      year={2022},\n      eprint={2007.01852},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2007.01852},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-minilm-l12-v2","title":"<code>sentence-transformers/all-MiniLM-L12-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 33.4M 127.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-minilm-l6-v2","title":"<code>sentence-transformers/all-MiniLM-L6-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 22.7M 87.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-mpnet-base-v2","title":"<code>sentence-transformers/all-mpnet-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 384 768 109.0M 418.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-base","title":"<code>sentence-transformers/gtr-t5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 110.0M 209.0 MB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-large","title":"<code>sentence-transformers/gtr-t5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.0M 639.0 MB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-xl","title":"<code>sentence-transformers/gtr-t5-xl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 1.2B 2.3 GB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-xxl","title":"<code>sentence-transformers/gtr-t5-xxl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 4.9B 9.1 GB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersmulti-qa-minilm-l6-cos-v1","title":"<code>sentence-transformers/multi-qa-MiniLM-L6-cos-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersparaphrase-multilingual-minilm-l12-v2","title":"<code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 118.0M 449.0 MB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersparaphrase-multilingual-mpnet-base-v2","title":"<code>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-base","title":"<code>sentence-transformers/sentence-t5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 110.0M 209.0 MB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-large","title":"<code>sentence-transformers/sentence-t5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.0M 639.0 MB 2022-02-09 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-xl","title":"<code>sentence-transformers/sentence-t5-xl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 3.0B 2.3 GB 2024-03-27 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-xxl","title":"<code>sentence-transformers/sentence-t5-xxl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 11.0B 9.1 GB 2024-03-27 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersstatic-similarity-mrl-multilingual-v1","title":"<code>sentence-transformers/static-similarity-mrl-multilingual-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 108.4M 413.0 MB 2025-01-15 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (49)"},{"location":"overview/available_models/text/#sergeyzhlabse-ru-turbo","title":"<code>sergeyzh/LaBSE-ru-turbo</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 129.0M 490.0 MB 2024-06-27 rus-Cyrl"},{"location":"overview/available_models/text/#sergeyzhrubert-tiny-turbo","title":"<code>sergeyzh/rubert-tiny-turbo</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 29.2M 111.0 MB 2024-06-21 rus-Cyrl"},{"location":"overview/available_models/text/#shibing624text2vec-base-chinese","title":"<code>shibing624/text2vec-base-chinese</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 102.0M 390.0 MB 2022-01-23 zho-Hans Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#shibing624text2vec-base-chinese-paraphrase","title":"<code>shibing624/text2vec-base-chinese-paraphrase</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 118.0M 450.0 MB 2023-06-19 zho-Hans Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#shibing624text2vec-base-multilingual","title":"<code>shibing624/text2vec-base-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 117.7M 449.0 MB 2023-06-22 deu-Latn, eng-Latn, fra-Latn, ita-Latn, nld-Latn, ... (10) Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#silma-aisilma-embeddding-matryoshka-v01","title":"<code>silma-ai/silma-embeddding-matryoshka-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 135.2M 516.0 MB 2024-10-12 ara-Arab, eng-Latn"},{"location":"overview/available_models/text/#thenlpergte-base","title":"<code>thenlper/gte-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 209.0 MB 2023-07-27 eng-Latn"},{"location":"overview/available_models/text/#thenlpergte-base-zh","title":"<code>thenlper/gte-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 102.0M 195.0 MB 2023-11-08 zho-Hans"},{"location":"overview/available_models/text/#thenlpergte-large","title":"<code>thenlper/gte-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 639.0 MB 2023-07-27 eng-Latn"},{"location":"overview/available_models/text/#thenlpergte-large-zh","title":"<code>thenlper/gte-large-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 326.0M 621.0 MB 2023-11-08 zho-Hans"},{"location":"overview/available_models/text/#thenlpergte-small","title":"<code>thenlper/gte-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 64.0 MB 2023-07-27 eng-Latn"},{"location":"overview/available_models/text/#thenlpergte-small-zh","title":"<code>thenlper/gte-small-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 30.3M 58.0 MB 2023-11-08 zho-Hans"},{"location":"overview/available_models/text/#unicamp-dlmt5-13b-mmarco-100k","title":"unicamp-dl/mt5-13b-mmarco-100k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-11-04 afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103)"},{"location":"overview/available_models/text/#unicamp-dlmt5-base-mmarco-v2","title":"unicamp-dl/mt5-base-mmarco-v2","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-01-05 afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103) Citation <pre><code>@misc{bonifacio2021mmarco,\n      title={mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset},\n      author={Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n      year={2021},\n      eprint={2108.13897},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#w601sxsb1ade-embed","title":"<code>w601sxs/b1ade-embed</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 1024 335.0M 1.2 GB 2025-03-10 eng-Latn Citation <pre><code>    @misc{bigscience_workshop_2022,\n    author       = { {Shreyas Subramanian} },\n    title        = { {b1ade series of models} },\n    year         = 2024,\n    url          = { https://huggingface.co/w601sxs/b1ade-embed },\n    publisher    = { Hugging Face }\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/","title":"Any2AnyMultilingualRetrieval","text":"<ul> <li>Number of tasks: 3</li> </ul>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#witt2iretrieval","title":"WITT2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>mteb/wit</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 ara, bul, dan, ell, eng, ... (11) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bugliarello2022iglue,\n  author = {Bugliarello, Emanuele and Liu, Fangyu and Pfeiffer, Jonas and Reddy, Siva and Elliott, Desmond and Ponti, Edoardo Maria and Vuli{\\'c}, Ivan},\n  booktitle = {International Conference on Machine Learning},\n  organization = {PMLR},\n  pages = {2370--2392},\n  title = {IGLUE: A benchmark for transfer learning across modalities, tasks, and languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#xflickr30kcot2iretrieval","title":"XFlickr30kCoT2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>floschne/xflickrco</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, ind, jpn, rus, ... (8) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bugliarello2022iglue,\n  author = {Bugliarello, Emanuele and Liu, Fangyu and Pfeiffer, Jonas and Reddy, Siva and Elliott, Desmond and Ponti, Edoardo Maria and Vuli{\\'c}, Ivan},\n  booktitle = {International Conference on Machine Learning},\n  organization = {PMLR},\n  pages = {2370--2392},\n  title = {IGLUE: A benchmark for transfer learning across modalities, tasks, and languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#xm3600t2iretrieval","title":"XM3600T2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>floschne/xm3600</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 ara, ben, ces, dan, deu, ... (38) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{thapliyal2022crossmodal,\n  author = {Thapliyal, Ashish V and Tuset, Jordi Pont and Chen, Xi and Soricut, Radu},\n  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},\n  pages = {715--729},\n  title = {Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/","title":"Any2AnyRetrieval","text":"<ul> <li>Number of tasks: 49</li> </ul>"},{"location":"overview/available_tasks/any2anyretrieval/#blinkit2iretrieval","title":"BLINKIT2IRetrieval","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>JamieSJS/blink-it2i</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) cv_recall_at_1 eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#blinkit2tretrieval","title":"BLINKIT2TRetrieval","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>JamieSJS/blink-it2t</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) cv_recall_at_1 eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cirrit2iretrieval","title":"CIRRIT2IRetrieval","text":"<p>Retrieve images based on texts and images.</p> <p>Dataset: <code>MRBench/mbeir_cirr_task7</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{liu2021image,\n  author = {Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {2125--2134},\n  title = {Image retrieval on real-life images with pre-trained vision-and-language models},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cub200i2iretrieval","title":"CUB200I2IRetrieval","text":"<p>Retrieve bird images from 200 classes.</p> <p>Dataset: <code>isaacchung/cub200_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@article{article,\n  author = {Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},\n  month = {09},\n  pages = {},\n  title = {Caltech-UCSD Birds 200},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#edist2itretrieval","title":"EDIST2ITRetrieval","text":"<p>Retrieve news images and titles based on news content.</p> <p>Dataset: <code>MRBench/mbeir_edis_task2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image, text (t2it) ndcg_at_10 eng News derived created Citation <pre><code>@inproceedings{liu2023edis,\n  author = {Liu, Siqi and Feng, Weixi and Fu, Tsu-Jui and Chen, Wenhu and Wang, William},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {4877--4894},\n  title = {EDIS: Entity-Driven Image Search over Multimodal Web Content},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#encyclopediavqait2itretrieval","title":"EncyclopediaVQAIT2ITRetrieval","text":"<p>Retrieval Wiki passage and image and passage to answer query about an image.</p> <p>Dataset: <code>izhx/UMRB-EncyclopediaVQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) cv_recall_at_5 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{mensink2023encyclopedic,\n  author = {Mensink, Thomas and Uijlings, Jasper and Castrejon, Lluis and Goel, Arushi and Cadar, Felipe and Zhou, Howard and Sha, Fei and Araujo, Andr{\\'e} and Ferrari, Vittorio},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {3113--3124},\n  title = {Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#forbi2iretrieval","title":"FORBI2IRetrieval","text":"<p>Retrieve flat object images from 8 classes.</p> <p>Dataset: <code>isaacchung/forb_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@misc{wu2023forbflatobjectretrieval,\n  archiveprefix = {arXiv},\n  author = {Pengxiang Wu and Siman Wang and Kevin Dela Rosa and Derek Hao Hu},\n  eprint = {2309.16249},\n  primaryclass = {cs.CV},\n  title = {FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding},\n  url = {https://arxiv.org/abs/2309.16249},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashion200ki2tretrieval","title":"Fashion200kI2TRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>MRBench/mbeir_fashion200k_task3</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{han2017automatic,\n  author = {Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},\n  booktitle = {Proceedings of the IEEE international conference on computer vision},\n  pages = {1463--1471},\n  title = {Automatic spatially-aware fashion concept discovery},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashion200kt2iretrieval","title":"Fashion200kT2IRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>MRBench/mbeir_fashion200k_task0</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{han2017automatic,\n  author = {Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},\n  booktitle = {Proceedings of the IEEE international conference on computer vision},\n  pages = {1463--1471},\n  title = {Automatic spatially-aware fashion concept discovery},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashioniqit2iretrieval","title":"FashionIQIT2IRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>MRBench/mbeir_fashioniq_task7</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{wu2021fashion,\n  author = {Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},\n  booktitle = {Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},\n  pages = {11307--11317},\n  title = {Fashion iq: A new dataset towards retrieving images by natural language feedback},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#flickr30ki2tretrieval","title":"Flickr30kI2TRetrieval","text":"<p>Retrieve captions based on images.</p> <p>Dataset: <code>isaacchung/flickr30ki2t</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@article{Young2014FromID,\n  author = {Peter Young and Alice Lai and Micah Hodosh and J. Hockenmaier},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {67-78},\n  title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n  url = {https://api.semanticscholar.org/CorpusID:3104920},\n  volume = {2},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#flickr30kt2iretrieval","title":"Flickr30kT2IRetrieval","text":"<p>Retrieve images based on captions.</p> <p>Dataset: <code>isaacchung/flickr30kt2i</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@article{Young2014FromID,\n  author = {Peter Young and Alice Lai and Micah Hodosh and J. Hockenmaier},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {67-78},\n  title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n  url = {https://api.semanticscholar.org/CorpusID:3104920},\n  volume = {2},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gldv2i2iretrieval","title":"GLDv2I2IRetrieval","text":"<p>Retrieve names of landmarks based on their image.</p> <p>Dataset: <code>gowitheflow/gld-v2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Weyand_2020_CVPR,\n  author = {Weyand, Tobias and Araujo, Andre and Cao, Bingyi and Sim, Jack},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gldv2i2tretrieval","title":"GLDv2I2TRetrieval","text":"<p>Retrieve names of landmarks based on their image.</p> <p>Dataset: <code>JamieSJS/gld-v2-i2t</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Weyand_2020_CVPR,\n  author = {Weyand, Tobias and Araujo, Andre and Cao, Bingyi and Sim, Jack},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hatefulmemesi2tretrieval","title":"HatefulMemesI2TRetrieval","text":"<p>Retrieve captions based on memes to assess OCR abilities.</p> <p>Dataset: <code>Ahren09/MMSoc_HatefulMemes</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{kiela2020hateful,\n  author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},\n  journal = {Advances in neural information processing systems},\n  pages = {2611--2624},\n  title = {The hateful memes challenge: Detecting hate speech in multimodal memes},\n  volume = {33},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hatefulmemest2iretrieval","title":"HatefulMemesT2IRetrieval","text":"<p>Retrieve captions based on memes to assess OCR abilities.</p> <p>Dataset: <code>Ahren09/MMSoc_HatefulMemes</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{kiela2020hateful,\n  author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},\n  journal = {Advances in neural information processing systems},\n  pages = {2611--2624},\n  title = {The hateful memes challenge: Detecting hate speech in multimodal memes},\n  volume = {33},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#imagecodet2iretrieval","title":"ImageCoDeT2IRetrieval","text":"<p>Retrieve a specific video frame based on a precise caption.</p> <p>Dataset: <code>JamieSJS/imagecode</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) cv_recall_at_3 eng Web, Written derived found Citation <pre><code>@article{krojer2022image,\n  author = {Krojer, Benno and Adlakha, Vaibhav and Vineet, Vibhav and Goyal, Yash and Ponti, Edoardo and Reddy, Siva},\n  journal = {arXiv preprint arXiv:2203.15867},\n  title = {Image retrieval from contextual descriptions},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#infoseekit2itretrieval","title":"InfoSeekIT2ITRetrieval","text":"<p>Retrieve source text and image information to answer questions about images.</p> <p>Dataset: <code>mteb/InfoSeekIT2ITRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{chen2023can,\n  author = {Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {14948--14968},\n  title = {Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#infoseekit2tretrieval","title":"InfoSeekIT2TRetrieval","text":"<p>Retrieve source information to answer questions about images.</p> <p>Dataset: <code>MRBench/mbeir_infoseek_task6</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{chen2023can,\n  author = {Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {14948--14968},\n  title = {Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#llavait2tretrieval","title":"LLaVAIT2TRetrieval","text":"<p>Retrieve responses to answer questions about images.</p> <p>Dataset: <code>izhx/UMRB-LLaVA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) cv_recall_at_5 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin-etal-2024-preflmr,\n  address = {Bangkok, Thailand},\n  author = {Lin, Weizhe  and\nMei, Jingbiao  and\nChen, Jinghong  and\nByrne, Bill},\n  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2024.acl-long.289},\n  editor = {Ku, Lun-Wei  and\nMartins, Andre  and\nSrikumar, Vivek},\n  month = aug,\n  pages = {5294--5316},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}re{FLMR}: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers},\n  url = {https://aclanthology.org/2024.acl-long.289},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#meti2iretrieval","title":"METI2IRetrieval","text":"<p>Retrieve photos of more than 224k artworks.</p> <p>Dataset: <code>JamieSJS/met</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{ypsilantis2021met,\n  author = {Ypsilantis, Nikolaos-Antonios and Garcia, Noa and Han, Guangxing and Ibrahimi, Sarah and Van Noord, Nanne and Tolias, Giorgos},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {The met dataset: Instance-level recognition for artworks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#mscocoi2tretrieval","title":"MSCOCOI2TRetrieval","text":"<p>Retrieve captions based on images.</p> <p>Dataset: <code>MRBench/mbeir_mscoco_task3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin2014microsoft,\n  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n  booktitle = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n  organization = {Springer},\n  pages = {740--755},\n  title = {Microsoft coco: Common objects in context},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#mscocot2iretrieval","title":"MSCOCOT2IRetrieval","text":"<p>Retrieve images based on captions.</p> <p>Dataset: <code>MRBench/mbeir_mscoco_task0</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin2014microsoft,\n  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n  booktitle = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n  organization = {Springer},\n  pages = {740--755},\n  title = {Microsoft coco: Common objects in context},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#memotioni2tretrieval","title":"MemotionI2TRetrieval","text":"<p>Retrieve captions based on memes.</p> <p>Dataset: <code>Ahren09/MMSoc_Memotion</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{sharma2020semeval,\n  author = {Sharma, Chhavi and Bhageria, Deepesh and Scott, William and Pykl, Srinivas and Das, Amitava and Chakraborty, Tanmoy and Pulabaigari, Viswanath and Gamb{\\\"a}ck, Bj{\\\"o}rn},\n  booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},\n  pages = {759--773},\n  title = {SemEval-2020 Task 8: Memotion Analysis-the Visuo-Lingual Metaphor!},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#memotiont2iretrieval","title":"MemotionT2IRetrieval","text":"<p>Retrieve memes based on captions.</p> <p>Dataset: <code>Ahren09/MMSoc_Memotion</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{sharma2020semeval,\n  author = {Sharma, Chhavi and Bhageria, Deepesh and Scott, William and Pykl, Srinivas and Das, Amitava and Chakraborty, Tanmoy and Pulabaigari, Viswanath and Gamb{\\\"a}ck, Bj{\\\"o}rn},\n  booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},\n  pages = {759--773},\n  title = {SemEval-2020 Task 8: Memotion Analysis-the Visuo-Lingual Metaphor!},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#nightsi2iretrieval","title":"NIGHTSI2IRetrieval","text":"<p>Retrieval identical image to the given image.</p> <p>Dataset: <code>MRBench/mbeir_nights_task4</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@article{fu2024dreamsim,\n  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},\n  journal = {Advances in Neural Information Processing Systems},\n  title = {DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},\n  volume = {36},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#okvqait2tretrieval","title":"OKVQAIT2TRetrieval","text":"<p>Retrieval a Wiki passage to answer query about an image.</p> <p>Dataset: <code>izhx/UMRB-OKVQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) cv_recall_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{marino2019ok,\n  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},\n  booktitle = {Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},\n  pages = {3195--3204},\n  title = {Ok-vqa: A visual question answering benchmark requiring external knowledge},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#ovenit2itretrieval","title":"OVENIT2ITRetrieval","text":"<p>Retrieval a Wiki image and passage to answer query about an image.</p> <p>Dataset: <code>MRBench/mbeir_oven_task8</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{hu2023open,\n  author = {Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {12065--12075},\n  title = {Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#ovenit2tretrieval","title":"OVENIT2TRetrieval","text":"<p>Retrieval a Wiki passage to answer query about an image.</p> <p>Dataset: <code>MRBench/mbeir_oven_task6</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{hu2023open,\n  author = {Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {12065--12075},\n  title = {Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordeasyi2iretrieval","title":"ROxfordEasyI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>JamieSJS/r-oxford-easy-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordhardi2iretrieval","title":"ROxfordHardI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>JamieSJS/r-oxford-hard-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordmediumi2iretrieval","title":"ROxfordMediumI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>JamieSJS/r-oxford-medium-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rp2ki2iretrieval","title":"RP2kI2IRetrieval","text":"<p>Retrieve photos of 39457 products.</p> <p>Dataset: <code>JamieSJS/rp2k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Web derived created Citation <pre><code>@article{peng2020rp2k,\n  author = {Peng, Jingtian and Xiao, Chang and Li, Yifan},\n  journal = {arXiv preprint arXiv:2006.12634},\n  title = {RP2K: A large-scale retail product dataset for fine-grained image classification},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rpariseasyi2iretrieval","title":"RParisEasyI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>JamieSJS/r-paris-easy-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rparishardi2iretrieval","title":"RParisHardI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>JamieSJS/r-paris-hard-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rparismediumi2iretrieval","title":"RParisMediumI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>JamieSJS/r-paris-medium-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#remuqit2tretrieval","title":"ReMuQIT2TRetrieval","text":"<p>Retrieval of a Wiki passage to answer a query about an image.</p> <p>Dataset: <code>izhx/UMRB-ReMuQ</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) cv_recall_at_5 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{luo-etal-2023-end,\n  address = {Toronto, Canada},\n  author = {Luo, Man  and\nFang, Zhiyuan  and\nGokhale, Tejas  and\nYang, Yezhou  and\nBaral, Chitta},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2023.acl-long.478},\n  editor = {Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki},\n  month = jul,\n  pages = {8573--8589},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-end Knowledge Retrieval with Multi-modal Queries},\n  url = {https://aclanthology.org/2023.acl-long.478},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sopi2iretrieval","title":"SOPI2IRetrieval","text":"<p>Retrieve product photos of 22634 online products.</p> <p>Dataset: <code>JamieSJS/stanford-online-products</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{oh2016deep,\n  author = {Oh Song, Hyun and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {4004--4012},\n  title = {Deep metric learning via lifted structured feature embedding},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#scimmiri2tretrieval","title":"SciMMIRI2TRetrieval","text":"<p>Retrieve captions based on figures and tables.</p> <p>Dataset: <code>m-a-p/SciMMIR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Academic derived found Citation <pre><code>@article{wu2024scimmir,\n  author = {Wu, Siwei and Li, Yizhi and Zhu, Kang and Zhang, Ge and Liang, Yiming and Ma, Kaijing and Xiao, Chenghao and Zhang, Haoran and Yang, Bohao and Chen, Wenhu and others},\n  journal = {arXiv preprint arXiv:2401.13478},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#scimmirt2iretrieval","title":"SciMMIRT2IRetrieval","text":"<p>Retrieve figures and tables based on captions.</p> <p>Dataset: <code>m-a-p/SciMMIR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Academic derived found Citation <pre><code>@article{wu2024scimmir,\n  author = {Wu, Siwei and Li, Yizhi and Zhu, Kang and Zhang, Ge and Liang, Yiming and Ma, Kaijing and Xiao, Chenghao and Zhang, Haoran and Yang, Bohao and Chen, Wenhu and others},\n  journal = {arXiv preprint arXiv:2401.13478},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sketchyi2iretrieval","title":"SketchyI2IRetrieval","text":"<p>Retrieve photos from sketches.</p> <p>Dataset: <code>JamieSJS/sketchy</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{ypsilantis2021met,\n  author = {Ypsilantis, Nikolaos-Antonios and Garcia, Noa and Han, Guangxing and Ibrahimi, Sarah and Van Noord, Nanne and Tolias, Giorgos},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {The met dataset: Instance-level recognition for artworks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#stanfordcarsi2iretrieval","title":"StanfordCarsI2IRetrieval","text":"<p>Retrieve car images from 196 makes.</p> <p>Dataset: <code>isaacchung/stanford_cars_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cv_recall_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#tuberlint2iretrieval","title":"TUBerlinT2IRetrieval","text":"<p>Retrieve sketch images based on text descriptions.</p> <p>Dataset: <code>gowitheflow/tu-berlin</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{eitz2012humans,\n  author = {Eitz, Mathias and Hays, James and Alexa, Marc},\n  journal = {ACM Transactions on graphics (TOG)},\n  number = {4},\n  pages = {1--10},\n  publisher = {Acm New York, NY, USA},\n  title = {How do humans sketch objects?},\n  volume = {31},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#vqa2it2tretrieval","title":"VQA2IT2TRetrieval","text":"<p>Retrieve the correct answer for a question about an image.</p> <p>Dataset: <code>JamieSJS/vqa-2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Web derived found Citation <pre><code>@inproceedings{Goyal_2017_CVPR,\n  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#visualnewsi2tretrieval","title":"VisualNewsI2TRetrieval","text":"<p>Retrieval entity-rich captions for news images.</p> <p>Dataset: <code>MRBench/mbeir_visualnews_task3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{liu2021visual,\n  author = {Liu, Fuxiao and Wang, Yinghan and Wang, Tianlu and Ordonez, Vicente},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  pages = {6761--6771},\n  title = {Visual News: Benchmark and Challenges in News Image Captioning},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#visualnewst2iretrieval","title":"VisualNewsT2IRetrieval","text":"<p>Retrieve news images with captions.</p> <p>Dataset: <code>MRBench/mbeir_visualnews_task0</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{liu2021visual,\n  author = {Liu, Fuxiao and Wang, Yinghan and Wang, Tianlu and Ordonez, Vicente},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  pages = {6761--6771},\n  title = {Visual News: Benchmark and Challenges in News Image Captioning},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#vizwizit2tretrieval","title":"VizWizIT2TRetrieval","text":"<p>Retrieve the correct answer for a question about an image.</p> <p>Dataset: <code>JamieSJS/vizwiz</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Web derived found Citation <pre><code>@inproceedings{gurari2018vizwiz,\n  author = {Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {3608--3617},\n  title = {Vizwiz grand challenge: Answering visual questions from blind people},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#webqat2itretrieval","title":"WebQAT2ITRetrieval","text":"<p>Retrieve sources of information based on questions.</p> <p>Dataset: <code>MRBench/mbeir_webqa_task2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image, text (t2it) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{chang2022webqa,\n  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},\n  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages = {16495--16504},\n  title = {Webqa: Multihop and multimodal qa},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#webqat2tretrieval","title":"WebQAT2TRetrieval","text":"<p>Retrieve sources of information based on questions.</p> <p>Dataset: <code>MRBench/mbeir_webqa_task1</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{chang2022webqa,\n  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},\n  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages = {16495--16504},\n  title = {Webqa: Multihop and multimodal qa},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/","title":"BitextMining","text":"<ul> <li>Number of tasks: 30</li> </ul>"},{"location":"overview/available_tasks/bitextmining/#bucc","title":"BUCC","text":"<p>BUCC bitext mining dataset</p> <p>Dataset: <code>mteb/BUCC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 cmn, deu, eng, fra, rus Written human-annotated human-translated Citation <pre><code>@inproceedings{zweigenbaum-etal-2017-overview,\n  address = {Vancouver, Canada},\n  author = {Zweigenbaum, Pierre  and\nSharoff, Serge  and\nRapp, Reinhard},\n  booktitle = {Proceedings of the 10th Workshop on Building and Using Comparable Corpora},\n  doi = {10.18653/v1/W17-2512},\n  editor = {Sharoff, Serge  and\nZweigenbaum, Pierre  and\nRapp, Reinhard},\n  month = aug,\n  pages = {60--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora},\n  url = {https://aclanthology.org/W17-2512},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#buccv2","title":"BUCC.v2","text":"<p>BUCC bitext mining dataset</p> <p>Dataset: <code>mteb/bucc-bitext-mining</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 cmn, deu, eng, fra, rus Written human-annotated human-translated Citation <pre><code>@inproceedings{zweigenbaum-etal-2017-overview,\n  address = {Vancouver, Canada},\n  author = {Zweigenbaum, Pierre  and\nSharoff, Serge  and\nRapp, Reinhard},\n  booktitle = {Proceedings of the 10th Workshop on Building and Using Comparable Corpora},\n  doi = {10.18653/v1/W17-2512},\n  editor = {Sharoff, Serge  and\nZweigenbaum, Pierre  and\nRapp, Reinhard},\n  month = aug,\n  pages = {60--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora},\n  url = {https://aclanthology.org/W17-2512},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#biblenlpbitextmining","title":"BibleNLPBitextMining","text":"<p>Partial Bible translations in 829 languages, aligned by verse.</p> <p>Dataset: <code>davidstap/biblenlp-corpus-mmteb</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 aai, aak, aau, aaz, abt, ... (829) Religious, Written expert-annotated created Citation <pre><code>@article{akerman2023ebible,\n  author = {Akerman, Vesa and Baines, David and Daspit, Damien and Hermjakob, Ulf and Jang, Taeho and Leong, Colin and Martin, Michael and Mathew, Joel and Robie, Jonathan and Schwarting, Marcus},\n  journal = {arXiv preprint arXiv:2304.09919},\n  title = {The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#bornholmbitextmining","title":"BornholmBitextMining","text":"<p>Danish Bornholmsk Parallel Corpus. Bornholmsk is a Danish dialect spoken on the island of Bornholm, Denmark. Historically it is a part of east Danish which was also spoken in Scania and Halland, Sweden.</p> <p>Dataset: <code>mteb/BornholmBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 dan Fiction, Social, Web, Written expert-annotated created Citation <pre><code>@inproceedings{derczynskiBornholmskNaturalLanguage2019,\n  author = {Derczynski, Leon and Kjeldsen, Alex Speed},\n  booktitle = {Proceedings of the Nordic Conference of Computational Linguistics (2019)},\n  date = {2019},\n  file = {Available Version (via Google Scholar):/Users/au554730/Zotero/storage/FBQ73ZYN/Derczynski and Kjeldsen - 2019 - Bornholmsk natural language processing Resources .pdf:application/pdf},\n  pages = {338--344},\n  publisher = {Link\u00f6ping University Electronic Press},\n  shorttitle = {Bornholmsk natural language processing},\n  title = {Bornholmsk natural language processing: Resources and tools},\n  url = {https://pure.itu.dk/ws/files/84551091/W19_6138.pdf},\n  urldate = {2024-04-24},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#danishmedicinesagencybitextmining","title":"DanishMedicinesAgencyBitextMining","text":"<p>A Bilingual English-Danish parallel corpus from The Danish Medicines Agency.</p> <p>Dataset: <code>mteb/english-danish-parallel-corpus</code> \u2022 License: https://opendefinition.org/od/2.1/en/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 dan, eng Medical, Written human-annotated found Citation <pre><code>@misc{elrc_danish_medicines_agency_2018,\n  author = {Rozis, Roberts},\n  institution = {European Union},\n  license = {Open Under-PSI},\n  note = {Dataset created within the European Language Resource Coordination (ELRC) project under the Connecting Europe Facility - Automated Translation (CEF.AT) actions SMART 2014/1074 and SMART 2015/1091.},\n  title = {Bilingual English-Danish Parallel Corpus from the Danish Medicines Agency},\n  url = {https://sprogteknologi.dk/dataset/bilingual-english-danish-parallel-corpus-from-the-danish-medicines-agency},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#diablabitextmining","title":"DiaBlaBitextMining","text":"<p>English-French Parallel Corpus. DiaBLa is an English-French dataset for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue.</p> <p>Dataset: <code>mteb/DiaBlaBitextMining</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, fra Social, Written human-annotated created Citation <pre><code>@inproceedings{gonzalez2019diabla,\n  author = {Gonz\u00e1lez, Matilde and Garc\u00eda, Clara and S\u00e1nchez, Luc\u00eda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4192--4198},\n  title = {DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#floresbitextmining","title":"FloresBitextMining","text":"<p>FLORES is a benchmark dataset for machine translation between English and low-resource languages.</p> <p>Dataset: <code>mteb/FloresBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ace, acm, acq, aeb, afr, ... (196) Encyclopaedic, Non-fiction, Written human-annotated created Citation <pre><code>@inproceedings{goyal2022flores,\n  author = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm{\\'a}n, Francisco},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  pages = {19--35},\n  title = {The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#in22convbitextmining","title":"IN22ConvBitextMining","text":"<p>IN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.</p> <p>Dataset: <code>mteb/IN22ConvBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, ben, brx, doi, eng, ... (23) Fiction, Social, Spoken, Spoken expert-annotated created Citation <pre><code>@article{gala2023indictrans,\n  author = {Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\n  issn = {2835-8856},\n  journal = {Transactions on Machine Learning Research},\n  note = {},\n  title = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\n  url = {https://openreview.net/forum?id=vfT4YuzAYA},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#in22genbitextmining","title":"IN22GenBitextMining","text":"<p>IN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.</p> <p>Dataset: <code>mteb/IN22GenBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, ben, brx, doi, eng, ... (23) Government, Legal, News, Non-fiction, Religious, ... (7) expert-annotated created Citation <pre><code>@article{gala2023indictrans,\n  author = {Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\n  issn = {2835-8856},\n  journal = {Transactions on Machine Learning Research},\n  note = {},\n  title = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\n  url = {https://openreview.net/forum?id=vfT4YuzAYA},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#iwslt2017bitextmining","title":"IWSLT2017BitextMining","text":"<p>The IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT system across all directions including English, German, Dutch, Italian and Romanian.</p> <p>Dataset: <code>mteb/IWSLT2017BitextMining</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, cmn, deu, eng, fra, ... (10) Fiction, Non-fiction, Written expert-annotated found Citation <pre><code>@inproceedings{cettolo-etal-2017-overview,\n  address = {Tokyo, Japan},\n  author = {Cettolo, Mauro  and\nFederico, Marcello  and\nBentivogli, Luisa  and\nNiehues, Jan  and\nSt{\\\"u}ker, Sebastian  and\nSudoh, Katsuhito  and\nYoshino, Koichiro  and\nFedermann, Christian},\n  booktitle = {Proceedings of the 14th International Conference on Spoken Language Translation},\n  editor = {Sakti, Sakriani  and\nUtiyama, Masao},\n  month = dec # { 14-15},\n  pages = {2--14},\n  publisher = {International Workshop on Spoken Language Translation},\n  title = {Overview of the {IWSLT} 2017 Evaluation Campaign},\n  url = {https://aclanthology.org/2017.iwslt-1.1},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#indicgenbenchfloresbitextmining","title":"IndicGenBenchFloresBitextMining","text":"<p>Flores-IN dataset is an extension of Flores dataset released as a part of the IndicGenBench by Google</p> <p>Dataset: <code>mteb/IndicGenBenchFloresBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, awa, ben, bgc, bho, ... (30) News, Web, Written expert-annotated human-translated and localized Citation <pre><code>@misc{singh2024indicgenbench,\n  archiveprefix = {arXiv},\n  author = {Harman Singh and Nitish Gupta and Shikhar Bharadwaj and Dinesh Tewari and Partha Talukdar},\n  eprint = {2404.16816},\n  primaryclass = {cs.CL},\n  title = {IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#lincemtbitextmining","title":"LinceMTBitextMining","text":"<p>LinceMT is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.</p> <p>Dataset: <code>gentaiscool/bitext_lincemt_miners</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hin Social, Written human-annotated found Citation <pre><code>@inproceedings{aguilar2020lince,\n  author = {Aguilar, Gustavo and Kar, Sudipta and Solorio, Thamar},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  pages = {1803--1813},\n  title = {LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#ntrexbitextmining","title":"NTREXBitextMining","text":"<p>NTREX is a News Test References dataset for Machine Translation Evaluation, covering translation from English into 128 languages. We select language pairs according to the M2M-100 language grouping strategy, resulting in 1916 directions.</p> <p>Dataset: <code>mteb/NTREXBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 afr, amh, arb, aze, bak, ... (119) News, Written expert-annotated human-translated and localized Citation <pre><code>@inproceedings{federmann-etal-2022-ntrex,\n  address = {Online},\n  author = {Federmann, Christian and Kocmi, Tom and Xin, Ying},\n  booktitle = {Proceedings of the First Workshop on Scaling Up Multilingual Evaluation},\n  month = {nov},\n  pages = {21--24},\n  publisher = {Association for Computational Linguistics},\n  title = {{NTREX}-128 {--} News Test References for {MT} Evaluation of 128 Languages},\n  url = {https://aclanthology.org/2022.sumeval-1.4},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nollysentibitextmining","title":"NollySentiBitextMining","text":"<p>NollySenti is Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yoruba.</p> <p>Dataset: <code>gentaiscool/bitext_nollysenti_miners</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hau, ibo, pcm, yor Reviews, Social, Written human-annotated found Citation <pre><code>@inproceedings{shode2023nollysenti,\n  author = {Shode, Iyanuoluwa and Adelani, David Ifeoluwa and Peng, Jing and Feldman, Anna},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n  pages = {986--998},\n  title = {NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#norwegiancourtsbitextmining","title":"NorwegianCourtsBitextMining","text":"<p>Nynorsk and Bokm\u00e5l parallel corpus from Norwegian courts. Norwegian courts have two standardised written languages. Bokm\u00e5l is a variant closer to Danish, while Nynorsk was created to resemble regional dialects of Norwegian.</p> <p>Dataset: <code>kardosdrur/norwegian-courts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 nno, nob Legal, Written human-annotated found Citation <pre><code>@inproceedings{opus4,\n  author = {Tiedemann, J{\\\"o}rg and Thottingal, Santhosh},\n  booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation (EAMT)},\n  title = {OPUS-MT \u2014 Building open translation services for the World},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nusatranslationbitextmining","title":"NusaTranslationBitextMining","text":"<p>NusaTranslation is a parallel dataset for machine translation on 11 Indonesia languages and English.</p> <p>Dataset: <code>gentaiscool/bitext_nusatranslation_miners</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 abs, bbc, bew, bhp, ind, ... (12) Social, Written human-annotated created Citation <pre><code>@inproceedings{cahyawijaya2023nusawrites,\n  author = {Cahyawijaya, Samuel and Lovenia, Holy and Koto, Fajri and Adhista, Dea and Dave, Emmanuel and Oktavianti, Sarah and Akbar, Salsabil and Lee, Jhonson and Shadieq, Nuur and Cenggoro, Tjeng Wawan and others},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {921--945},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nusaxbitextmining","title":"NusaXBitextMining","text":"<p>NusaX is a parallel dataset for machine translation and sentiment analysis on 11 Indonesia languages and English.</p> <p>Dataset: <code>gentaiscool/bitext_nusax_miners</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ace, ban, bbc, bjn, bug, ... (12) Reviews, Written human-annotated created Citation <pre><code>@inproceedings{winata2023nusax,\n  author = {Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya, Samuel and Mahendra, Rahmad and Koto, Fajri and Romadhony, Ade and Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and Fung, Pascale and others},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  pages = {815--834},\n  title = {NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages},\n  year = {2023},\n}\n\n@misc{winata2024miners,\n  archiveprefix = {arXiv},\n  author = {Genta Indra Winata and Ruochen Zhang and David Ifeoluwa Adelani},\n  eprint = {2406.07424},\n  primaryclass = {cs.CL},\n  title = {MINERS: Multilingual Language Models as Semantic Retrievers},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#phincbitextmining","title":"PhincBitextMining","text":"<p>Phinc is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.</p> <p>Dataset: <code>gentaiscool/bitext_phinc_miners</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hin Social, Written human-annotated found Citation <pre><code>@inproceedings{srivastava2020phinc,\n  author = {Srivastava, Vivek and Singh, Mayank},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {41--49},\n  title = {PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#pubchemsmilesbitextmining","title":"PubChemSMILESBitextMining","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSMILESBitextMining</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#romatalesbitextmining","title":"RomaTalesBitextMining","text":"<p>Parallel corpus of Roma Tales in Lovari with Hungarian translations.</p> <p>Dataset: <code>kardosdrur/roma-tales</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 hun, rom Fiction, Written expert-annotated created"},{"location":"overview/available_tasks/bitextmining/#ruscibenchbitextmining","title":"RuSciBenchBitextMining","text":"<p>This task focuses on finding translations of scientific articles.         The dataset is sourced from eLibrary, Russia's largest electronic library of scientific publications.         Russian authors often provide English translations for their abstracts and titles,         and the data consists of these paired titles and abstracts. The task evaluates a model's ability         to match an article's Russian title and abstract to its English counterpart, or vice versa.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_bitext_mining</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#samsumfa","title":"SAMSumFa","text":"<p>Translated Version of SAMSum Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/samsum-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated machine-translated"},{"location":"overview/available_tasks/bitextmining/#srncorpusbitextmining","title":"SRNCorpusBitextMining","text":"<p>SRNCorpus is a machine translation corpus for creole language Sranantongo and Dutch.</p> <p>Dataset: <code>mteb/SRNCorpusBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 nld, srn Social, Web, Written human-annotated found Citation <pre><code>@article{zwennicker2022towards,\n  author = {Zwennicker, Just and Stap, David},\n  journal = {arXiv preprint arXiv:2212.06383},\n  title = {Towards a general purpose machine translation system for Sranantongo},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#synperchatbotragsumsretrieval","title":"SynPerChatbotRAGSumSRetrieval","text":"<p>Synthetic Persian Chatbot RAG Summary Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-summary-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#synperchatbotsumsretrieval","title":"SynPerChatbotSumSRetrieval","text":"<p>Synthetic Persian Chatbot Summary Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-summary-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#tatoeba","title":"Tatoeba","text":"<p>1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus</p> <p>Dataset: <code>mteb/tatoeba-bitext-mining</code> \u2022 License: cc-by-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 afr, amh, ang, ara, arq, ... (113) Written human-annotated found Citation <pre><code>@misc{tatoeba,\n  author = {Tatoeba community},\n  title = {Tatoeba: Collection of sentences and translations},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#tbilisicityhallbitextmining","title":"TbilisiCityHallBitextMining","text":"<p>Parallel news titles from the Tbilisi City Hall website (https://tbilisi.gov.ge/).</p> <p>Dataset: <code>jupyterjazz/tbilisi-city-hall-titles</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, kat News, Written derived created"},{"location":"overview/available_tasks/bitextmining/#viemedevbitextmining","title":"VieMedEVBitextMining","text":"<p>A high-quality Vietnamese-English parallel data from the medical domain for machine translation</p> <p>Dataset: <code>mteb/VieMedEVBitextMining</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, vie Medical, Written expert-annotated human-translated and localized Citation <pre><code>@inproceedings{medev,\n  author = {Nhu Vo and Dat Quoc Nguyen and Dung D. Le and Massimo Piccardi and Wray Buntine},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)},\n  title = {{Improving Vietnamese-English Medical Machine Translation}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#webfaqbitextminingqas","title":"WebFAQBitextMiningQAs","text":"<p>The WebFAQ Bitext Dataset consists of natural FAQ-style Question-Answer pairs that align across languages. A sentence in the \"WebFAQBitextMiningQAs\" task is a concatenation of a question and its corresponding answer. The dataset is sourced from FAQ pages on the web.</p> <p>Dataset: <code>PaDaS-Lab/webfaq-bitexts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, aze, ben, bul, cat, ... (49) Web, Written human-annotated human-translated Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#webfaqbitextminingquestions","title":"WebFAQBitextMiningQuestions","text":"<p>The WebFAQ Bitext Dataset consists of natural FAQ-style Question-Answer pairs that align across languages. A sentence in the \"WebFAQBitextMiningQuestions\" task is the question originating from an aligned QA. The dataset is sourced from FAQ pages on the web.</p> <p>Dataset: <code>PaDaS-Lab/webfaq-bitexts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, aze, ben, bul, cat, ... (49) Web, Written human-annotated human-translated Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/","title":"Classification","text":"<ul> <li>Number of tasks: 472</li> </ul>"},{"location":"overview/available_tasks/classification/#ajgt","title":"AJGT","text":"<p>Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.</p> <p>Dataset: <code>komari6/ajgt_twitter_ar</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{alomari2017arabic,\n  author = {Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\n  booktitle = {International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},\n  organization = {Springer},\n  pages = {602--610},\n  title = {Arabic tweets sentimental analysis using machine learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ajgtv2","title":"AJGT.v2","text":"<p>Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets (900 for training and 900 for testing) annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ajgt</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{alomari2017arabic,\n  author = {Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\n  booktitle = {International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},\n  organization = {Springer},\n  pages = {602--610},\n  title = {Arabic tweets sentimental analysis using machine learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#afrisenticlassification","title":"AfriSentiClassification","text":"<p>AfriSenti is the largest sentiment analysis dataset for under-represented African languages.</p> <p>Dataset: <code>mteb/AfriSentiClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, arq, ary, hau, ibo, ... (12) Social, Written derived found Citation <pre><code>@inproceedings{Muhammad2023AfriSentiAT,\n  author = {Shamsuddeen Hassan Muhammad and Idris Abdulmumin and Abinew Ali Ayele and Nedjma Ousidhoum and David Ifeoluwa Adelani and Seid Muhie Yimam and Ibrahim Sa'id Ahmad and Meriem Beloucif and Saif Mohammad and Sebastian Ruder and Oumaima Hourrane and Pavel Brazdil and Felermino D'ario M'ario Ant'onio Ali and Davis Davis and Salomey Osei and Bello Shehu Bello and Falalu Ibrahim and Tajuddeen Gwadabe and Samuel Rutunda and Tadesse Belay and Wendimu Baye Messelle and Hailu Beshada Balcha and Sisay Adugna Chala and Hagos Tesfahun Gebremichael and Bernard Opoku and Steven Arthur},\n  title = {AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#afrisentilangclassification","title":"AfriSentiLangClassification","text":"<p>AfriSentiLID is the largest LID classification dataset for African Languages.</p> <p>Dataset: <code>HausaNLP/afrisenti-lid-data</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, arq, ary, hau, ibo, ... (12) Social, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#allegroreviews","title":"AllegroReviews","text":"<p>A Polish dataset for sentiment classification on reviews from e-commerce marketplace Allegro.</p> <p>Dataset: <code>PL-MTEB/allegro-reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Reviews derived found Citation <pre><code>@inproceedings{rybak-etal-2020-klej,\n  address = {Online},\n  author = {Rybak, Piotr  and\nMroczkowski, Robert  and\nTracz, Janusz  and\nGawlik, Ireneusz},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.111},\n  editor = {Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel},\n  month = jul,\n  pages = {1191--1201},\n  publisher = {Association for Computational Linguistics},\n  title = {{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding},\n  url = {https://aclanthology.org/2020.acl-main.111/},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#allegroreviewsv2","title":"AllegroReviews.v2","text":"<p>A Polish dataset for sentiment classification on reviews from e-commerce marketplace Allegro.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/allegro_reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Reviews derived found Citation <pre><code>@inproceedings{rybak-etal-2020-klej,\n  address = {Online},\n  author = {Rybak, Piotr  and\nMroczkowski, Robert  and\nTracz, Janusz  and\nGawlik, Ireneusz},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.111},\n  editor = {Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel},\n  month = jul,\n  pages = {1191--1201},\n  publisher = {Association for Computational Linguistics},\n  title = {{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding},\n  url = {https://aclanthology.org/2020.acl-main.111/},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazoncounterfactualclassification","title":"AmazonCounterfactualClassification","text":"<p>A collection of Amazon customer reviews annotated for counterfactual detection pair classification.</p> <p>Dataset: <code>mteb/amazon_counterfactual</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, jpn Reviews, Written human-annotated found Citation <pre><code>@inproceedings{oneill-etal-2021-wish,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {O{'}Neill, James  and\nRozenshtein, Polina  and\nKiryo, Ryuichi  and\nKubota, Motoko  and\nBollegala, Danushka},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.568},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {7092--7108},\n  publisher = {Association for Computational Linguistics},\n  title = {{I} Wish {I} Would Have Loved This One, But {I} Didn{'}t {--} A Multilingual Dataset for Counterfactual Detection in Product Review},\n  url = {https://aclanthology.org/2021.emnlp-main.568},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazoncounterfactualvnclassification","title":"AmazonCounterfactualVNClassification","text":"<p>A collection of translated Amazon customer reviews annotated for counterfactual detection pair classification.         The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:         - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.         - Applies advanced embedding models to filter the translations.         - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-counterfactual-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityclassification","title":"AmazonPolarityClassification","text":"<p>Amazon Polarity Classification Dataset.</p> <p>Dataset: <code>mteb/amazon_polarity</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@article{McAuley2013HiddenFA,\n  author = {Julian McAuley and Jure Leskovec},\n  journal = {Proceedings of the 7th ACM conference on Recommender systems},\n  title = {Hidden factors and hidden topics: understanding rating dimensions with review text},\n  url = {https://api.semanticscholar.org/CorpusID:6440341},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityclassificationv2","title":"AmazonPolarityClassification.v2","text":"<p>Amazon Polarity Classification Dataset.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/amazon_polarity</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@article{McAuley2013HiddenFA,\n  author = {Julian McAuley and Jure Leskovec},\n  journal = {Proceedings of the 7th ACM conference on Recommender systems},\n  title = {Hidden factors and hidden topics: understanding rating dimensions with review text},\n  url = {https://api.semanticscholar.org/CorpusID:6440341},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityvnclassification","title":"AmazonPolarityVNClassification","text":"<p>A collection of translated Amazon customer reviews annotated for polarity classification.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-polarity-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonreviewsclassification","title":"AmazonReviewsClassification","text":"<p>A collection of Amazon reviews specifically designed to aid research in multilingual text classification.</p> <p>Dataset: <code>mteb/AmazonReviewsClassification</code> \u2022 License: https://docs.opendata.aws/amazon-reviews-ml/license.txt \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn, deu, eng, fra, jpn, ... (6) Reviews, Written human-annotated found Citation <pre><code>@misc{keung2020multilingual,\n  archiveprefix = {arXiv},\n  author = {Phillip Keung and Yichao Lu and Gy\u00f6rgy Szarvas and Noah A. Smith},\n  eprint = {2010.02573},\n  primaryclass = {cs.CL},\n  title = {The Multilingual Amazon Reviews Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonreviewsvnclassification","title":"AmazonReviewsVNClassification","text":"<p>A collection of translated Amazon reviews specifically designed to aid research in multilingual text classification.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-reviews-multi-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#angrytweetsclassification","title":"AngryTweetsClassification","text":"<p>A sentiment dataset with 3 classes (positive, negative, neutral) for Danish tweets</p> <p>Dataset: <code>DDSC/angry-tweets</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written human-annotated found Citation <pre><code>@inproceedings{pauli2021danlp,\n  author = {Pauli, Amalie Brogaard and Barrett, Maria and Lacroix, Oph{\\'e}lie and Hvingelby, Rasmus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  pages = {460--466},\n  title = {DaNLP: An open-source toolkit for Danish Natural Language Processing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#angrytweetsclassificationv2","title":"AngryTweetsClassification.v2","text":"<p>A sentiment dataset with 3 classes (positive, negative, neutral) for Danish tweets         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/angry_tweets</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written human-annotated found Citation <pre><code>@inproceedings{pauli2021danlp,\n  author = {Pauli, Amalie Brogaard and Barrett, Maria and Lacroix, Oph{\\'e}lie and Hvingelby, Rasmus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  pages = {460--466},\n  title = {DaNLP: An open-source toolkit for Danish Natural Language Processing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#arxivclassification","title":"ArxivClassification","text":"<p>Classification Dataset of Arxiv Papers</p> <p>Dataset: <code>mteb/ArxivClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Academic, Written derived found Citation <pre><code>@article{8675939,\n  author = {He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},\n  doi = {10.1109/ACCESS.2019.2907992},\n  journal = {IEEE Access},\n  number = {},\n  pages = {40707-40718},\n  title = {Long Document Classification From Local Word Glimpses via Recurrent Attention Learning},\n  volume = {7},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#arxivclassificationv2","title":"ArxivClassification.v2","text":"<p>Classification Dataset of Arxiv Papers         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/arxiv</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Academic, Written derived found Citation <pre><code>@article{8675939,\n  author = {He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},\n  doi = {10.1109/ACCESS.2019.2907992},\n  journal = {IEEE Access},\n  number = {},\n  pages = {40707-40718},\n  title = {Long Document Classification From Local Word Glimpses via Recurrent Attention Learning},\n  volume = {7},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77classification","title":"Banking77Classification","text":"<p>Dataset composed of online banking queries annotated with their corresponding intents.</p> <p>Dataset: <code>mteb/banking77</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Written human-annotated found Citation <pre><code>@inproceedings{casanueva-etal-2020-efficient,\n  address = {Online},\n  author = {Casanueva, I{\\~n}igo  and\nTem{\\v{c}}inas, Tadas  and\nGerz, Daniela  and\nHenderson, Matthew  and\nVuli{\\'c}, Ivan},\n  booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},\n  doi = {10.18653/v1/2020.nlp4convai-1.5},\n  editor = {Wen, Tsung-Hsien  and\nCelikyilmaz, Asli  and\nYu, Zhou  and\nPapangelis, Alexandros  and\nEric, Mihail  and\nKumar, Anuj  and\nCasanueva, I{\\~n}igo  and\nShah, Rushin},\n  month = jul,\n  pages = {38--45},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Intent Detection with Dual Sentence Encoders},\n  url = {https://aclanthology.org/2020.nlp4convai-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77classificationv2","title":"Banking77Classification.v2","text":"<p>Dataset composed of online banking queries annotated with their corresponding intents.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/banking77</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Written human-annotated found Citation <pre><code>@inproceedings{casanueva-etal-2020-efficient,\n  address = {Online},\n  author = {Casanueva, I{\\~n}igo  and\nTem{\\v{c}}inas, Tadas  and\nGerz, Daniela  and\nHenderson, Matthew  and\nVuli{\\'c}, Ivan},\n  booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},\n  doi = {10.18653/v1/2020.nlp4convai-1.5},\n  editor = {Wen, Tsung-Hsien  and\nCelikyilmaz, Asli  and\nYu, Zhou  and\nPapangelis, Alexandros  and\nEric, Mihail  and\nKumar, Anuj  and\nCasanueva, I{\\~n}igo  and\nShah, Rushin},\n  month = jul,\n  pages = {38--45},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Intent Detection with Dual Sentence Encoders},\n  url = {https://aclanthology.org/2020.nlp4convai-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77vnclassification","title":"Banking77VNClassification","text":"<p>A translated dataset composed of online banking queries annotated with their corresponding intents.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/banking77-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalidocumentclassification","title":"BengaliDocumentClassification","text":"<p>Dataset for News Classification, categorized with 13 domains.</p> <p>Dataset: <code>dialect-ai/shironaam</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ben News, Written derived found Citation <pre><code>@inproceedings{akash-etal-2023-shironaam,\n  address = {Dubrovnik, Croatia},\n  author = {Akash, Abu Ubaida  and\nNayeem, Mir Tafseer  and\nShohan, Faisal Tareque  and\nIslam, Tanvir},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {52--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Shironaam: {B}engali News Headline Generation using Auxiliary Information},\n  url = {https://aclanthology.org/2023.eacl-main.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalidocumentclassificationv2","title":"BengaliDocumentClassification.v2","text":"<p>Dataset for News Classification, categorized with 13 domains.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_document</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ben News, Written derived found Citation <pre><code>@inproceedings{akash-etal-2023-shironaam,\n  address = {Dubrovnik, Croatia},\n  author = {Akash, Abu Ubaida  and\nNayeem, Mir Tafseer  and\nShohan, Faisal Tareque  and\nIslam, Tanvir},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {52--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Shironaam: {B}engali News Headline Generation using Auxiliary Information},\n  url = {https://aclanthology.org/2023.eacl-main.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalihatespeechclassification","title":"BengaliHateSpeechClassification","text":"<p>The Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text.</p> <p>Dataset: <code>rezacsedu/bn_hate_speech</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben News, Written expert-annotated found Citation <pre><code>@inproceedings{karim2020BengaliNLP,\n  author = {Karim, Md. Rezaul and Chakravarti, Bharathi Raja and P. McCrae, John and Cochez, Michael},\n  booktitle = {7th IEEE International Conference on Data Science and Advanced Analytics (IEEE DSAA,2020)},\n  publisher = {IEEE},\n  title = {Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalihatespeechclassificationv2","title":"BengaliHateSpeechClassification.v2","text":"<p>The Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_hate_speech</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben News, Written expert-annotated found Citation <pre><code>@inproceedings{karim2020BengaliNLP,\n  author = {Karim, Md. Rezaul and Chakravarti, Bharathi Raja and P. McCrae, John and Cochez, Michael},\n  booktitle = {7th IEEE International Conference on Data Science and Advanced Analytics (IEEE DSAA,2020)},\n  publisher = {IEEE},\n  title = {Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalisentimentanalysis","title":"BengaliSentimentAnalysis","text":"<p>dataset contains 3307 Negative reviews and 8500 Positive reviews collected and manually annotated from Youtube Bengali drama.</p> <p>Dataset: <code>Akash190104/bengali_sentiment_analysis</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben Reviews, Written human-annotated found Citation <pre><code>@inproceedings{sazzed2020cross,\n  author = {Sazzed, Salim},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {50--60},\n  title = {Cross-lingual sentiment classification in low-resource Bengali language},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalisentimentanalysisv2","title":"BengaliSentimentAnalysis.v2","text":"<p>dataset contains 2854 Negative reviews and 7238 Positive reviews collected and manually annotated from Youtube Bengali drama.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_sentiment_analysis</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben Reviews, Written human-annotated found Citation <pre><code>@inproceedings{sazzed2020cross,\n  author = {Sazzed, Salim},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {50--60},\n  title = {Cross-lingual sentiment classification in low-resource Bengali language},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bulgarianstorereviewsentimentclassfication","title":"BulgarianStoreReviewSentimentClassfication","text":"<p>Bulgarian online store review dataset for sentiment classification.</p> <p>Dataset: <code>artist/Bulgarian-Online-Store-Feedback-Text-Analysis</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bul Reviews, Written human-annotated found Citation <pre><code>@data{DVN/TXIK9P_2018,\n  author = {Georgieva-Trifonova, Tsvetanka and Stefanova, Milena and Kalchev, Stefan},\n  doi = {10.7910/DVN/TXIK9P},\n  publisher = {Harvard Dataverse},\n  title = {{Dataset for ``Customer Feedback Text Analysis for Online Stores Reviews in Bulgarian''}},\n  url = {https://doi.org/10.7910/DVN/TXIK9P},\n  version = {V1},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cbd","title":"CBD","text":"<p>Polish Tweets annotated for cyberbullying detection.</p> <p>Dataset: <code>PL-MTEB/cbd</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written human-annotated found Citation <pre><code>@proceedings{ogr:kob:19:poleval,\n  address = {Warsaw, Poland},\n  editor = {Maciej Ogrodniczuk and \u0141ukasz Kobyli\u0144ski},\n  isbn = {978-83-63159-28-3},\n  publisher = {Institute of Computer Science, Polish Academy of Sciences},\n  title = {{Proceedings of the PolEval 2019 Workshop}},\n  url = {http://2019.poleval.pl/files/poleval2019.pdf},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cbdv2","title":"CBD.v2","text":"<p>Polish Tweets annotated for cyberbullying detection.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/cbd</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written human-annotated found Citation <pre><code>@proceedings{ogr:kob:19:poleval,\n  address = {Warsaw, Poland},\n  editor = {Maciej Ogrodniczuk and \u0141ukasz Kobyli\u0144ski},\n  isbn = {978-83-63159-28-3},\n  publisher = {Institute of Computer Science, Polish Academy of Sciences},\n  title = {{Proceedings of the PolEval 2019 Workshop}},\n  url = {http://2019.poleval.pl/files/poleval2019.pdf},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdczmoviereviewsentimentclassification","title":"CSFDCZMovieReviewSentimentClassification","text":"<p>The dataset contains 30k user reviews from csfd.cz in Czech.</p> <p>Dataset: <code>fewshot-goes-multilingual/cs_csfd-movie-reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdczmoviereviewsentimentclassificationv2","title":"CSFDCZMovieReviewSentimentClassification.v2","text":"<p>The dataset contains 30k user reviews from csfd.cz in Czech.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/csfdcz_movie_review_sentiment</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdskmoviereviewsentimentclassification","title":"CSFDSKMovieReviewSentimentClassification","text":"<p>The dataset contains 30k user reviews from csfd.cz in Slovak.</p> <p>Dataset: <code>fewshot-goes-multilingual/sk_csfd-movie-reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdskmoviereviewsentimentclassificationv2","title":"CSFDSKMovieReviewSentimentClassification.v2","text":"<p>The dataset contains 30k user reviews from csfd.cz in Slovak.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/csfdsk_movie_review_sentiment</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadaffiliatelicenselicenseelegalbenchclassification","title":"CUADAffiliateLicenseLicenseeLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if a clause describes a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor.</p> <p>Dataset: <code>mteb/CUADAffiliateLicenseLicenseeLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadaffiliatelicenselicensorlegalbenchclassification","title":"CUADAffiliateLicenseLicensorLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause describes a license grant by affiliates of the licensor or that includes intellectual property of affiliates of the licensor.</p> <p>Dataset: <code>mteb/CUADAffiliateLicenseLicensorLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadantiassignmentlegalbenchclassification","title":"CUADAntiAssignmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires consent or notice of a party if the contract is assigned to a third party.</p> <p>Dataset: <code>mteb/CUADAntiAssignmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadauditrightslegalbenchclassification","title":"CUADAuditRightsLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause gives a party the right to audit the books, records, or physical locations of the counterparty to ensure compliance with the contract.</p> <p>Dataset: <code>mteb/CUADAuditRightsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcaponliabilitylegalbenchclassification","title":"CUADCapOnLiabilityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a cap on liability upon the breach of a party's obligation. This includes time limitation for the counterparty to bring claims or maximum amount for recovery.</p> <p>Dataset: <code>mteb/CUADCapOnLiabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadchangeofcontrollegalbenchclassification","title":"CUADChangeOfControlLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause gives one party the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law.</p> <p>Dataset: <code>mteb/CUADChangeOfControlLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcompetitiverestrictionexceptionlegalbenchclassification","title":"CUADCompetitiveRestrictionExceptionLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause mentions exceptions or carveouts to Non-Compete, Exclusivity and No-Solicit of Customers.</p> <p>Dataset: <code>mteb/CUADCompetitiveRestrictionExceptionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcovenantnottosuelegalbenchclassification","title":"CUADCovenantNotToSueLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party is restricted from contesting the validity of the counterparty's ownership of intellectual property or otherwise bringing a claim against the counterparty for matters unrelated to the contract.</p> <p>Dataset: <code>mteb/CUADCovenantNotToSueLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadeffectivedatelegalbenchclassification","title":"CUADEffectiveDateLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the agreement becomes effective.</p> <p>Dataset: <code>mteb/CUADEffectiveDateLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadexclusivitylegalbenchclassification","title":"CUADExclusivityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies exclusive dealing commitment with the counterparty. This includes a commitment to procure all 'requirements' from one party of certain technology, goods, or services or a prohibition on licensing or selling technology, goods or services to third parties, or a prohibition on collaborating or working with other parties), whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADExclusivityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadexpirationdatelegalbenchclassification","title":"CUADExpirationDateLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the initial term expires.</p> <p>Dataset: <code>mteb/CUADExpirationDateLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadgoverninglawlegalbenchclassification","title":"CUADGoverningLawLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies which state/country\u2019s law governs the contract.</p> <p>Dataset: <code>mteb/CUADGoverningLawLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadipownershipassignmentlegalbenchclassification","title":"CUADIPOwnershipAssignmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that intellectual property created by one party become the property of the counterparty, either per the terms of the contract or upon the occurrence of certain events.</p> <p>Dataset: <code>mteb/CUADIPOwnershipAssignmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadinsurancelegalbenchclassification","title":"CUADInsuranceLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if clause creates a requirement for insurance that must be maintained by one party for the benefit of the counterparty.</p> <p>Dataset: <code>mteb/CUADInsuranceLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadirrevocableorperpetuallicenselegalbenchclassification","title":"CUADIrrevocableOrPerpetualLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a license grant that is irrevocable or perpetual.</p> <p>Dataset: <code>mteb/CUADIrrevocableOrPerpetualLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadjointipownershiplegalbenchclassification","title":"CUADJointIPOwnershipLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause provides for joint or shared ownership of intellectual property between the parties to the contract.</p> <p>Dataset: <code>mteb/CUADJointIPOwnershipLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadlicensegrantlegalbenchclassification","title":"CUADLicenseGrantLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause contains a license granted by one party to its counterparty.</p> <p>Dataset: <code>mteb/CUADLicenseGrantLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadliquidateddamageslegalbenchclassification","title":"CUADLiquidatedDamagesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause awards either party liquidated damages for breach or a fee upon the termination of a contract (termination fee).</p> <p>Dataset: <code>mteb/CUADLiquidatedDamagesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadminimumcommitmentlegalbenchclassification","title":"CUADMinimumCommitmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a minimum order size or minimum amount or units per time period that one party must buy from the counterparty.</p> <p>Dataset: <code>mteb/CUADMinimumCommitmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadmostfavorednationlegalbenchclassification","title":"CUADMostFavoredNationLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if a third party gets better terms on the licensing or sale of technology/goods/services described in the contract, the buyer of such technology/goods/services under the contract shall be entitled to those better terms.</p> <p>Dataset: <code>mteb/CUADMostFavoredNationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnosolicitofcustomerslegalbenchclassification","title":"CUADNoSolicitOfCustomersLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party from contracting or soliciting customers or partners of the counterparty, whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADNoSolicitOfCustomersLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnosolicitofemployeeslegalbenchclassification","title":"CUADNoSolicitOfEmployeesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party's soliciting or hiring employees and/or contractors from the counterparty, whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADNoSolicitOfEmployeesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnoncompetelegalbenchclassification","title":"CUADNonCompeteLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts the ability of a party to compete with the counterparty or operate in a certain geography or business or technology sector.</p> <p>Dataset: <code>mteb/CUADNonCompeteLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnondisparagementlegalbenchclassification","title":"CUADNonDisparagementLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires a party not to disparage the counterparty.</p> <p>Dataset: <code>mteb/CUADNonDisparagementLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnontransferablelicenselegalbenchclassification","title":"CUADNonTransferableLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause limits the ability of a party to transfer the license being granted to a third party.</p> <p>Dataset: <code>mteb/CUADNonTransferableLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnoticeperiodtoterminaterenewallegalbenchclassification","title":"CUADNoticePeriodToTerminateRenewalLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a notice period required to terminate renewal.</p> <p>Dataset: <code>mteb/CUADNoticePeriodToTerminateRenewalLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadpostterminationserviceslegalbenchclassification","title":"CUADPostTerminationServicesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause subjects a party to obligations after the termination or expiration of a contract, including any post-termination transition, payment, transfer of IP, wind-down, last-buy, or similar commitments.</p> <p>Dataset: <code>mteb/CUADPostTerminationServicesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadpricerestrictionslegalbenchclassification","title":"CUADPriceRestrictionsLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause places a restriction on the ability of a party to raise or reduce prices of technology, goods, or services provided.</p> <p>Dataset: <code>mteb/CUADPriceRestrictionsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrenewaltermlegalbenchclassification","title":"CUADRenewalTermLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a renewal term.</p> <p>Dataset: <code>mteb/CUADRenewalTermLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrevenueprofitsharinglegalbenchclassification","title":"CUADRevenueProfitSharingLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause require a party to share revenue or profit with the counterparty for any technology, goods, or services.</p> <p>Dataset: <code>mteb/CUADRevenueProfitSharingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrofrroforofnlegalbenchclassification","title":"CUADRofrRofoRofnLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause grant one party a right of first refusal, right of first offer or right of first negotiation to purchase, license, market, or distribute equity interest, technology, assets, products or services.</p> <p>Dataset: <code>mteb/CUADRofrRofoRofnLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadsourcecodeescrowlegalbenchclassification","title":"CUADSourceCodeEscrowLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires one party to deposit its source code into escrow with a third party, which can be released to the counterparty upon the occurrence of certain events (bankruptcy, insolvency, etc.).</p> <p>Dataset: <code>mteb/CUADSourceCodeEscrowLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadterminationforconveniencelegalbenchclassification","title":"CUADTerminationForConvenienceLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that one party can terminate this contract without cause (solely by giving a notice and allowing a waiting period to expire).</p> <p>Dataset: <code>mteb/CUADTerminationForConvenienceLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadthirdpartybeneficiarylegalbenchclassification","title":"CUADThirdPartyBeneficiaryLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that that there a non-contracting party who is a beneficiary to some or all of the clauses in the contract and therefore can enforce its rights against a contracting party.</p> <p>Dataset: <code>mteb/CUADThirdPartyBeneficiaryLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuaduncappedliabilitylegalbenchclassification","title":"CUADUncappedLiabilityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party's liability is uncapped upon the breach of its obligation in the contract. This also includes uncap liability for a particular type of breach such as IP infringement or breach of confidentiality obligation.</p> <p>Dataset: <code>mteb/CUADUncappedLiabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadunlimitedallyoucaneatlicenselegalbenchclassification","title":"CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause grants one party an \u201centerprise,\u201d \u201call you can eat\u201d or unlimited usage license.</p> <p>Dataset: <code>mteb/CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadvolumerestrictionlegalbenchclassification","title":"CUADVolumeRestrictionLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a fee increase or consent requirement, etc. if one party's use of the product/services exceeds certain threshold.</p> <p>Dataset: <code>mteb/CUADVolumeRestrictionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadwarrantydurationlegalbenchclassification","title":"CUADWarrantyDurationLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a duration of any warranty against defects or errors in technology, products, or services provided under the contract.</p> <p>Dataset: <code>mteb/CUADWarrantyDurationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#canadataxcourtoutcomeslegalbenchclassification","title":"CanadaTaxCourtOutcomesLegalBenchClassification","text":"<p>The input is an excerpt of text from Tax Court of Canada decisions involving appeals of tax related matters. The task is to classify whether the excerpt includes the outcome of the appeal, and if so, to specify whether the appeal was allowed or dismissed. Partial success (e.g. appeal granted on one tax year but dismissed on another) counts as allowed (with the exception of costs orders which are disregarded). Where the excerpt does not clearly articulate an outcome, the system should indicate other as the outcome. Categorizing case outcomes is a common task that legal researchers complete in order to gather datasets involving outcomes in legal processes for the purposes of quantitative empirical legal research.</p> <p>Dataset: <code>mteb/CanadaTaxCourtOutcomesLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cataloniatweetclassification","title":"CataloniaTweetClassification","text":"<p>This dataset contains two corpora in Spanish and Catalan that consist of annotated Twitter         messages for automatic stance detection. The data was collected over 12 days during February and March         of 2019 from tweets posted in Barcelona, and during September of 2018 from tweets posted in the town of Terrassa, Catalonia.         Each corpus is annotated with three classes: AGAINST, FAVOR and NEUTRAL, which express the stance         towards the target - independence of Catalonia.</p> <p>Dataset: <code>community-datasets/catalonia_independence</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cat, spa Government, Social, Written expert-annotated created Citation <pre><code>@inproceedings{zotova-etal-2020-multilingual,\n  author = {Zotova, Elena  and\nAgerri, Rodrigo  and\nNu{\\~n}ez, Manuel  and\nRigau, German},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  month = may,\n  pages = {1368--1375},\n  publisher = {European Language Resources Association},\n  title = {Multilingual Stance Detection in Tweets: The {C}atalonia Independence Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliconfidentialityofagreementlegalbenchclassification","title":"ContractNLIConfidentialityOfAgreementLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA provides that the Receiving Party shall not disclose the fact that Agreement was agreed or negotiated.</p> <p>Dataset: <code>mteb/ContractNLIConfidentialityOfAgreementLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliexplicitidentificationlegalbenchclassification","title":"ContractNLIExplicitIdentificationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that all Confidential Information shall be expressly identified by the Disclosing Party.</p> <p>Dataset: <code>mteb/ContractNLIExplicitIdentificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliinclusionofverballyconveyedinformationlegalbenchclassification","title":"ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that Confidential Information may include verbally conveyed information.</p> <p>Dataset: <code>mteb/ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlilimiteduselegalbenchclassification","title":"ContractNLILimitedUseLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall not use any Confidential Information for any purpose other than the purposes stated in Agreement.</p> <p>Dataset: <code>mteb/ContractNLILimitedUseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlinolicensinglegalbenchclassification","title":"ContractNLINoLicensingLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Agreement shall not grant Receiving Party any right to Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLINoLicensingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlinoticeoncompelleddisclosurelegalbenchclassification","title":"ContractNLINoticeOnCompelledDisclosureLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall notify Disclosing Party in case Receiving Party is required by law, regulation or judicial process to disclose any Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLINoticeOnCompelledDisclosureLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissibleacquirementofsimilarinformationlegalbenchclassification","title":"ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may acquire information similar to Confidential Information from a third party.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissiblecopylegalbenchclassification","title":"ContractNLIPermissibleCopyLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may create a copy of some Confidential Information in some circumstances.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleCopyLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissibledevelopmentofsimilarinformationlegalbenchclassification","title":"ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may independently develop information similar to Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissiblepostagreementpossessionlegalbenchclassification","title":"ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may retain some Confidential Information even after the return or destruction of Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlireturnofconfidentialinformationlegalbenchclassification","title":"ContractNLIReturnOfConfidentialInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.</p> <p>Dataset: <code>mteb/ContractNLIReturnOfConfidentialInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisharingwithemployeeslegalbenchclassification","title":"ContractNLISharingWithEmployeesLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some of Receiving Party's employees.</p> <p>Dataset: <code>mteb/ContractNLISharingWithEmployeesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisharingwiththirdpartieslegalbenchclassification","title":"ContractNLISharingWithThirdPartiesLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some third-parties (including consultants, agents and professional advisors).</p> <p>Dataset: <code>mteb/ContractNLISharingWithThirdPartiesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisurvivalofobligationslegalbenchclassification","title":"ContractNLISurvivalOfObligationsLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that some obligations of Agreement may survive termination of Agreement.</p> <p>Dataset: <code>mteb/ContractNLISurvivalOfObligationsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#corporatelobbyinglegalbenchclassification","title":"CorporateLobbyingLegalBenchClassification","text":"<p>The Corporate Lobbying task consists of determining whether a proposed Congressional bill may be relevant to a company based on a company's self-description in its SEC 10K filing.</p> <p>Dataset: <code>mteb/CorporateLobbyingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cyrillicturkiclangclassification","title":"CyrillicTurkicLangClassification","text":"<p>Cyrillic dataset of 8 Turkic languages spoken in Russia and former USSR</p> <p>Dataset: <code>tatiana-merz/cyrillic_turkic_langs</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bak, chv, kaz, kir, krc, ... (9) Web, Written derived found Citation <pre><code>@inproceedings{goldhahn2012building,\n  author = {Goldhahn, Dirk and Eckart, Thomas and Quasthoff, Uwe},\n  booktitle = {Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)},\n  title = {Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechproductreviewsentimentclassification","title":"CzechProductReviewSentimentClassification","text":"<p>User reviews of products on Czech e-shop Mall.cz with 3 sentiment classes (positive, neutral, negative)</p> <p>Dataset: <code>fewshot-goes-multilingual/cs_mall-product-reviews</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechproductreviewsentimentclassificationv2","title":"CzechProductReviewSentimentClassification.v2","text":"<p>User reviews of products on Czech e-shop Mall.cz with 3 sentiment classes (positive, neutral, negative)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/czech_product_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsomesentimentclassification","title":"CzechSoMeSentimentClassification","text":"<p>User comments on Facebook</p> <p>Dataset: <code>fewshot-goes-multilingual/cs_facebook-comments</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsomesentimentclassificationv2","title":"CzechSoMeSentimentClassification.v2","text":"<p>User comments on Facebook         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/czech_so_me_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsubjectivityclassification","title":"CzechSubjectivityClassification","text":"<p>An Czech dataset for subjectivity classification.</p> <p>Dataset: <code>pauli31/czech-subjectivity-dataset</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written human-annotated found Citation <pre><code>@inproceedings{priban-steinberger-2022-czech,\n  address = {Marseille, France},\n  author = {P{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {1381--1391},\n  publisher = {European Language Resources Association},\n  title = {\\{C\\}zech Dataset for Cross-lingual Subjectivity Classification},\n  url = {https://aclanthology.org/2022.lrec-1.148},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dbpediaclassification","title":"DBpediaClassification","text":"<p>DBpedia14 is a dataset of English texts from Wikipedia articles, categorized into 14 non-overlapping classes based on their DBpedia ontology.</p> <p>Dataset: <code>fancyzhx/dbpedia_14</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dbpediaclassificationv2","title":"DBpediaClassification.v2","text":"<p>DBpedia14 is a dataset of English texts from Wikipedia articles, categorized into 14 non-overlapping classes based on their DBpedia ontology.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/d_bpedia</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dkhateclassification","title":"DKHateClassification","text":"<p>Danish Tweets annotated for Hate Speech either being Offensive or not</p> <p>Dataset: <code>DDSC/dkhate</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written expert-annotated found Citation <pre><code>@inproceedings{sigurbergsson-derczynski-2020-offensive,\n  address = {Marseille, France},\n  author = {Sigurbergsson, Gudbjartur Ingi  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {3498--3508},\n  publisher = {European Language Resources Association},\n  title = {Offensive Language and Hate Speech Detection for {D}anish},\n  url = {https://aclanthology.org/2020.lrec-1.430},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dkhateclassificationv2","title":"DKHateClassification.v2","text":"<p>Danish Tweets annotated for Hate Speech either being Offensive or not         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/dk_hate</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written expert-annotated found Citation <pre><code>@inproceedings{sigurbergsson-derczynski-2020-offensive,\n  address = {Marseille, France},\n  author = {Sigurbergsson, Gudbjartur Ingi  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {3498--3508},\n  publisher = {European Language Resources Association},\n  title = {Offensive Language and Hate Speech Detection for {D}anish},\n  url = {https://aclanthology.org/2020.lrec-1.430},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dadoevalcoarseclassification","title":"DadoEvalCoarseClassification","text":"<p>The DaDoEval dataset is a curated collection of 2,759 documents authored by Alcide De Gasperi, spanning the period from 1901 to 1954. Each document in the dataset is manually tagged with its date of issue.</p> <p>Dataset: <code>MattiaSangermano/DaDoEval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Written derived found Citation <pre><code>@inproceedings{menini2020dadoeval,\n  author = {Menini, Stefano and Moretti, Giovanni and Sprugnoli, Rachele and Tonelli, Sara and others},\n  booktitle = {Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)},\n  organization = {Accademia University Press},\n  pages = {391--397},\n  title = {DaDoEval@ EVALITA 2020: Same-genre and cross-genre dating of historical documents},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dalajclassification","title":"DalajClassification","text":"<p>A Swedish dataset for linguistic acceptability. Available as a part of Superlim.</p> <p>Dataset: <code>mteb/DalajClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Non-fiction, Written expert-annotated created Citation <pre><code>@misc{2105.06681,\n  author = {Elena Volodina and Yousuf Ali Mohammed and Julia Klezl},\n  eprint = {arXiv:2105.06681},\n  title = {DaLAJ - a dataset for linguistic acceptability judgments for Swedish: Format, baseline, sharing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dalajclassificationv2","title":"DalajClassification.v2","text":"<p>A Swedish dataset for linguistic acceptability. Available as a part of Superlim.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/dalaj</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Non-fiction, Written expert-annotated created Citation <pre><code>@misc{2105.06681,\n  author = {Elena Volodina and Yousuf Ali Mohammed and Julia Klezl},\n  eprint = {arXiv:2105.06681},\n  title = {DaLAJ - a dataset for linguistic acceptability judgments for Swedish: Format, baseline, sharing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#danishpoliticalcommentsclassification","title":"DanishPoliticalCommentsClassification","text":"<p>A dataset of Danish political comments rated for sentiment</p> <p>Dataset: <code>mteb/DanishPoliticalCommentsClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written derived found Citation <pre><code>@techreport{SAMsentiment,\n  author = {Mads Guldborg Kjeldgaard Kongsbak and Steffan Eybye Christensen and Lucas H\u00f8yberg Puvis~de~Chavannes and Peter Due Jensen},\n  institution = {IT University of Copenhagen},\n  title = {Sentiment Analysis Multitool, SAM},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#danishpoliticalcommentsclassificationv2","title":"DanishPoliticalCommentsClassification.v2","text":"<p>A dataset of Danish political comments rated for sentiment         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/danish_political_comments</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written derived found Citation <pre><code>@techreport{SAMsentiment,\n  author = {Mads Guldborg Kjeldgaard Kongsbak and Steffan Eybye Christensen and Lucas H\u00f8yberg Puvis~de~Chavannes and Peter Due Jensen},\n  institution = {IT University of Copenhagen},\n  title = {Sentiment Analysis Multitool, SAM},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ddisco","title":"Ddisco","text":"<p>A Danish Discourse dataset with values for coherence and source (Wikipedia or Reddit)</p> <p>Dataset: <code>DDSC/ddisco</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Non-fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ddiscov2","title":"Ddisco.v2","text":"<p>A Danish Discourse dataset with values for coherence and source (Wikipedia or Reddit)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ddisco_cohesion</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Non-fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#deepsentipers","title":"DeepSentiPers","text":"<p>Persian Sentiment Analysis Dataset</p> <p>Dataset: <code>PartAI/DeepSentiPers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#deepsentipersv2","title":"DeepSentiPers.v2","text":"<p>Persian Sentiment Analysis Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/deep_senti_pers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#definitionclassificationlegalbenchclassification","title":"DefinitionClassificationLegalBenchClassification","text":"<p>This task consists of determining whether or not a sentence from a Supreme Court opinion offers a definition of a term.</p> <p>Dataset: <code>mteb/DefinitionClassificationLegalBenchClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#digikalamagclassification","title":"DigikalamagClassification","text":"<p>A total of 8,515 articles scraped from Digikala Online Magazine. This dataset includes seven different classes.</p> <p>Dataset: <code>mteb/DigikalamagClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity1legalbenchclassification","title":"Diversity1LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 1).</p> <p>Dataset: <code>mteb/Diversity1LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity2legalbenchclassification","title":"Diversity2LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 2).</p> <p>Dataset: <code>mteb/Diversity2LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity3legalbenchclassification","title":"Diversity3LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 3).</p> <p>Dataset: <code>mteb/Diversity3LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity4legalbenchclassification","title":"Diversity4LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 4).</p> <p>Dataset: <code>mteb/Diversity4LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity5legalbenchclassification","title":"Diversity5LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 5).</p> <p>Dataset: <code>mteb/Diversity5LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity6legalbenchclassification","title":"Diversity6LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 6).</p> <p>Dataset: <code>mteb/Diversity6LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchbookreviewsentimentclassification","title":"DutchBookReviewSentimentClassification","text":"<p>A Dutch book review for sentiment classification.</p> <p>Dataset: <code>mteb/DutchBookReviewSentimentClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nld Reviews, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1910-00896,\n  archiveprefix = {arXiv},\n  author = {Benjamin, van der Burgh and\nSuzan, Verberne},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  eprint = {1910.00896},\n  journal = {CoRR},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  title = {The merits of Universal Language Model Fine-tuning for Small Datasets\n- a case with Dutch book reviews},\n  url = {http://arxiv.org/abs/1910.00896},\n  volume = {abs/1910.00896},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchbookreviewsentimentclassificationv2","title":"DutchBookReviewSentimentClassification.v2","text":"<p>A Dutch book review for sentiment classification.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/dutch_book_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nld Reviews, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1910-00896,\n  archiveprefix = {arXiv},\n  author = {Benjamin, van der Burgh and\nSuzan, Verberne},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  eprint = {1910.00896},\n  journal = {CoRR},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  title = {The merits of Universal Language Model Fine-tuning for Small Datasets\n- a case with Dutch book reviews},\n  url = {http://arxiv.org/abs/1910.00896},\n  volume = {abs/1910.00896},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchcolaclassification","title":"DutchColaClassification","text":"<p>Dutch CoLA is a corpus of linguistic acceptability for Dutch.</p> <p>Dataset: <code>clips/mteb-nl-dutch-cola</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Written expert-annotated found Citation <pre><code>@misc{gronlp_2024,\n  author = {Bylinina, Lisa and Abdi, Silvana and Brouwer, Hylke and Elzinga, Martine and Gunput, Shenza and Huisman, Sem and Krooneman, Collin and Poot, David and Top, Jelmer and Weideman, Cain},\n  doi = { 10.57967/hf/3825 },\n  publisher = { Hugging Face },\n  title = { {Dutch-CoLA (Revision 5a4196c)} },\n  url = { https://huggingface.co/datasets/GroNLP/dutch-cola },\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchgovernmentbiasclassification","title":"DutchGovernmentBiasClassification","text":"<p>The Dutch Government Data for Bias Detection (DGDB) is a dataset sourced from the Dutch House of Representatives and annotated for bias by experts</p> <p>Dataset: <code>clips/mteb-nl-dutch-government-bias-detection</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Government, Written expert-annotated found Citation <pre><code>@inproceedings{de2025detecting,\n  author = {de Swart, Milena and Den Hengst, Floris and Chen, Jieying},\n  booktitle = {Proceedings of the ACM on Web Conference 2025},\n  pages = {5034--5044},\n  title = {Detecting Linguistic Bias in Government Documents Using Large language Models},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchnewsarticlesclassification","title":"DutchNewsArticlesClassification","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld News, Written derived found"},{"location":"overview/available_tasks/classification/#dutchsarcasticheadlinesclassification","title":"DutchSarcasticHeadlinesClassification","text":"<p>This dataset contains news headlines of two Dutch news websites. All sarcastic headlines were collected from the Speld.nl (the Dutch equivalent of The Onion) whereas all 'normal' headlines were collected from the news website Nu.nl.</p> <p>Dataset: <code>clips/mteb-nl-sarcastic-headlines</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Fiction, News, Written derived found"},{"location":"overview/available_tasks/classification/#emotionclassification","title":"EmotionClassification","text":"<p>Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>mteb/emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#emotionclassificationv2","title":"EmotionClassification.v2","text":"<p>Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#emotionvnclassification","title":"EmotionVNClassification","text":"<p>Emotion is a translated dataset of Vietnamese from English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/emotion-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#estonianvalenceclassification","title":"EstonianValenceClassification","text":"<p>Dataset containing annotated Estonian news data from the Postimees and \u00d5htuleht newspapers.</p> <p>Dataset: <code>kardosdrur/estonian-valence</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy est News, Written human-annotated found Citation <pre><code>@article{Pajupuu2023,\n  author = {Hille Pajupuu and Jaan Pajupuu and Rene Altrov and Kairi Tamuri},\n  doi = {10.6084/m9.figshare.24517054.v1},\n  month = {11},\n  title = {{Estonian Valence Corpus  / Eesti valentsikorpus}},\n  url = {https://figshare.com/articles/dataset/Estonian_Valence_Corpus_Eesti_valentsikorpus/24517054},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#estonianvalenceclassificationv2","title":"EstonianValenceClassification.v2","text":"<p>Dataset containing annotated Estonian news data from the Postimees and \u00d5htuleht newspapers.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/estonian_valence</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy est News, Written human-annotated found Citation <pre><code>@article{Pajupuu2023,\n  author = {Hille Pajupuu and Jaan Pajupuu and Rene Altrov and Kairi Tamuri},\n  doi = {10.6084/m9.figshare.24517054.v1},\n  month = {11},\n  title = {{Estonian Valence Corpus  / Eesti valentsikorpus}},\n  url = {https://figshare.com/articles/dataset/Estonian_Valence_Corpus_Eesti_valentsikorpus/24517054},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#faintentclassification","title":"FaIntentClassification","text":"<p>Questions in 4 different categories that a user might ask their voice assistant to do</p> <p>Dataset: <code>MCINext/FaIntent</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinohatespeechclassification","title":"FilipinoHateSpeechClassification","text":"<p>Filipino Twitter dataset for sentiment classification.</p> <p>Dataset: <code>mteb/FilipinoHateSpeechClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{Cabasag-2019-hate-speech,\n  author = {Neil Vicente Cabasag, Vicente Raphael Chan, Sean Christian Lim, Mark Edward Gonzales, and Charibeth Cheng},\n  journal = {Philippine Computing Journal},\n  month = {August},\n  number = {1},\n  title = {Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing.},\n  volume = {XIV},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinohatespeechclassificationv2","title":"FilipinoHateSpeechClassification.v2","text":"<p>Filipino Twitter dataset for sentiment classification.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/filipino_hate_speech</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{Cabasag-2019-hate-speech,\n  author = {Neil Vicente Cabasag, Vicente Raphael Chan, Sean Christian Lim, Mark Edward Gonzales, and Charibeth Cheng},\n  journal = {Philippine Computing Journal},\n  month = {August},\n  number = {1},\n  title = {Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing.},\n  volume = {XIV},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinoshopeereviewsclassification","title":"FilipinoShopeeReviewsClassification","text":"<p>The Shopee reviews tl 15 dataset is constructed by randomly taking 2100 training samples and 450 samples for testing and validation for each review star from 1 to 5. In total, there are 10500 training samples and 2250 each in validation and testing samples.</p> <p>Dataset: <code>scaredmeow/shopee-reviews-tl-stars</code> \u2022 License: mpl-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{riegoenhancement,\n  author = {Riego, Neil Christian R. and Villarba, Danny Bell and Sison, Ariel Antwaun Rolando C. and Pineda, Fernandez C. and Lagunzad, Hermini\u00f1o C.},\n  issue = {08},\n  journal = {United International Journal for Research &amp; Technology},\n  pages = {72--82},\n  title = {Enhancement to Low-Resource Text Classification via Sequential Transfer Learning},\n  volume = {04},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#fintoxicityclassification","title":"FinToxicityClassification","text":"<pre><code>    This dataset is a DeepL -based machine translated version of the Jigsaw toxicity dataset for Finnish. The dataset is originally from a Kaggle competition https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data.\n    The original dataset poses a multi-label text classification problem and includes the labels identity_attack, insult, obscene, severe_toxicity, threat and toxicity.\n    Here adapted for toxicity classification, which is the most represented class.\n</code></pre> <p>Dataset: <code>TurkuNLP/jigsaw_toxicity_pred_fi</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 fin News, Written derived machine-translated Citation <pre><code>@inproceedings{eskelinen-etal-2023-toxicity,\n  author = {Eskelinen, Anni  and\nSilvala, Laura  and\nGinter, Filip  and\nPyysalo, Sampo  and\nLaippala, Veronika},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  title = {Toxicity Detection in {F}innish Using Machine Translation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#fintoxicityclassificationv2","title":"FinToxicityClassification.v2","text":"<pre><code>    This dataset is a DeepL -based machine translated version of the Jigsaw toxicity dataset for Finnish. The dataset is originally from a Kaggle competition https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data.\n    The original dataset poses a multi-label text classification problem and includes the labels identity_attack, insult, obscene, severe_toxicity, threat and toxicity.\n    Here adapted for toxicity classification, which is the most represented class.\n\n    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/fin_toxicity</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 fin News, Written derived machine-translated Citation <pre><code>@inproceedings{eskelinen-etal-2023-toxicity,\n  author = {Eskelinen, Anni  and\nSilvala, Laura  and\nGinter, Filip  and\nPyysalo, Sampo  and\nLaippala, Veronika},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  title = {Toxicity Detection in {F}innish Using Machine Translation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#financialphrasebankclassification","title":"FinancialPhrasebankClassification","text":"<p>Polar sentiment dataset of sentences from financial news, categorized by sentiment into positive, negative, or neutral.</p> <p>Dataset: <code>mteb/FinancialPhrasebankClassification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Financial, News, Written expert-annotated found Citation <pre><code>@article{Malo2014GoodDO,\n  author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\n  journal = {Journal of the Association for Information Science and Technology},\n  title = {Good debt or bad debt: Detecting semantic orientations in economic texts},\n  volume = {65},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#financialphrasebankclassificationv2","title":"FinancialPhrasebankClassification.v2","text":"<p>Polar sentiment dataset of sentences from financial news, categorized by sentiment into positive, negative, or neutral.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/financial_phrasebank</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Financial, News, Written expert-annotated found Citation <pre><code>@article{Malo2014GoodDO,\n  author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\n  journal = {Journal of the Association for Information Science and Technology},\n  title = {Good debt or bad debt: Detecting semantic orientations in economic texts},\n  volume = {65},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenchbookreviews","title":"FrenchBookReviews","text":"<p>It is a French book reviews dataset containing a huge number of reader reviews on French books. Each review is pared with a rating that ranges from 0.5 to 5 (with 0.5 increment).</p> <p>Dataset: <code>Abirate/french_book_reviews</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenchbookreviewsv2","title":"FrenchBookReviews.v2","text":"<p>It is a French book reviews dataset containing a huge number of reader reviews on French books. Each review is pared with a rating that ranges from 0.5 to 5 (with 0.5 increment).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/french_book_reviews</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkenclassification","title":"FrenkEnClassification","text":"<p>English subset of the FRENK dataset</p> <p>Dataset: <code>mteb/FrenkEnClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkenclassificationv2","title":"FrenkEnClassification.v2","text":"<p>English subset of the FRENK dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_en</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkhrclassification","title":"FrenkHrClassification","text":"<p>Croatian subset of the FRENK dataset</p> <p>Dataset: <code>mteb/FrenkHrClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hrv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkhrclassificationv2","title":"FrenkHrClassification.v2","text":"<p>Croatian subset of the FRENK dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_hr</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hrv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkslclassification","title":"FrenkSlClassification","text":"<p>Slovenian subset of the FRENK dataset. Also available on HuggingFace dataset hub: English subset, Croatian subset.</p> <p>Dataset: <code>mteb/FrenkSlClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkslclassificationv2","title":"FrenkSlClassification.v2","text":"<p>Slovenian subset of the FRENK dataset. Also available on HuggingFace dataset hub: English subset, Croatian subset.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_sl</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#functionofdecisionsectionlegalbenchclassification","title":"FunctionOfDecisionSectionLegalBenchClassification","text":"<p>The task is to classify a paragraph extracted from a written court decision into one of seven possible categories:             1. Facts - The paragraph describes the faction background that led up to the present lawsuit.             2. Procedural History - The paragraph describes the course of litigation that led to the current proceeding before the court.             3. Issue - The paragraph describes the legal or factual issue that must be resolved by the court.             4. Rule - The paragraph describes a rule of law relevant to resolving the issue.             5. Analysis - The paragraph analyzes the legal issue by applying the relevant legal principles to the facts of the present dispute.             6. Conclusion - The paragraph presents a conclusion of the court.             7. Decree - The paragraph constitutes a decree resolving the dispute.</p> <p>Dataset: <code>mteb/FunctionOfDecisionSectionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#georeviewclassification","title":"GeoreviewClassification","text":"<p>Review classification (5-point scale) based on Yandex Georeview dataset</p> <p>Dataset: <code>mteb/GeoreviewClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#georeviewclassificationv2","title":"GeoreviewClassification.v2","text":"<p>Review classification (5-point scale) based on Yandex Georeview dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/georeview</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#georgiansentimentclassification","title":"GeorgianSentimentClassification","text":"<p>Goergian Sentiment Dataset</p> <p>Dataset: <code>asparius/Georgian-Sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kat Reviews, Written derived found Citation <pre><code>@inproceedings{stefanovitch-etal-2022-resources,\n  address = {Marseille, France},\n  author = {Stefanovitch, Nicolas  and\nPiskorski, Jakub  and\nKharazi, Sopho},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {1613--1621},\n  publisher = {European Language Resources Association},\n  title = {Resources and Experiments on Sentiment Classification for {G}eorgian},\n  url = {https://aclanthology.org/2022.lrec-1.173},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#germanpoliticianstwittersentimentclassification","title":"GermanPoliticiansTwitterSentimentClassification","text":"<p>GermanPoliticiansTwitterSentiment is a dataset of German tweets categorized with their sentiment (3 classes).</p> <p>Dataset: <code>Alienmaster/german_politicians_twitter_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu Government, Social, Written human-annotated found Citation <pre><code>@inproceedings{schmidt-etal-2022-sentiment,\n  address = {Potsdam, Germany},\n  author = {Schmidt, Thomas  and\nFehle, Jakob  and\nWeissenbacher, Maximilian  and\nRichter, Jonathan  and\nGottschalk, Philipp  and\nWolff, Christian},\n  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},\n  editor = {Schaefer, Robin  and\nBai, Xiaoyu  and\nStede, Manfred  and\nZesch, Torsten},\n  month = {12--15 } # sep,\n  pages = {74--87},\n  publisher = {KONVENS 2022 Organizers},\n  title = {Sentiment Analysis on {T}witter for the Major {G}erman Parties during the 2021 {G}erman Federal Election},\n  url = {https://aclanthology.org/2022.konvens-1.9},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#germanpoliticianstwittersentimentclassificationv2","title":"GermanPoliticiansTwitterSentimentClassification.v2","text":"<p>GermanPoliticiansTwitterSentiment is a dataset of German tweets categorized with their sentiment (3 classes).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/german_politicians_twitter_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu Government, Social, Written human-annotated found Citation <pre><code>@inproceedings{schmidt-etal-2022-sentiment,\n  address = {Potsdam, Germany},\n  author = {Schmidt, Thomas  and\nFehle, Jakob  and\nWeissenbacher, Maximilian  and\nRichter, Jonathan  and\nGottschalk, Philipp  and\nWolff, Christian},\n  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},\n  editor = {Schaefer, Robin  and\nBai, Xiaoyu  and\nStede, Manfred  and\nZesch, Torsten},\n  month = {12--15 } # sep,\n  pages = {74--87},\n  publisher = {KONVENS 2022 Organizers},\n  title = {Sentiment Analysis on {T}witter for the Major {G}erman Parties during the 2021 {G}erman Federal Election},\n  url = {https://aclanthology.org/2022.konvens-1.9},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#greeklegalcodeclassification","title":"GreekLegalCodeClassification","text":"<p>Greek Legal Code Dataset for Classification. (subset = chapter)</p> <p>Dataset: <code>AI-team-UoA/greek_legal_code</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ell Legal, Written human-annotated found Citation <pre><code>@inproceedings{papaloukas-etal-2021-glc,\n  address = {Punta Cana, Dominican Republic},\n  author = {Papaloukas, Christos and Chalkidis, Ilias and Athinaios, Konstantinos and Pantazi, Despina-Athanasia and Koubarakis, Manolis},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  doi = {10.48550/arXiv.2109.15298},\n  pages = {63--75},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-granular Legal Topic Classification on Greek Legislation},\n  url = {https://arxiv.org/abs/2109.15298},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#gujaratinewsclassification","title":"GujaratiNewsClassification","text":"<p>A Gujarati dataset for 3-class classification of Gujarati news articles</p> <p>Dataset: <code>mlexplorer008/gujarati_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj News, Written derived found"},{"location":"overview/available_tasks/classification/#gujaratinewsclassificationv2","title":"GujaratiNewsClassification.v2","text":"<p>A Gujarati dataset for 3-class classification of Gujarati news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/gujarati_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj News, Written derived found"},{"location":"overview/available_tasks/classification/#humeemotionclassification","title":"HUMEEmotionClassification","text":"<p>Human evaluation subset of Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>mteb/HUMEEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humemultilingualsentimentclassification","title":"HUMEMultilingualSentimentClassification","text":"<p>Human evaluation subset of Sentiment classification dataset with binary (positive vs negative sentiment) labels. Includes 4 languages.</p> <p>Dataset: <code>mteb/HUMEMultilingualSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy ara, eng, nob, rus Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humetoxicconversationsclassification","title":"HUMEToxicConversationsClassification","text":"<p>Human evaluation subset of Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.</p> <p>Dataset: <code>mteb/HUMEToxicConversationsClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humetweetsentimentextractionclassification","title":"HUMETweetSentimentExtractionClassification","text":"<p>Human evaluation subset of Tweet Sentiment Extraction dataset.</p> <p>Dataset: <code>mteb/HUMETweetSentimentExtractionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hatespeechportugueseclassification","title":"HateSpeechPortugueseClassification","text":"<p>HateSpeechPortugueseClassification is a dataset of Portuguese tweets categorized with their sentiment (2 classes).</p> <p>Dataset: <code>mteb/HateSpeechPortugueseClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy por Social, Written expert-annotated found Citation <pre><code>@inproceedings{fortuna-etal-2019-hierarchically,\n  address = {Florence, Italy},\n  author = {Fortuna, Paula  and\nRocha da Silva, Jo{\\~a}o  and\nSoler-Company, Juan  and\nWanner, Leo  and\nNunes, S{\\'e}rgio},\n  booktitle = {Proceedings of the Third Workshop on Abusive Language Online},\n  doi = {10.18653/v1/W19-3510},\n  editor = {Roberts, Sarah T.  and\nTetreault, Joel  and\nPrabhakaran, Vinodkumar  and\nWaseem, Zeerak},\n  month = aug,\n  pages = {94--104},\n  publisher = {Association for Computational Linguistics},\n  title = {A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset},\n  url = {https://aclanthology.org/W19-3510},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#headlineclassification","title":"HeadlineClassification","text":"<p>Headline rubric classification based on the paraphraser plus dataset.</p> <p>Dataset: <code>ai-forever/headline-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus News, Written derived found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  editor = {Birch, Alexandra  and\nFinch, Andrew  and\nHayashi, Hiroaki  and\nHeafield, Kenneth  and\nJunczys-Dowmunt, Marcin  and\nKonstas, Ioannis  and\nLi, Xian  and\nNeubig, Graham  and\nOda, Yusuke},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#headlineclassificationv2","title":"HeadlineClassification.v2","text":"<p>Headline rubric classification based on the paraphraser plus dataset.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/headline</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus News, Written derived found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  editor = {Birch, Alexandra  and\nFinch, Andrew  and\nHayashi, Hiroaki  and\nHeafield, Kenneth  and\nJunczys-Dowmunt, Marcin  and\nKonstas, Ioannis  and\nLi, Xian  and\nNeubig, Graham  and\nOda, Yusuke},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hebrewsentimentanalysis","title":"HebrewSentimentAnalysis","text":"<p>HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel\u2019s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president\u2019s posts in the period of June \u2013 August 2014, the first three months of Rivlin\u2019s presidency.2 While the president\u2019s posts aimed at reconciling tensions and called for tolerance and empathy, the sentiment expressed in the comments to the president\u2019s posts was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his policy.</p> <p>Dataset: <code>mteb/HebrewSentimentAnalysis</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy heb Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{amram-etal-2018-representations,\n  address = {Santa Fe, New Mexico, USA},\n  author = {Amram, Adam and Ben David, Anat and Tsarfaty, Reut},\n  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},\n  month = aug,\n  pages = {2242--2252},\n  publisher = {Association for Computational Linguistics},\n  title = {Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew},\n  url = {https://www.aclweb.org/anthology/C18-1190},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hebrewsentimentanalysisv2","title":"HebrewSentimentAnalysis.v2","text":"<p>HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel\u2019s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president\u2019s posts in the period of June \u2013 August 2014, the first three months of Rivlin\u2019s presidency.2 While the president\u2019s posts aimed at reconciling tensions and called for tolerance and empathy, the sentiment expressed in the comments to the president\u2019s posts was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his policy.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/hebrew_sentiment_analysis</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy heb Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{amram-etal-2018-representations,\n  address = {Santa Fe, New Mexico, USA},\n  author = {Amram, Adam and Ben David, Anat and Tsarfaty, Reut},\n  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},\n  month = aug,\n  pages = {2242--2252},\n  publisher = {Association for Computational Linguistics},\n  title = {Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew},\n  url = {https://www.aclweb.org/anthology/C18-1190},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindialectclassification","title":"HinDialectClassification","text":"<p>HinDialect: 26 Hindi-related languages and dialects of the Indic Continuum in North India</p> <p>Dataset: <code>mlexplorer008/hin_dialect_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 anp, awa, ben, bgc, bhb, ... (21) Social, Spoken, Written expert-annotated found Citation <pre><code>@misc{11234/1-4839,\n  author = {Bafna, Niyati and {\\v Z}abokrtsk{\\'y}, Zden{\\v e}k and Espa{\\~n}a-Bonet, Cristina and van Genabith, Josef and Kumar, Lalit \"Samyak Lalit\" and Suman, Sharda and Shivay, Rahul},\n  copyright = {Creative Commons - Attribution-{NonCommercial}-{ShareAlike} 4.0 International ({CC} {BY}-{NC}-{SA} 4.0)},\n  note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\\'U}FAL}), Faculty of Mathematics and Physics, Charles University},\n  title = {{HinDialect} 1.1: 26 Hindi-related languages and dialects of the Indic Continuum in North India},\n  url = {http://hdl.handle.net/11234/1-4839},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindidiscourseclassification","title":"HindiDiscourseClassification","text":"<p>A Hindi Discourse dataset in Hindi with values for coherence.</p> <p>Dataset: <code>mteb/HindiDiscourseClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hin Fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dhanwal-etal-2020-annotated,\n  address = {Marseille, France},\n  author = {Dhanwal, Swapnil  and\nDutta, Hritwik  and\nNankani, Hitesh  and\nShrivastava, Nilay  and\nKumar, Yaman  and\nLi, Junyi Jessy  and\nMahata, Debanjan  and\nGosangi, Rakesh  and\nZhang, Haimin  and\nShah, Rajiv Ratn  and\nStent, Amanda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  publisher = {European Language Resources Association},\n  title = {An Annotated Dataset of Discourse Modes in {H}indi Stories},\n  url = {https://www.aclweb.org/anthology/2020.lrec-1.149},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindidiscourseclassificationv2","title":"HindiDiscourseClassification.v2","text":"<p>A Hindi Discourse dataset in Hindi with values for coherence.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/hindi_discourse</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hin Fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dhanwal-etal-2020-annotated,\n  address = {Marseille, France},\n  author = {Dhanwal, Swapnil  and\nDutta, Hritwik  and\nNankani, Hitesh  and\nShrivastava, Nilay  and\nKumar, Yaman  and\nLi, Junyi Jessy  and\nMahata, Debanjan  and\nGosangi, Rakesh  and\nZhang, Haimin  and\nShah, Rajiv Ratn  and\nStent, Amanda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  publisher = {European Language Resources Association},\n  title = {An Annotated Dataset of Discourse Modes in {H}indi Stories},\n  url = {https://www.aclweb.org/anthology/2020.lrec-1.149},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hotelreviewsentimentclassification","title":"HotelReviewSentimentClassification","text":"<p>HARD is a dataset of Arabic hotel reviews collected from the Booking.com website.</p> <p>Dataset: <code>mteb/HotelReviewSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@article{elnagar2018hotel,\n  author = {Elnagar, Ashraf and Khalifa, Yasmin S and Einea, Anas},\n  journal = {Intelligent natural language processing: Trends and applications},\n  pages = {35--52},\n  publisher = {Springer},\n  title = {Hotel Arabic-reviews dataset construction for sentiment analysis applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hotelreviewsentimentclassificationv2","title":"HotelReviewSentimentClassification.v2","text":"<p>HARD is a dataset of Arabic hotel reviews collected from the Booking.com website.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/HotelReviewSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@article{elnagar2018hotel,\n  author = {Elnagar, Ashraf and Khalifa, Yasmin S and Einea, Anas},\n  journal = {Intelligent natural language processing: Trends and applications},\n  pages = {35--52},\n  publisher = {Springer},\n  title = {Hotel Arabic-reviews dataset construction for sentiment analysis applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iflytek","title":"IFlyTek","text":"<p>Long Text classification for the description of Apps</p> <p>Dataset: <code>C-MTEB/IFlyTek-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iflytekv2","title":"IFlyTek.v2","text":"<p>Long Text classification for the description of Apps         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/i_fly_tek</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iconclassclassification","title":"IconclassClassification","text":"<p>Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. The task is to classify the first layer of Iconclass</p> <p>Dataset: <code>clips/mteb-nl-iconclass-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Fiction, Written expert-annotated found Citation <pre><code>@article{banar2023transfer,\n  author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},\n  journal = {ACM Journal on Computing and Cultural Heritage},\n  number = {2},\n  pages = {1--16},\n  publisher = {ACM New York, NY},\n  title = {Transfer learning for the visual arts: The multi-modal retrieval of iconclass codes},\n  volume = {16},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbclassification","title":"ImdbClassification","text":"<p>Large Movie Review Dataset</p> <p>Dataset: <code>mteb/imdb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{maas-etal-2011-learning,\n  address = {Portland, Oregon, USA},\n  author = {Maas, Andrew L.  and\nDaly, Raymond E.  and\nPham, Peter T.  and\nHuang, Dan  and\nNg, Andrew Y.  and\nPotts, Christopher},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  editor = {Lin, Dekang  and\nMatsumoto, Yuji  and\nMihalcea, Rada},\n  month = jun,\n  pages = {142--150},\n  publisher = {Association for Computational Linguistics},\n  title = {Learning Word Vectors for Sentiment Analysis},\n  url = {https://aclanthology.org/P11-1015},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbclassificationv2","title":"ImdbClassification.v2","text":"<p>Large Movie Review Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/imdb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{maas-etal-2011-learning,\n  address = {Portland, Oregon, USA},\n  author = {Maas, Andrew L.  and\nDaly, Raymond E.  and\nPham, Peter T.  and\nHuang, Dan  and\nNg, Andrew Y.  and\nPotts, Christopher},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  editor = {Lin, Dekang  and\nMatsumoto, Yuji  and\nMihalcea, Rada},\n  month = jun,\n  pages = {142--150},\n  publisher = {Association for Computational Linguistics},\n  title = {Learning Word Vectors for Sentiment Analysis},\n  url = {https://aclanthology.org/P11-1015},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbvnclassification","title":"ImdbVNClassification","text":"<p>A translated dataset of large movie reviews annotated for sentiment classification.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/imdb-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassification","title":"InappropriatenessClassification","text":"<p>Inappropriateness identification in the form of binary classification</p> <p>Dataset: <code>ai-forever/inappropriateness-classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassificationv2","title":"InappropriatenessClassification.v2","text":"<p>Inappropriateness identification in the form of binary classification         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/inappropriateness</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassificationv2_1","title":"InappropriatenessClassificationv2","text":"<p>Inappropriateness identification in the form of binary classification</p> <p>Dataset: <code>mteb/InappropriatenessClassificationv2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indiclangclassification","title":"IndicLangClassification","text":"<p>A language identification test set for native-script as well as Romanized text which spans 22 Indic languages.</p> <p>Dataset: <code>ai4bharat/Bhasha-Abhijnaanam</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy asm, ben, brx, doi, gom, ... (22) Non-fiction, Web, Written expert-annotated created Citation <pre><code>@inproceedings{madhani-etal-2023-bhasa,\n  address = {Toronto, Canada},\n  author = {Madhani, Yash  and\nKhapra, Mitesh M.  and\nKunchukuttan, Anoop},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n  doi = {10.18653/v1/2023.acl-short.71},\n  editor = {Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki},\n  month = jul,\n  pages = {816--826},\n  publisher = {Association for Computational Linguistics},\n  title = {Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 {I}ndic languages},\n  url = {https://aclanthology.org/2023.acl-short.71},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indicnlpnewsclassification","title":"IndicNLPNewsClassification","text":"<p>A News classification dataset in multiple Indian regional languages.</p> <p>Dataset: <code>Sakshamrzt/IndicNLP-Multilingual</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj, kan, mal, mar, ori, ... (8) News, Written expert-annotated found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indicsentimentclassification","title":"IndicSentimentClassification","text":"<p>A new, multilingual, and n-way parallel dataset for sentiment analysis in 13 Indic languages.</p> <p>Dataset: <code>mteb/IndicSentiment</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy asm, ben, brx, guj, hin, ... (13) Reviews, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianidclickbaitclassification","title":"IndonesianIdClickbaitClassification","text":"<p>The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers.</p> <p>Dataset: <code>manandey/id_clickbait</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind News, Written expert-annotated found Citation <pre><code>@article{WILLIAM2020106231,\n  author = {Andika William and Yunita Sari},\n  doi = {https://doi.org/10.1016/j.dib.2020.106231},\n  issn = {2352-3409},\n  journal = {Data in Brief},\n  keywords = {Indonesian, Natural Language Processing, News articles, Clickbait, Text-classification},\n  pages = {106231},\n  title = {CLICK-ID: A novel dataset for Indonesian clickbait headlines},\n  url = {http://www.sciencedirect.com/science/article/pii/S2352340920311252},\n  volume = {32},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianidclickbaitclassificationv2","title":"IndonesianIdClickbaitClassification.v2","text":"<p>The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/indonesian_id_clickbait</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind News, Written expert-annotated found Citation <pre><code>@article{WILLIAM2020106231,\n  author = {Andika William and Yunita Sari},\n  doi = {https://doi.org/10.1016/j.dib.2020.106231},\n  issn = {2352-3409},\n  journal = {Data in Brief},\n  keywords = {Indonesian, Natural Language Processing, News articles, Clickbait, Text-classification},\n  pages = {106231},\n  title = {CLICK-ID: A novel dataset for Indonesian clickbait headlines},\n  url = {http://www.sciencedirect.com/science/article/pii/S2352340920311252},\n  volume = {32},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianmongabayconservationclassification","title":"IndonesianMongabayConservationClassification","text":"<p>Conservation dataset that was collected from mongabay.co.id contains topic-classification task (multi-label format) and sentiment classification. This task only covers sentiment analysis (positive, neutral negative)</p> <p>Dataset: <code>Datasaur/mongabay-experiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind Web, Written derived found Citation <pre><code>@inproceedings{fransiska-etal-2023-utilizing,\n  address = {Nusa Dua, Bali, Indonesia},\n  author = {Fransiska, Mega  and\nPitaloka, Diah  and\nSaripudin, Saripudin  and\nPutra, Satrio  and\nSutawika*, Lintang},\n  booktitle = {Proceedings of the First Workshop in South East Asian Language Processing},\n  doi = {10.18653/v1/2023.sealp-1.4},\n  editor = {Wijaya, Derry  and\nAji, Alham Fikri  and\nVania, Clara  and\nWinata, Genta Indra  and\nPurwarianti, Ayu},\n  month = nov,\n  pages = {30--54},\n  publisher = {Association for Computational Linguistics},\n  title = {Utilizing Weak Supervision to Generate {I}ndonesian Conservation Datasets},\n  url = {https://aclanthology.org/2023.sealp-1.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianmongabayconservationclassificationv2","title":"IndonesianMongabayConservationClassification.v2","text":"<p>Conservation dataset that was collected from mongabay.co.id contains topic-classification task (multi-label format) and sentiment classification. This task only covers sentiment analysis (positive, neutral negative)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/indonesian_mongabay_conservation</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind Web, Written derived found Citation <pre><code>@inproceedings{fransiska-etal-2023-utilizing,\n  address = {Nusa Dua, Bali, Indonesia},\n  author = {Fransiska, Mega  and\nPitaloka, Diah  and\nSaripudin, Saripudin  and\nPutra, Satrio  and\nSutawika*, Lintang},\n  booktitle = {Proceedings of the First Workshop in South East Asian Language Processing},\n  doi = {10.18653/v1/2023.sealp-1.4},\n  editor = {Wijaya, Derry  and\nAji, Alham Fikri  and\nVania, Clara  and\nWinata, Genta Indra  and\nPurwarianti, Ayu},\n  month = nov,\n  pages = {30--54},\n  publisher = {Association for Computational Linguistics},\n  title = {Utilizing Weak Supervision to Generate {I}ndonesian Conservation Datasets},\n  url = {https://aclanthology.org/2023.sealp-1.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#insurancepolicyinterpretationlegalbenchclassification","title":"InsurancePolicyInterpretationLegalBenchClassification","text":"<p>Given an insurance claim and policy, determine whether the claim is covered by the policy.</p> <p>Dataset: <code>mteb/InsurancePolicyInterpretationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#internationalcitizenshipquestionslegalbenchclassification","title":"InternationalCitizenshipQuestionsLegalBenchClassification","text":"<p>Answer questions about citizenship law from across the world. Dataset was made using the GLOBALCIT citizenship law dataset, by constructing questions about citizenship law as Yes or No questions.</p> <p>Dataset: <code>mteb/InternationalCitizenshipQuestionsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@misc{vink2023globalcit,\n  author = {Vink, Maarten and van der Baaren, Luuk and Baub\u00f6ck, Rainer and D\u017eanki\u0107, Jelena and Honohan, Iseult and Manby, Bronwen},\n  howpublished = {https://hdl.handle.net/1814/73190},\n  publisher = {Global Citizenship Observatory},\n  title = {GLOBALCIT Citizenship Law Dataset, v2.0, Country-Year-Mode Data (Acquisition)},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#isizulunewsclassification","title":"IsiZuluNewsClassification","text":"<p>isiZulu News Classification Dataset</p> <p>Dataset: <code>isaacchung/isizulu-news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy zul News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#isizulunewsclassificationv2","title":"IsiZuluNewsClassification.v2","text":"<p>isiZulu News Classification Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/isi_zulu_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy zul News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacaseholdclassification","title":"ItaCaseholdClassification","text":"<p>An Italian Dataset consisting of 1101 pairs of judgments and their official holdings between the years 2019 and 2022 from the archives of Italian Administrative Justice categorized with 64 subjects.</p> <p>Dataset: <code>itacasehold/itacasehold</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Government, Legal, Written expert-annotated found Citation <pre><code>@inproceedings{10.1145/3594536.3595177,\n  address = {New York, NY, USA},\n  author = {Licari, Daniele and Bushipaka, Praveen and Marino, Gabriele and Comand\\'{e}, Giovanni and Cucinotta, Tommaso},\n  booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},\n  doi = {10.1145/3594536.3595177},\n  isbn = {9798400701979},\n  keywords = {Italian-LEGAL-BERT, Holding Extraction, Extractive Text Summarization, Benchmark Dataset},\n  location = {&lt;conf-loc&gt;, &lt;city&gt;Braga&lt;/city&gt;, &lt;country&gt;Portugal&lt;/country&gt;, &lt;/conf-loc&gt;},\n  numpages = {9},\n  pages = {148\u2013156},\n  publisher = {Association for Computing Machinery},\n  series = {ICAIL '23},\n  title = {Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization},\n  url = {https://doi.org/10.1145/3594536.3595177},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacola","title":"Itacola","text":"<p>An Italian Corpus of Linguistic Acceptability taken from linguistic literature with a binary annotation made by the original authors themselves.</p> <p>Dataset: <code>mteb/Itacola</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Non-fiction, Spoken, Written expert-annotated found Citation <pre><code>@inproceedings{trotta-etal-2021-monolingual-cross,\n  address = {Punta Cana, Dominican Republic},\n  author = {Trotta, Daniela  and\nGuarasci, Raffaele  and\nLeonardelli, Elisa  and\nTonelli, Sara},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.250},\n  month = nov,\n  pages = {2929--2940},\n  publisher = {Association for Computational Linguistics},\n  title = {Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus},\n  url = {https://aclanthology.org/2021.findings-emnlp.250},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacolav2","title":"Itacola.v2","text":"<p>An Italian Corpus of Linguistic Acceptability taken from linguistic literature with a binary annotation made by the original authors themselves.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/italian_linguistic_acceptability</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Non-fiction, Spoken, Written expert-annotated found Citation <pre><code>@inproceedings{trotta-etal-2021-monolingual-cross,\n  address = {Punta Cana, Dominican Republic},\n  author = {Trotta, Daniela  and\nGuarasci, Raffaele  and\nLeonardelli, Elisa  and\nTonelli, Sara},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.250},\n  month = nov,\n  pages = {2929--2940},\n  publisher = {Association for Computational Linguistics},\n  title = {Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus},\n  url = {https://aclanthology.org/2021.findings-emnlp.250},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jcrewblockerlegalbenchclassification","title":"JCrewBlockerLegalBenchClassification","text":"<p>The J.Crew Blocker, also known as the J.Crew Protection, is a provision included in leveraged loan documents to prevent companies from removing security by transferring intellectual property (IP) into new subsidiaries and raising additional debt. The task consists of determining whether the J.Crew Blocker is present in the document.</p> <p>Dataset: <code>mteb/JCrewBlockerLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jcrewblockerlegalbenchclassificationv2","title":"JCrewBlockerLegalBenchClassification.v2","text":"<p>The J.Crew Blocker, also known as the J.Crew Protection, is a provision included in leveraged loan documents to prevent companies from removing security by transferring intellectual property (IP) into new subsidiaries and raising additional debt. The task consists of determining whether the J.Crew Blocker is present in the document.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/j_crew_blocker_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jdreview","title":"JDReview","text":"<p>review for iphone</p> <p>Dataset: <code>C-MTEB/JDReview-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@article{xiao2023c,\n  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},\n  journal = {arXiv preprint arXiv:2309.07597},\n  title = {C-pack: Packaged resources to advance general chinese embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jdreviewv2","title":"JDReview.v2","text":"<p>review for iphone         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/jd_review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@article{xiao2023c,\n  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},\n  journal = {arXiv preprint arXiv:2309.07597},\n  title = {C-pack: Packaged resources to advance general chinese embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#japanesesentimentclassification","title":"JapaneseSentimentClassification","text":"<p>Japanese sentiment classification dataset with binary (positive vs negative sentiment) labels. This version reverts the morphological analysis from the original multilingual dataset to restore natural Japanese text without artificial spaces.</p> <p>Dataset: <code>mteb/JapaneseSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#javaneseimdbclassification","title":"JavaneseIMDBClassification","text":"<p>Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets.</p> <p>Dataset: <code>mteb/JavaneseIMDBClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jav Reviews, Written human-annotated found Citation <pre><code>@inproceedings{wongso2021causal,\n  author = {Wongso, Wilson and Setiawan, David Samuel and Suhartono, Derwin},\n  booktitle = {2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},\n  organization = {IEEE},\n  pages = {1--7},\n  title = {Causal and Masked Language Modeling of Javanese Language using Transformer-based Architectures},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#javaneseimdbclassificationv2","title":"JavaneseIMDBClassification.v2","text":"<p>Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/javanese_imdb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jav Reviews, Written human-annotated found Citation <pre><code>@inproceedings{wongso2021causal,\n  author = {Wongso, Wilson and Setiawan, David Samuel and Suhartono, Derwin},\n  booktitle = {2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},\n  organization = {IEEE},\n  pages = {1--7},\n  title = {Causal and Masked Language Modeling of Javanese Language using Transformer-based Architectures},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#klue-tc","title":"KLUE-TC","text":"<p>Topic classification dataset of human-annotated news headlines. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#klue-tcv2","title":"KLUE-TC.v2","text":"<p>Topic classification dataset of human-annotated news headlines. Part of the Korean Language Understanding Evaluation (KLUE).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/klue_tc</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kannadanewsclassification","title":"KannadaNewsClassification","text":"<p>The Kannada news dataset contains only the headlines of news article in three categories: Entertainment, Tech, and Sports. The data set contains around 6300 news article headlines which are collected from Kannada news websites. The data set has been cleaned and contains train and test set using which can be used to benchmark topic classification models in Kannada.</p> <p>Dataset: <code>Akash190104/kannada_news_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kannadanewsclassificationv2","title":"KannadaNewsClassification.v2","text":"<p>The Kannada news dataset contains only the headlines of news article in three categories: Entertainment, Tech, and Sports. The data set contains around 6300 news article headlines which are collected from Kannada news websites. The data set has been cleaned and contains train and test set using which can be used to benchmark topic classification models in Kannada.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kannada_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kinopoiskclassification","title":"KinopoiskClassification","text":"<p>Kinopoisk review sentiment classification</p> <p>Dataset: <code>ai-forever/kinopoisk-sentiment-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@article{blinov2013research,\n  author = {Blinov, PD and Klekovkina, Maria and Kotelnikov, Eugeny and Pestov, Oleg},\n  journal = {Computational Linguistics and Intellectual Technologies},\n  number = {12},\n  pages = {48--58},\n  title = {Research of lexical approach and machine learning methods for sentiment analysis},\n  volume = {2},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korfin","title":"KorFin","text":"<p>The KorFin-ASC is an extension of KorFin-ABSA, which is a financial sentiment analysis dataset including 8818 samples with (aspect, polarity) pairs annotated. The samples were collected from KLUE-TC and analyst reports from Naver Finance.</p> <p>Dataset: <code>amphora/korfin-asc</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Financial, News, Written expert-annotated found Citation <pre><code>@article{son2023removing,\n  author = {Son, Guijin and Lee, Hanwool and Kang, Nahyeon and Hahm, Moonjeong},\n  journal = {arXiv preprint arXiv:2301.03136},\n  title = {Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korhateclassification","title":"KorHateClassification","text":"<p>The dataset was created to provide the first human-labeled Korean corpus for         toxic speech detection from a Korean online entertainment news aggregator. Recently,         two young Korean celebrities suffered from a series of tragic incidents that led to two         major Korean web portals to close the comments section on their platform. However, this only         serves as a temporary solution, and the fundamental issue has not been solved yet. This dataset         hopes to improve Korean hate speech detection. Annotation was performed by 32 annotators,         consisting of 29 annotators from the crowdsourcing platform DeepNatural AI and three NLP researchers.</p> <p>Dataset: <code>mteb/KorHateClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{moon2020beep,\n  archiveprefix = {arXiv},\n  author = {Jihyung Moon and Won Ik Cho and Junbum Lee},\n  eprint = {2005.12503},\n  primaryclass = {cs.CL},\n  title = {BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korhateclassificationv2","title":"KorHateClassification.v2","text":"<p>The dataset was created to provide the first human-labeled Korean corpus for         toxic speech detection from a Korean online entertainment news aggregator. Recently,         two young Korean celebrities suffered from a series of tragic incidents that led to two         major Korean web portals to close the comments section on their platform. However, this only         serves as a temporary solution, and the fundamental issue has not been solved yet. This dataset         hopes to improve Korean hate speech detection. Annotation was performed by 32 annotators,         consisting of 29 annotators from the crowdsourcing platform DeepNatural AI and three NLP researchers.</p> <pre><code>    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/kor_hate</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{moon2020beep,\n  archiveprefix = {arXiv},\n  author = {Jihyung Moon and Won Ik Cho and Junbum Lee},\n  eprint = {2005.12503},\n  primaryclass = {cs.CL},\n  title = {BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korsarcasmclassification","title":"KorSarcasmClassification","text":"<pre><code>    The Korean Sarcasm Dataset was created to detect sarcasm in text, which can significantly alter the original\n    meaning of a sentence. 9319 tweets were collected from Twitter and labeled for sarcasm or not_sarcasm. These\n    tweets were gathered by querying for: irony sarcastic, and\n    sarcasm.\n    The dataset was created by gathering HTML data from Twitter. Queries for hashtags that include sarcasm\n    and variants of it were used to return tweets. It was preprocessed by removing the keyword\n    hashtag, urls and mentions of the user to preserve anonymity.\n</code></pre> <p>Dataset: <code>mteb/KorSarcasmClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{kim2019kocasm,\n  author = {Kim, Jiwon and Cho, Won Ik},\n  howpublished = {https://github.com/SpellOnYou/korean-sarcasm},\n  journal = {GitHub repository},\n  publisher = {GitHub},\n  title = {Kocasm: Korean Automatic Sarcasm Detection},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korsarcasmclassificationv2","title":"KorSarcasmClassification.v2","text":"<pre><code>    The Korean Sarcasm Dataset was created to detect sarcasm in text, which can significantly alter the original\n    meaning of a sentence. 9319 tweets were collected from Twitter and labeled for sarcasm or not_sarcasm. These\n    tweets were gathered by querying for: irony sarcastic, and\n    sarcasm.\n    The dataset was created by gathering HTML data from Twitter. Queries for hashtags that include sarcasm\n    and variants of it were used to return tweets. It was preprocessed by removing the keyword\n    hashtag, urls and mentions of the user to preserve anonymity.\n\n    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/kor_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{kim2019kocasm,\n  author = {Kim, Jiwon and Cho, Won Ik},\n  howpublished = {https://github.com/SpellOnYou/korean-sarcasm},\n  journal = {GitHub repository},\n  publisher = {GitHub},\n  title = {Kocasm: Korean Automatic Sarcasm Detection},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kurdishsentimentclassification","title":"KurdishSentimentClassification","text":"<p>Kurdish Sentiment Dataset</p> <p>Dataset: <code>asparius/Kurdish-Sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kur Web, Written derived found Citation <pre><code>@article{article,\n  author = {Badawi, Soran and Kazemi, Arefeh and Rezaie, Vali},\n  doi = {10.1007/s10579-023-09716-6},\n  journal = {Language Resources and Evaluation},\n  month = {01},\n  pages = {1-20},\n  title = {KurdiSent: a corpus for kurdish sentiment analysis},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kurdishsentimentclassificationv2","title":"KurdishSentimentClassification.v2","text":"<p>Kurdish Sentiment Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kurdish_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kur Web, Written derived found Citation <pre><code>@article{article,\n  author = {Badawi, Soran and Kazemi, Arefeh and Rezaie, Vali},\n  doi = {10.1007/s10579-023-09716-6},\n  journal = {Language Resources and Evaluation},\n  month = {01},\n  pages = {1-20},\n  title = {KurdiSent: a corpus for kurdish sentiment analysis},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#languageclassification","title":"LanguageClassification","text":"<p>A language identification dataset for 20 languages.</p> <p>Dataset: <code>papluca/language-identification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, bul, cmn, deu, ell, ... (20) Fiction, Government, Non-fiction, Reviews, Web, ... (6) derived found Citation <pre><code>@inproceedings{conneau2018xnli,\n  author = {Conneau, Alexis\nand Rinott, Ruty\nand Lample, Guillaume\nand Williams, Adina\nand Bowman, Samuel R.\nand Schwenk, Holger\nand Stoyanov, Veselin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#lccsentimentclassification","title":"LccSentimentClassification","text":"<p>The leipzig corpora collection, annotated for sentiment</p> <p>Dataset: <code>DDSC/lcc</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{quasthoff-etal-2006-corpus,\n  address = {Genoa, Italy},\n  author = {Quasthoff, Uwe  and\nRichter, Matthias  and\nBiemann, Christian},\n  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nGangemi, Aldo  and\nMaegaard, Bente  and\nMariani, Joseph  and\nOdijk, Jan  and\nTapias, Daniel},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {Corpus Portal for Search in Monolingual Corpora},\n  url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/641_pdf.pdf},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsbenefitslegalbenchclassification","title":"LearnedHandsBenefitsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal post discusses public benefits and social services that people can get from the government, like for food, disability, old age, housing, medical help, unemployment, child care, or other social needs.</p> <p>Dataset: <code>mteb/LearnedHandsBenefitsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsbusinesslegalbenchclassification","title":"LearnedHandsBusinessLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal question discusses issues faced by people who run small businesses or nonprofits, including around incorporation, licenses, taxes, regulations, and other concerns. It also includes options when there are disasters, bankruptcies, or other problems.</p> <p>Dataset: <code>mteb/LearnedHandsBusinessLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsconsumerlegalbenchclassification","title":"LearnedHandsConsumerLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues people face regarding money, insurance, consumer goods and contracts, taxes, and small claims about quality of service.</p> <p>Dataset: <code>mteb/LearnedHandsConsumerLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandscourtslegalbenchclassification","title":"LearnedHandsCourtsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses the logistics of how a person can interact with a lawyer or the court system. It applies to situations about procedure, rules, how to file lawsuits, how to hire lawyers, how to represent oneself, and other practical matters about dealing with these systems.</p> <p>Dataset: <code>mteb/LearnedHandsCourtsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandscrimelegalbenchclassification","title":"LearnedHandsCrimeLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues in the criminal system including when people are charged with crimes, go to a criminal trial, go to prison, or are a victim of a crime.</p> <p>Dataset: <code>mteb/LearnedHandsCrimeLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsdivorcelegalbenchclassification","title":"LearnedHandsDivorceLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues around filing for divorce, separation, or annulment, getting spousal support, splitting money and property, and following the court processes.</p> <p>Dataset: <code>mteb/LearnedHandsDivorceLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsdomesticviolencelegalbenchclassification","title":"LearnedHandsDomesticViolenceLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses dealing with domestic violence and abuse, including getting protective orders, enforcing them, understanding abuse, reporting abuse, and getting resources and status if there is abuse.</p> <p>Dataset: <code>mteb/LearnedHandsDomesticViolenceLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandseducationlegalbenchclassification","title":"LearnedHandsEducationLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues around school, including accommodations for special needs, discrimination, student debt, discipline, and other issues in education.</p> <p>Dataset: <code>mteb/LearnedHandsEducationLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsemploymentlegalbenchclassification","title":"LearnedHandsEmploymentLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues related to working at a job, including discrimination and harassment, worker's compensation, workers rights, unions, getting paid, pensions, being fired, and more.</p> <p>Dataset: <code>mteb/LearnedHandsEmploymentLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsestateslegalbenchclassification","title":"LearnedHandsEstatesLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses planning for end-of-life, possible incapacitation, and other special circumstances that would prevent a person from making decisions about their own well-being, finances, and property. This includes issues around wills, powers of attorney, advance directives, trusts, guardianships, conservatorships, and other estate issues that people and families deal with.</p> <p>Dataset: <code>mteb/LearnedHandsEstatesLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsfamilylegalbenchclassification","title":"LearnedHandsFamilyLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues that arise within a family, like divorce, adoption, name change, guardianship, domestic violence, child custody, and other issues.</p> <p>Dataset: <code>mteb/LearnedHandsFamilyLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandshealthlegalbenchclassification","title":"LearnedHandsHealthLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues with accessing health services, paying for medical care, getting public benefits for health care, protecting one's rights in medical settings, and other issues related to health.</p> <p>Dataset: <code>mteb/LearnedHandsHealthLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandshousinglegalbenchclassification","title":"LearnedHandsHousingLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues with paying your rent or mortgage, landlord-tenant issues, housing subsidies and public housing, eviction, and other problems with your apartment, mobile home, or house.</p> <p>Dataset: <code>mteb/LearnedHandsHousingLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsimmigrationlegalbenchclassification","title":"LearnedHandsImmigrationLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses visas, asylum, green cards, citizenship, migrant work and benefits, and other issues faced by people who are not full citizens in the US.</p> <p>Dataset: <code>mteb/LearnedHandsImmigrationLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandstortslegalbenchclassification","title":"LearnedHandsTortsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal question discusses problems that one person has with another person (or animal), like when there is a car accident, a dog bite, bullying or possible harassment, or neighbors treating each other badly.</p> <p>Dataset: <code>mteb/LearnedHandsTortsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandstrafficlegalbenchclassification","title":"LearnedHandsTrafficLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal post discusses problems with traffic and parking tickets, fees, driver's licenses, and other issues experienced with the traffic system. It also concerns issues with car accidents and injuries, cars' quality, repairs, purchases, and other contracts.</p> <p>Dataset: <code>mteb/LearnedHandsTrafficLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#legalreasoningcausalitylegalbenchclassification","title":"LegalReasoningCausalityLegalBenchClassification","text":"<p>Given an excerpt from a district court opinion, classify if it relies on statistical evidence in its reasoning.</p> <p>Dataset: <code>mteb/LegalReasoningCausalityLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#legalreasoningcausalitylegalbenchclassificationv2","title":"LegalReasoningCausalityLegalBenchClassification.v2","text":"<p>Given an excerpt from a district court opinion, classify if it relies on statistical evidence in its reasoning.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/legal_reasoning_causality_legal_bench</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#maudlegalbenchclassification","title":"MAUDLegalBenchClassification","text":"<p>This task was constructed from the MAUD dataset, which consists of over 47,000 labels across 152 merger agreements annotated to identify 92 questions in each agreement used by the 2021 American Bar Association (ABA) Public Target Deal Points Study. Each dataset is formatted as a series of multiple-choice questions, where given a segment of the merger agreement and a Deal Point question, the model is to choose the answer that best characterizes the agreement as response.</p> <pre><code>    This is a combination of all 34 of the MAUD Legal Bench datasets:\n    1. MAUD Ability To Consummate Concept Is Subject To MAE Carveouts: Given an excerpt from a merger agreement and the task is to answer: is the \u201cability to consummate\u201d concept subject to Material Adverse Effect (MAE) carveouts? amongst the multiple choice options.\n    2. MAUD Accuracy Of Fundamental Target RWS Bringdown Standard: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    3. MAUD Accuracy Of Target Capitalization RW Outstanding Shares Bringdown Standard Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    4. MAUD Accuracy Of Target General RW Bringdown Timing Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    5. MAUD Additional Matching Rights Period For Modifications Cor: Given an excerpt from a merger agreement and the task is to answer: how long is the additional matching rights period for modifications in case the board changes its recommendation, amongst the multiple choice options.\n    6. MAUD Application Of Buyer Consent Requirement Negative Interim Covenant: Given an excerpt from a merger agreement and the task is to answer: what negative covenants does the requirement of Buyer consent apply to, amongst the multiple choice options.\n    7. MAUD Buyer Consent Requirement Ordinary Course: Given an excerpt from a merger agreement and the task is to answer: in case the Buyer's consent for the acquired company's ordinary business operations is required, are there any limitations on the Buyer's right to condition, withhold, or delay their consent, amongst the multiple choice options.\n    8. MAUD Change In Law Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in law that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    9. MAUD Changes In GAAP Or Other Accounting Principles Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in GAAP or other accounting principles that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    10. MAUD COR Permitted In Response To Intervening Event: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted in response to an intervening event, amongst the multiple choice options.\n    11. MAUD COR Permitted With Board Fiduciary Determination Only: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted as long as the board determines that such change is required to fulfill its fiduciary obligations, amongst the multiple choice options.\n    12. MAUD COR Standard Intervening Event: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in response to an intervening event, amongst the multiple choice options.\n    13. MAUD COR Standard Superior Offer: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in connection with a superior offer, amongst the multiple choice options.\n    14. MAUD Definition Contains Knowledge Requirement Answer: Given an excerpt from a merger agreement and the task is to answer: what is the knowledge requirement in the definition of \u201cIntervening Event\u201d, amongst the multiple choice options.\n    15. MAUD Definition Includes Asset Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of asset deals, amongst the multiple choice options.\n    16. MAUD Definition Includes Stock Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of stock deals, amongst the multiple choice options.\n    17. MAUD Fiduciary Exception Board Determination Standard: Given an excerpt from a merger agreement and the task is to answer: under what circumstances could the Board take actions on a different acquisition proposal notwithstanding the no-shop provision, amongst the multiple choice options.\n    18. MAUD Fiduciary Exception Board Determination Trigger No Shop: Given an excerpt from a merger agreement and the task is to answer: what type of offer could the Board take actions on notwithstanding the no-shop provision, amongst the multiple choice options.\n    19. MAUD Financial Point Of View Is The Sole Consideration: Given an excerpt from a merger agreement and the task is to answer: is \u201cfinancial point of view\u201d the sole consideration when determining whether an offer is superior, amongst the multiple choice options.\n    20. MAUD FLS MAE Standard: Given an excerpt from a merger agreement and the task is to answer: what is the Forward Looking Standard (FLS) with respect to Material Adverse Effect (MAE), amongst the multiple choice options.\n    21. MAUD General Economic and Financial Conditions Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes caused by general economic and financial conditions that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    22. MAUD Includes Consistent With Past Practice: Given an excerpt from a merger agreement and the task is to answer: does the wording of the Efforts Covenant clause include \u201cconsistent with past practice\u201d, amongst the multiple choice options.\n    23. MAUD Initial Matching Rights Period COR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in case the board changes its recommendation, amongst the multiple choice options.\n    24. MAUD Initial Matching Rights Period FTR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in connection with the Fiduciary Termination Right (FTR), amongst the multiple choice options.\n    25. MAUDInterveningEventRequiredToOccurAfterSigningAnswer: Given an excerpt from a merger agreement and the task is to answer: is an \u201cIntervening Event\u201d required to occur after signing, amongst the multiple choice options.\n    26. MAUD Knowledge Definition: Given an excerpt from a merger agreement and the task is to answer: what counts as Knowledge, amongst the multiple choice options.\n    27. MAUDLiabilityStandardForNoShopBreachByTargetNonDORepresentatives: Given an excerpt from a merger agreement and the task is to answer:  what is the liability standard for no-shop breach by Target Non-D&amp;O Representatives, amongst the multiple choice options.\n    28. MAUD Ordinary Course Efforts Standard: Given an excerpt from a merger agreement and the task is to answer: what is the efforts standard, amongst the multiple choice options.\n    29. MAUD Pandemic Or Other Public Health Event Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do pandemics or other public health events have to have disproportionate impact to qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    30. MAUD Pandemic Or Other Public Health Event Specific Reference To Pandemic Related Governmental Responses Or Measures: Given an excerpt from a merger agreement and the task is to answer: is there specific reference to pandemic-related governmental responses or measures in the clause that qualifies pandemics or other public health events for Material Adverse Effect (MAE), amongst the multiple choice options.\n    31. MAUD Relational Language MAE Applies To: Given an excerpt from a merger agreement and the task is to answer: what carveouts pertaining to Material Adverse Effect (MAE) does the relational language apply to?, amongst the multiple choice options.\n    32. MAUD Specific Performance: Given an excerpt from a merger agreement and the task is to answer: what is the wording of the Specific Performance clause regarding the parties' entitlement in the event of a contractual breach, amongst the multiple choice options.\n    33. MAUD Tail Period Length: Given an excerpt from a merger agreement and the task is to answer: how long is the Tail Period, amongst the multiple choice options.\n    34. MAUD Type Of Consideration: Given an excerpt from a merger agreement and the task is to answer: what type of consideration is specified in this agreement, amongst the multiple choice options.\n</code></pre> <p>Dataset: <code>mteb/MAUDLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#maudlegalbenchclassificationv2","title":"MAUDLegalBenchClassification.v2","text":"<p>This task was constructed from the MAUD dataset, which consists of over 47,000 labels across 152 merger agreements annotated to identify 92 questions in each agreement used by the 2021 American Bar Association (ABA) Public Target Deal Points Study. Each dataset is formatted as a series of multiple-choice questions, where given a segment of the merger agreement and a Deal Point question, the model is to choose the answer that best characterizes the agreement as response.</p> <pre><code>    This is a combination of all 34 of the MAUD Legal Bench datasets:\n    1. MAUD Ability To Consummate Concept Is Subject To MAE Carveouts: Given an excerpt from a merger agreement and the task is to answer: is the \u201cability to consummate\u201d concept subject to Material Adverse Effect (MAE) carveouts? amongst the multiple choice options.\n    2. MAUD Accuracy Of Fundamental Target RWS Bringdown Standard: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    3. MAUD Accuracy Of Target Capitalization RW Outstanding Shares Bringdown Standard Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    4. MAUD Accuracy Of Target General RW Bringdown Timing Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options.\n    5. MAUD Additional Matching Rights Period For Modifications Cor: Given an excerpt from a merger agreement and the task is to answer: how long is the additional matching rights period for modifications in case the board changes its recommendation, amongst the multiple choice options.\n    6. MAUD Application Of Buyer Consent Requirement Negative Interim Covenant: Given an excerpt from a merger agreement and the task is to answer: what negative covenants does the requirement of Buyer consent apply to, amongst the multiple choice options.\n    7. MAUD Buyer Consent Requirement Ordinary Course: Given an excerpt from a merger agreement and the task is to answer: in case the Buyer's consent for the acquired company's ordinary business operations is required, are there any limitations on the Buyer's right to condition, withhold, or delay their consent, amongst the multiple choice options.\n    8. MAUD Change In Law Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in law that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    9. MAUD Changes In GAAP Or Other Accounting Principles Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in GAAP or other accounting principles that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    10. MAUD COR Permitted In Response To Intervening Event: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted in response to an intervening event, amongst the multiple choice options.\n    11. MAUD COR Permitted With Board Fiduciary Determination Only: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted as long as the board determines that such change is required to fulfill its fiduciary obligations, amongst the multiple choice options.\n    12. MAUD COR Standard Intervening Event: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in response to an intervening event, amongst the multiple choice options.\n    13. MAUD COR Standard Superior Offer: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in connection with a superior offer, amongst the multiple choice options.\n    14. MAUD Definition Contains Knowledge Requirement Answer: Given an excerpt from a merger agreement and the task is to answer: what is the knowledge requirement in the definition of \u201cIntervening Event\u201d, amongst the multiple choice options.\n    15. MAUD Definition Includes Asset Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of asset deals, amongst the multiple choice options.\n    16. MAUD Definition Includes Stock Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of stock deals, amongst the multiple choice options.\n    17. MAUD Fiduciary Exception Board Determination Standard: Given an excerpt from a merger agreement and the task is to answer: under what circumstances could the Board take actions on a different acquisition proposal notwithstanding the no-shop provision, amongst the multiple choice options.\n    18. MAUD Fiduciary Exception Board Determination Trigger No Shop: Given an excerpt from a merger agreement and the task is to answer: what type of offer could the Board take actions on notwithstanding the no-shop provision, amongst the multiple choice options.\n    19. MAUD Financial Point Of View Is The Sole Consideration: Given an excerpt from a merger agreement and the task is to answer: is \u201cfinancial point of view\u201d the sole consideration when determining whether an offer is superior, amongst the multiple choice options.\n    20. MAUD FLS MAE Standard: Given an excerpt from a merger agreement and the task is to answer: what is the Forward Looking Standard (FLS) with respect to Material Adverse Effect (MAE), amongst the multiple choice options.\n    21. MAUD General Economic and Financial Conditions Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes caused by general economic and financial conditions that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    22. MAUD Includes Consistent With Past Practice: Given an excerpt from a merger agreement and the task is to answer: does the wording of the Efforts Covenant clause include \u201cconsistent with past practice\u201d, amongst the multiple choice options.\n    23. MAUD Initial Matching Rights Period COR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in case the board changes its recommendation, amongst the multiple choice options.\n    24. MAUD Initial Matching Rights Period FTR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in connection with the Fiduciary Termination Right (FTR), amongst the multiple choice options.\n    25. MAUDInterveningEventRequiredToOccurAfterSigningAnswer: Given an excerpt from a merger agreement and the task is to answer: is an \u201cIntervening Event\u201d required to occur after signing, amongst the multiple choice options.\n    26. MAUD Knowledge Definition: Given an excerpt from a merger agreement and the task is to answer: what counts as Knowledge, amongst the multiple choice options.\n    27. MAUDLiabilityStandardForNoShopBreachByTargetNonDORepresentatives: Given an excerpt from a merger agreement and the task is to answer:  what is the liability standard for no-shop breach by Target Non-D&amp;O Representatives, amongst the multiple choice options.\n    28. MAUD Ordinary Course Efforts Standard: Given an excerpt from a merger agreement and the task is to answer: what is the efforts standard, amongst the multiple choice options.\n    29. MAUD Pandemic Or Other Public Health Event Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do pandemics or other public health events have to have disproportionate impact to qualify for Material Adverse Effect (MAE), amongst the multiple choice options.\n    30. MAUD Pandemic Or Other Public Health Event Specific Reference To Pandemic Related Governmental Responses Or Measures: Given an excerpt from a merger agreement and the task is to answer: is there specific reference to pandemic-related governmental responses or measures in the clause that qualifies pandemics or other public health events for Material Adverse Effect (MAE), amongst the multiple choice options.\n    31. MAUD Relational Language MAE Applies To: Given an excerpt from a merger agreement and the task is to answer: what carveouts pertaining to Material Adverse Effect (MAE) does the relational language apply to?, amongst the multiple choice options.\n    32. MAUD Specific Performance: Given an excerpt from a merger agreement and the task is to answer: what is the wording of the Specific Performance clause regarding the parties' entitlement in the event of a contractual breach, amongst the multiple choice options.\n    33. MAUD Tail Period Length: Given an excerpt from a merger agreement and the task is to answer: how long is the Tail Period, amongst the multiple choice options.\n    34. MAUD Type Of Consideration: Given an excerpt from a merger agreement and the task is to answer: what type of consideration is specified in this agreement, amongst the multiple choice options.\n\n    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/maud_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopdomainclassification","title":"MTOPDomainClassification","text":"<p>MTOP: Multilingual Task-Oriented Semantic Parsing</p> <p>Dataset: <code>mteb/MTOPDomainClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, fra, hin, spa, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@inproceedings{li-etal-2021-mtop,\n  address = {Online},\n  author = {Li, Haoran  and\nArora, Abhinav  and\nChen, Shuohui  and\nGupta, Anchit  and\nGupta, Sonal  and\nMehdad, Yashar},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  doi = {10.18653/v1/2021.eacl-main.257},\n  editor = {Merlo, Paola  and\nTiedemann, Jorg  and\nTsarfaty, Reut},\n  month = apr,\n  pages = {2950--2962},\n  publisher = {Association for Computational Linguistics},\n  title = {{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark},\n  url = {https://aclanthology.org/2021.eacl-main.257},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopdomainvnclassification","title":"MTOPDomainVNClassification","text":"<p>A translated dataset from MTOP: Multilingual Task-Oriented Semantic Parsing             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/mtop-domain-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken, Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopintentclassification","title":"MTOPIntentClassification","text":"<p>MTOP: Multilingual Task-Oriented Semantic Parsing</p> <p>Dataset: <code>mteb/MTOPIntentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, fra, hin, spa, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@inproceedings{li-etal-2021-mtop,\n  address = {Online},\n  author = {Li, Haoran  and\nArora, Abhinav  and\nChen, Shuohui  and\nGupta, Anchit  and\nGupta, Sonal  and\nMehdad, Yashar},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  doi = {10.18653/v1/2021.eacl-main.257},\n  editor = {Merlo, Paola  and\nTiedemann, Jorg  and\nTsarfaty, Reut},\n  month = apr,\n  pages = {2950--2962},\n  publisher = {Association for Computational Linguistics},\n  title = {{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark},\n  url = {https://aclanthology.org/2021.eacl-main.257},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopintentvnclassification","title":"MTOPIntentVNClassification","text":"<p>A translated dataset from MTOP: Multilingual Task-Oriented Semantic Parsing             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/mtop-intent-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken, Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#macedoniantweetsentimentclassification","title":"MacedonianTweetSentimentClassification","text":"<p>An Macedonian dataset for tweet sentiment classification.</p> <p>Dataset: <code>isaacchung/macedonian-tweet-sentiment-classification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mkd Social, Written human-annotated found Citation <pre><code>@inproceedings{jovanoski-etal-2015-sentiment,\n  address = {Hissar, Bulgaria},\n  author = {Jovanoski, Dame  and\nPachovski, Veno  and\nNakov, Preslav},\n  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},\n  editor = {Mitkov, Ruslan  and\nAngelova, Galia  and\nBontcheva, Kalina},\n  month = sep,\n  pages = {249--257},\n  publisher = {INCOMA Ltd. Shoumen, BULGARIA},\n  title = {Sentiment Analysis in {T}witter for {M}acedonian},\n  url = {https://aclanthology.org/R15-1034},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#macedoniantweetsentimentclassificationv2","title":"MacedonianTweetSentimentClassification.v2","text":"<p>An Macedonian dataset for tweet sentiment classification.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/macedonian_tweet_sentiment</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mkd Social, Written human-annotated found Citation <pre><code>@inproceedings{jovanoski-etal-2015-sentiment,\n  address = {Hissar, Bulgaria},\n  author = {Jovanoski, Dame  and\nPachovski, Veno  and\nNakov, Preslav},\n  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},\n  editor = {Mitkov, Ruslan  and\nAngelova, Galia  and\nBontcheva, Kalina},\n  month = sep,\n  pages = {249--257},\n  publisher = {INCOMA Ltd. Shoumen, BULGARIA},\n  title = {Sentiment Analysis in {T}witter for {M}acedonian},\n  url = {https://aclanthology.org/R15-1034},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#malayalamnewsclassification","title":"MalayalamNewsClassification","text":"<p>A Malayalam dataset for 3-class classification of Malayalam news articles</p> <p>Dataset: <code>mlexplorer008/malayalam_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mal News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#malayalamnewsclassificationv2","title":"MalayalamNewsClassification.v2","text":"<p>A Malayalam dataset for 3-class classification of Malayalam news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/malayalam_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mal News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#marathinewsclassification","title":"MarathiNewsClassification","text":"<p>A Marathi dataset for 3-class classification of Marathi news articles</p> <p>Dataset: <code>mlexplorer008/marathi_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 mar News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#marathinewsclassificationv2","title":"MarathiNewsClassification.v2","text":"<p>A Marathi dataset for 3-class classification of Marathi news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/marathi_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 mar News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#masakhanewsclassification","title":"MasakhaNEWSClassification","text":"<p>MasakhaNEWS is the largest publicly available dataset for news topic classification in 16 languages widely spoken in Africa. The train/validation/test sets are available for all the 16 languages.</p> <p>Dataset: <code>mteb/masakhanews</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, eng, fra, hau, ibo, ... (16) News, Written expert-annotated found Citation <pre><code>@misc{adelani2023masakhanews,\n  archiveprefix = {arXiv},\n  author = {David Ifeoluwa Adelani and Marek Masiak and Israel Abebe Azime and Jesujoba Alabi and Atnafu Lambebo Tonja and Christine Mwase and Odunayo Ogundepo and Bonaventure F. P. Dossou and Akintunde Oladipo and Doreen Nixdorf and Chris Chinenye Emezue and sana al-azzawi and Blessing Sibanda and Davis David and Lolwethu Ndolela and Jonathan Mukiibi and Tunde Ajayi and Tatiana Moteu and Brian Odhiambo and Abraham Owodunni and Nnaemeka Obiefuna and Muhidin Mohamed and Shamsuddeen Hassan Muhammad and Teshome Mulugeta Ababu and Saheed Abdullahi Salahudeen and Mesay Gemeda Yigezu and Tajuddeen Gwadabe and Idris Abdulmumin and Mahlet Taye and Oluwabusayo Awoyomi and Iyanuoluwa Shode and Tolulope Adelani and Habiba Abdulganiyu and Abdul-Hakeem Omotayo and Adetola Adeeko and Abeeb Afolabi and Anuoluwapo Aremu and Olanrewaju Samuel and Clemencia Siro and Wangari Kimotho and Onyekachi Ogbu and Chinedu Mbonu and Chiamaka Chukwuneke and Samuel Fanijo and Jessica Ojo and Oyinkansola Awosan and Tadesse Kebede and Toadoum Sari Sakayo and Pamela Nyatsine and Freedmore Sidume and Oreen Yousuf and Mardiyyah Oduwole and Tshinu Tshinu and Ussen Kimanuka and Thina Diko and Siyanda Nxakama and Sinodos Nigusse and Abdulmejid Johar and Shafie Mohamed and Fuad Mire Hassan and Moges Ahmed Mehamed and Evrard Ngabire and Jules Jules and Ivan Ssenkungu and Pontus Stenetorp},\n  eprint = {2304.09972},\n  primaryclass = {cs.CL},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massiveintentclassification","title":"MassiveIntentClassification","text":"<p>MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</p> <p>Dataset: <code>mteb/amazon_massive_intent</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, amh, ara, aze, ben, ... (50) Spoken human-annotated human-translated and localized Citation <pre><code>@misc{fitzgerald2022massive,\n  archiveprefix = {arXiv},\n  author = {Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},\n  eprint = {2204.08582},\n  primaryclass = {cs.CL},\n  title = {MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massiveintentvnclassification","title":"MassiveIntentVNClassification","text":"<p>A translated dataset from MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-massive-intent-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massivescenarioclassification","title":"MassiveScenarioClassification","text":"<p>MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</p> <p>Dataset: <code>mteb/amazon_massive_scenario</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, amh, ara, aze, ben, ... (50) Spoken human-annotated human-translated and localized Citation <pre><code>@misc{fitzgerald2022massive,\n  archiveprefix = {arXiv},\n  author = {Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},\n  eprint = {2204.08582},\n  primaryclass = {cs.CL},\n  title = {MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massivescenariovnclassification","title":"MassiveScenarioVNClassification","text":"<p>A translated dataset from MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-massive-scenario-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moroco","title":"Moroco","text":"<p>The Moldavian and Romanian Dialectal Corpus. The MOROCO data set contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: (0) culture, (1) finance, (2) politics, (3) science, (4) sports, (5) tech</p> <p>Dataset: <code>mteb/Moroco</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron News, Written derived found Citation <pre><code>@inproceedings{Butnaru-ACL-2019,\n  author = {Andrei M. Butnaru and Radu Tudor Ionescu},\n  booktitle = {Proceedings of ACL},\n  pages = {688--698},\n  title = {{MOROCO: The Moldavian and Romanian Dialectal Corpus}},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#morocov2","title":"Moroco.v2","text":"<p>The Moldavian and Romanian Dialectal Corpus. The MOROCO data set contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: (0) culture, (1) finance, (2) politics, (3) science, (4) sports, (5) tech         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/moroco</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron News, Written derived found Citation <pre><code>@inproceedings{Butnaru-ACL-2019,\n  author = {Andrei M. Butnaru and Radu Tudor Ionescu},\n  booktitle = {Proceedings of ACL},\n  pages = {688--698},\n  title = {{MOROCO: The Moldavian and Romanian Dialectal Corpus}},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moviereviewsentimentclassification","title":"MovieReviewSentimentClassification","text":"<p>The Allocin\u00e9 dataset is a French-language dataset for sentiment analysis that contains movie reviews produced by the online community of the Allocin\u00e9.fr website.</p> <p>Dataset: <code>tblard/allocine</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>@software{blard2020,\n  author = {Th\u00e9ophile Blard},\n  title = {French sentiment analysis with BERT},\n  url = {https://github.com/TheophileBlard/french-sentiment-analysis-with-bert},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moviereviewsentimentclassificationv2","title":"MovieReviewSentimentClassification.v2","text":"<p>The Allocin\u00e9 dataset is a French-language dataset for sentiment analysis that contains movie reviews produced by the online community of the Allocin\u00e9.fr website.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/movie_review_sentiment</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>@software{blard2020,\n  author = {Th\u00e9ophile Blard},\n  title = {French sentiment analysis with BERT},\n  url = {https://github.com/TheophileBlard/french-sentiment-analysis-with-bert},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multihateclassification","title":"MultiHateClassification","text":"<p>Hate speech detection dataset with binary                        (hateful vs non-hateful) labels. Includes 25+ distinct types of hate                        and challenging non-hate, and 11 languages.</p> <p>Dataset: <code>mteb/multi-hatecheck</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, cmn, deu, eng, fra, ... (11) Constructed, Written expert-annotated created Citation <pre><code>@inproceedings{rottger-etal-2021-hatecheck,\n  address = {Online},\n  author = {R{\\\"o}ttger, Paul  and\nVidgen, Bertie  and\nNguyen, Dong  and\nWaseem, Zeerak  and\nMargetts, Helen  and\nPierrehumbert, Janet},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.4},\n  editor = {Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto},\n  month = aug,\n  pages = {41--58},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}ate{C}heck: Functional Tests for Hate Speech Detection Models},\n  url = {https://aclanthology.org/2021.acl-long.4},\n  year = {2021},\n}\n\n@inproceedings{rottger-etal-2022-multilingual,\n  address = {Seattle, Washington (Hybrid)},\n  author = {R{\\\"o}ttger, Paul  and\nSeelawi, Haitham  and\nNozza, Debora  and\nTalat, Zeerak  and\nVidgen, Bertie},\n  booktitle = {Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)},\n  doi = {10.18653/v1/2022.woah-1.15},\n  editor = {Narang, Kanika  and\nMostafazadeh Davani, Aida  and\nMathias, Lambert  and\nVidgen, Bertie  and\nTalat, Zeerak},\n  month = jul,\n  pages = {154--169},\n  publisher = {Association for Computational Linguistics},\n  title = {Multilingual {H}ate{C}heck: Functional Tests for Multilingual Hate Speech Detection Models},\n  url = {https://aclanthology.org/2022.woah-1.15},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multilingualsentiment","title":"MultilingualSentiment","text":"<p>A collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative</p> <p>Dataset: <code>C-MTEB/MultilingualSentiment-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified"},{"location":"overview/available_tasks/classification/#multilingualsentimentv2","title":"MultilingualSentiment.v2","text":"<p>A collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/multilingual_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified"},{"location":"overview/available_tasks/classification/#multilingualsentimentclassification","title":"MultilingualSentimentClassification","text":"<p>Sentiment classification dataset with binary (positive vs negative sentiment) labels. Includes 30 languages and dialects.</p> <p>Dataset: <code>mteb/multilingual-sentiment-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, bam, bul, cmn, cym, ... (31) Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#myanmarnews","title":"MyanmarNews","text":"<p>The Myanmar News dataset on Hugging Face contains news articles in Burmese. It is designed for tasks such as text classification, sentiment analysis, and language modeling. The dataset includes a variety of news topics in 4 categories, providing a rich resource for natural language processing applications involving Burmese which is a low resource language.</p> <p>Dataset: <code>mteb/MyanmarNews</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mya News, Written derived found Citation <pre><code>@inproceedings{Khine2017,\n  author = {A. H. Khine and K. T. Nwet and K. M. Soe},\n  booktitle = {15th Proceedings of International Conference on Computer Applications},\n  month = {February},\n  pages = {401--408},\n  title = {Automatic Myanmar News Classification},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#myanmarnewsv2","title":"MyanmarNews.v2","text":"<p>The Myanmar News dataset on Hugging Face contains news articles in Burmese. It is designed for tasks such as text classification, sentiment analysis, and language modeling. The dataset includes a variety of news topics in 4 categories, providing a rich resource for natural language processing applications involving Burmese which is a low resource language.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/myanmar_news</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mya News, Written derived found Citation <pre><code>@inproceedings{Khine2017,\n  author = {A. H. Khine and K. T. Nwet and K. M. Soe},\n  booktitle = {15th Proceedings of International Conference on Computer Applications},\n  month = {February},\n  pages = {401--408},\n  title = {Automatic Myanmar News Classification},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nlptwitteranalysisclassification","title":"NLPTwitterAnalysisClassification","text":"<p>Twitter Analysis Classification</p> <p>Dataset: <code>hamedhf/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#nlptwitteranalysisclassificationv2","title":"NLPTwitterAnalysisClassification.v2","text":"<p>Twitter Analysis Classification         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#nysjudicialethicslegalbenchclassification","title":"NYSJudicialEthicsLegalBenchClassification","text":"<p>Answer questions on judicial ethics from the New York State Unified Court System Advisory Committee.</p> <p>Dataset: <code>mteb/NYSJudicialEthicsLegalBenchClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#naijasenti","title":"NaijaSenti","text":"<p>NaijaSenti is the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken languages in Nigeria \u2014 Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1 \u2014 consisting of around 30,000 annotated tweets per language, including a significant fraction of code-mixed tweets.</p> <p>Dataset: <code>mteb/NaijaSenti</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hau, ibo, pcm, yor Social, Written expert-annotated found Citation <pre><code>@inproceedings{muhammad-etal-2022-naijasenti,\n  address = {Marseille, France},\n  author = {Muhammad, Shamsuddeen Hassan  and\nAdelani, David Ifeoluwa  and\nRuder, Sebastian  and\nAhmad, Ibrahim Sa{'}id  and\nAbdulmumin, Idris  and\nBello, Bello Shehu  and\nChoudhury, Monojit  and\nEmezue, Chris Chinenye  and\nAbdullahi, Saheed Salahudeen  and\nAremu, Anuoluwapo  and\nJorge, Al{\\'\\i}pio  and\nBrazdil, Pavel},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {590--602},\n  publisher = {European Language Resources Association},\n  title = {{N}aija{S}enti: A {N}igerian {T}witter Sentiment Corpus for Multilingual Sentiment Analysis},\n  url = {https://aclanthology.org/2022.lrec-1.63},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nepalinewsclassification","title":"NepaliNewsClassification","text":"<p>A Nepali dataset for 7500 news articles </p> <p>Dataset: <code>bpHigh/iNLTK_Nepali_News_Dataset</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nep News, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nepalinewsclassificationv2","title":"NepaliNewsClassification.v2","text":"<p>A Nepali dataset for 7500 news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/nepali_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nep News, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#newsclassification","title":"NewsClassification","text":"<p>Large News Classification Dataset</p> <p>Dataset: <code>fancyzhx/ag_news</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Written expert-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#newsclassificationv2","title":"NewsClassification.v2","text":"<p>Large News Classification Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/news</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Written expert-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norecclassification","title":"NoRecClassification","text":"<p>A Norwegian dataset for sentiment classification on review</p> <p>Dataset: <code>mteb/norec_classification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Reviews, Written derived found Citation <pre><code>@inproceedings{velldal-etal-2018-norec,\n  address = {Miyazaki, Japan},\n  author = {Velldal, Erik  and\n{\\\\O}vrelid, Lilja  and\nBergem, Eivind Alexander  and\nStadsnes, Cathrine  and\nTouileb, Samia  and\nJ{\\\\o}rgensen, Fredrik},\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nHasida, Koiti  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios  and\nTokunaga, Takenobu},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {{N}o{R}e{C}: The {N}orwegian Review Corpus},\n  url = {https://aclanthology.org/L18-1661},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norecclassificationv2","title":"NoRecClassification.v2","text":"<p>A Norwegian dataset for sentiment classification on review         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/no_rec</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Reviews, Written derived found Citation <pre><code>@inproceedings{velldal-etal-2018-norec,\n  address = {Miyazaki, Japan},\n  author = {Velldal, Erik  and\n{\\\\O}vrelid, Lilja  and\nBergem, Eivind Alexander  and\nStadsnes, Cathrine  and\nTouileb, Samia  and\nJ{\\\\o}rgensen, Fredrik},\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nHasida, Koiti  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios  and\nTokunaga, Takenobu},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {{N}o{R}e{C}: The {N}orwegian Review Corpus},\n  url = {https://aclanthology.org/L18-1661},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nordiclangclassification","title":"NordicLangClassification","text":"<p>A dataset for Nordic language identification.</p> <p>Dataset: <code>mteb/NordicLangClassification</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, fao, isl, nno, nob, ... (6) Encyclopaedic derived found Citation <pre><code>@inproceedings{haas-derczynski-2021-discriminating,\n  address = {Kiyv, Ukraine},\n  author = {Haas, Ren{\\'e}  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects},\n  editor = {Zampieri, Marcos  and\nNakov, Preslav  and\nLjube{\\v{s}}i{\\'c}, Nikola  and\nTiedemann, J{\\\"o}rg  and\nScherrer, Yves  and\nJauhiainen, Tommi},\n  month = apr,\n  pages = {67--75},\n  publisher = {Association for Computational Linguistics},\n  title = {Discriminating Between Similar {N}ordic Languages},\n  url = {https://aclanthology.org/2021.vardial-1.8},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norwegianparliamentclassification","title":"NorwegianParliamentClassification","text":"<p>Norwegian parliament speeches annotated for sentiment</p> <p>Dataset: <code>mteb/NorwegianParliamentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Government, Spoken derived found Citation <pre><code>@inproceedings{kummervold-etal-2021-operationalizing,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kummervold, Per E  and\nDe la Rosa, Javier  and\nWetjen, Freddy  and\nBrygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {20--29},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  url = {https://aclanthology.org/2021.nodalida-main.3},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norwegianparliamentclassificationv2","title":"NorwegianParliamentClassification.v2","text":"<p>Norwegian parliament speeches annotated for sentiment         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/norwegian_parliament</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Government, Spoken derived found Citation <pre><code>@inproceedings{kummervold-etal-2021-operationalizing,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kummervold, Per E  and\nDe la Rosa, Javier  and\nWetjen, Freddy  and\nBrygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {20--29},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  url = {https://aclanthology.org/2021.nodalida-main.3},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusaparagraphemotionclassification","title":"NusaParagraphEmotionClassification","text":"<p>NusaParagraphEmotionClassification is a multi-class emotion classification on 10 Indonesian languages from the NusaParagraph dataset.</p> <p>Dataset: <code>gentaiscool/nusaparagraph_emot</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 bbc, bew, bug, jav, mad, ... (10) Fiction, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{cahyawijaya-etal-2023-nusawrites,\n  address = {Nusa Dua, Bali},\n  author = {Cahyawijaya, Samuel  and  Lovenia, Holy  and Koto, Fajri  and  Adhista, Dea  and  Dave, Emmanuel  and  Oktavianti, Sarah  and  Akbar, Salsabil  and  Lee, Jhonson  and  Shadieq, Nuur  and  Cenggoro, Tjeng Wawan  and  Linuwih, Hanung  and  Wilie, Bryan  and  Muridan, Galih  and  Winata, Genta  and  Moeljadi, David  and  Aji, Alham Fikri  and  Purwarianti, Ayu  and  Fung, Pascale},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  editor = {Park, Jong C.  and  Arase, Yuki  and  Hu, Baotian  and  Lu, Wei  and  Wijaya, Derry  and  Purwarianti, Ayu  and  Krisnadhi, Adila Alfa},\n  month = nov,\n  pages = {921--945},\n  publisher = {Association for Computational Linguistics},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  url = {https://aclanthology.org/2023.ijcnlp-main.60},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusaparagraphtopicclassification","title":"NusaParagraphTopicClassification","text":"<p>NusaParagraphTopicClassification is a multi-class topic classification on 10 Indonesian languages.</p> <p>Dataset: <code>gentaiscool/nusaparagraph_topic</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 bbc, bew, bug, jav, mad, ... (10) Fiction, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{cahyawijaya-etal-2023-nusawrites,\n  address = {Nusa Dua, Bali},\n  author = {Cahyawijaya, Samuel  and  Lovenia, Holy  and Koto, Fajri  and  Adhista, Dea  and  Dave, Emmanuel  and  Oktavianti, Sarah  and  Akbar, Salsabil  and  Lee, Jhonson  and  Shadieq, Nuur  and  Cenggoro, Tjeng Wawan  and  Linuwih, Hanung  and  Wilie, Bryan  and  Muridan, Galih  and  Winata, Genta  and  Moeljadi, David  and  Aji, Alham Fikri  and  Purwarianti, Ayu  and  Fung, Pascale},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  editor = {Park, Jong C.  and  Arase, Yuki  and  Hu, Baotian  and  Lu, Wei  and  Wijaya, Derry  and  Purwarianti, Ayu  and  Krisnadhi, Adila Alfa},\n  month = nov,\n  pages = {921--945},\n  publisher = {Association for Computational Linguistics},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  url = {https://aclanthology.org/2023.ijcnlp-main.60},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusax-senti","title":"NusaX-senti","text":"<p>NusaX is a high-quality multilingual parallel corpus that covers 12 languages, Indonesian, English, and 10 Indonesian local languages, namely Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Ngaju, Sundanese, and Toba Batak. NusaX-Senti is a 3-labels (positive, neutral, negative) sentiment analysis dataset for 10 Indonesian local languages + Indonesian and English.</p> <p>Dataset: <code>mteb/NusaX-senti</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ace, ban, bbc, bjn, bug, ... (12) Constructed, Reviews, Social, Web, Written expert-annotated found Citation <pre><code>@misc{winata2022nusax,\n  archiveprefix = {arXiv},\n  author = {Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya,\nSamuel and Mahendra, Rahmad and Koto, Fajri and Romadhony,\nAde and Kurniawan, Kemal and Moeljadi, David and Prasojo,\nRadityo Eko and Fung, Pascale and Baldwin, Timothy and Lau,\nJey Han and Sennrich, Rico and Ruder, Sebastian},\n  eprint = {2205.15960},\n  primaryclass = {cs.CL},\n  title = {NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115dataretentionlegalbenchclassification","title":"OPP115DataRetentionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how long user information is stored.</p> <p>Dataset: <code>mteb/OPP115DataRetentionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115datasecuritylegalbenchclassification","title":"OPP115DataSecurityLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how user information is protected.</p> <p>Dataset: <code>mteb/OPP115DataSecurityLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115datasecuritylegalbenchclassificationv2","title":"OPP115DataSecurityLegalBenchClassification.v2","text":"<p>Given a clause from a privacy policy, classify if the clause describes how user information is protected.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_data_security_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115donottracklegalbenchclassification","title":"OPP115DoNotTrackLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how Do Not Track signals for online tracking and advertising are honored.</p> <p>Dataset: <code>mteb/OPP115DoNotTrackLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115donottracklegalbenchclassificationv2","title":"OPP115DoNotTrackLegalBenchClassification.v2","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how Do Not Track signals for online tracking and advertising are honored.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_do_not_track_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115firstpartycollectionuselegalbenchclassification","title":"OPP115FirstPartyCollectionUseLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how and why a service provider collects user information.</p> <p>Dataset: <code>mteb/OPP115FirstPartyCollectionUseLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115internationalandspecificaudienceslegalbenchclassification","title":"OPP115InternationalAndSpecificAudiencesLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describe practices that pertain only to a specific group of users (e.g., children, Europeans, or California residents).</p> <p>Dataset: <code>mteb/OPP115InternationalAndSpecificAudiencesLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115policychangelegalbenchclassification","title":"OPP115PolicyChangeLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how users will be informed about changes to the privacy policy.</p> <p>Dataset: <code>mteb/OPP115PolicyChangeLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115thirdpartysharingcollectionlegalbenchclassification","title":"OPP115ThirdPartySharingCollectionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describe how user information may be shared with or collected by third parties.</p> <p>Dataset: <code>mteb/OPP115ThirdPartySharingCollectionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115useraccesseditanddeletionlegalbenchclassification","title":"OPP115UserAccessEditAndDeletionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how users may access, edit, or delete their information.</p> <p>Dataset: <code>mteb/OPP115UserAccessEditAndDeletionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115userchoicecontrollegalbenchclassification","title":"OPP115UserChoiceControlLegalBenchClassification","text":"<p>Given a clause fro ma privacy policy, classify if the clause describes the choices and control options available to users.</p> <p>Dataset: <code>mteb/OPP115UserChoiceControlLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115userchoicecontrollegalbenchclassificationv2","title":"OPP115UserChoiceControlLegalBenchClassification.v2","text":"<p>Given a clause fro ma privacy policy, classify if the clause describes the choices and control options available to users.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_user_choice_control_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#odianewsclassification","title":"OdiaNewsClassification","text":"<p>A Odia dataset for 3-class classification of Odia news articles</p> <p>Dataset: <code>mlexplorer008/odia_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ory News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#odianewsclassificationv2","title":"OdiaNewsClassification.v2","text":"<p>A Odia dataset for 3-class classification of Odia news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/odia_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ory News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#onlineshopping","title":"OnlineShopping","text":"<p>Sentiment Analysis of User Reviews on Online Shopping Websites</p> <p>Dataset: <code>C-MTEB/OnlineShopping-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@article{xiao2023c,\n  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},\n  journal = {arXiv preprint arXiv:2309.07597},\n  title = {C-pack: Packaged resources to advance general chinese embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#onlinestorereviewsentimentclassification","title":"OnlineStoreReviewSentimentClassification","text":"<p>This dataset contains Arabic reviews of products from the SHEIN online store.</p> <p>Dataset: <code>Ruqiya/Arabic_Reviews_of_SHEIN</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#onlinestorereviewsentimentclassificationv2","title":"OnlineStoreReviewSentimentClassification.v2","text":"<p>This dataset contains Arabic reviews of products from the SHEIN online store.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/online_store_review_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#opentenderclassification","title":"OpenTenderClassification","text":"<p>This dataset contains Belgian and Dutch tender calls from OpenTender in Dutch</p> <p>Dataset: <code>clips/mteb-nl-opentender-cls-pr</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Government, Written human-annotated found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#oralargumentquestionpurposelegalbenchclassification","title":"OralArgumentQuestionPurposeLegalBenchClassification","text":"<p>This task classifies questions asked by Supreme Court justices at oral argument into seven categories:         1. Background - questions seeking factual or procedural information that is missing or not clear in the briefing         2. Clarification - questions seeking to get an advocate to clarify her position or the scope of the rule being advocated for         3. Implications - questions about the limits of a rule or its implications for future cases         4. Support - questions offering support for the advocate\u2019s position         5. Criticism - questions criticizing an advocate\u2019s position         6. Communicate - question designed primarily to communicate with other justices         7. Humor - questions designed to interject humor into the argument and relieve tension</p> <p>Dataset: <code>mteb/OralArgumentQuestionPurposeLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#oralargumentquestionpurposelegalbenchclassificationv2","title":"OralArgumentQuestionPurposeLegalBenchClassification.v2","text":"<p>This task classifies questions asked by Supreme Court justices at oral argument into seven categories:         1. Background - questions seeking factual or procedural information that is missing or not clear in the briefing         2. Clarification - questions seeking to get an advocate to clarify her position or the scope of the rule being advocated for         3. Implications - questions about the limits of a rule or its implications for future cases         4. Support - questions offering support for the advocate\u2019s position         5. Criticism - questions criticizing an advocate\u2019s position         6. Communicate - question designed primarily to communicate with other justices         7. Humor - questions designed to interject humor into the argument and relieve tension</p> <pre><code>    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/oral_argument_question_purpose_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#overrulinglegalbenchclassification","title":"OverrulingLegalBenchClassification","text":"<p>This task consists of classifying whether or not a particular sentence of case law overturns the decision of a previous case.</p> <p>Dataset: <code>mteb/OverrulingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#overrulinglegalbenchclassificationv2","title":"OverrulingLegalBenchClassification.v2","text":"<p>This task consists of classifying whether or not a particular sentence of case law overturns the decision of a previous case.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/overruling_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pac","title":"PAC","text":"<p>Polish Paraphrase Corpus</p> <p>Dataset: <code>mteb/PAC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Legal, Written not specified not specified Citation <pre><code>@misc{augustyniak2022waydesigningcompilinglepiszcze,\n  archiveprefix = {arXiv},\n  author = {\u0141ukasz Augustyniak and Kamil Tagowski and Albert Sawczyn and Denis Janiak and Roman Bartusiak and Adrian Szymczak and Marcin W\u0105troba and Arkadiusz Janz and Piotr Szyma\u0144ski and Miko\u0142aj Morzy and Tomasz Kajdanowicz and Maciej Piasecki},\n  eprint = {2211.13112},\n  primaryclass = {cs.CL},\n  title = {This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish},\n  url = {https://arxiv.org/abs/2211.13112},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pacv2","title":"PAC.v2","text":"<p>Polish Paraphrase Corpus         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/pac</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Legal, Written not specified not specified Citation <pre><code>@misc{augustyniak2022waydesigningcompilinglepiszcze,\n  archiveprefix = {arXiv},\n  author = {\u0141ukasz Augustyniak and Kamil Tagowski and Albert Sawczyn and Denis Janiak and Roman Bartusiak and Adrian Szymczak and Marcin W\u0105troba and Arkadiusz Janz and Piotr Szyma\u0144ski and Miko\u0142aj Morzy and Tomasz Kajdanowicz and Maciej Piasecki},\n  eprint = {2211.13112},\n  primaryclass = {cs.CL},\n  title = {This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish},\n  url = {https://arxiv.org/abs/2211.13112},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#proalegalbenchclassification","title":"PROALegalBenchClassification","text":"<p>Given a statute, determine if the text contains an explicit private right of action. Given a privacy policy clause and a description of the clause, determine if the description is correct. A private right of action (PROA) exists when a statute empowers an ordinary individual (i.e., a private person) to legally enforce their rights by bringing an action in court. In short, a PROA creates the ability for an individual to sue someone in order to recover damages or halt some offending conduct. PROAs are ubiquitous in antitrust law (in which individuals harmed by anti-competitive behavior can sue offending firms for compensation) and environmental law (in which individuals can sue entities which release hazardous substances for damages).</p> <p>Dataset: <code>mteb/PROALegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#patentclassification","title":"PatentClassification","text":"<p>Classification Dataset of Patents and Abstract</p> <p>Dataset: <code>mteb/PatentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written derived found Citation <pre><code>@inproceedings{sharma-etal-2019-bigpatent,\n  address = {Florence, Italy},\n  author = {Sharma, Eva  and\nLi, Chen  and\nWang, Lu},\n  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/P19-1212},\n  editor = {Korhonen, Anna  and\nTraum, David  and\nM{\\`a}rquez, Llu{\\'\\i}s},\n  month = jul,\n  pages = {2204--2213},\n  publisher = {Association for Computational Linguistics},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {https://aclanthology.org/P19-1212},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#patentclassificationv2","title":"PatentClassification.v2","text":"<p>Classification Dataset of Patents and Abstract         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/patent</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written derived found Citation <pre><code>@inproceedings{sharma-etal-2019-bigpatent,\n  address = {Florence, Italy},\n  author = {Sharma, Eva  and\nLi, Chen  and\nWang, Lu},\n  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/P19-1212},\n  editor = {Korhonen, Anna  and\nTraum, David  and\nM{\\`a}rquez, Llu{\\'\\i}s},\n  month = jul,\n  pages = {2204--2213},\n  publisher = {Association for Computational Linguistics},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {https://aclanthology.org/P19-1212},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pershopdomainclassification","title":"PerShopDomainClassification","text":"<p>PerSHOP - A Persian dataset for shopping dialogue systems modeling</p> <p>Dataset: <code>MCINext/pershop-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken human-annotated created Citation <pre><code>@article{mahmoudi2024pershop,\n  author = {Mahmoudi, Keyvan and Faili, Heshaam},\n  journal = {arXiv preprint arXiv:2401.00811},\n  title = {PerSHOP--A Persian dataset for shopping dialogue systems modeling},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pershopintentclassification","title":"PerShopIntentClassification","text":"<p>PerSHOP - A Persian dataset for shopping dialogue systems modeling</p> <p>Dataset: <code>MCINext/pershop-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken human-annotated created Citation <pre><code>@article{mahmoudi2024pershop,\n  author = {Mahmoudi, Keyvan and Faili, Heshaam},\n  journal = {arXiv preprint arXiv:2401.00811},\n  title = {PerSHOP--A Persian dataset for shopping dialogue systems modeling},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#persianfoodsentimentclassification","title":"PersianFoodSentimentClassification","text":"<p>Persian Food Review Dataset</p> <p>Dataset: <code>asparius/Persian-Food-Sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews, Written derived found Citation <pre><code>@article{ParsBERT,\n  author = {Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},\n  journal = {ArXiv},\n  title = {ParsBERT: Transformer-based Model for Persian Language Understanding},\n  volume = {abs/2005.12515},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#persiantextemotion","title":"PersianTextEmotion","text":"<p>Emotion is a Persian dataset with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>SeyedAli/Persian-Text-Emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#persiantextemotionv2","title":"PersianTextEmotion.v2","text":"<p>Emotion is a Persian dataset with six basic emotions: anger, fear, joy, love, sadness, and surprise.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/persian_text_emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#personaljurisdictionlegalbenchclassification","title":"PersonalJurisdictionLegalBenchClassification","text":"<p>Given a fact pattern describing the set of contacts between a plaintiff, defendant, and forum, determine if a court in that forum could exercise personal jurisdiction over the defendant.</p> <p>Dataset: <code>mteb/PersonalJurisdictionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#poemsentimentclassification","title":"PoemSentimentClassification","text":"<p>Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg.</p> <p>Dataset: <code>mteb/PoemSentimentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written human-annotated found Citation <pre><code>@misc{sheng2020investigating,\n  archiveprefix = {arXiv},\n  author = {Emily Sheng and David Uthus},\n  eprint = {2011.02686},\n  primaryclass = {cs.CL},\n  title = {Investigating Societal Biases in a Poetry Composition System},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#poemsentimentclassificationv2","title":"PoemSentimentClassification.v2","text":"<p>Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/poem_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written human-annotated found Citation <pre><code>@misc{sheng2020investigating,\n  archiveprefix = {arXiv},\n  author = {Emily Sheng and David Uthus},\n  eprint = {2011.02686},\n  primaryclass = {cs.CL},\n  title = {Investigating Societal Biases in a Poetry Composition System},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-in","title":"PolEmo2.0-IN","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-IN task is to predict the sentiment of in-domain (medicine and hotels) reviews.</p> <p>Dataset: <code>PL-MTEB/polemo2_in</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-inv2","title":"PolEmo2.0-IN.v2","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-IN task is to predict the sentiment of in-domain (medicine and hotels) reviews.</p> <p>Dataset: <code>mteb/pol_emo2_in</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-out","title":"PolEmo2.0-OUT","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-OUT task is to predict the sentiment of out-of-domain (products and school) reviews using models train on reviews from medicine and hotels domains.</p> <p>Dataset: <code>PL-MTEB/polemo2_out</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written not specified not specified"},{"location":"overview/available_tasks/classification/#polemo20-outv2","title":"PolEmo2.0-OUT.v2","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-OUT task is to predict the sentiment of out-of-domain (products and school) reviews using models train on reviews from medicine and hotels domains.</p> <p>Dataset: <code>mteb/pol_emo2_out</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written not specified not specified"},{"location":"overview/available_tasks/classification/#punjabinewsclassification","title":"PunjabiNewsClassification","text":"<p>A Punjabi dataset for 2-class classification of Punjabi news articles</p> <p>Dataset: <code>mlexplorer008/punjabi_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#restaurantreviewsentimentclassification","title":"RestaurantReviewSentimentClassification","text":"<p>Dataset of 8364 restaurant reviews from qaym.com in Arabic for sentiment analysis</p> <p>Dataset: <code>hadyelsahar/ar_res_reviews</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@inproceedings{elsahar2015building,\n  author = {ElSahar, Hady and El-Beltagy, Samhaa R},\n  booktitle = {International conference on intelligent text processing and computational linguistics},\n  organization = {Springer},\n  pages = {23--34},\n  title = {Building large arabic multi-domain resources for sentiment analysis},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#restaurantreviewsentimentclassificationv2","title":"RestaurantReviewSentimentClassification.v2","text":"<p>Dataset of 8156 restaurant reviews from qaym.com in Arabic for sentiment analysis         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/restaurant_review_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@inproceedings{elsahar2015building,\n  author = {ElSahar, Hady and El-Beltagy, Samhaa R},\n  booktitle = {International conference on intelligent text processing and computational linguistics},\n  organization = {Springer},\n  pages = {23--34},\n  title = {Building large arabic multi-domain resources for sentiment analysis},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romanianreviewssentiment","title":"RomanianReviewsSentiment","text":"<p>LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian</p> <p>Dataset: <code>mteb/RomanianReviewsSentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written derived found Citation <pre><code>@article{tache2101clustering,\n  author = {Anca Maria Tache and Mihaela Gaman and Radu Tudor Ionescu},\n  journal = {ArXiv},\n  title = {Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa -- A Large Romanian Sentiment Data Set},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romanianreviewssentimentv2","title":"RomanianReviewsSentiment.v2","text":"<p>LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/romanian_reviews_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written derived found Citation <pre><code>@article{tache2101clustering,\n  author = {Anca Maria Tache and Mihaela Gaman and Radu Tudor Ionescu},\n  journal = {ArXiv},\n  title = {Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa -- A Large Romanian Sentiment Data Set},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romaniansentimentclassification","title":"RomanianSentimentClassification","text":"<p>An Romanian dataset for sentiment classification.</p> <p>Dataset: <code>mteb/RomanianSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written human-annotated found Citation <pre><code>@article{dumitrescu2020birth,\n  author = {Dumitrescu, Stefan Daniel and Avram, Andrei-Marius and Pyysalo, Sampo},\n  journal = {arXiv preprint arXiv:2009.08712},\n  title = {The birth of Romanian BERT},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romaniansentimentclassificationv2","title":"RomanianSentimentClassification.v2","text":"<p>An Romanian dataset for sentiment classification.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/romanian_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written human-annotated found Citation <pre><code>@article{dumitrescu2020birth,\n  author = {Dumitrescu, Stefan Daniel and Avram, Andrei-Marius and Pyysalo, Sampo},\n  journal = {arXiv preprint arXiv:2009.08712},\n  title = {The birth of Romanian BERT},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#runluintentclassification","title":"RuNLUIntentClassification","text":"<p>Contains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms.</p> <p>Dataset: <code>mteb/RuNLUIntentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified human-annotated found Citation <pre><code>@misc{liu2019benchmarkingnaturallanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Xingkun Liu and Arash Eshghi and Pawel Swietojanski and Verena Rieser},\n  eprint = {1903.05566},\n  primaryclass = {cs.CL},\n  title = {Benchmarking Natural Language Understanding Services for building Conversational Agents},\n  url = {https://arxiv.org/abs/1903.05566},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rureviewsclassification","title":"RuReviewsClassification","text":"<p>Product review classification (3-point scale) based on RuRevies dataset</p> <p>Dataset: <code>ai-forever/ru-reviews-classification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@inproceedings{Smetanin-SA-2019,\n  author = {Sergey Smetanin and Michail Komarov},\n  booktitle = {2019 IEEE 21st Conference on Business Informatics (CBI)},\n  doi = {10.1109/CBI.2019.00062},\n  issn = {2378-1963},\n  month = {July},\n  number = {},\n  pages = {482-486},\n  title = {Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks},\n  volume = {01},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rureviewsclassificationv2","title":"RuReviewsClassification.v2","text":"<p>Product review classification (3-point scale) based on RuRevies dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ru_reviews</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@inproceedings{Smetanin-SA-2019,\n  author = {Sergey Smetanin and Michail Komarov},\n  booktitle = {2019 IEEE 21st Conference on Business Informatics (CBI)},\n  doi = {10.1109/CBI.2019.00062},\n  issn = {2378-1963},\n  month = {July},\n  number = {},\n  pages = {482-486},\n  title = {Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks},\n  volume = {01},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchcoreriscclassification","title":"RuSciBenchCoreRiscClassification","text":"<p>This binary classification task aims to determine whether a scientific paper         (based on its title and abstract) belongs to the Core of the Russian Science Citation Index (RISC).         The RISC includes a wide range of publications, but the Core RISC comprises the most cited and prestigious         journals, dissertations, theses, monographs, and studies. The task is provided for both Russian and English         versions of the paper's title and abstract.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchgrnticlassification","title":"RuSciBenchGRNTIClassification","text":"<p>Classification of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-grnti-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Academic, Written derived found"},{"location":"overview/available_tasks/classification/#ruscibenchgrnticlassificationv2","title":"RuSciBenchGRNTIClassification.v2","text":"<p>Classification of scientific papers based on the GRNTI (State Rubricator of Scientific and         Technical Information) rubricator. GRNTI is a universal hierarchical classification of knowledge domains         adopted in Russia and CIS countries to systematize the entire flow of scientific and technical information.         This task uses the first level of the GRNTI hierarchy and top 28 classes by frequency.</p> <pre><code>    In this version, English language support has been added and data partitioning has been slightly modified.\n</code></pre> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchoecdclassification","title":"RuSciBenchOECDClassification","text":"<p>Classification of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-oecd-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Academic, Written derived found"},{"location":"overview/available_tasks/classification/#ruscibenchoecdclassificationv2","title":"RuSciBenchOECDClassification.v2","text":"<p>Classification of scientific papers based on the OECD         (Organization for Economic Co-operation and Development) rubricator. OECD provides         a hierarchical 3-level system of classes for labeling scientific articles.         This task uses the first two levels of the OECD hierarchy, top 29 classes.</p> <pre><code>    In this version, English language support has been added and data partitioning has been slightly modified.\n</code></pre> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchpubtypeclassification","title":"RuSciBenchPubTypeClassification","text":"<p>This task involves classifying scientific papers (based on their title and abstract)         into different publication types. The dataset identifies the following types:         'Article', 'Conference proceedings', 'Survey', 'Miscellanea', 'Short message', 'Review', and 'Personalia'.         This task is available for both Russian and English versions of the paper's title and abstract.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupclassification","title":"RuToxicOKMLCUPClassification","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day.</p> <p>Dataset: <code>mteb/RuToxicOKMLCUPClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupclassificationv2","title":"RuToxicOKMLCUPClassification.v2","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ru_toxic_okmlcup</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupmultilabelclassification","title":"RuToxicOKMLCUPMultilabelClassification","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day.</p> <p>Dataset: <code>mteb/RuToxicOKMLCUPClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#scdbpaccountabilitylegalbenchclassification","title":"SCDBPAccountabilityLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer maintains internal compliance procedures on company standards regarding human trafficking and slavery? This includes any type of internal accountability mechanism. Requiring independently of the supply to comply with laws does not qualify or asking for documentary evidence of compliance does not count either.'</p> <p>Dataset: <code>mteb/SCDBPAccountabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpauditslegalbenchclassification","title":"SCDBPAuditsLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'</p> <p>Dataset: <code>mteb/SCDBPAuditsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpcertificationlegalbenchclassification","title":"SCDBPCertificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'</p> <p>Dataset: <code>mteb/SCDBPCertificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbptraininglegalbenchclassification","title":"SCDBPTrainingLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  provides training to employees on human trafficking and slavery? Broad policies such as ongoing dialogue on mitigating risks of human trafficking and slavery or increasing managers and purchasers knowledge about health, safety and labor practices qualify as training. Providing training to contractors who failed to comply with human trafficking laws counts as training.'</p> <p>Dataset: <code>mteb/SCDBPTrainingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpverificationlegalbenchclassification","title":"SCDBPVerificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer engages in verification and auditing as one practice, expresses that it may conduct an audit, or expressess that it is assessing supplier risks through a review of the US Dept. of Labor's List?'</p> <p>Dataset: <code>mteb/SCDBPVerificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddaccountabilitylegalbenchclassification","title":"SCDDAccountabilityLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer maintains internal accountability standards and procedures for employees or contractors failing to meet company standards regarding slavery and trafficking?'</p> <p>Dataset: <code>mteb/SCDDAccountabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddauditslegalbenchclassification","title":"SCDDAuditsLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer conducts audits of suppliers to evaluate supplier compliance with company standards for trafficking and slavery in supply chains? The disclosure shall specify if the verification was not an independent, unannounced audit.'</p> <p>Dataset: <code>mteb/SCDDAuditsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddcertificationlegalbenchclassification","title":"SCDDCertificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer requires direct suppliers to certify that materials incorporated into the product comply with the laws regarding slavery and human trafficking of the country or countries in which they are doing business?'</p> <p>Dataset: <code>mteb/SCDDCertificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddtraininglegalbenchclassification","title":"SCDDTrainingLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer provides company employees and management, who have direct responsibility for supply chain management, training on human trafficking and slavery, particularly with respect to mitigating risks within the supply chains of products?'</p> <p>Dataset: <code>mteb/SCDDTrainingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddverificationlegalbenchclassification","title":"SCDDVerificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer engages in verification of product supply chains to evaluate and address risks of human trafficking and slavery? If the company conducts verification], the disclosure shall specify if the verification was not conducted by a third party.'</p> <p>Dataset: <code>mteb/SCDDVerificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdseyeprotectionclassification","title":"SDSEyeProtectionClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/SDSEyeProtectionClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdseyeprotectionclassificationv2","title":"SDSEyeProtectionClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sds_eye_protection</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdsglovesclassification","title":"SDSGlovesClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/SDSGlovesClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdsglovesclassificationv2","title":"SDSGlovesClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sds_gloves</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sib200classification","title":"SIB200Classification","text":"<p>SIB-200 is the largest publicly available topic classification         dataset based on Flores-200 covering 205 languages and dialects annotated. The dataset is         annotated in English for the topics,  science/technology, travel, politics, sports,         health, entertainment, and geography. The labels are then transferred to the other languages         in Flores-200 which are human-translated.</p> <p>Dataset: <code>mteb/sib200</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ace, acm, acq, aeb, afr, ... (197) News, Written expert-annotated human-translated and localized Citation <pre><code>@article{adelani2023sib,\n  author = {Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},\n  journal = {arXiv preprint arXiv:2309.07445},\n  title = {SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sidclassification","title":"SIDClassification","text":"<p>SID Classification</p> <p>Dataset: <code>MCINext/sid-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sidclassificationv2","title":"SIDClassification.v2","text":"<p>SID Classification         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sid</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sanskritshlokasclassification","title":"SanskritShlokasClassification","text":"<p>This data set contains ~500 Shlokas  </p> <p>Dataset: <code>bpHigh/iNLTK_Sanskrit_Shlokas_Dataset</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy san Religious, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sardistanceclassification","title":"SardiStanceClassification","text":"<p>SardiStance is a unique dataset designed for the task of stance detection in Italian tweets. It consists of tweets related to the Sardines movement, providing a valuable resource for researchers and practitioners in the field of NLP.</p> <p>Dataset: <code>MattiaSangermano/SardiStance</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Social derived found Citation <pre><code>@inproceedings{cignarella2020sardistance,\n  author = {Cignarella, Alessandra Teresa and Lai, Mirko and Bosco, Cristina and Patti, Viviana and Rosso, Paolo and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {Ceur},\n  pages = {1--10},\n  title = {Sardistance@ evalita2020: Overview of the task on stance detection in italian tweets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scalaclassification","title":"ScalaClassification","text":"<p>ScaLa a linguistic acceptability dataset for the mainland Scandinavian languages automatically constructed from dependency annotations in Universal Dependencies Treebanks.         Published as part of 'ScandEval: A Benchmark for Scandinavian Natural Language Processing'</p> <p>Dataset: <code>mteb/multilingual-scala-classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, nno, nob, swe Blog, Fiction, News, Non-fiction, Spoken, ... (7) human-annotated created Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scandisentclassification","title":"ScandiSentClassification","text":"<p>The corpus is crawled from se.trustpilot.com, no.trustpilot.com, dk.trustpilot.com, fi.trustpilot.com and trustpilot.com.</p> <p>Dataset: <code>mteb/scandisent</code> \u2022 License: openrail \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, eng, fin, nob, swe Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{isbister-etal-2021-stop,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Isbister, Tim  and\nCarlsson, Fredrik  and\nSahlgren, Magnus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {385--390},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?},\n  url = {https://aclanthology.org/2021.nodalida-main.42/},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentirueval2016","title":"SentiRuEval2016","text":"<p>Russian sentiment analysis evaluation SentiRuEval-2016 devoted to reputation monitoring of banks and telecom companies in Twitter. We describe the task, data, the procedure of data preparation, and participants\u2019 results.</p> <p>Dataset: <code>mteb/SentiRuEval2016</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found Citation <pre><code>@inproceedings{loukachevitch2016sentirueval,\n  author = {Loukachevitch, NV and Rubtsova, Yu V},\n  booktitle = {Computational Linguistics and Intellectual Technologies},\n  pages = {416--426},\n  title = {SentiRuEval-2016: overcoming time gap and data sparsity in tweet sentiment analysis},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentirueval2016v2","title":"SentiRuEval2016.v2","text":"<p>Russian sentiment analysis evaluation SentiRuEval-2016 devoted to reputation monitoring of banks and telecom companies in Twitter. We describe the task, data, the procedure of data preparation, and participants\u2019 results.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/senti_ru_eval2016</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found Citation <pre><code>@inproceedings{loukachevitch2016sentirueval,\n  author = {Loukachevitch, NV and Rubtsova, Yu V},\n  booktitle = {Computational Linguistics and Intellectual Technologies},\n  pages = {416--426},\n  title = {SentiRuEval-2016: overcoming time gap and data sparsity in tweet sentiment analysis},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentanalysishindi","title":"SentimentAnalysisHindi","text":"<p>Hindi Sentiment Analysis Dataset</p> <p>Dataset: <code>OdiaGenAI/sentiment_analysis_hindi</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 hin Reviews, Written derived found Citation <pre><code>@misc{OdiaGenAI,\n  author = {Shantipriya Parida and Sambit Sekhar and Soumendra Kumar Sahoo and Swateek Jena and Abhijeet Parida and Satya Ranjan Dash and Guneet Singh Kohli},\n  howpublished = {{https://huggingface.co/OdiaGenAI}},\n  journal = {Hugging Face repository},\n  publisher = {Hugging Face},\n  title = {OdiaGenAI: Generative AI and LLM Initiative for the Odia Language},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentanalysishindiv2","title":"SentimentAnalysisHindi.v2","text":"<p>Hindi Sentiment Analysis Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sentiment_analysis_hindi</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 hin Reviews, Written derived found Citation <pre><code>@misc{OdiaGenAI,\n  author = {Shantipriya Parida and Sambit Sekhar and Soumendra Kumar Sahoo and Swateek Jena and Abhijeet Parida and Satya Ranjan Dash and Guneet Singh Kohli},\n  howpublished = {{https://huggingface.co/OdiaGenAI}},\n  journal = {Hugging Face repository},\n  publisher = {Hugging Face},\n  title = {OdiaGenAI: Generative AI and LLM Initiative for the Odia Language},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentdksf","title":"SentimentDKSF","text":"<p>The Sentiment DKSF (Digikala/Snappfood comments) is a dataset for sentiment analysis.</p> <p>Dataset: <code>hezarai/sentiment-dksf</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentdksfv2","title":"SentimentDKSF.v2","text":"<p>The Sentiment DKSF (Digikala/Snappfood comments) is a dataset for sentiment analysis.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sentiment_dksf</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewsclassification","title":"SinhalaNewsClassification","text":"<p>This file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015).</p> <p>Dataset: <code>NLPC-UOM/Sinhala-News-Category-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{deSilva2015,\n  author = {Nisansa de Silva},\n  journal = {Year of Publication},\n  title = {Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language},\n  year = {2015},\n}\n\n@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewsclassificationv2","title":"SinhalaNewsClassification.v2","text":"<p>This file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sinhala_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{deSilva2015,\n  author = {Nisansa de Silva},\n  journal = {Year of Publication},\n  title = {Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language},\n  year = {2015},\n}\n\n@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewssourceclassification","title":"SinhalaNewsSourceClassification","text":"<p>This dataset contains Sinhala news headlines extracted from 9 news sources (websites) (Sri Lanka Army, Dinamina, GossipLanka, Hiru, ITN, Lankapuwath, NewsLK, Newsfirst, World Socialist Web Site-Sinhala).</p> <p>Dataset: <code>NLPC-UOM/Sinhala-News-Source-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewssourceclassificationv2","title":"SinhalaNewsSourceClassification.v2","text":"<p>This dataset contains Sinhala news headlines extracted from 9 news sources (websites) (Sri Lanka Army, Dinamina, GossipLanka, Hiru, ITN, Lankapuwath, NewsLK, Newsfirst, World Socialist Web Site-Sinhala).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sinhala_news_source</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#siswatinewsclassification","title":"SiswatiNewsClassification","text":"<p>Siswati News Classification Dataset</p> <p>Dataset: <code>isaacchung/siswati-news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ssw News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#siswatinewsclassificationv2","title":"SiswatiNewsClassification.v2","text":"<p>Siswati News Classification Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/siswati_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ssw News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#slovakhatespeechclassification","title":"SlovakHateSpeechClassification","text":"<p>The dataset contains posts from a social network with human annotations for hateful or offensive language in Slovak.</p> <p>Dataset: <code>TUKE-KEMT/hate_speech_slovak</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Social, Written human-annotated found"},{"location":"overview/available_tasks/classification/#slovakhatespeechclassificationv2","title":"SlovakHateSpeechClassification.v2","text":"<p>The dataset contains posts from a social network with human annotations for hateful or offensive language in Slovak.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/slovak_hate_speech</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Social, Written human-annotated found"},{"location":"overview/available_tasks/classification/#slovakmoviereviewsentimentclassification","title":"SlovakMovieReviewSentimentClassification","text":"<p>User reviews of movies on the CSFD movie database, with 2 sentiment classes (positive, negative)</p> <p>Dataset: <code>janko/sk_csfd-movie-reviews</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@article{vstefanik2023resources,\n  author = {{\\v{S}}tef{\\'a}nik, Michal and Kadl{\\v{c}}{\\'\\i}k, Marek and Gramacki, Piotr and Sojka, Petr},\n  journal = {arXiv preprint arXiv:2304.01922},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#slovakmoviereviewsentimentclassificationv2","title":"SlovakMovieReviewSentimentClassification.v2","text":"<p>User reviews of movies on the CSFD movie database, with 2 sentiment classes (positive, negative)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/slovak_movie_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@article{vstefanik2023resources,\n  author = {{\\v{S}}tef{\\'a}nik, Michal and Kadl{\\v{c}}{\\'\\i}k, Marek and Gramacki, Piotr and Sojka, Petr},\n  journal = {arXiv preprint arXiv:2304.01922},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#southafricanlangclassification","title":"SouthAfricanLangClassification","text":"<p>A language identification test set for 11 South African Languages.</p> <p>Dataset: <code>mlexplorer008/south_african_language_identification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, eng, nbl, nso, sot, ... (11) Non-fiction, Web, Written expert-annotated found Citation <pre><code>@misc{south-african-language-identification,\n  author = {ExploreAI Academy, Joanne M},\n  publisher = {Kaggle},\n  title = {South African Language Identification},\n  url = {https://kaggle.com/competitions/south-african-language-identification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishnewsclassification","title":"SpanishNewsClassification","text":"<p>A Spanish dataset for news classification. The dataset includes articles from reputable Spanish news sources spanning 12 different categories.</p> <p>Dataset: <code>MarcOrfilaCarreras/spanish-news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishnewsclassificationv2","title":"SpanishNewsClassification.v2","text":"<p>A Spanish dataset for news classification. The dataset includes articles from reputable Spanish news sources spanning 12 different categories.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/spanish_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishsentimentclassification","title":"SpanishSentimentClassification","text":"<p>A Spanish dataset for sentiment classification.</p> <p>Dataset: <code>sepidmnorozy/Spanish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishsentimentclassificationv2","title":"SpanishSentimentClassification.v2","text":"<p>A Spanish dataset for sentiment classification.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/spanish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#styleclassification","title":"StyleClassification","text":"<p>A dataset containing formal and informal sentences in Persian for style classification.</p> <p>Dataset: <code>MCINext/style-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#swahilinewsclassification","title":"SwahiliNewsClassification","text":"<p>Dataset for Swahili News Classification, categorized with 6 domains (Local News (Kitaifa), International News (Kimataifa), Finance News (Uchumi), Health News (Afya), Sports News (Michezo), and Entertainment News (Burudani)). Building and Optimizing Swahili Language Models: Techniques, Embeddings, and Datasets</p> <p>Dataset: <code>Mollel/SwahiliNewsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swa News, Written derived found Citation <pre><code>@inproceedings{davis2020swahili,\n  author = {Davis, David},\n  doi = {10.5281/zenodo.5514203},\n  publisher = {Zenodo},\n  title = {Swahili: News Classification Dataset (0.2)},\n  url = {https://doi.org/10.5281/zenodo.5514203},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swahilinewsclassificationv2","title":"SwahiliNewsClassification.v2","text":"<p>Dataset for Swahili News Classification, categorized with 6 domains (Local News (Kitaifa), International News (Kimataifa), Finance News (Uchumi), Health News (Afya), Sports News (Michezo), and Entertainment News (Burudani)). Building and Optimizing Swahili Language Models: Techniques, Embeddings, and Datasets         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swahili_news</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swa News, Written derived found Citation <pre><code>@inproceedings{davis2020swahili,\n  author = {Davis, David},\n  doi = {10.5281/zenodo.5514203},\n  publisher = {Zenodo},\n  title = {Swahili: News Classification Dataset (0.2)},\n  url = {https://doi.org/10.5281/zenodo.5514203},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swerecclassification","title":"SweRecClassification","text":"<p>A Swedish dataset for sentiment classification on review</p> <p>Dataset: <code>mteb/swerec_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swerecclassificationv2","title":"SweRecClassification.v2","text":"<p>A Swedish dataset for sentiment classification on review         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swe_rec</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swedishsentimentclassification","title":"SwedishSentimentClassification","text":"<p>Dataset of Swedish reviews scarped from various public available websites</p> <p>Dataset: <code>mteb/SwedishSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#swedishsentimentclassificationv2","title":"SwedishSentimentClassification.v2","text":"<p>Dataset of Swedish reviews scarped from various public available websites         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swedish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#swissjudgementclassification","title":"SwissJudgementClassification","text":"<p>Multilingual, diachronic dataset of Swiss Federal Supreme Court cases annotated with the respective binarized judgment outcome (approval/dismissal)</p> <p>Dataset: <code>mteb/SwissJudgementClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, fra, ita Legal, Written expert-annotated found Citation <pre><code>@misc{niklaus2022empirical,\n  archiveprefix = {arXiv},\n  author = {Joel Niklaus and Matthias St\u00fcrmer and Ilias Chalkidis},\n  eprint = {2209.12325},\n  primaryclass = {cs.CL},\n  title = {An Empirical Study on Cross-X Transfer for Legal Judgment Prediction},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsaanger","title":"SynPerChatbotConvSAAnger","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Anger</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-anger</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsafear","title":"SynPerChatbotConvSAFear","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Fear</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-fear</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsafriendship","title":"SynPerChatbotConvSAFriendship","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Friendship</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-friendship</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsahappiness","title":"SynPerChatbotConvSAHappiness","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Happiness</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-happiness</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsajealousy","title":"SynPerChatbotConvSAJealousy","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Jealousy</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-jealousy</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsalove","title":"SynPerChatbotConvSALove","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Love</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-love</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasadness","title":"SynPerChatbotConvSASadness","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Sadness</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-sadness</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasatisfaction","title":"SynPerChatbotConvSASatisfaction","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Satisfaction</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-satisfaction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasurprise","title":"SynPerChatbotConvSASurprise","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Surprise</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-surprise</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsatonechatbotclassification","title":"SynPerChatbotConvSAToneChatbotClassification","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsatoneuserclassification","title":"SynPerChatbotConvSAToneUserClassification","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Tone User</p> <p>Dataset: <code>MCINext/chatbot-conversational-sentiment-analysis-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotragtonechatbotclassification","title":"SynPerChatbotRAGToneChatbotClassification","text":"<p>Synthetic Persian Chatbot RAG Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotragtoneuserclassification","title":"SynPerChatbotRAGToneUserClassification","text":"<p>Synthetic Persian Chatbot RAG Tone User Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotsatisfactionlevelclassification","title":"SynPerChatbotSatisfactionLevelClassification","text":"<p>Synthetic Persian Chatbot Satisfaction Level Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-satisfaction-level-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbottonechatbotclassification","title":"SynPerChatbotToneChatbotClassification","text":"<p>Synthetic Persian Chatbot Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbottoneuserclassification","title":"SynPerChatbotToneUserClassification","text":"<p>Synthetic Persian Chatbot Tone User Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassification","title":"SynPerTextToneClassification","text":"<p>Persian Text Tone</p> <p>Dataset: <code>MCINext/synthetic-persian-text-tone-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassificationv2","title":"SynPerTextToneClassification.v2","text":"<p>Persian Text Tone         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/syn_per_text_tone</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassificationv3","title":"SynPerTextToneClassification.v3","text":"<p>This version of the Persian text tone classification dataset is an improved version of its predecessors.          It excludes several classes identified as having low-quality data, leading to a more reliable benchmark.</p> <p>Dataset: <code>MCINext/synthetic-persian-text-tone-classification-v3</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#tnews","title":"TNews","text":"<p>Short Text Classification for News</p> <p>Dataset: <code>C-MTEB/TNews-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tnewsv2","title":"TNews.v2","text":"<p>Short Text Classification for News         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/t_news</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tamilnewsclassification","title":"TamilNewsClassification","text":"<p>A Tamil dataset for 6-class classification of Tamil news articles</p> <p>Dataset: <code>mlexplorer008/tamil_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tam News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tamilnewsclassificationv2","title":"TamilNewsClassification.v2","text":"<p>A Tamil dataset for 6-class classification of Tamil news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tamil_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tam News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#telemarketingsalesrulelegalbenchclassification","title":"TelemarketingSalesRuleLegalBenchClassification","text":"<p>Determine how 16 C.F.R. \u00a7 310.3(a)(1) and 16 C.F.R. \u00a7 310.3(a)(2) (governing deceptive practices) apply to different fact patterns. This dataset is designed to test a model\u2019s ability to apply 16 C.F.R. \u00a7 310.3(a)(1) and 16 C.F.R. \u00a7 310.3(a)(2) of the Telemarketing Sales Rule to a simple fact pattern with a clear outcome. Each fact pattern ends with the question: \u201cIs this a violation of the Telemarketing Sales Rule?\u201d Each fact pattern is paired with the answer \u201cYes\u201d or the answer \u201cNo.\u201d Fact patterns are listed in the column \u201ctext,\u201d and answers are listed in the column \u201clabel.\u201d</p> <p>Dataset: <code>mteb/TelemarketingSalesRuleLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#teluguandhrajyotinewsclassification","title":"TeluguAndhraJyotiNewsClassification","text":"<p>A Telugu dataset for 5-class classification of Telugu news articles</p> <p>Dataset: <code>mlexplorer008/telugu_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tel News, Written derived found"},{"location":"overview/available_tasks/classification/#teluguandhrajyotinewsclassificationv2","title":"TeluguAndhraJyotiNewsClassification.v2","text":"<p>A Telugu dataset for 5-class classification of Telugu news articles         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/telugu_andhra_jyoti_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tel News, Written derived found"},{"location":"overview/available_tasks/classification/#tenkgnadclassification","title":"TenKGnadClassification","text":"<p>10k German News Articles Dataset (10kGNAD) contains news articles from the online Austrian newspaper website DER Standard with their topic classification (9 classes).</p> <p>Dataset: <code>mteb/TenKGnadClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu News, Written expert-annotated found Citation <pre><code>@inproceedings{Schabus2017,\n  address = {Tokyo, Japan},\n  author = {Dietmar Schabus and Marcin Skowron and Martin Trapp},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},\n  doi = {10.1145/3077136.3080711},\n  month = aug,\n  pages = {1241--1244},\n  title = {One Million Posts: A Data Set of German Online Discussions},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tenkgnadclassificationv2","title":"TenKGnadClassification.v2","text":"<p>10k German News Articles Dataset (10kGNAD) contains news articles from the online Austrian newspaper website DER Standard with their topic classification (9 classes).         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ten_k_gnad</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu News, Written expert-annotated found Citation <pre><code>@inproceedings{Schabus2017,\n  address = {Tokyo, Japan},\n  author = {Dietmar Schabus and Marcin Skowron and Martin Trapp},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},\n  doi = {10.1145/3077136.3080711},\n  month = aug,\n  pages = {1241--1244},\n  title = {One Million Posts: A Data Set of German Online Discussions},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#textualismtooldictionarieslegalbenchclassification","title":"TextualismToolDictionariesLegalBenchClassification","text":"<p>Determine if a paragraph from a judicial opinion is applying a form textualism that relies on the dictionary meaning of terms.</p> <p>Dataset: <code>mteb/TextualismToolDictionariesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#textualismtoolplainlegalbenchclassification","title":"TextualismToolPlainLegalBenchClassification","text":"<p>Determine if a paragraph from a judicial opinion is applying a form textualism that relies on the ordinary (\u201cplain\u201d) meaning of terms.</p> <p>Dataset: <code>mteb/TextualismToolPlainLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicchatclassification","title":"ToxicChatClassification","text":"<p>This dataset contains toxicity annotations on 10K user             prompts collected from the Vicuna online demo. We utilize a human-AI             collaborative annotation framework to guarantee the quality of annotation             while maintaining a feasible annotation workload. The details of data             collection, pre-processing, and annotation can be found in our paper.             We believe that ToxicChat can be a valuable resource to drive further             advancements toward building a safe and healthy environment for user-AI             interactions.             Only human annotated samples are selected here.</p> <p>Dataset: <code>lmsys/toxic-chat</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Constructed, Written expert-annotated found Citation <pre><code>@misc{lin2023toxicchat,\n  archiveprefix = {arXiv},\n  author = {Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},\n  eprint = {2310.17389},\n  primaryclass = {cs.CL},\n  title = {ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicchatclassificationv2","title":"ToxicChatClassification.v2","text":"<p>This dataset contains toxicity annotations on 10K user             prompts collected from the Vicuna online demo. We utilize a human-AI             collaborative annotation framework to guarantee the quality of annotation             while maintaining a feasible annotation workload. The details of data             collection, pre-processing, and annotation can be found in our paper.             We believe that ToxicChat can be a valuable resource to drive further             advancements toward building a safe and healthy environment for user-AI             interactions.             Only human annotated samples are selected here.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/toxic_chat</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Constructed, Written expert-annotated found Citation <pre><code>@misc{lin2023toxicchat,\n  archiveprefix = {arXiv},\n  author = {Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},\n  eprint = {2310.17389},\n  primaryclass = {cs.CL},\n  title = {ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsclassification","title":"ToxicConversationsClassification","text":"<p>Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.</p> <p>Dataset: <code>mteb/toxic_conversations_50k</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsclassificationv2","title":"ToxicConversationsClassification.v2","text":"<p>Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/toxic_conversations</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsvnclassification","title":"ToxicConversationsVNClassification","text":"<p>A translated dataset from Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/toxic-conversations-50k-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tswananewsclassification","title":"TswanaNewsClassification","text":"<p>Tswana News Classification Dataset</p> <p>Dataset: <code>dsfsi/daily-news-dikgang</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tsn News, Written derived found Citation <pre><code>@inproceedings{marivate2023puoberta,\n  author = {Vukosi Marivate and Moseli Mots'Oehli and Valencia Wagner and Richard Lastrucci and Isheanesu Dzingirai},\n  booktitle = {SACAIR 2023 (To Appear)},\n  dataset_url = {https://github.com/dsfsi/PuoBERTa},\n  keywords = {NLP},\n  preprint_url = {https://arxiv.org/abs/2310.09141},\n  software_url = {https://huggingface.co/dsfsi/PuoBERTa},\n  title = {PuoBERTa: Training and evaluation of a curated language model for Setswana},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tswananewsclassificationv2","title":"TswanaNewsClassification.v2","text":"<p>Tswana News Classification Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tswana_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tsn News, Written derived found Citation <pre><code>@inproceedings{marivate2023puoberta,\n  author = {Vukosi Marivate and Moseli Mots'Oehli and Valencia Wagner and Richard Lastrucci and Isheanesu Dzingirai},\n  booktitle = {SACAIR 2023 (To Appear)},\n  dataset_url = {https://github.com/dsfsi/PuoBERTa},\n  keywords = {NLP},\n  preprint_url = {https://arxiv.org/abs/2310.09141},\n  software_url = {https://huggingface.co/dsfsi/PuoBERTa},\n  title = {PuoBERTa: Training and evaluation of a curated language model for Setswana},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkicclassification","title":"TurkicClassification","text":"<p>A dataset of news classification in three Turkic languages.</p> <p>Dataset: <code>Electrotubbie/classification_Turkic_languages</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bak, kaz, kir News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishmoviesentimentclassification","title":"TurkishMovieSentimentClassification","text":"<p>Turkish Movie Review Dataset</p> <p>Dataset: <code>asparius/Turkish-Movie-Review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishmoviesentimentclassificationv2","title":"TurkishMovieSentimentClassification.v2","text":"<p>Turkish Movie Review Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/turkish_movie_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishproductsentimentclassification","title":"TurkishProductSentimentClassification","text":"<p>Turkish Product Review Dataset</p> <p>Dataset: <code>asparius/Turkish-Product-Review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishproductsentimentclassificationv2","title":"TurkishProductSentimentClassification.v2","text":"<p>Turkish Product Review Dataset         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/turkish_product_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetemotionclassification","title":"TweetEmotionClassification","text":"<p>A dataset of 10,000 tweets that was created with the aim of covering the most frequently used emotion categories in Arabic tweets.</p> <p>Dataset: <code>mteb/TweetEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{al2018emotional,\n  author = {Al-Khatib, Amr and El-Beltagy, Samhaa R},\n  booktitle = {Computational Linguistics and Intelligent Text Processing: 18th International Conference, CICLing 2017, Budapest, Hungary, April 17--23, 2017, Revised Selected Papers, Part II 18},\n  organization = {Springer},\n  pages = {105--114},\n  title = {Emotional tone detection in arabic tweets},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetemotionclassificationv2","title":"TweetEmotionClassification.v2","text":"<p>A dataset of 10,012 tweets that was created with the aim of covering the most frequently used emotion categories in Arabic tweets.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/TweetEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{al2018emotional,\n  author = {Al-Khatib, Amr and El-Beltagy, Samhaa R},\n  booktitle = {Computational Linguistics and Intelligent Text Processing: 18th International Conference, CICLing 2017, Budapest, Hungary, April 17--23, 2017, Revised Selected Papers, Part II 18},\n  organization = {Springer},\n  pages = {105--114},\n  title = {Emotional tone detection in arabic tweets},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsarcasmclassification","title":"TweetSarcasmClassification","text":"<p>Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets.</p> <p>Dataset: <code>iabufarha/ar_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{abu-farha-magdy-2020-arabic,\n  address = {Marseille, France},\n  author = {Abu Farha, Ibrahim  and\nMagdy, Walid},\n  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},\n  editor = {Al-Khalifa, Hend  and\nMagdy, Walid  and\nDarwish, Kareem  and\nElsayed, Tamer  and\nMubarak, Hamdy},\n  isbn = {979-10-95546-51-1},\n  language = {English},\n  month = may,\n  pages = {32--39},\n  publisher = {European Language Resource Association},\n  title = {From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset},\n  url = {https://aclanthology.org/2020.osact-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsarcasmclassificationv2","title":"TweetSarcasmClassification.v2","text":"<p>Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tweet_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{abu-farha-magdy-2020-arabic,\n  address = {Marseille, France},\n  author = {Abu Farha, Ibrahim  and\nMagdy, Walid},\n  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},\n  editor = {Al-Khalifa, Hend  and\nMagdy, Walid  and\nDarwish, Kareem  and\nElsayed, Tamer  and\nMubarak, Hamdy},\n  isbn = {979-10-95546-51-1},\n  language = {English},\n  month = may,\n  pages = {32--39},\n  publisher = {European Language Resource Association},\n  title = {From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset},\n  url = {https://aclanthology.org/2020.osact-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentclassification","title":"TweetSentimentClassification","text":"<p>A multilingual Sentiment Analysis dataset consisting of tweets in 8 different languages.</p> <p>Dataset: <code>mteb/tweet_sentiment_multilingual</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, deu, eng, fra, hin, ... (8) Social, Written human-annotated found Citation <pre><code>@inproceedings{barbieri-etal-2022-xlm,\n  address = {Marseille, France},\n  author = {Barbieri, Francesco  and\nEspinosa Anke, Luis  and\nCamacho-Collados, Jose},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {258--266},\n  publisher = {European Language Resources Association},\n  title = {{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond},\n  url = {https://aclanthology.org/2022.lrec-1.27},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionclassification","title":"TweetSentimentExtractionClassification","text":"<p>Dataset: <code>mteb/tweet_sentiment_extraction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionclassificationv2","title":"TweetSentimentExtractionClassification.v2","text":"<pre><code>    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/tweet_sentiment_extraction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionvnclassification","title":"TweetSentimentExtractionVNClassification","text":"<p>A collection of translated tweets annotated for sentiment extraction.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/tweet-sentiment-extraction-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweettopicsingleclassification","title":"TweetTopicSingleClassification","text":"<p>Topic classification dataset on Twitter with 6 labels. Each instance of         TweetTopic comes with a timestamp which distributes from September 2019 to August 2021.         Tweets were preprocessed before the annotation to normalize some artifacts, converting         URLs into a special token {{URL}} and non-verified usernames into {{USERNAME}}. For verified         usernames, we replace its display name (or account name) with symbols {@}.</p> <p>Dataset: <code>mteb/TweetTopicSingleClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dimosthenis-etal-2022-twitter,\n  address = {Gyeongju, Republic of Korea},\n  author = {Antypas, Dimosthenis  and\nUshio, Asahi  and\nCamacho-Collados, Jose  and\nNeves, Leonardo  and\nSilva, Vitor  and\nBarbieri, Francesco},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  publisher = {International Committee on Computational Linguistics},\n  title = {{T}witter {T}opic {C}lassification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweettopicsingleclassificationv2","title":"TweetTopicSingleClassification.v2","text":"<p>Topic classification dataset on Twitter with 6 labels. Each instance of         TweetTopic comes with a timestamp which distributes from September 2019 to August 2021.         Tweets were preprocessed before the annotation to normalize some artifacts, converting         URLs into a special token {{URL}} and non-verified usernames into {{USERNAME}}. For verified         usernames, we replace its display name (or account name) with symbols {@}.</p> <pre><code>    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/tweet_topic_single</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dimosthenis-etal-2022-twitter,\n  address = {Gyeongju, Republic of Korea},\n  author = {Antypas, Dimosthenis  and\nUshio, Asahi  and\nCamacho-Collados, Jose  and\nNeves, Leonardo  and\nSilva, Vitor  and\nBarbieri, Francesco},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  publisher = {International Committee on Computational Linguistics},\n  title = {{T}witter {T}opic {C}lassification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#uccvcommonlawlegalbenchclassification","title":"UCCVCommonLawLegalBenchClassification","text":"<p>Determine if a contract is governed by the Uniform Commercial Code (UCC) or the common law of contracts.</p> <p>Dataset: <code>mteb/UCCVCommonLawLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ukrformalityclassification","title":"UkrFormalityClassification","text":"<pre><code>    This dataset contains Ukrainian Formality Classification dataset obtained by\n    trainslating English GYAFC data.\n    English data source: https://aclanthology.org/N18-1012/\n    Translation into Ukrainian language using model: https://huggingface.co/facebook/nllb-200-distilled-600M\n    Additionally, the dataset was balanced, with labels: 0 - informal, 1 - formal.\n</code></pre> <p>Dataset: <code>ukr-detect/ukr-formality-dataset-translated-gyafc</code> \u2022 License: openrail++ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ukr News, Written derived machine-translated Citation <pre><code>@inproceedings{rao-tetreault-2018-dear,\n  author = {Rao, Sudha  and\nTetreault, Joel},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  month = jun,\n  publisher = {Association for Computational Linguistics},\n  title = {Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},\n  url = {https://aclanthology.org/N18-1012},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ukrformalityclassificationv2","title":"UkrFormalityClassification.v2","text":"<pre><code>    This dataset contains Ukrainian Formality Classification dataset obtained by\n    trainslating English GYAFC data.\n    English data source: https://aclanthology.org/N18-1012/\n    Translation into Ukrainian language using model: https://huggingface.co/facebook/nllb-200-distilled-600M\n    Additionally, the dataset was balanced, with labels: 0 - informal, 1 - formal.\n\n    This version corrects errors found in the original data. For details, see [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900)\n</code></pre> <p>Dataset: <code>mteb/ukr_formality</code> \u2022 License: openrail++ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ukr News, Written derived machine-translated Citation <pre><code>@inproceedings{rao-tetreault-2018-dear,\n  author = {Rao, Sudha  and\nTetreault, Joel},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  month = jun,\n  publisher = {Association for Computational Linguistics},\n  title = {Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},\n  url = {https://aclanthology.org/N18-1012},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#unfairtoslegalbenchclassification","title":"UnfairTOSLegalBenchClassification","text":"<p>Given a clause from a terms-of-service contract, determine the category the clause belongs to. The purpose of this task is classifying clauses in Terms of Service agreements. Clauses have been annotated by into nine categories: ['Arbitration', 'Unilateral change', 'Content removal', 'Jurisdiction', 'Choice of law', 'Limitation of liability', 'Unilateral termination', 'Contract by using', 'Other']. The first eight categories correspond to clauses that would potentially be deemed potentially unfair. The last category (Other) corresponds to clauses in agreements which don\u2019t fit into these categories.</p> <p>Dataset: <code>mteb/UnfairTOSLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{lippi2019claudette,\n  author = {Lippi, Marco and Pa{\\l}ka, Przemys{\\l}aw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},\n  journal = {Artificial Intelligence and Law},\n  pages = {117--139},\n  publisher = {Springer},\n  title = {CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service},\n  volume = {27},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#urduromansentimentclassification","title":"UrduRomanSentimentClassification","text":"<p>The Roman Urdu dataset is a data corpus comprising of more than 20000 records tagged for sentiment (Positive, Negative, Neutral)</p> <p>Dataset: <code>mteb/UrduRomanSentimentClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 urd Social, Written derived found Citation <pre><code>@misc{misc_roman_urdu_data_set_458,\n  author = {Sharf,Zareen},\n  howpublished = {UCI Machine Learning Repository},\n  note = {{DOI}: https://doi.org/10.24432/C58325},\n  title = {{Roman Urdu Data Set}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#urduromansentimentclassificationv2","title":"UrduRomanSentimentClassification.v2","text":"<p>The Roman Urdu dataset is a data corpus comprising of more than 20000 records tagged for sentiment (Positive, Negative, Neutral)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/urdu_roman_sentiment</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 urd Social, Written derived found Citation <pre><code>@misc{misc_roman_urdu_data_set_458,\n  author = {Sharf,Zareen},\n  howpublished = {UCI Machine Learning Repository},\n  note = {{DOI}: https://doi.org/10.24432/C58325},\n  title = {{Roman Urdu Data Set}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#vaccinchatnlclassification","title":"VaccinChatNLClassification","text":"<p>VaccinChatNL is a Flemish Dutch FAQ dataset on the topic of COVID-19 vaccinations in Flanders.</p> <p>Dataset: <code>clips/VaccinChatNL</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Spoken, Web expert-annotated created Citation <pre><code>@inproceedings{buhmann-etal-2022-domain,\n  address = {Gyeongju, Republic of Korea},\n  author = {Buhmann, Jeska and De Bruyn, Maxime and Lotfi, Ehsan and Daelemans, Walter},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {3539--3549},\n  publisher = {International Committee on Computational Linguistics},\n  title = {Domain- and Task-Adaptation for {V}accin{C}hat{NL}, a {D}utch {COVID}-19 {FAQ} Answering Corpus and Classification Model},\n  url = {https://aclanthology.org/2022.coling-1.312},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#viestudentfeedbackclassification","title":"VieStudentFeedbackClassification","text":"<p>A Vietnamese dataset for classification of student feedback</p> <p>Dataset: <code>mteb/VieStudentFeedbackClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written human-annotated created Citation <pre><code>@inproceedings{8573337,\n  author = {Nguyen, Kiet Van and Nguyen, Vu Duc and Nguyen, Phu X. V. and Truong, Tham T. H. and Nguyen, Ngan Luu-Thuy},\n  booktitle = {2018 10th International Conference on Knowledge and Systems Engineering (KSE)},\n  doi = {10.1109/KSE.2018.8573337},\n  number = {},\n  pages = {19-24},\n  title = {UIT-VSFC: Vietnamese Students\u2019 Feedback Corpus for Sentiment Analysis},\n  volume = {},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#viestudentfeedbackclassificationv2","title":"VieStudentFeedbackClassification.v2","text":"<p>A Vietnamese dataset for classification of student feedback         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/vie_student_feedback</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written human-annotated created Citation <pre><code>@inproceedings{8573337,\n  author = {Nguyen, Kiet Van and Nguyen, Vu Duc and Nguyen, Phu X. V. and Truong, Tham T. H. and Nguyen, Ngan Luu-Thuy},\n  booktitle = {2018 10th International Conference on Knowledge and Systems Engineering (KSE)},\n  doi = {10.1109/KSE.2018.8573337},\n  number = {},\n  pages = {19-24},\n  title = {UIT-VSFC: Vietnamese Students\u2019 Feedback Corpus for Sentiment Analysis},\n  volume = {},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wrimeclassification","title":"WRIMEClassification","text":"<p>A dataset of Japanese social network rated for sentiment</p> <p>Dataset: <code>mteb/WRIMEClassification</code> \u2022 License: https://huggingface.co/datasets/shunk031/wrime#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Social, Written human-annotated found Citation <pre><code>@inproceedings{kajiwara-etal-2021-wrime,\n  address = {Online},\n  author = {Kajiwara, Tomoyuki  and\nChu, Chenhui  and\nTakemura, Noriko  and\nNakashima, Yuta  and\nNagahara, Hajime},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.169},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {2095--2104},\n  publisher = {Association for Computational Linguistics},\n  title = {{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations},\n  url = {https://aclanthology.org/2021.naacl-main.169},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wrimeclassificationv2","title":"WRIMEClassification.v2","text":"<p>A dataset of Japanese social network rated for sentiment         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wrime</code> \u2022 License: https://huggingface.co/datasets/shunk031/wrime#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Social, Written human-annotated found Citation <pre><code>@inproceedings{kajiwara-etal-2021-wrime,\n  address = {Online},\n  author = {Kajiwara, Tomoyuki  and\nChu, Chenhui  and\nTakemura, Noriko  and\nNakashima, Yuta  and\nNagahara, Hajime},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.169},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {2095--2104},\n  publisher = {Association for Computational Linguistics},\n  title = {{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations},\n  url = {https://aclanthology.org/2021.naacl-main.169},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#waimai","title":"Waimai","text":"<p>Sentiment Analysis of user reviews on takeaway platforms</p> <p>Dataset: <code>C-MTEB/waimai-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@article{xiao2023c,\n  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},\n  journal = {arXiv preprint arXiv:2309.07597},\n  title = {C-pack: Packaged resources to advance general chinese embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#waimaiv2","title":"Waimai.v2","text":"<p>Sentiment Analysis of user reviews on takeaway platforms         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/waimai</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn not specified not specified not specified Citation <pre><code>@article{xiao2023c,\n  author = {Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighof, Niklas},\n  journal = {arXiv preprint arXiv:2309.07597},\n  title = {C-pack: Packaged resources to advance general chinese embedding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiometchemclassification","title":"WikipediaBioMetChemClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2GeneExpressionVsMetallurgyClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiometchemclassificationv2","title":"WikipediaBioMetChemClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_bio_met_chem</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiolumneurochemclassification","title":"WikipediaBiolumNeurochemClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2BioluminescenceVsNeurochemistryClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemengspecialtiesclassification","title":"WikipediaChemEngSpecialtiesClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium5Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemfieldsclassification","title":"WikipediaChemFieldsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEZ10Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemfieldsclassificationv2","title":"WikipediaChemFieldsClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_chem_fields</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemistrytopicsclassification","title":"WikipediaChemistryTopicsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy10Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacompchemspectroscopyclassification","title":"WikipediaCompChemSpectroscopyClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2ComputationalVsSpectroscopistsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacompchemspectroscopyclassificationv2","title":"WikipediaCompChemSpectroscopyClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_comp_chem_spectroscopy</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacryobiologyseparationclassification","title":"WikipediaCryobiologySeparationClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy5Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacrystallographyanalyticalclassification","title":"WikipediaCrystallographyAnalyticalClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2CrystallographyVsChromatographyTitrationpHClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacrystallographyanalyticalclassificationv2","title":"WikipediaCrystallographyAnalyticalClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_crystallography_analytical</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediagreenhouseenantiopureclassification","title":"WikipediaGreenhouseEnantiopureClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2GreenhouseVsEnantiopureClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediaisotopesfissionclassification","title":"WikipediaIsotopesFissionClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2IsotopesVsFissionProductsNuclearFissionClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipedialuminescenceclassification","title":"WikipediaLuminescenceClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2BioluminescenceVsLuminescenceClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediaorganicinorganicclassification","title":"WikipediaOrganicInorganicClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2SpecialClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediasaltssemiconductorsclassification","title":"WikipediaSaltsSemiconductorsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2SaltsVsSemiconductorMaterialsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediasolidstatecolloidalclassification","title":"WikipediaSolidStateColloidalClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2SolidStateVsColloidalClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediatheoreticalappliedclassification","title":"WikipediaTheoreticalAppliedClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEZ2Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediatheoreticalappliedclassificationv2","title":"WikipediaTheoreticalAppliedClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_theoretical_applied</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wisesightsentimentclassification","title":"WisesightSentimentClassification","text":"<p>Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question)</p> <p>Dataset: <code>mteb/WisesightSentimentClassification</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tha News, Social, Written expert-annotated found Citation <pre><code>@software{bact_2019_3457447,\n  author = {Suriyawongkul, Arthit and\nChuangsuwanich, Ekapol and\nChormai, Pattarawat and\nPolpanumas, Charin},\n  doi = {10.5281/zenodo.3457447},\n  month = sep,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/wisesight-sentiment: First release},\n  url = {https://doi.org/10.5281/zenodo.3457447},\n  version = {v1.0},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wisesightsentimentclassificationv2","title":"WisesightSentimentClassification.v2","text":"<p>Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question)         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wisesight_sentiment</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tha News, Social, Written expert-annotated found Citation <pre><code>@software{bact_2019_3457447,\n  author = {Suriyawongkul, Arthit and\nChuangsuwanich, Ekapol and\nChormai, Pattarawat and\nPolpanumas, Charin},\n  doi = {10.5281/zenodo.3457447},\n  month = sep,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/wisesight-sentiment: First release},\n  url = {https://doi.org/10.5281/zenodo.3457447},\n  version = {v1.0},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wongnaireviewsclassification","title":"WongnaiReviewsClassification","text":"<p>Wongnai features over 200,000 restaurants, beauty salons, and spas across Thailand on its platform, with detailed information about each merchant and user reviews. In this dataset there are 5 classes corresponding each star rating</p> <p>Dataset: <code>Wongnai/wongnai_reviews</code> \u2022 License: lgpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tha Reviews, Written derived found Citation <pre><code>@software{cstorm125_2020_3852912,\n  author = {cstorm125 and lukkiddd},\n  doi = {10.5281/zenodo.3852912},\n  month = may,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/classification-benchmarks: v0.1-alpha},\n  url = {https://doi.org/10.5281/zenodo.3852912},\n  version = {v0.1-alpha},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yahooanswerstopicsclassification","title":"YahooAnswersTopicsClassification","text":"<p>Dataset composed of questions and answers from Yahoo Answers, categorized into topics.</p> <p>Dataset: <code>mteb/YahooAnswersTopicsClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Web, Written human-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yahooanswerstopicsclassificationv2","title":"YahooAnswersTopicsClassification.v2","text":"<p>Dataset composed of questions and answers from Yahoo Answers, categorized into topics.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yahoo_answers_topics</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Web, Written human-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yelpreviewfullclassification","title":"YelpReviewFullClassification","text":"<p>Yelp Review Full is a dataset for sentiment analysis, containing 5 classes corresponding to ratings 1-5.</p> <p>Dataset: <code>Yelp/yelp_review_full</code> \u2022 License: https://huggingface.co/datasets/Yelp/yelp_review_full#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yelpreviewfullclassificationv2","title":"YelpReviewFullClassification.v2","text":"<p>Yelp Review Full is a dataset for sentiment analysis, containing 5 classes corresponding to ratings 1-5.         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yelp_review_full</code> \u2022 License: https://huggingface.co/datasets/Yelp/yelp_review_full#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yueopenricereviewclassification","title":"YueOpenriceReviewClassification","text":"<p>A Cantonese dataset for review classification</p> <p>Dataset: <code>izhx/yue-openrice-review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy yue Reviews, Spoken human-annotated found Citation <pre><code>@inproceedings{xiang2019sentiment,\n  author = {Xiang, Rong and Jiao, Ying and Lu, Qin},\n  booktitle = {Proceedings of the 8th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)},\n  organization = {KDD WISDOM},\n  pages = {1--9},\n  title = {Sentiment Augmented Attention Network for Cantonese Restaurant Review Analysis},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yueopenricereviewclassificationv2","title":"YueOpenriceReviewClassification.v2","text":"<p>A Cantonese dataset for review classification         This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yue_openrice_review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy yue Reviews, Spoken human-annotated found Citation <pre><code>@inproceedings{xiang2019sentiment,\n  author = {Xiang, Rong and Jiao, Ying and Lu, Qin},\n  booktitle = {Proceedings of the 8th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)},\n  organization = {KDD WISDOM},\n  pages = {1--9},\n  title = {Sentiment Augmented Attention Network for Cantonese Restaurant Review Analysis},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/","title":"Clustering","text":"<ul> <li>Number of tasks: 109</li> </ul>"},{"location":"overview/available_tasks/clustering/#alloprofclusteringp2p","title":"AlloProfClusteringP2P","text":"<p>Clustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringP2P</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusteringp2pv2","title":"AlloProfClusteringP2P.v2","text":"<p>Clustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringP2P.v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusterings2s","title":"AlloProfClusteringS2S","text":"<p>Clustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringS2S</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusterings2sv2","title":"AlloProfClusteringS2S.v2","text":"<p>Clustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringS2S.v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivhierarchicalclusteringp2p","title":"ArXivHierarchicalClusteringP2P","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#arxivhierarchicalclusterings2s","title":"ArXivHierarchicalClusteringS2S","text":"<p>Clustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#arxivclusteringp2p","title":"ArxivClusteringP2P","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivclusteringp2pv2","title":"ArxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivclusterings2s","title":"ArxivClusteringS2S","text":"<p>Clustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#beytooteclustering","title":"BeytooteClustering","text":"<p>Beytoote Website Articles Clustering</p> <p>Dataset: <code>MCINext/beytoote-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas News derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#bigpatentclustering","title":"BigPatentClustering","text":"<p>Clustering of documents from the Big Patent dataset. Test set only includes documents belonging to a single category, with a total of 9 categories.</p> <p>Dataset: <code>jinaai/big-patent-clustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT:} {A} Large-Scale Dataset for Abstractive and Coherent\nSummarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#bigpatentclusteringv2","title":"BigPatentClustering.v2","text":"<p>Clustering of documents from the Big Patent dataset. Test set only includes documents belonging to a single category, with a total of 9 categories.</p> <p>Dataset: <code>mteb/big-patent</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT:} {A} Large-Scale Dataset for Abstractive and Coherent\nSummarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#biorxivclusteringp2p","title":"BiorxivClusteringP2P","text":"<p>Clustering of titles+abstract from biorxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/biorxiv-clustering-p2p</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusteringp2pv2","title":"BiorxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from biorxiv across 26 categories.</p> <p>Dataset: <code>mteb/biorxiv-clustering-p2p</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusterings2s","title":"BiorxivClusteringS2S","text":"<p>Clustering of titles from biorxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/biorxiv-clustering-s2s</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusterings2sv2","title":"BiorxivClusteringS2S.v2","text":"<p>Clustering of titles from biorxiv across 26 categories.</p> <p>Dataset: <code>mteb/biorxiv-clustering-s2s</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#blurbsclusteringp2p","title":"BlurbsClusteringP2P","text":"<p>Clustering of book titles+blurbs. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Written not specified not specified Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusteringp2pv2","title":"BlurbsClusteringP2P.v2","text":"<p>Clustering of book titles+blurbs. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-p2p</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusterings2s","title":"BlurbsClusteringS2S","text":"<p>Clustering of book titles. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-s2s</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Written not specified not specified Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusterings2sv2","title":"BlurbsClusteringS2S.v2","text":"<p>Clustering of book titles. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-s2s</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#builtbenchclusteringp2p","title":"BuiltBenchClusteringP2P","text":"<p>Clustering of built asset item descriptions based on categories identified within industry classification systems such as IFC, Uniclass, etc.</p> <p>Dataset: <code>mehrzad-shahin/BuiltBench-clustering-p2p</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#builtbenchclusterings2s","title":"BuiltBenchClusteringS2S","text":"<p>Clustering of built asset names/titles based on categories identified within industry classification systems such as IFC, Uniclass, etc.</p> <p>Dataset: <code>mehrzad-shahin/BuiltBench-clustering-s2s</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusteringp2p","title":"CLSClusteringP2P","text":"<p>Clustering of titles + abstract from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringP2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn not specified not specified not specified Citation <pre><code>@article{li2022csl,\n  author = {Li, Yudong and Zhang, Yuqing and Zhao, Zhe and Shen, Linlin and Liu, Weijie and Mao, Weiquan and Zhang, Hui},\n  journal = {arXiv preprint arXiv:2209.05034},\n  title = {CSL: A large-scale Chinese scientific literature dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusteringp2pv2","title":"CLSClusteringP2P.v2","text":"<p>Clustering of titles + abstract from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringP2P</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@misc{li2022csl,\n  archiveprefix = {arXiv},\n  author = {Yudong Li and Yuqing Zhang and Zhe Zhao and Linlin Shen and Weijie Liu and Weiquan Mao and Hui Zhang},\n  eprint = {2209.05034},\n  primaryclass = {cs.CL},\n  title = {CSL: A Large-scale Chinese Scientific Literature Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusterings2s","title":"CLSClusteringS2S","text":"<p>Clustering of titles from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn not specified not specified not specified Citation <pre><code>@article{li2022csl,\n  author = {Li, Yudong and Zhang, Yuqing and Zhao, Zhe and Shen, Linlin and Liu, Weijie and Mao, Weiquan and Zhang, Hui},\n  journal = {arXiv preprint arXiv:2209.05034},\n  title = {CSL: A large-scale Chinese scientific literature dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusterings2sv2","title":"CLSClusteringS2S.v2","text":"<p>Clustering of titles from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringS2S</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@misc{li2022csl,\n  archiveprefix = {arXiv},\n  author = {Yudong Li and Yuqing Zhang and Zhe Zhao and Linlin Shen and Weijie Liu and Weiquan Mao and Hui Zhang},\n  eprint = {2209.05034},\n  primaryclass = {cs.CL},\n  title = {CSL: A Large-scale Chinese Scientific Literature Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clustrec-covid","title":"ClusTREC-Covid","text":"<p>A Topical Clustering Benchmark for COVID-19 Scientific Research across 50 covid-19 related topics.</p> <p>Dataset: <code>Uri-ka/ClusTREC-Covid</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written expert-annotated created Citation <pre><code>@inproceedings{katz-etal-2024-knowledge,\n  address = {Miami, Florida, USA},\n  author = {Katz, Uri  and\nLevy, Mosh  and\nGoldberg, Yoav},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},\n  month = nov,\n  pages = {8838--8855},\n  publisher = {Association for Computational Linguistics},\n  title = {Knowledge Navigator: {LLM}-guided Browsing Framework for Exploratory Search in Scientific Literature},\n  url = {https://aclanthology.org/2024.findings-emnlp.516},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#digikalamagclustering","title":"DigikalamagClustering","text":"<p>A total of 8,515 articles scraped from Digikala Online Magazine. This dataset includes seven different classes.</p> <p>Dataset: <code>mteb/DigikalamagClustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#dutchnewsarticlesclusteringp2p","title":"DutchNewsArticlesClusteringP2P","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld News, Written derived found"},{"location":"overview/available_tasks/clustering/#dutchnewsarticlesclusterings2s","title":"DutchNewsArticlesClusteringS2S","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld News, Written derived found"},{"location":"overview/available_tasks/clustering/#eighttagsclustering","title":"EightTagsClustering","text":"<p>Clustering of headlines from social media posts in Polish belonging to 8 categories: film, history, food, medicine, motorization, work, sport and technology.</p> <p>Dataset: <code>PL-MTEB/8tags-clustering</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Social, Written derived found Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\\\l}kiewicz, Micha{\\\\l}  and\nPo{\\\\'s}wiata, Rafa{\\\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#eighttagsclusteringv2","title":"EightTagsClustering.v2","text":"<p>Clustering of headlines from social media posts in Polish belonging to 8 categories: film, history, food, medicine, motorization, work, sport and technology.</p> <p>Dataset: <code>PL-MTEB/8tags-clustering</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Social, Written derived found Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\\\l}kiewicz, Micha{\\\\l}  and\nPo{\\\\'s}wiata, Rafa{\\\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\\\'e}chet, Fr{\\\\'e}d{\\\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#georeviewclusteringp2p","title":"GeoreviewClusteringP2P","text":"<p>Review clustering based on Yandex Georeview dataset</p> <p>Dataset: <code>ai-forever/georeview-clustering-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Reviews, Written derived found"},{"location":"overview/available_tasks/clustering/#halclusterings2s","title":"HALClusteringS2S","text":"<p>Clustering of titles from HAL (https://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s)</p> <p>Dataset: <code>lyon-nlp/clustering-hal-s2s</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Academic, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#halclusterings2sv2","title":"HALClusteringS2S.v2","text":"<p>Clustering of titles from HAL (https://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s)</p> <p>Dataset: <code>lyon-nlp/clustering-hal-s2s</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Academic, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humearxivclusteringp2p","title":"HUMEArxivClusteringP2P","text":"<p>Human evaluation subset of Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/mteb-human-arxiv-clustering</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humeredditclusteringp2p","title":"HUMERedditClusteringP2P","text":"<p>Human evaluation subset of Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/mteb-human-reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humesib200clusterings2s","title":"HUMESIB200ClusteringS2S","text":"<p>Human evaluation subset of Clustering of news article headlines from SIB-200. Clustering of 10 sets, each with 8 categories and 10 texts per category.</p> <p>Dataset: <code>mteb/mteb-human-sib200-clustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure ara, dan, eng, fra, rus News, Written derived found Citation <pre><code>@inproceedings{adelani-etal-2023-sib,\n  address = {Toronto, Canada},\n  author = {Adelani, David Ifeoluwa  and\nHedderich, Michael A.  and\nZhu, Dawei  and\nvan den Berg, Esther  and\nKlakow, Dietrich},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2023.acl-long.660},\n  month = jul,\n  pages = {11784--11801},\n  publisher = {Association for Computational Linguistics},\n  title = {{SIB}-200: A Large-Scale News Classification Dataset for Over 200 Languages},\n  url = {https://aclanthology.org/2023.acl-long.660},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humewikicitiesclustering","title":"HUMEWikiCitiesClustering","text":"<p>Human evaluation subset of Clustering of Wikipedia articles of cities by country from https://huggingface.co/datasets/wikipedia. Test set includes 126 countries, and a total of 3531 cities.</p> <p>Dataset: <code>mteb/mteb-human-wikicities-clustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Encyclopaedic, Written derived found Citation <pre><code>@online{wikidump,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#hamshahriclustring","title":"HamshahriClustring","text":"<p>These datasets have been extracted from the RSS feed of two Farsi news agency websites.</p> <p>Dataset: <code>community-datasets/farsi_news</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas News derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#iconclassclusterings2s","title":"IconclassClusteringS2S","text":"<p>Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. The task is to classify the first layer of Iconclass</p> <p>Dataset: <code>clips/mteb-nl-iconclass-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Fiction, Written derived found Citation <pre><code>@article{banar2023transfer,\n  author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},\n  journal = {ACM Journal on Computing and Cultural Heritage},\n  number = {2},\n  pages = {1--16},\n  publisher = {ACM New York, NY},\n  title = {Transfer learning for the visual arts: The multi-modal retrieval of iconclass codes},\n  volume = {16},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#indicreviewsclusteringp2p","title":"IndicReviewsClusteringP2P","text":"<p>Clustering of reviews from IndicSentiment dataset. Clustering of 14 sets on the generic categories label.</p> <p>Dataset: <code>mteb/IndicReviewsClusteringP2P</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure asm, ben, brx, guj, hin, ... (13) Reviews, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#kluemrcdomainclustering","title":"KlueMrcDomainClustering","text":"<p>this dataset is a processed and redistributed version of the KLUE-MRC dataset. Domain: Game / Media / Automotive / Finance / Real Estate / Education</p> <p>Dataset: <code>mteb/KlueMrcDomainClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#klueynatmrccategoryclustering","title":"KlueYnatMrcCategoryClustering","text":"<p>this dataset is a processed and redistributed version of the KLUE-Ynat &amp; KLUE-MRC  dataset. News_category: IT/Science, Sports, Media/Culture, Ecomomy/Finance, Real Estate</p> <p>Dataset: <code>mteb/KlueYnatMrcCategoryClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#livedoornewsclustering","title":"LivedoorNewsClustering","text":"<p>Clustering of the news reports of a Japanese news site, Livedoor News by RONDHUIT Co, Ltd. in 2012. It contains over 7,000 news report texts across 9 categories (topics).</p> <p>Dataset: <code>mteb/LivedoorNewsClustering</code> \u2022 License: cc-by-nd-2.1-jp \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found"},{"location":"overview/available_tasks/clustering/#livedoornewsclusteringv2","title":"LivedoorNewsClustering.v2","text":"<p>Clustering of the news reports of a Japanese news site, Livedoor News by RONDHUIT Co, Ltd. in 2012. It contains over 7,000 news report texts across 9 categories (topics). Version 2 updated on LivedoorNewsClustering by removing pairs where one of entries contain an empty sentences.</p> <p>Dataset: <code>mteb/LivedoorNewsClustering.v2</code> \u2022 License: cc-by-nd-2.1-jp \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found"},{"location":"overview/available_tasks/clustering/#mlsumclusteringp2p","title":"MLSUMClusteringP2P","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusteringp2pv2","title":"MLSUMClusteringP2P.v2","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusterings2s","title":"MLSUMClusteringS2S","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusterings2sv2","title":"MLSUMClusteringS2S.v2","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#masakhanewsclusteringp2p","title":"MasakhaNEWSClusteringP2P","text":"<p>Clustering of news article headlines and texts from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.</p> <p>Dataset: <code>mteb/MasakhaNEWSClusteringP2P</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure amh, eng, fra, hau, ibo, ... (16) News, Non-fiction, Written derived found Citation <pre><code>@article{adelani2023masakhanews,\n  author = {David Ifeoluwa Adelani and  Marek Masiak and  Israel Abebe Azime and  Jesujoba Oluwadara Alabi and  Atnafu Lambebo Tonja and  Christine Mwase and  Odunayo Ogundepo and  Bonaventure F. P. Dossou and  Akintunde Oladipo and  Doreen Nixdorf and  Chris Chinenye Emezue and  Sana Sabah al-azzawi and  Blessing K. Sibanda and  Davis David and  Lolwethu Ndolela and  Jonathan Mukiibi and  Tunde Oluwaseyi Ajayi and  Tatiana Moteu Ngoli and  Brian Odhiambo and  Abraham Toluwase Owodunni and  Nnaemeka C. Obiefuna and  Shamsuddeen Hassan Muhammad and  Saheed Salahudeen Abdullahi and  Mesay Gemeda Yigezu and  Tajuddeen Gwadabe and  Idris Abdulmumin and  Mahlet Taye Bame and  Oluwabusayo Olufunke Awoyomi and  Iyanuoluwa Shode and  Tolulope Anu Adelani and  Habiba Abdulganiy Kailani and  Abdul-Hakeem Omotayo and  Adetola Adeeko and  Afolabi Abeeb and  Anuoluwapo Aremu and  Olanrewaju Samuel and  Clemencia Siro and  Wangari Kimotho and  Onyekachi Raphael Ogbu and  Chinedu E. Mbonu and  Chiamaka I. Chukwuneke and  Samuel Fanijo and  Jessica Ojo and  Oyinkansola F. Awosan and  Tadesse Kebede Guge and  Sakayo Toadoum Sari and  Pamela Nyatsine and  Freedmore Sidume and  Oreen Yousuf and  Mardiyyah Oduwole and  Ussen Kimanuka and  Kanda Patrick Tshinu and  Thina Diko and  Siyanda Nxakama and   Abdulmejid Tuni Johar and  Sinodos Gebre and  Muhidin Mohamed and  Shafie Abdi Mohamed and  Fuad Mire Hassan and  Moges Ahmed Mehamed and  Evrard Ngabire and  and Pontus Stenetorp},\n  journal = {ArXiv},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  volume = {},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#masakhanewsclusterings2s","title":"MasakhaNEWSClusteringS2S","text":"<p>Clustering of news article headlines from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.</p> <p>Dataset: <code>mteb/MasakhaNEWSClusteringS2S</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure amh, eng, fra, hau, ibo, ... (16) News, Written human-annotated not specified Citation <pre><code>@article{adelani2023masakhanews,\n  author = {David Ifeoluwa Adelani and  Marek Masiak and  Israel Abebe Azime and  Jesujoba Oluwadara Alabi and  Atnafu Lambebo Tonja and  Christine Mwase and  Odunayo Ogundepo and  Bonaventure F. P. Dossou and  Akintunde Oladipo and  Doreen Nixdorf and  Chris Chinenye Emezue and  Sana Sabah al-azzawi and  Blessing K. Sibanda and  Davis David and  Lolwethu Ndolela and  Jonathan Mukiibi and  Tunde Oluwaseyi Ajayi and  Tatiana Moteu Ngoli and  Brian Odhiambo and  Abraham Toluwase Owodunni and  Nnaemeka C. Obiefuna and  Shamsuddeen Hassan Muhammad and  Saheed Salahudeen Abdullahi and  Mesay Gemeda Yigezu and  Tajuddeen Gwadabe and  Idris Abdulmumin and  Mahlet Taye Bame and  Oluwabusayo Olufunke Awoyomi and  Iyanuoluwa Shode and  Tolulope Anu Adelani and  Habiba Abdulganiy Kailani and  Abdul-Hakeem Omotayo and  Adetola Adeeko and  Afolabi Abeeb and  Anuoluwapo Aremu and  Olanrewaju Samuel and  Clemencia Siro and  Wangari Kimotho and  Onyekachi Raphael Ogbu and  Chinedu E. Mbonu and  Chiamaka I. Chukwuneke and  Samuel Fanijo and  Jessica Ojo and  Oyinkansola F. Awosan and  Tadesse Kebede Guge and  Sakayo Toadoum Sari and  Pamela Nyatsine and  Freedmore Sidume and  Oreen Yousuf and  Mardiyyah Oduwole and  Ussen Kimanuka and  Kanda Patrick Tshinu and  Thina Diko and  Siyanda Nxakama and   Abdulmejid Tuni Johar and  Sinodos Gebre and  Muhidin Mohamed and  Shafie Abdi Mohamed and  Fuad Mire Hassan and  Moges Ahmed Mehamed and  Evrard Ngabire and  and Pontus Stenetorp},\n  journal = {ArXiv},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  volume = {},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#medrxivclusteringp2p","title":"MedrxivClusteringP2P","text":"<p>Clustering of titles+abstract from medrxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/medrxiv-clustering-p2p</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusteringp2pv2","title":"MedrxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from medrxiv across 51 categories.</p> <p>Dataset: <code>mteb/medrxiv-clustering-p2p</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusterings2s","title":"MedrxivClusteringS2S","text":"<p>Clustering of titles from medrxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/medrxiv-clustering-s2s</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusterings2sv2","title":"MedrxivClusteringS2S.v2","text":"<p>Clustering of titles from medrxiv across 51 categories.</p> <p>Dataset: <code>mteb/medrxiv-clustering-s2s</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#mewsc16jaclustering","title":"MewsC16JaClustering","text":"<p>MewsC-16 (Multilingual Short Text Clustering Dataset for News in 16 languages) is constructed from Wikinews.         This dataset is the Japanese split of MewsC-16, containing topic sentences from Wikinews articles in 12 categories.         More detailed information is available in the Appendix E of the citation.</p> <p>Dataset: <code>mteb/MewsC16JaClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found Citation <pre><code>@inproceedings{nishikawa-etal-2022-ease,\n  address = {Seattle, United States},\n  author = {Nishikawa, Sosuke  and\nRi, Ryokan  and\nYamada, Ikuya  and\nTsuruoka, Yoshimasa  and\nEchizen, Isao},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  month = jul,\n  pages = {3870--3885},\n  publisher = {Association for Computational Linguistics},\n  title = {{EASE}: Entity-Aware Contrastive Learning of Sentence Embedding},\n  url = {https://aclanthology.org/2022.naacl-main.284},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#nlptwitteranalysisclustering","title":"NLPTwitterAnalysisClustering","text":"<p>Clustering of tweets from twitter across 26 categories.</p> <p>Dataset: <code>hamedhf/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#opentenderclusteringp2p","title":"OpenTenderClusteringP2P","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-opentender-cls-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#opentenderclusterings2s","title":"OpenTenderClusteringS2S","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-opentender-clst-s2s-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#plscclusteringp2p","title":"PlscClusteringP2P","text":"<p>Clustering of Polish article titles+abstracts from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusteringp2pv2","title":"PlscClusteringP2P.v2","text":"<p>Clustering of Polish article titles+abstracts from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusterings2s","title":"PlscClusteringS2S","text":"<p>Clustering of Polish article titles from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusterings2sv2","title":"PlscClusteringS2S.v2","text":"<p>Clustering of Polish article titles from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#redditclustering","title":"RedditClustering","text":"<p>Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclustering-vn","title":"RedditClustering-VN","text":"<p>A translated dataset from Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/reddit-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Social, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringv2","title":"RedditClustering.v2","text":"<p>Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2p","title":"RedditClusteringP2P","text":"<p>Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/reddit-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2p-vn","title":"RedditClusteringP2P-VN","text":"<p>A translated dataset from Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/reddit-clustering-p2p-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Social, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2pv2","title":"RedditClusteringP2P.v2","text":"<p>Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/reddit-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#romanibibleclustering","title":"RomaniBibleClustering","text":"<p>Clustering verses from the Bible in Kalderash Romani by book.</p> <p>Dataset: <code>kardosdrur/romani-bible</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rom Religious, Written derived human-translated and localized"},{"location":"overview/available_tasks/clustering/#ruscibenchgrnticlusteringp2p","title":"RuSciBenchGRNTIClusteringP2P","text":"<p>Clustering of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-grnti-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#ruscibenchoecdclusteringp2p","title":"RuSciBenchOECDClusteringP2P","text":"<p>Clustering of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-oecd-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#sib200clusterings2s","title":"SIB200ClusteringS2S","text":"<p>SIB-200 is the largest publicly available topic classification         dataset based on Flores-200 covering 205 languages and dialects annotated. The dataset is         annotated in English for the topics,  science/technology, travel, politics, sports,         health, entertainment, and geography. The labels are then transferred to the other languages         in Flores-200 which are human-translated.</p> <p>Dataset: <code>mteb/sib200</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure ace, acm, acq, aeb, afr, ... (197) News, Written expert-annotated human-translated and localized Citation <pre><code>@article{adelani2023sib,\n  author = {Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},\n  journal = {arXiv preprint arXiv:2309.07445},\n  title = {SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#sidclustring","title":"SIDClustring","text":"<p>Clustering of summariesfrom SIDClustring across categories.</p> <p>Dataset: <code>MCINext/sid-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlclustering","title":"SNLClustering","text":"<p>Webscraped articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>adrlau/navjordj-SNL_summarization_copy</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlhierarchicalclusteringp2p","title":"SNLHierarchicalClusteringP2P","text":"<p>Webscrabed articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>mteb/SNLHierarchicalClusteringP2P</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlhierarchicalclusterings2s","title":"SNLHierarchicalClusteringS2S","text":"<p>Webscrabed articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>mteb/SNLHierarchicalClusteringS2S</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#spanishnewsclusteringp2p","title":"SpanishNewsClusteringP2P","text":"<p>Clustering of news articles, 7 topics in total.</p> <p>Dataset: <code>jinaai/spanish_news_clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure spa not specified not specified not specified"},{"location":"overview/available_tasks/clustering/#stackexchangeclustering","title":"StackExchangeClustering","text":"<p>Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/stackexchange-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclustering-vn","title":"StackExchangeClustering-VN","text":"<p>A translated dataset from Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stackexchange-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringv2","title":"StackExchangeClustering.v2","text":"<p>Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/stackexchange-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2p","title":"StackExchangeClusteringP2P","text":"<p>Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs.</p> <p>Dataset: <code>mteb/stackexchange-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2p-vn","title":"StackExchangeClusteringP2P-VN","text":"<p>A translated Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stackexchange-clustering-p2p-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2pv2","title":"StackExchangeClusteringP2P.v2","text":"<p>Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs.</p> <p>Dataset: <code>mteb/stackexchange-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclustering","title":"SwednClustering","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclusteringp2p","title":"SwednClusteringP2P","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClusteringP2P</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclusterings2s","title":"SwednClusteringS2S","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClusteringS2S</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#tenkgnadclusteringp2p","title":"TenKGnadClusteringP2P","text":"<p>Clustering of news article titles+subheadings+texts. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-p2p</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Web, Written not specified found"},{"location":"overview/available_tasks/clustering/#tenkgnadclusteringp2pv2","title":"TenKGnadClusteringP2P.v2","text":"<p>Clustering of news article titles+subheadings+texts. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-p2p</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written derived found"},{"location":"overview/available_tasks/clustering/#tenkgnadclusterings2s","title":"TenKGnadClusteringS2S","text":"<p>Clustering of news article titles. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-s2s</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written not specified not specified"},{"location":"overview/available_tasks/clustering/#tenkgnadclusterings2sv2","title":"TenKGnadClusteringS2S.v2","text":"<p>Clustering of news article titles. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-s2s</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written derived found"},{"location":"overview/available_tasks/clustering/#thunewsclusteringp2p","title":"ThuNewsClusteringP2P","text":"<p>Clustering of titles + abstracts from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringP2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn not specified not specified not specified Citation <pre><code>@inproceedings{eisner2007proceedings,\n  author = {Eisner, Jason},\n  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},\n  title = {Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n  year = {2007},\n}\n\n@inproceedings{li2006comparison,\n  author = {Li, Jingyang and Sun, Maosong and Zhang, Xian},\n  booktitle = {proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics},\n  pages = {545--552},\n  title = {A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusteringp2pv2","title":"ThuNewsClusteringP2P.v2","text":"<p>Clustering of titles + abstracts from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringP2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@software{THUCTC,\n  author = {Sun, M. and Li, J. and Guo, Z. and Yu, Z. and Zheng, Y. and Si, X. and Liu, Z.},\n  note = {THU Chinese Text Classification Toolkit},\n  publisher = {THU Natural Language Processing Lab},\n  title = {THUCTC: An Efficient Chinese Text Classifier},\n  url = {https://github.com/thunlp/THUCTC},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusterings2s","title":"ThuNewsClusteringS2S","text":"<p>Clustering of titles from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn not specified not specified not specified Citation <pre><code>@inproceedings{eisner2007proceedings,\n  author = {Eisner, Jason},\n  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},\n  title = {Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n  year = {2007},\n}\n\n@inproceedings{li2006comparison,\n  author = {Li, Jingyang and Sun, Maosong and Zhang, Xian},\n  booktitle = {proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics},\n  pages = {545--552},\n  title = {A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusterings2sv2","title":"ThuNewsClusteringS2S.v2","text":"<p>Clustering of titles from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@software{THUCTC,\n  author = {Sun, M. and Li, J. and Guo, Z. and Yu, Z. and Zheng, Y. and Si, X. and Liu, Z.},\n  note = {THU Chinese Text Classification Toolkit},\n  publisher = {THU Natural Language Processing Lab},\n  title = {THUCTC: An Efficient Chinese Text Classifier},\n  url = {https://github.com/thunlp/THUCTC},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclustering","title":"TwentyNewsgroupsClustering","text":"<p>Clustering of the 20 Newsgroups dataset (subject only).</p> <p>Dataset: <code>mteb/twentynewsgroups-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng News, Written derived found Citation <pre><code>@incollection{LANG1995331,\n  address = {San Francisco (CA)},\n  author = {Ken Lang},\n  booktitle = {Machine Learning Proceedings 1995},\n  doi = {https://doi.org/10.1016/B978-1-55860-377-6.50048-7},\n  editor = {Armand Prieditis and Stuart Russell},\n  isbn = {978-1-55860-377-6},\n  pages = {331-339},\n  publisher = {Morgan Kaufmann},\n  title = {NewsWeeder: Learning to Filter Netnews},\n  url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500487},\n  year = {1995},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclustering-vn","title":"TwentyNewsgroupsClustering-VN","text":"<p>A translated dataset from Clustering of the 20 Newsgroups dataset (subject only).             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twentynewsgroups-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie News, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclusteringv2","title":"TwentyNewsgroupsClustering.v2","text":"<p>Clustering of the 20 Newsgroups dataset (subject only).</p> <p>Dataset: <code>mteb/twentynewsgroups-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng News, Written derived found Citation <pre><code>@incollection{LANG1995331,\n  address = {San Francisco (CA)},\n  author = {Ken Lang},\n  booktitle = {Machine Learning Proceedings 1995},\n  doi = {https://doi.org/10.1016/B978-1-55860-377-6.50048-7},\n  editor = {Armand Prieditis and Stuart Russell},\n  isbn = {978-1-55860-377-6},\n  pages = {331-339},\n  publisher = {Morgan Kaufmann},\n  title = {NewsWeeder: Learning to Filter Netnews},\n  url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500487},\n  year = {1995},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vabbclusteringp2p","title":"VABBClusteringP2P","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vabbclusterings2s","title":"VABBClusteringS2S","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vgclustering","title":"VGClustering","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vghierarchicalclusteringp2p","title":"VGHierarchicalClusteringP2P","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vghierarchicalclusterings2s","title":"VGHierarchicalClusteringS2S","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikicitiesclustering","title":"WikiCitiesClustering","text":"<p>Clustering of Wikipedia articles of cities by country from https://huggingface.co/datasets/wikipedia. Test set includes 126 countries, and a total of 3531 cities.</p> <p>Dataset: <code>jinaai/cities_wiki_clustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Encyclopaedic, Written derived found Citation <pre><code>@online{wikidump,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikiclusteringp2p","title":"WikiClusteringP2P","text":"<p>Clustering of wikipedia articles inspired by BlubrbsClusteringP2P. Labels are taken from top-level categories of the respective languages (e.g., https://lv.wikipedia.org/wiki/Kategorija:Pamatkategorijas).</p> <p>Dataset: <code>ryzzlestrizzle/multi-wiki-clustering-p2p</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure bos, cat, ces, dan, eus, ... (14) Encyclopaedic, Written derived created"},{"location":"overview/available_tasks/clustering/#wikiclusteringp2pv2","title":"WikiClusteringP2P.v2","text":"<p>Clustering of wikipedia articles inspired by BlubrbsClusteringP2P. Labels are taken from top-level categories of the respective languages (e.g., https://lv.wikipedia.org/wiki/Kategorija:Pamatkategorijas).</p> <p>Dataset: <code>ryzzlestrizzle/multi-wiki-clustering-p2p</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure bos, cat, ces, dan, eus, ... (14) Encyclopaedic, Written derived created"},{"location":"overview/available_tasks/clustering/#wikipediachemistrytopicsclustering","title":"WikipediaChemistryTopicsClustering","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy10Clustering</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikipediaspecialtiesinchemistryclustering","title":"WikipediaSpecialtiesInChemistryClustering","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium5Clustering</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/","title":"Compositionality","text":"<ul> <li>Number of tasks: 7</li> </ul>"},{"location":"overview/available_tasks/compositionality/#arococoorder","title":"AROCocoOrder","text":"<p>Compositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.</p> <p>Dataset: <code>gowitheflow/ARO-COCO-order</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#aroflickrorder","title":"AROFlickrOrder","text":"<p>Compositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.</p> <p>Dataset: <code>gowitheflow/ARO-Flickr-Order</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#arovisualattribution","title":"AROVisualAttribution","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>gowitheflow/ARO-Visual-Attribution</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#arovisualrelation","title":"AROVisualRelation","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>gowitheflow/ARO-Visual-Relation</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#imagecode","title":"ImageCoDe","text":"<p>Identify the correct image from a set of similar images based on a precise caption.</p> <p>Dataset: <code>JamieSJS/imagecode-multi</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) image_acc eng Web, Written derived found Citation <pre><code>@article{krojer2022image,\n  author = {Krojer, Benno and Adlakha, Vaibhav and Vineet, Vibhav and Goyal, Yash and Ponti, Edoardo and Reddy, Siva},\n  journal = {arXiv preprint arXiv:2203.15867},\n  title = {Image retrieval from contextual descriptions},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#sugarcrepe","title":"SugarCrepe","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>yjkimstats/SUGARCREPE_fmt</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@article{hsieh2024sugarcrepe,\n  author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},\n  journal = {Advances in neural information processing systems},\n  title = {Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality},\n  volume = {36},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#winoground","title":"Winoground","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>facebook/winoground</code> \u2022 License: https://huggingface.co/datasets/facebook/winoground/blob/main/license_agreement.txt \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Social expert-annotated created Citation <pre><code>@misc{thrush2022winogroundprobingvisionlanguage,\n  archiveprefix = {arXiv},\n  author = {Tristan Thrush and Ryan Jiang and Max Bartolo and Amanpreet Singh and Adina Williams and Douwe Kiela and Candace Ross},\n  eprint = {2204.03162},\n  primaryclass = {cs.CV},\n  title = {Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},\n  url = {https://arxiv.org/abs/2204.03162},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/","title":"DocumentUnderstanding","text":"<ul> <li>Number of tasks: 58</li> </ul>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrairbnbsyntheticretrieval","title":"JinaVDRAirbnbSyntheticRetrieval","text":"<p>Retrieve rendered tables from Airbnb listings based on templated queries.</p> <p>Dataset: <code>jinaai/airbnb-synthetic-retrieval_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, deu, eng, fra, hin, ... (10) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarabicchartqaretrieval","title":"JinaVDRArabicChartQARetrieval","text":"<p>Retrieve Arabic charts based on queries.</p> <p>Dataset: <code>jinaai/arabic_chartqa_ar_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarabicinfographicsvqaretrieval","title":"JinaVDRArabicInfographicsVQARetrieval","text":"<p>Retrieve Arabic infographics based on queries.</p> <p>Dataset: <code>jinaai/arabic_infographicsvqa_ar_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarxivqaretrieval","title":"JinaVDRArxivQARetrieval","text":"<p>Retrieve figures from scientific papers from arXiv based on LLM generated queries.</p> <p>Dataset: <code>jinaai/arxivqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrautomobilecatelogretrieval","title":"JinaVDRAutomobileCatelogRetrieval","text":"<p>Retrieve automobile marketing documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/automobile_catalogue_jp_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Engineering, Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrbeveragescatalogueretrieval","title":"JinaVDRBeveragesCatalogueRetrieval","text":"<p>Retrieve beverages marketing documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/beverages_catalogue_ru_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 rus Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrcharxivocrretrieval","title":"JinaVDRCharXivOCRRetrieval","text":"<p>Retrieve charts from scientific papers based on human annotated queries.</p> <p>Dataset: <code>jinaai/CharXiv-en_beir</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrchartqaretrieval","title":"JinaVDRChartQARetrieval","text":"<p>Retrieve charts based on LLM generated queries.</p> <p>Dataset: <code>jinaai/ChartQA_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqaai","title":"JinaVDRDocQAAI","text":"<p>Retrieve AI documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/docqa_artificial_intelligence_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqaenergyretrieval","title":"JinaVDRDocQAEnergyRetrieval","text":"<p>Retrieve energy industry documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/docqa_energy_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqagovreportretrieval","title":"JinaVDRDocQAGovReportRetrieval","text":"<p>Retrieve government reports based on LLM generated queries.</p> <p>Dataset: <code>jinaai/docqa_gov_report_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Government derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqahealthcareindustryretrieval","title":"JinaVDRDocQAHealthcareIndustryRetrieval","text":"<p>Retrieve healthcare industry documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/docqa_healthcare_industry_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocvqaretrieval","title":"JinaVDRDocVQARetrieval","text":"<p>Retrieve industry documents based on human annotated queries.</p> <p>Dataset: <code>jinaai/docvqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdonutvqaisynhmpretrieval","title":"JinaVDRDonutVQAISynHMPRetrieval","text":"<p>Retrieve medical records based on templated queries.</p> <p>Dataset: <code>jinaai/donut_vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanadenewsretrieval","title":"JinaVDREuropeanaDeNewsRetrieval","text":"<p>Retrieve German news articles based on LLM generated queries.</p> <p>Dataset: <code>jinaai/europeana-de-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanaesnewsretrieval","title":"JinaVDREuropeanaEsNewsRetrieval","text":"<p>Retrieve Spanish news articles based on LLM generated queries.</p> <p>Dataset: <code>jinaai/europeana-es-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 spa News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanafrnewsretrieval","title":"JinaVDREuropeanaFrNewsRetrieval","text":"<p>Retrieve French news articles from Europeana based on LLM generated queries.</p> <p>Dataset: <code>jinaai/europeana-fr-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 fra News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanaitscansretrieval","title":"JinaVDREuropeanaItScansRetrieval","text":"<p>Retrieve Italian historical articles based on LLM generated queries.</p> <p>Dataset: <code>jinaai/europeana-it-scans_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ita News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeananllegalretrieval","title":"JinaVDREuropeanaNlLegalRetrieval","text":"<p>Retrieve Dutch historical legal documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/europeana-nl-legal_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 nld Legal LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrgithubreadmeretrieval","title":"JinaVDRGitHubReadmeRetrieval","text":"<p>Retrieve GitHub readme files based their description.</p> <p>Dataset: <code>jinaai/github-readme-retrieval-multilingual_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fra, ... (17) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrhindigovvqaretrieval","title":"JinaVDRHindiGovVQARetrieval","text":"<p>Retrieve Hindi government documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/hindi-gov-vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 hin Government LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrhungariandocqaretrieval","title":"JinaVDRHungarianDocQARetrieval","text":"<p>Retrieve Hungarian documents in various formats based on human annotated queries.</p> <p>Dataset: <code>jinaai/hungarian_doc_qa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 hun Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrinfovqaretrieval","title":"JinaVDRInfovqaRetrieval","text":"<p>Retrieve infographics based on human annotated queries.</p> <p>Dataset: <code>jinaai/infovqa_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrjdocqaretrieval","title":"JinaVDRJDocQARetrieval","text":"<p>Retrieve Japanese documents in various formats based on human annotated queries.</p> <p>Dataset: <code>jinaai/jdocqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrjina2024yearlybookretrieval","title":"JinaVDRJina2024YearlyBookRetrieval","text":"<p>Retrieve pages from the 2024 Jina yearbook based on human annotated questions.</p> <p>Dataset: <code>jinaai/jina_2024_yearly_book_beir</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmmtabretrieval","title":"JinaVDRMMTabRetrieval","text":"<p>Retrieve tables from the MMTab dataset based on queries.</p> <p>Dataset: <code>jinaai/MMTab_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmpmqaretrieval","title":"JinaVDRMPMQARetrieval","text":"<p>Retrieve product manuals based on human annotated queries.</p> <p>Dataset: <code>jinaai/mpmqa_small_beir</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmedicalprescriptionsretrieval","title":"JinaVDRMedicalPrescriptionsRetrieval","text":"<p>Retrieve medical prescriptions based on templated queries.</p> <p>Dataset: <code>jinaai/medical-prescriptions_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrowidchartsretrieval","title":"JinaVDROWIDChartsRetrieval","text":"<p>Retrieve charts from the OWID dataset based on accompanied text snippets.</p> <p>Dataset: <code>jinaai/owid_charts_en_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdropenainewsretrieval","title":"JinaVDROpenAINewsRetrieval","text":"<p>Retrieve news articles from the OpenAI news website based on human annotated queries.</p> <p>Dataset: <code>jinaai/openai-news_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng News, Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrplotqaretrieval","title":"JinaVDRPlotQARetrieval","text":"<p>Retrieve plots from the PlotQA dataset based on LLM generated queries.</p> <p>Dataset: <code>jinaai/plotqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrramensbenchmarkretrieval","title":"JinaVDRRamensBenchmarkRetrieval","text":"<p>Retrieve ramen restaurant marketing documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/ramen_benchmark_jp_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrshanghaimasterplanretrieval","title":"JinaVDRShanghaiMasterPlanRetrieval","text":"<p>Retrieve pages from the Shanghai Master Plan based on human annotated queries.</p> <p>Dataset: <code>jinaai/shanghai_master_plan_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 zho Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrshiftprojectretrieval","title":"JinaVDRShiftProjectRetrieval","text":"<p>Retrieve documents with graphs from the Shift Project based on LLM generated queries.</p> <p>Dataset: <code>jinaai/shiftproject_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrstanfordslideretrieval","title":"JinaVDRStanfordSlideRetrieval","text":"<p>Retrieve scientific and engineering slides based on human annotated queries.</p> <p>Dataset: <code>jinaai/stanford_slide_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrstudentenrollmentsyntheticretrieval","title":"JinaVDRStudentEnrollmentSyntheticRetrieval","text":"<p>Retrieve student enrollment data based on templated queries.</p> <p>Dataset: <code>jinaai/student-enrollment_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtqaretrieval","title":"JinaVDRTQARetrieval","text":"<p>Retrieve textbook pages (images and text) based on LLM generated queries from the text.</p> <p>Dataset: <code>jinaai/tqa_beir</code> \u2022 License: cc-by-nc-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtabfquadretrieval","title":"JinaVDRTabFQuadRetrieval","text":"<p>Retrieve tables from industry documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/tabfquad_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtablevqaretrieval","title":"JinaVDRTableVQARetrieval","text":"<p>Retrieve scientific tables based on LLM generated queries.</p> <p>Dataset: <code>jinaai/table-vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtatqaretrieval","title":"JinaVDRTatQARetrieval","text":"<p>Retrieve financial reports based on human annotated queries.</p> <p>Dataset: <code>jinaai/tatqa_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtweetstocksyntheticsretrieval","title":"JinaVDRTweetStockSyntheticsRetrieval","text":"<p>Retrieve rendered tables of stock prices based on templated queries.</p> <p>Dataset: <code>jinaai/tweet-stock-synthetic-retrieval_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, deu, eng, fra, hin, ... (10) Social derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrwikimediacommonsdocumentsretrieval","title":"JinaVDRWikimediaCommonsDocumentsRetrieval","text":"<p>Retrieve historical documents from Wikimedia Commons based on their description.</p> <p>Dataset: <code>jinaai/wikimedia-commons-documents-ml_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fra, ... (20) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrwikimediacommonsmapsretrieval","title":"JinaVDRWikimediaCommonsMapsRetrieval","text":"<p>Retrieve maps from Wikimedia Commons based on their description.</p> <p>Dataset: <code>jinaai/wikimedia-commons-maps_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#miraclvisionretrieval","title":"MIRACLVisionRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>nvidia/miracl-vision</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fas, ... (18) Encyclopaedic derived created Citation <pre><code>@article{osmulski2025miraclvisionlargemultilingualvisual,\n  author = {Radek Osmulski and Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Benedikt Schifferer and Even Oldridge},\n  eprint = {2505.11651},\n  journal = {arxiv},\n  title = {{MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark}},\n  url = {https://arxiv.org/abs/2505.11651},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2biomedicallecturesretrieval","title":"Vidore2BioMedicalLecturesRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/biomedical_lectures_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2esgreportshlretrieval","title":"Vidore2ESGReportsHLRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/esg_reports_human_labeled_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2esgreportsretrieval","title":"Vidore2ESGReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/esg_reports_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2economicsreportsretrieval","title":"Vidore2EconomicsReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/economics_reports_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidorearxivqaretrieval","title":"VidoreArxivQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/arxivqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoredocvqaretrieval","title":"VidoreDocVQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/docvqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoreinfovqaretrieval","title":"VidoreInfoVQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/infovqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoreshiftprojectretrieval","title":"VidoreShiftProjectRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/shiftproject_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqaairetrieval","title":"VidoreSyntheticDocQAAIRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/syntheticDocQA_artificial_intelligence_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqaenergyretrieval","title":"VidoreSyntheticDocQAEnergyRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/syntheticDocQA_energy_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqagovernmentreportsretrieval","title":"VidoreSyntheticDocQAGovernmentReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/syntheticDocQA_government_reports_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqahealthcareindustryretrieval","title":"VidoreSyntheticDocQAHealthcareIndustryRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/syntheticDocQA_healthcare_industry_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoretabfquadretrieval","title":"VidoreTabfquadRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/tabfquad_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoretatdqaretrieval","title":"VidoreTatdqaRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/tatdqa_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/","title":"ImageClassification","text":"<ul> <li>Number of tasks: 22</li> </ul>"},{"location":"overview/available_tasks/imageclassification/#birdsnap","title":"Birdsnap","text":"<p>Classifying bird images from 500 species.</p> <p>Dataset: <code>isaacchung/birdsnap</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Berg_2014_CVPR,\n  author = {Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L. and Jacobs, David W. and Belhumeur, Peter N.},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Birdsnap: Large-scale Fine-grained Visual Categorization of Birds},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#cifar10","title":"CIFAR10","text":"<p>Classifying images from 10 classes.</p> <p>Dataset: <code>uoft-cs/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#cifar100","title":"CIFAR100","text":"<p>Classifying images from 100 classes.</p> <p>Dataset: <code>uoft-cs/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#caltech101","title":"Caltech101","text":"<p>Classifying images of 101 widely varied objects.</p> <p>Dataset: <code>mteb/Caltech101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{1384978,\n  author = {Li Fei-Fei and Fergus, R. and Perona, P.},\n  booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n  doi = {10.1109/CVPR.2004.383},\n  keywords = {Bayesian methods;Testing;Humans;Maximum likelihood estimation;Assembly;Shape;Machine vision;Image recognition;Parameter estimation;Image databases},\n  number = {},\n  pages = {178-178},\n  title = {Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},\n  volume = {},\n  year = {2004},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#country211","title":"Country211","text":"<p>Classifying images of 211 countries.</p> <p>Dataset: <code>clip-benchmark/wds_country211</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@article{radford2021learning,\n  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},\n  journal = {arXiv preprint arXiv:2103.00020},\n  title = {Learning Transferable Visual Models From Natural Language Supervision},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#dtd","title":"DTD","text":"<p>Describable Textures Dataset in 47 categories.</p> <p>Dataset: <code>tanganke/dtd</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{cimpoi14describing,\n  author = {M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and and A. Vedaldi},\n  booktitle = {Proceedings of the {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},\n  title = {Describing Textures in the Wild},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#eurosat","title":"EuroSAT","text":"<p>Classifying satellite images.</p> <p>Dataset: <code>timm/eurosat-rgb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{8736785,\n  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n  doi = {10.1109/JSTARS.2019.2918242},\n  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n  keywords = {Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing;Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images},\n  number = {7},\n  pages = {2217-2226},\n  title = {EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},\n  volume = {12},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#fer2013","title":"FER2013","text":"<p>Classifying facial emotions.</p> <p>Dataset: <code>clip-benchmark/wds_fer2013</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{goodfellow2015explainingharnessingadversarialexamples,\n  archiveprefix = {arXiv},\n  author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},\n  eprint = {1412.6572},\n  primaryclass = {stat.ML},\n  title = {Explaining and Harnessing Adversarial Examples},\n  url = {https://arxiv.org/abs/1412.6572},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#fgvcaircraft","title":"FGVCAircraft","text":"<p>Classifying aircraft images from 41 manufacturers and 102 variants.</p> <p>Dataset: <code>mteb/FGVCAircraft</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#food101classification","title":"Food101Classification","text":"<p>Classifying food.</p> <p>Dataset: <code>ethz/food101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Web derived created Citation <pre><code>@inproceedings{bossard14,\n  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n  booktitle = {European Conference on Computer Vision},\n  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#gtsrb","title":"GTSRB","text":"<p>The German Traffic Sign Recognition Benchmark (GTSRB) is a multi-class classification dataset for traffic signs. It consists of dataset of more than 50,000 traffic sign images. The dataset comprises 43 classes with unbalanced class frequencies.</p> <p>Dataset: <code>clip-benchmark/wds_gtsrb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@inproceedings{6033395,\n  author = {Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},\n  booktitle = {The 2011 International Joint Conference on Neural Networks},\n  doi = {10.1109/IJCNN.2011.6033395},\n  keywords = {Humans;Training;Image color analysis;Benchmark testing;Lead;Histograms;Image resolution},\n  number = {},\n  pages = {1453-1460},\n  title = {The German Traffic Sign Recognition Benchmark: A multi-class classification competition},\n  volume = {},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#imagenet1k","title":"Imagenet1k","text":"<p>ImageNet, a large-scale ontology of images built upon the backbone of the WordNet structure.</p> <p>Dataset: <code>clip-benchmark/wds_imagenet1k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene human-annotated created Citation <pre><code>@article{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  organization = {Ieee},\n  pages = {248--255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#mnist","title":"MNIST","text":"<p>Classifying handwritten digits.</p> <p>Dataset: <code>ylecun/mnist</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{lecun2010mnist,\n  author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},\n  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n  title = {MNIST handwritten digit database},\n  volume = {2},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#oxfordflowersclassification","title":"OxfordFlowersClassification","text":"<p>Classifying flowers</p> <p>Dataset: <code>nelorth/oxford-flowers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Reviews derived found Citation <pre><code>@inproceedings{4756141,\n  author = {Nilsback, Maria-Elena and Zisserman, Andrew},\n  booktitle = {2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing},\n  doi = {10.1109/ICVGIP.2008.47},\n  keywords = {Shape;Kernel;Distributed computing;Support vector machines;Support vector machine classification;object classification;segmentation},\n  number = {},\n  pages = {722-729},\n  title = {Automated Flower Classification over a Large Number of Classes},\n  volume = {},\n  year = {2008},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#oxfordpets","title":"OxfordPets","text":"<p>Classifying animal images.</p> <p>Dataset: <code>isaacchung/OxfordPets</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{6248092,\n  author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},\n  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2012.6248092},\n  keywords = {Positron emission tomography;Image segmentation;Cats;Dogs;Layout;Deformable models;Head},\n  number = {},\n  pages = {3498-3505},\n  title = {Cats and dogs},\n  volume = {},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#patchcamelyon","title":"PatchCamelyon","text":"<p>Histopathology diagnosis classification dataset.</p> <p>Dataset: <code>clip-benchmark/wds_vtab-pcam</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Medical derived created Citation <pre><code>@inproceedings{10.1007/978-3-030-00934-2_24,\n  address = {Cham},\n  author = {Veeling, Bastiaan S.\nand Linmans, Jasper\nand Winkens, Jim\nand Cohen, Taco\nand Welling, Max},\n  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},\n  editor = {Frangi, Alejandro F.\nand Schnabel, Julia A.\nand Davatzikos, Christos\nand Alberola-L{\\'o}pez, Carlos\nand Fichtinger, Gabor},\n  isbn = {978-3-030-00934-2},\n  pages = {210--218},\n  publisher = {Springer International Publishing},\n  title = {Rotation Equivariant CNNs for Digital Pathology},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#resisc45","title":"RESISC45","text":"<p>Remote Sensing Image Scene Classification by Northwestern Polytechnical University (NWPU).</p> <p>Dataset: <code>timm/resisc45</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{7891544,\n  author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},\n  doi = {10.1109/JPROC.2017.2675998},\n  journal = {Proceedings of the IEEE},\n  keywords = {Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification;Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning},\n  number = {10},\n  pages = {1865-1883},\n  title = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},\n  volume = {105},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#stl10","title":"STL10","text":"<p>Classifying 96x96 images from 10 classes.</p> <p>Dataset: <code>tanganke/stl10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{pmlr-v15-coates11a,\n  address = {Fort Lauderdale, FL, USA},\n  author = {Coates, Adam and Ng, Andrew and Lee, Honglak},\n  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},\n  editor = {Gordon, Geoffrey and Dunson, David and Dud\u00edk, Miroslav},\n  month = {11--13 Apr},\n  pages = {215--223},\n  pdf = {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},\n  publisher = {PMLR},\n  series = {Proceedings of Machine Learning Research},\n  title = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},\n  url = {https://proceedings.mlr.press/v15/coates11a.html},\n  volume = {15},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#sun397","title":"SUN397","text":"<p>Large scale scene recognition in 397 categories.</p> <p>Dataset: <code>dpdl-benchmark/sun397</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{5539970,\n  author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},\n  booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2010.5539970},\n  number = {},\n  pages = {3485-3492},\n  title = {SUN database: Large-scale scene recognition from abbey to zoo},\n  volume = {},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#stanfordcars","title":"StanfordCars","text":"<p>Classifying car images from 196 makes.</p> <p>Dataset: <code>isaacchung/StanfordCars</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#ucf101","title":"UCF101","text":"<p>UCF101 is an action recognition data set of realistic action videos collected from YouTube, having 101 action categories. This version of the dataset does not contain images but images saved frame by frame. Train and test splits are generated based on the authors' first version train/test list.</p> <p>Dataset: <code>flwrlabs/ucf101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@misc{soomro2012ucf101dataset101human,\n  archiveprefix = {arXiv},\n  author = {Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},\n  eprint = {1212.0402},\n  primaryclass = {cs.CV},\n  title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\n  url = {https://arxiv.org/abs/1212.0402},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#voc2007","title":"VOC2007","text":"<p>Classifying bird images from 500 species.</p> <p>Dataset: <code>mteb/VOC2007</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) lrap eng Encyclopaedic derived created Citation <pre><code>@article{Everingham10,\n  author = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},\n  journal = {International Journal of Computer Vision},\n  month = jun,\n  number = {2},\n  pages = {303--338},\n  title = {The Pascal Visual Object Classes (VOC) Challenge},\n  volume = {88},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/","title":"ImageClustering","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/imageclustering/#cifar100clustering","title":"CIFAR100Clustering","text":"<p>Clustering images from 100 classes.</p> <p>Dataset: <code>uoft-cs/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) nmi eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#cifar10clustering","title":"CIFAR10Clustering","text":"<p>Clustering images from 10 classes.</p> <p>Dataset: <code>uoft-cs/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#imagenet10clustering","title":"ImageNet10Clustering","text":"<p>Clustering images from an 10-class subset of ImageNet which are generally easy to distinguish.</p> <p>Dataset: <code>JamieSJS/imagenet-10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) nmi eng Web derived created Citation <pre><code>@inproceedings{5206848,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#imagenetdog15clustering","title":"ImageNetDog15Clustering","text":"<p>Clustering images from a 15-class dogs-only subset of the dog classes in ImageNet.</p> <p>Dataset: <code>JamieSJS/imagenet-dog-15</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Web derived created Citation <pre><code>@inproceedings{5206848,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#tinyimagenetclustering","title":"TinyImageNetClustering","text":"<p>Clustering over 200 classes.</p> <p>Dataset: <code>zh-plus/tiny-imagenet</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Reviews derived found"},{"location":"overview/available_tasks/instructionreranking/","title":"InstructionReranking","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/instructionreranking/#core17instructionretrieval","title":"Core17InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on Core17 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/core17-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#news21instructionretrieval","title":"News21InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on News21 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/news21-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#robust04instructionretrieval","title":"Robust04InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on Robust04 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/robust04-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#mfollowir","title":"mFollowIR","text":"<p>This tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages.</p> <p>Dataset: <code>jhu-clsp/mFollowIR-parquet-mteb</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{weller2024mfollowir,\n  author = {Weller, Orion and Chang, Benjamin and Yang, Eugene and Yarmohammadi, Mahsa and Barham, Sam and MacAvaney, Sean and Cohan, Arman and Soldaini, Luca and Van Durme, Benjamin and Lawrie, Dawn},\n  journal = {arXiv preprint TODO},\n  title = {{mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#mfollowircrosslingual","title":"mFollowIRCrossLingual","text":"<p>This tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages with English queries/instructions.</p> <p>Dataset: <code>jhu-clsp/mFollowIR-cross-lingual-parquet-mteb</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng, fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{weller2024mfollowir,\n  author = {Weller, Orion and Chang, Benjamin and Yang, Eugene and Yarmohammadi, Mahsa and Barham, Sam and MacAvaney, Sean and Cohan, Arman and Soldaini, Luca and Van Durme, Benjamin and Lawrie, Dawn},\n  journal = {arXiv preprint TODO},\n  title = {{mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/","title":"InstructionRetrieval","text":"<ul> <li>Number of tasks: 8</li> </ul>"},{"location":"overview/available_tasks/instructionretrieval/#ifiraila","title":"IFIRAila","text":"<p>Benchmark aila subset in aila within instruction following abilities. The instructions simulate lawyers' or legal assistants' nuanced queries to retrieve relevant legal documents. </p> <p>Dataset: <code>if-ir/aila</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Legal, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifircds","title":"IFIRCds","text":"<p>Benchmark IFIR cds subset within instruction following abilities. The instructions simulate a doctor's nuanced queries to retrieve suitable clinical trails, treatment and diagnosis information. </p> <p>Dataset: <code>if-ir/cds</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirfiqa","title":"IFIRFiQA","text":"<p>Benchmark IFIR fiqa subset within instruction following abilities. The instructions simulate people's daily life queries to retrieve suitable financial suggestions. </p> <p>Dataset: <code>if-ir/fiqa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Financial, Written human-annotated created Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirfire","title":"IFIRFire","text":"<p>Benchmark IFIR fire subset within instruction following abilities. The instructions simulate lawyers' or legal assistants' nuanced queries to retrieve relevant legal documents. </p> <p>Dataset: <code>if-ir/fire</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Legal, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirnfcorpus","title":"IFIRNFCorpus","text":"<p>Benchmark IFIR nfcorpus subset within instruction following abilities. The instructions in this dataset simulate nuanced queries from students or researchers to retrieve relevant science literature in the medical and biological domains. </p> <p>Dataset: <code>if-ir/nfcorpus</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Academic, Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirpm","title":"IFIRPm","text":"<p>Benchmark IFIR pm subset within instruction following abilities. The instructions simulate a doctor's nuanced queries to retrieve suitable clinical trails, treatment and diagnosis information. </p> <p>Dataset: <code>if-ir/pm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirscifact","title":"IFIRScifact","text":"<p>Benchmark IFIR scifact_open subset within instruction following abilities. The instructions in this dataset simulate nuanced queries from students or researchers to retrieve relevant science literature. </p> <p>Dataset: <code>if-ir/scifact_open</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Academic, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#instructir","title":"InstructIR","text":"<p>A benchmark specifically designed to evaluate the instruction following ability in information retrieval models. Our approach focuses on user-aligned instructions tailored to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios. NOTE: scores on this may differ unless you include instruction first, then \"[SEP]\" and then the query via redefining <code>combine_query_and_instruction</code> in your model.</p> <p>Dataset: <code>mteb/InstructIR-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) robustness_at_10 eng Web human-annotated created Citation <pre><code>@article{oh2024instructir,\n  archiveprefix = {{arXiv}},\n  author = {{Hanseok Oh and Hyunji Lee and Seonghyeon Ye and Haebin Shin and Hansol Jang and Changwook Jun and Minjoon Seo}},\n  eprint = {{2402.14334}},\n  primaryclass = {{cs.CL}},\n  title = {{INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models}},\n  year = {{2024}},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/","title":"MultilabelClassification","text":"<ul> <li>Number of tasks: 11</li> </ul>"},{"location":"overview/available_tasks/multilabelclassification/#braziliantoxictweetsclassification","title":"BrazilianToxicTweetsClassification","text":"<pre><code>    ToLD-Br is the biggest dataset for toxic tweets in Brazilian Portuguese, crowdsourced by 42 annotators selected from\n    a pool of 129 volunteers. Annotators were selected aiming to create a plural group in terms of demographics (ethnicity,\n    sexual orientation, age, gender). Each tweet was labeled by three annotators in 6 possible categories: LGBTQ+phobia,\n    Xenophobia, Obscene, Insult, Misogyny and Racism.\n</code></pre> <p>Dataset: <code>mteb/BrazilianToxicTweetsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy por Constructed, Written expert-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-2010-04543,\n  author = {Joao Augusto Leite and\nDiego F. Silva and\nKalina Bontcheva and\nCarolina Scarton},\n  eprint = {2010.04543},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Tue, 15 Dec 2020 16:10:16 +0100},\n  title = {Toxic Language Detection in Social Media for Brazilian Portuguese:\nNew Dataset and Multilingual Analysis},\n  url = {https://arxiv.org/abs/2010.04543},\n  volume = {abs/2010.04543},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#cedrclassification","title":"CEDRClassification","text":"<p>Classification of sentences by emotions, labeled into 5 categories (joy, sadness, surprise, fear, and anger).</p> <p>Dataset: <code>ai-forever/cedr-classification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Blog, Social, Web, Written human-annotated found Citation <pre><code>@article{sboev2021data,\n  author = {Sboev, Alexander and Naumov, Aleksandr and Rybka, Roman},\n  journal = {Procedia Computer Science},\n  pages = {637--642},\n  publisher = {Elsevier},\n  title = {Data-Driven Model for Emotion Detection in Russian Texts},\n  volume = {190},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#coviddisinformationnlmultilabelclassification","title":"CovidDisinformationNLMultiLabelClassification","text":"<p>The dataset is curated to address questions of interest to journalists, fact-checkers, social media platforms, policymakers, and the general public.</p> <p>Dataset: <code>clips/mteb-nl-COVID-19-disinformation</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{alam-etal-2021-fighting-covid,\n  address = {Punta Cana, Dominican Republic},\n  author = {Alam, Firoj  and\nShaar, Shaden  and\nDalvi, Fahim  and\nSajjad, Hassan  and\nNikolov, Alex  and\nMubarak, Hamdy  and\nDa San Martino, Giovanni  and\nAbdelali, Ahmed  and\nDurrani, Nadir  and\nDarwish, Kareem  and\nAl-Homaid, Abdulaziz  and\nZaghouani, Wajdi  and\nCaselli, Tommaso  and\nDanoe, Gijs  and\nStolk, Friso  and\nBruntink, Britt  and\nNakov, Preslav},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.56},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {611--649},\n  publisher = {Association for Computational Linguistics},\n  title = {Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society},\n  url = {https://aclanthology.org/2021.findings-emnlp.56/},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#emitclassification","title":"EmitClassification","text":"<p>The EMit dataset is a comprehensive resource for the detection of emotions in Italian social media texts.         The EMit dataset consists of social media messages about TV shows, TV series, music videos, and advertisements.         Each message is annotated with one or more of the 8 primary emotions defined by Plutchik         (anger, anticipation, disgust, fear, joy, sadness, surprise, trust), as well as an additional label \u201clove.\u201d</p> <p>Dataset: <code>MattiaSangermano/emit</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Social, Written expert-annotated found Citation <pre><code>@inproceedings{araque2023emit,\n  author = {Araque, O and Frenda, S and Sprugnoli, R and Nozza, D and Patti, V and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {CEUR-WS},\n  pages = {1--8},\n  title = {EMit at EVALITA 2023: Overview of the Categorical Emotion Detection in Italian Social Media Task},\n  volume = {3473},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#korhatespeechmlclassification","title":"KorHateSpeechMLClassification","text":"<pre><code>    The Korean Multi-label Hate Speech Dataset, K-MHaS, consists of 109,692 utterances from Korean online news comments,\n    labelled with 8 fine-grained hate speech classes (labels: Politics, Origin, Physical, Age, Gender, Religion, Race, Profanity)\n    or Not Hate Speech class. Each utterance provides from a single to four labels that can handles Korean language patterns effectively.\n    For more details, please refer to the paper about K-MHaS, published at COLING 2022.\n    This dataset is based on the Korean online news comments available on Kaggle and Github.\n    The unlabeled raw data was collected between January 2018 and June 2020.\n    The language producers are users who left the comments on the Korean online news platform between 2018 and 2020.\n</code></pre> <p>Dataset: <code>mteb/KorHateSpeechMLClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@inproceedings{lee-etal-2022-k,\n  address = {Gyeongju, Republic of Korea},\n  author = {Lee, Jean  and\nLim, Taejun  and\nLee, Heejun  and\nJo, Bogeun  and\nKim, Yangsok  and\nYoon, Heegeun  and\nHan, Soyeon Caren},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {3530--3538},\n  publisher = {International Committee on Computational Linguistics},\n  title = {K-{MH}a{S}: A Multi-label Hate Speech Detection Dataset in {K}orean Online News Comment},\n  url = {https://aclanthology.org/2022.coling-1.311},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#maltesenewsclassification","title":"MalteseNewsClassification","text":"<p>A multi-label topic classification dataset for Maltese News         Articles. The data was collected from the press_mt subset from Korpus         Malti v4.0. Article contents were cleaned to filter out JavaScript, CSS,         &amp; repeated non-Maltese sub-headings. The labels are based on the category         field from this corpus.</p> <p>Dataset: <code>MLRS/maltese_news_categories</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mlt Constructed, Written expert-annotated found Citation <pre><code>@inproceedings{maltese-news-datasets,\n  author = {Chaudhary, Amit Kumar  and\nMicallef, Kurt  and\nBorg, Claudia},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},\n  month = may,\n  publisher = {Association for Computational Linguistics},\n  title = {Topic Classification and Headline Generation for {M}altese using a Public News Corpus},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#multieurlexmultilabelclassification","title":"MultiEURLEXMultilabelClassification","text":"<p>EU laws in 23 EU languages containing annotated labels for 21 EUROVOC concepts.</p> <p>Dataset: <code>mteb/eurlex-multilingual</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bul, ces, dan, deu, ell, ... (23) Government, Legal, Written expert-annotated found Citation <pre><code>@inproceedings{chalkidis-etal-2021-multieurlex,\n  author = {Chalkidis, Ilias\nand Fergadiotis, Manos\nand Androutsopoulos, Ion},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Punta Cana, Dominican Republic},\n  publisher = {Association for Computational Linguistics},\n  title = {MultiEURLEX -- A multi-lingual and multi-label legal document\nclassification dataset for zero-shot cross-lingual transfer},\n  url = {https://arxiv.org/abs/2109.00904},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#sensitivetopicsclassification","title":"SensitiveTopicsClassification","text":"<p>Multilabel classification of sentences across 18 sensitive topics.</p> <p>Dataset: <code>ai-forever/sensitive-topics-classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#swedishpatentcpcgroupclassification","title":"SwedishPatentCPCGroupClassification","text":"<p>This dataset contains historical Swedish patent documents (1885-1972) classified according to the Cooperative Patent Classification (CPC) system at the group level. Each document can have multiple labels, making this a challenging multi-label classification task with significant class imbalance and data sparsity characteristics. The dataset includes patent claims text extracted from digitally recreated versions of historical Swedish patents, generated using Optical Character Recognition (OCR) from original paper documents. The text quality varies due to OCR limitations, but all CPC labels were manually assigned by patent engineers at PRV (Swedish Patent and Registration Office), ensuring high reliability for machine learning applications.</p> <p>Dataset: <code>atheer2104/swedish-patent-cpc-group-new</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy swe Government, Legal expert-annotated found Citation <pre><code>@mastersthesis{Salim1987995,\n  author = {Salim, Atheer},\n  institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  keywords = {Multi-label Text Classification, Machine Learning, Patent Classification, Deep Learning, Natural Language Processing, Textklassificering med flera Klasser, Maskininl\u00e4rning, Patentklassificering, Djupinl\u00e4rning, Spr\u00e5kteknologi},\n  number = {2025:571},\n  pages = {70},\n  school = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  series = {TRITA-EECS-EX},\n  title = {Machine Learning for Classifying Historical Swedish Patents : A Comparison of Textual and Combined Data Approaches},\n  url = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-368254},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#swedishpatentcpcsubclassclassification","title":"SwedishPatentCPCSubclassClassification","text":"<p>This dataset contains historical Swedish patent documents (1885-1972) classified according to the Cooperative Patent Classification (CPC) system. Each document can have multiple labels, making this a multi-label classification task with significant implications for patent retrieval and prior art search.         The dataset includes patent claims text extracted from digitally recreated versions of historical Swedish patents, generated using Optical Character Recognition (OCR) from original paper documents. The text quality varies due to OCR limitations, but all CPC labels were manually assigned by patent engineers at PRV (Swedish Patent and Registration Office), ensuring high reliability for machine learning applications.</p> <p>Dataset: <code>atheer2104/swedish-patent-cpc-subclass-new</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy swe Government, Legal expert-annotated found Citation <pre><code>@mastersthesis{Salim1987995,\n  author = {Salim, Atheer},\n  institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  keywords = {Multi-label Text Classification, Machine Learning, Patent Classification, Deep Learning, Natural Language Processing, Textklassificering med flera Klasser, Maskininl\u00e4rning, Patentklassificering, Djupinl\u00e4rning, Spr\u00e5kteknologi},\n  number = {2025:571},\n  pages = {70},\n  school = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  series = {TRITA-EECS-EX},\n  title = {Machine Learning for Classifying Historical Swedish Patents : A Comparison of Textual and Combined Data Approaches},\n  url = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-368254},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#vabbmultilabelclassification","title":"VABBMultiLabelClassification","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-mlcls-pr</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Academic, Written human-annotated found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/","title":"PairClassification","text":"<ul> <li>Number of tasks: 46</li> </ul>"},{"location":"overview/available_tasks/pairclassification/#arentail","title":"ArEntail","text":"<p>A manually-curated Arabic natural language inference dataset from news headlines.</p> <p>Dataset: <code>arbml/ArEntail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ara News, Written human-annotated found Citation <pre><code>@article{obeidat2024arentail,\n  author = {Obeidat, Rasha and Al-Harahsheh, Yara and Al-Ayyoub, Mahmoud and Gharaibeh, Maram},\n  journal = {Language Resources and Evaluation},\n  pages = {1--27},\n  publisher = {Springer},\n  title = {ArEntail: manually-curated Arabic natural language inference dataset from news headlines},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#armenianparaphrasepc","title":"ArmenianParaphrasePC","text":"<p>asparius/Armenian-Paraphrase-PC</p> <p>Dataset: <code>asparius/Armenian-Paraphrase-PC</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap hye News, Written derived found Citation <pre><code>@misc{malajyan2020arpa,\n  archiveprefix = {arXiv},\n  author = {Arthur Malajyan and Karen Avetisyan and Tsolak Ghukasyan},\n  eprint = {2009.12615},\n  primaryclass = {cs.CL},\n  title = {ARPA: Armenian Paraphrase Detection Corpus and Models},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#assin2rte","title":"Assin2RTE","text":"<p>Recognizing Textual Entailment part of the ASSIN 2, an evaluation shared task collocated with STIL 2019.</p> <p>Dataset: <code>nilc-nlp/assin2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap por Written human-annotated found Citation <pre><code>@inproceedings{real2020assin,\n  author = {Real, Livy and Fonseca, Erick and Oliveira, Hugo Goncalo},\n  booktitle = {International Conference on Computational Processing of the Portuguese Language},\n  organization = {Springer},\n  pages = {406--412},\n  title = {The assin 2 shared task: a quick overview},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cdsc-e","title":"CDSC-E","text":"<p>Compositional Distributional Semantics Corpus for textual entailment.</p> <p>Dataset: <code>PL-MTEB/cdsce-pairclassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Written human-annotated found Citation <pre><code>@inproceedings{wroblewska-krasnowska-kieras-2017-polish,\n  address = {Vancouver, Canada},\n  author = {Wr{\\'o}blewska, Alina  and\nKrasnowska-Kiera{\\'s}, Katarzyna},\n  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/P17-1073},\n  editor = {Barzilay, Regina  and\nKan, Min-Yen},\n  month = jul,\n  pages = {784--792},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}olish evaluation dataset for compositional distributional semantics models},\n  url = {https://aclanthology.org/P17-1073},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cexappc","title":"CExaPPC","text":"<p>ExaPPC is a large paraphrase corpus consisting of monolingual sentence-level paraphrases using different sources.</p> <p>Dataset: <code>PNLPhub/C-ExaPPC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Social, Web derived found Citation <pre><code>@inproceedings{9786243,\n  author = {Sadeghi, Reyhaneh and Karbasi, Hamed and Akbari, Ahmad},\n  booktitle = {2022 8th International Conference on Web Research (ICWR)},\n  doi = {10.1109/ICWR54782.2022.9786243},\n  keywords = {Data mining;Task analysis;Paraphrase Identification;Semantic Similarity;Deep Learning;Paraphrasing Corpora},\n  number = {},\n  pages = {168-175},\n  title = {ExaPPC: a Large-Scale Persian Paraphrase Detection Corpus},\n  volume = {},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ctkfactsnli","title":"CTKFactsNLI","text":"<p>Czech Natural Language Inference dataset of around 3K evidence-claim pairs labelled with SUPPORTS, REFUTES or NOT ENOUGH INFO veracity labels. Extracted from a round of fact-checking experiments.</p> <p>Dataset: <code>mteb/CTKFactsNLI</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ces News, Written human-annotated found Citation <pre><code>@article{ullrich2023csfever,\n  author = {Ullrich, Herbert and Drchal, Jan and R{\\\\`y}par, Martin and Vincourov{\\\\'a}, Hana and Moravec, V{\\\\'a}clav},\n  journal = {Language Resources and Evaluation},\n  number = {4},\n  pages = {1571--1605},\n  publisher = {Springer},\n  title = {CsFEVER and CTKFacts: acquiring Czech data for fact verification},\n  volume = {57},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cmnli","title":"Cmnli","text":"<p>Chinese Multi-Genre NLI</p> <p>Dataset: <code>C-MTEB/CMNLI</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy cmn not specified not specified not specified Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai  and\nZhang, Xuanwei  and\nLi, Lu  and\nCao, Chenjie  and\nLi, Yudong  and\nXu, Yechen  and\nSun, Kai  and\nYu, Dian  and\nYu, Cong  and\nTian, Yin  and\nDong, Qianqian  and\nLiu, Weitang  and\nShi, Bo  and\nCui, Yiming  and\nLi, Junyi  and\nZeng, Jun  and\nWang, Rongzhao  and\nXie, Weijian  and\nLi, Yanting  and\nPatterson, Yina  and\nTian, Zuoyu  and\nZhang, Yiwen  and\nZhou, He  and\nLiu, Shaoweihua  and\nZhao, Zhe  and\nZhao, Qipeng  and\nYue, Cong  and\nZhang, Xinrui  and\nYang, Zhengliang  and\nRichardson, Kyle  and\nLan, Zhenzhong},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#discotexpairclassification","title":"DisCoTexPairClassification","text":"<p>The DisCoTEX dataset aims at assessing discourse coherence in Italian texts. This dataset focuses on Italian real-world texts and provides resources to model coherence in natural language.</p> <p>Dataset: <code>MattiaSangermano/DisCoTex-last-sentence</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ita Social, Written derived found Citation <pre><code>@inproceedings{brunato2023discotex,\n  author = {Brunato, Dominique and Colla, Davide and Dell'Orletta, Felice and Dini, Irene and Radicioni, Daniele Paolo and Ravelli, Andrea Amelio and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {CEUR},\n  pages = {1--8},\n  title = {DisCoTex at EVALITA 2023: overview of the assessing discourse coherence in Italian texts task},\n  volume = {3473},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#falsefriendsgermanenglish","title":"FalseFriendsGermanEnglish","text":"<p>A dataset to identify False Friends / false cognates between English and German. A generally challenging task for multilingual models.</p> <p>Dataset: <code>aari1995/false_friends_de_en_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu Written human-annotated created Citation <pre><code>@misc{Chibb_2022,\n  author = {Chibb, Aaron},\n  month = {Sep},\n  title = {{German-English False Friends in Multilingual Transformer Models: An Evaluation on Robustness and Word-to-Word Fine-Tuning}},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#farstail","title":"FarsTail","text":"<p>This dataset, named FarsTail, includes 10,367 samples which are provided in both the Persian language as well as the indexed format to be useful for non-Persian researchers. The samples are generated from 3,539 multiple-choice questions with the least amount of annotator interventions in a way similar to the SciTail dataset</p> <p>Dataset: <code>azarijafari/FarsTail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Academic, Written human-annotated found Citation <pre><code>@article{amirkhani2023farstail,\n  author = {Amirkhani, Hossein and AzariJafari, Mohammad and Faridan-Jahromi, Soroush and Kouhkan, Zeinab and Pourjafari, Zohreh and Amirak, Azadeh},\n  doi = {10.1007/s00500-023-08959-3},\n  journal = {Soft Computing},\n  publisher = {Springer},\n  title = {FarsTail: a Persian natural language inference dataset},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#farsiparaphrasedetection","title":"FarsiParaphraseDetection","text":"<p>Farsi Paraphrase Detection</p> <p>Dataset: <code>alighasemi/farsi_paraphrase_detection</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#indicxnlipairclassification","title":"IndicXnliPairClassification","text":"<p>INDICXNLI is similar to existing XNLI dataset in shape/form, but focuses on Indic language family. The train (392,702), validation (2,490), and evaluation sets (5,010) of English XNLI were translated from English into each of the eleven Indic languages. IndicTrans is a large Transformer-based sequence to sequence model. It is trained on Samanantar dataset (Ramesh et al., 2021), which is the largest parallel multi- lingual corpus over eleven Indic languages.</p> <p>Dataset: <code>mteb/IndicXnliPairClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap asm, ben, guj, hin, kan, ... (11) Fiction, Government, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{aggarwal_gupta_kunch_22,\n  author = {Aggarwal, Divyanshu and Gupta, Vivek and Kunchukuttan, Anoop},\n  copyright = {Creative Commons Attribution 4.0 International},\n  doi = {10.48550/ARXIV.2204.08776},\n  publisher = {arXiv},\n  title = {IndicXNLI: Evaluating Multilingual Inference for Indian Languages},\n  url = {https://arxiv.org/abs/2204.08776},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#klue-nli","title":"KLUE-NLI","text":"<p>Textual Entailment between a hypothesis sentence and a premise sentence. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap kor Encyclopaedic, News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#legalbenchpc","title":"LegalBenchPC","text":"<p>This LegalBench pair classification task is a combination of the following datasets:</p> <pre><code>    - Citation Prediction Classification: Given a legal statement and a case citation, determine if the citation is supportive of the legal statement.\n    - Consumer Contracts QA: The task consists of 400 yes/no questions relating to consumer contracts (specifically, online terms of service) and is relevant to the legal skill of contract interpretation.\n    - Contract QA: Answer yes/no questions about whether contractual clauses discuss particular issues like confidentiality requirements, BIPA consent, PII data breaches, breach of contract etc.\n    - Hearsay: Classify if a particular piece of evidence qualifies as hearsay. Each sample in the dataset describes (1) an issue being litigated or an assertion a party wishes to prove, and (2) a piece of evidence a party wishes to introduce. The goal is to determine if\u2014as it relates to the issue\u2014the evidence would be considered hearsay under the definition provided above.\n    - Privacy Policy Entailment: Given a privacy policy clause and a description of the clause, determine if the description is correct. This is a binary classification task in which the LLM is provided with a clause from a privacy policy, and a description of that clause (e.g., \u201cThe policy describes collection of the user\u2019s HTTP cookies, flash cookies, pixel tags, or similar identifiers by a party to the contract.\u201d).\n    - Privacy Policy QA: Given a question and a clause from a privacy policy, determine if the clause contains enough information to answer the question. This is a binary classification task in which the LLM is provided with a question (e.g., \u201cdo you publish my data\u201d) and a clause from a privacy policy. The LLM must determine if the clause contains an answer to the question, and classify the question-clause pair.\n</code></pre> <p>Dataset: <code>mteb/LegalBenchPC</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{kolt2022predicting,\n  author = {Kolt, Noam},\n  journal = {Berkeley Tech. LJ},\n  pages = {71},\n  publisher = {HeinOnline},\n  title = {Predicting consumer contracts},\n  volume = {37},\n  year = {2022},\n}\n\n@article{ravichander2019question,\n  author = {Ravichander, Abhilasha and Black, Alan W and Wilson, Shomir and Norton, Thomas and Sadeh, Norman},\n  journal = {arXiv preprint arXiv:1911.00841},\n  title = {Question answering for privacy policies: Combining computational and legal perspectives},\n  year = {2019},\n}\n\n@article{zimmeck2019maps,\n  author = {Zimmeck, Sebastian and Story, Peter and Smullen, Daniel and Ravichander, Abhilasha and Wang, Ziqi and Reidenberg, Joel R and Russell, N Cameron and Sadeh, Norman},\n  journal = {Proc. Priv. Enhancing Tech.},\n  pages = {66},\n  title = {Maps: Scaling privacy compliance analysis to a million apps},\n  volume = {2019},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ocnli","title":"Ocnli","text":"<p>Original Chinese Natural Language Inference dataset</p> <p>Dataset: <code>C-MTEB/OCNLI</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy cmn not specified not specified not specified Citation <pre><code>@misc{hu2020ocnli,\n  archiveprefix = {arXiv},\n  author = {Hai Hu and Kyle Richardson and Liang Xu and Lu Li and Sandra Kuebler and Lawrence S. Moss},\n  eprint = {2010.05444},\n  primaryclass = {cs.CL},\n  title = {OCNLI: Original Chinese Natural Language Inference},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#opusparcuspc","title":"OpusparcusPC","text":"<p>Opusparcus is a paraphrase corpus for six European language: German, English, Finnish, French, Russian, and Swedish. The paraphrases consist of subtitles from movies and TV shows.</p> <p>Dataset: <code>mteb/OpusparcusPC</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, eng, fin, fra, rus, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@misc{creutz2018open,\n  archiveprefix = {arXiv},\n  author = {Mathias Creutz},\n  eprint = {1809.06142},\n  primaryclass = {cs.CL},\n  title = {Open Subtitles Paraphrase Corpus for Six Languages},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#psc","title":"PSC","text":"<p>Polish Summaries Corpus</p> <p>Dataset: <code>PL-MTEB/psc-pairclassification</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol News, Written derived found Citation <pre><code>@inproceedings{ogrodniczuk-kopec-2014-polish,\n  address = {Reykjavik, Iceland},\n  author = {Ogrodniczuk, Maciej  and\nKope{\\'c}, Mateusz},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {3712--3715},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {The {P}olish Summaries Corpus},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/1211_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#parsinluentail","title":"ParsinluEntail","text":"<p>A Persian textual entailment task (deciding sent1 entails sent2). The questions are partially translated from the SNLI dataset and partially generated by expert annotators.</p> <p>Dataset: <code>mteb/ParsinluEntail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Reviews, Written derived found Citation <pre><code>@misc{khashabi2021parsinlusuitelanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Daniel Khashabi and Arman Cohan and Siamak Shakeri and Pedram Hosseini and Pouya Pezeshkpour and Malihe Alikhani and Moin Aminnaseri and Marzieh Bitaab and Faeze Brahman and Sarik Ghazarian and Mozhdeh Gheini and Arman Kabiri and Rabeeh Karimi Mahabadi and Omid Memarrast and Ahmadreza Mosallanezhad and Erfan Noury and Shahab Raji and Mohammad Sadegh Rasooli and Sepideh Sadeghi and Erfan Sadeqi Azer and Niloofar Safi Samghabadi and Mahsa Shafaei and Saber Sheybani and Ali Tazarv and Yadollah Yaghoobzadeh},\n  eprint = {2012.06154},\n  primaryclass = {cs.CL},\n  title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n  url = {https://arxiv.org/abs/2012.06154},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#parsinluqueryparaphpc","title":"ParsinluQueryParaphPC","text":"<p>A Persian query paraphrasng task (deciding whether two questions are paraphrases of each other). The questions are partially generated from Google auto-complete, and partially translated from the Quora paraphrasing dataset.</p> <p>Dataset: <code>mteb/ParsinluQueryParaphPC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Reviews, Written derived found Citation <pre><code>@misc{khashabi2021parsinlusuitelanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Daniel Khashabi and Arman Cohan and Siamak Shakeri and Pedram Hosseini and Pouya Pezeshkpour and Malihe Alikhani and Moin Aminnaseri and Marzieh Bitaab and Faeze Brahman and Sarik Ghazarian and Mozhdeh Gheini and Arman Kabiri and Rabeeh Karimi Mahabadi and Omid Memarrast and Ahmadreza Mosallanezhad and Erfan Noury and Shahab Raji and Mohammad Sadegh Rasooli and Sepideh Sadeghi and Erfan Sadeqi Azer and Niloofar Safi Samghabadi and Mahsa Shafaei and Saber Sheybani and Ali Tazarv and Yadollah Yaghoobzadeh},\n  eprint = {2012.06154},\n  primaryclass = {cs.CL},\n  title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n  url = {https://arxiv.org/abs/2012.06154},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pawsxpairclassification","title":"PawsXPairClassification","text":"<p>{PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification</p> <p>Dataset: <code>mteb/PawsXPairClassification</code> \u2022 License: https://huggingface.co/datasets/google-research-datasets/paws-x#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap cmn, deu, eng, fra, jpn, ... (7) Encyclopaedic, Web, Written human-annotated human-translated Citation <pre><code>@misc{yang2019pawsx,\n  archiveprefix = {arXiv},\n  author = {Yinfei Yang and Yuan Zhang and Chris Tar and Jason Baldridge},\n  eprint = {1908.11828},\n  primaryclass = {cs.CL},\n  title = {PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ppcpc","title":"PpcPC","text":"<p>Polish Paraphrase Corpus</p> <p>Dataset: <code>PL-MTEB/ppc-pairclassification</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Fiction, News, Non-fiction, Social, Spoken, ... (7) derived found Citation <pre><code>@misc{dadas2022training,\n  archiveprefix = {arXiv},\n  author = {S\u0142awomir Dadas},\n  eprint = {2207.12759},\n  primaryclass = {cs.CL},\n  title = {Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemaisentenceparaphrasepc","title":"PubChemAISentenceParaphrasePC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemAISentenceParaphrasePC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry LM-generated created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemsmilespc","title":"PubChemSMILESPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSMILESPairClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemsynonympc","title":"PubChemSynonymPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSynonymPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemwikipairclassification","title":"PubChemWikiPairClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemWikiMultilingualPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ces, deu, eng, fra, hin, ... (13) Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\\\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemwikiparagraphspc","title":"PubChemWikiParagraphsPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemWikiParagraphsPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#rte3","title":"RTE3","text":"<p>Recognising Textual Entailment Challenge (RTE-3) aim to provide the NLP community with a benchmark to test progress in recognizing textual entailment</p> <p>Dataset: <code>maximoss/rte3-multi</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, eng, fra, ita Encyclopaedic, News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{giampiccolo-etal-2007-third,\n  address = {Prague},\n  author = {Giampiccolo, Danilo  and\nMagnini, Bernardo  and\nDagan, Ido  and\nDolan, Bill},\n  booktitle = {Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing},\n  month = jun,\n  pages = {1--9},\n  publisher = {Association for Computational Linguistics},\n  title = {The Third {PASCAL} Recognizing Textual Entailment Challenge},\n  url = {https://aclanthology.org/W07-1401},\n  year = {2007},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sick-br-pc","title":"SICK-BR-PC","text":"<p>SICK-BR is a Portuguese inference corpus, human translated from SICK</p> <p>Dataset: <code>eduagarcia/sick-br</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap por Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{real18,\n  author = {Real, Livy\nand Rodrigues, Ana\nand Vieira e Silva, Andressa\nand Albiero, Beatriz\nand Thalenberg, Bruna\nand Guide, Bruno\nand Silva, Cindy\nand de Oliveira Lima, Guilherme\nand C{\\^a}mara, Igor C. S.\nand Stanojevi{\\'{c}}, Milo{\\v{s}}\nand Souza, Rodrigo\nand de Paiva, Valeria},\n  booktitle = {{Computational Processing of the Portuguese Language. PROPOR 2018.}},\n  doi = {10.1007/978-3-319-99722-3_31},\n  isbn = {978-3-319-99722-3},\n  title = {{SICK-BR: A Portuguese Corpus for Inference}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sick-e-pl","title":"SICK-E-PL","text":"<p>Polish version of SICK dataset for textual entailment.</p> <p>Dataset: <code>PL-MTEB/sicke-pl-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Reviews not specified not specified Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\l}kiewicz, Micha{\\l}  and\nPo{\\'s}wiata, Rafa{\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sicknlpairclassification","title":"SICKNLPairClassification","text":"<p>SICK-NL is a Dutch translation of SICK </p> <p>Dataset: <code>clips/mteb-nl-sick-pcls-pr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap nld Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{wijnholds2021sick,\n  author = {Wijnholds, Gijs and Moortgat, Michael},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  pages = {1474--1479},\n  title = {SICK-NL: A Dataset for Dutch Natural Language Inference},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sprintduplicatequestions","title":"SprintDuplicateQuestions","text":"<p>Duplicate questions from the Sprint community.</p> <p>Dataset: <code>mteb/sprintduplicatequestions-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Programming, Written derived found Citation <pre><code>@inproceedings{shah-etal-2018-adversarial,\n  address = {Brussels, Belgium},\n  author = {Shah, Darsh  and\nLei, Tao  and\nMoschitti, Alessandro  and\nRomeo, Salvatore  and\nNakov, Preslav},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1131},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {1056--1063},\n  publisher = {Association for Computational Linguistics},\n  title = {Adversarial Domain Adaptation for Duplicate Question Detection},\n  url = {https://aclanthology.org/D18-1131},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sprintduplicatequestions-vn","title":"SprintDuplicateQuestions-VN","text":"<p>A translated dataset from Duplicate questions from the Sprint community.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/sprintduplicatequestions-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Programming, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synperchatbotragfaqpc","title":"SynPerChatbotRAGFAQPC","text":"<p>Synthetic Persian Chatbot RAG FAQ Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-faq-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synperqapc","title":"SynPerQAPC","text":"<p>Synthetic Persian QA Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-qa-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synpertextkeywordspc","title":"SynPerTextKeywordsPC","text":"<p>Synthetic Persian Text Keywords Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-text-keyword-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#terra","title":"TERRa","text":"<p>Textual Entailment Recognition for Russian. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.</p> <p>Dataset: <code>ai-forever/terra-pairclassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap rus News, Web, Written human-annotated found Citation <pre><code>@article{shavrina2020russiansuperglue,\n  author = {Shavrina, Tatiana\nand Fenogenova, Alena\nand Emelyanov, Anton\nand Shevelev, Denis\nand Artemova, Ekaterina\nand Malykh, Valentin\nand Mikhailov, Vladislav\nand Tikhonova, Maria\nand Chertok, Andrey\nand Evlampiev, Andrey},\n  journal = {arXiv preprint arXiv:2010.15925},\n  title = {RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#talemaaderpc","title":"TalemaaderPC","text":"<p>The Danish Language and Literature Society has developed a dataset for evaluating language models in Danish. The dataset contains a total of 1000 Danish idioms and fixed expressions with transferred meanings based on the Danish Dictionary's collection of fixed expressions with associated definitions. For each of the 1000 idioms and fixed expressions, three false definitions have also been prepared. The dataset can be used to test the performance of language models in identifying correct definitions for Danish idioms and fixed expressions.</p> <p>Dataset: <code>mteb/talemaader_pc</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy dan Academic, Written derived created Citation <pre><code>@misc{DSLDK1000Talemader,\n  author = {{Det Danske Sprog- og Litteraturselskab}},\n  howpublished = {Sprogteknologi.dk},\n  language = {Danish},\n  note = {CC-BY licensed dataset of 1000 Danish sayings and expressions},\n  publisher = {Digitaliseringsstyrelsen \\&amp; Det Danske Sprog- og Litteraturselskab},\n  title = {1000 danske talem\u00e5der - evalueringsdatas\u00e6t},\n  url = {https://sprogteknologi.dk/dataset/1000-talemader-evalueringsdatasaet},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twittersemeval2015","title":"TwitterSemEval2015","text":"<p>Paraphrase-Pairs of Tweets from the SemEval 2015 workshop.</p> <p>Dataset: <code>mteb/twittersemeval2015-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Social, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2015-semeval,\n  address = {Denver, Colorado},\n  author = {Xu, Wei  and\nCallison-Burch, Chris  and\nDolan, Bill},\n  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)},\n  doi = {10.18653/v1/S15-2001},\n  editor = {Nakov, Preslav  and\nZesch, Torsten  and\nCer, Daniel  and\nJurgens, David},\n  month = jun,\n  pages = {1--11},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2015 Task 1: Paraphrase and Semantic Similarity in {T}witter ({PIT})},\n  url = {https://aclanthology.org/S15-2001},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twittersemeval2015-vn","title":"TwitterSemEval2015-VN","text":"<p>A translated dataset from Paraphrase-Pairs of Tweets from the SemEval 2015 workshop.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twittersemeval2015-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twitterurlcorpus","title":"TwitterURLCorpus","text":"<p>Paraphrase-Pairs of Tweets.</p> <p>Dataset: <code>mteb/twitterurlcorpus-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Social, Written derived found Citation <pre><code>@inproceedings{lan-etal-2017-continuously,\n  address = {Copenhagen, Denmark},\n  author = {Lan, Wuwei  and\nQiu, Siyu  and\nHe, Hua  and\nXu, Wei},\n  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D17-1126},\n  editor = {Palmer, Martha  and\nHwa, Rebecca  and\nRiedel, Sebastian},\n  month = sep,\n  pages = {1224--1234},\n  publisher = {Association for Computational Linguistics},\n  title = {A Continuously Growing Dataset of Sentential Paraphrases},\n  url = {https://aclanthology.org/D17-1126},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twitterurlcorpus-vn","title":"TwitterURLCorpus-VN","text":"<p>A translated dataset from Paraphrase-Pairs of Tweets.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twitterurlcorpus-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xlwicnlpairclassification","title":"XLWICNLPairClassification","text":"<p>The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. </p> <p>Dataset: <code>clips/mteb-nl-xlwic</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap nld Written derived created Citation <pre><code>@inproceedings{raganato2020xl,\n  author = {Raganato, A and Pasini, T and Camacho-Collados, J and Pilehvar, M and others},\n  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  organization = {Association for Computational Linguistics (ACL)},\n  pages = {7193--7206},\n  title = {XL-WiC: A multilingual benchmark for evaluating semantic contextualization},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xnli","title":"XNLI","text":"<p>Dataset: <code>mteb/xnli</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ara, bul, deu, ell, eng, ... (14) Fiction, Government, Non-fiction, Written expert-annotated created Citation <pre><code>@inproceedings{conneau2018xnli,\n  author = {Conneau, Alexis\nand Rinott, Ruty\nand Lample, Guillaume\nand Williams, Adina\nand Bowman, Samuel R.\nand Schwenk, Holger\nand Stoyanov, Veselin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xnliv2","title":"XNLIV2","text":"<p>This is subset of 'XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding' with languages that were not part of the original XNLI plus three (verified) languages that are not strongly covered in MTEB</p> <p>Dataset: <code>mteb/XNLIV2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap asm, ben, bho, ell, guj, ... (13) Fiction, Government, Non-fiction, Written expert-annotated machine-translated and verified Citation <pre><code>@inproceedings{upadhyay2023xnli,\n  author = {Upadhyay, Ankit Kumar and Upadhya, Harsit Kumar},\n  booktitle = {2023 IEEE 8th International Conference for Convergence in Technology (I2CT)},\n  organization = {IEEE},\n  pages = {1--6},\n  title = {XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding (XLU)},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xstance","title":"XStance","text":"<p>A Multilingual Multi-Target Dataset for Stance Detection in French, German, and Italian.</p> <p>Dataset: <code>mteb/XStance</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, fra, ita Social, Written human-annotated created Citation <pre><code>@inproceedings{vamvas2020xstance,\n  address = {Zurich, Switzerland},\n  author = {Vamvas, Jannis and Sennrich, Rico},\n  booktitle = {Proceedings of the 5th Swiss Text Analytics Conference (SwissText)  16th Conference on Natural Language Processing (KONVENS)},\n  month = {jun},\n  title = {{X-Stance}: A Multilingual Multi-Target Dataset for Stance Detection},\n  url = {http://ceur-ws.org/Vol-2624/paper9.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#indonli","title":"indonli","text":"<p>IndoNLI is the first human-elicited Natural Language Inference (NLI) dataset for Indonesian. IndoNLI is annotated by both crowd workers and experts.</p> <p>Dataset: <code>mteb/indonli</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ind Encyclopaedic, News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{mahendra-etal-2021-indonli,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Mahendra, Rahmad and Aji, Alham Fikri and Louvan, Samuel and Rahman, Fahrurrozi and Vania, Clara},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  month = nov,\n  pages = {10511--10527},\n  publisher = {Association for Computational Linguistics},\n  title = {{I}ndo{NLI}: A Natural Language Inference Dataset for {I}ndonesian},\n  url = {https://aclanthology.org/2021.emnlp-main.821},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/regression/","title":"Regression","text":"<ul> <li>Number of tasks: 2</li> </ul>"},{"location":"overview/available_tasks/regression/#ruscibenchcitedcountregression","title":"RuSciBenchCitedCountRegression","text":"<p>Predicts the number of times a scientific article has been cited by other papers.         The prediction is based on the article's title and abstract. The data is sourced from the Russian electronic         library of scientific publications (eLibrary.ru) and includes papers with both Russian and English abstracts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) kendalltau eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/regression/#ruscibenchyearpublregression","title":"RuSciBenchYearPublRegression","text":"<p>Predicts the publication year of a scientific article. The prediction is based on the         article's title and abstract. The data is sourced from the Russian electronic library of scientific         publications (eLibrary.ru) and includes papers with both Russian and English abstracts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) kendalltau eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/","title":"Reranking","text":"<ul> <li>Number of tasks: 39</li> </ul>"},{"location":"overview/available_tasks/reranking/#alloprofreranking","title":"AlloprofReranking","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>mteb/AlloprofReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 fra Academic, Web, Written expert-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#askubuntudupquestions","title":"AskUbuntuDupQuestions","text":"<p>AskUbuntu Question Dataset - Questions from AskUbuntu with manual annotations marking pairs of questions as similar or non-similar</p> <p>Dataset: <code>mteb/AskUbuntuDupQuestions</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Programming, Web human-annotated found Citation <pre><code>@article{wang-2021-TSDAE,\n  author = {Wang, Kexin and Reimers, Nils and  Gurevych, Iryna},\n  journal = {arXiv preprint arXiv:2104.06979},\n  month = {4},\n  title = {TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning},\n  url = {https://arxiv.org/abs/2104.06979},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#askubuntudupquestions-vn","title":"AskUbuntuDupQuestions-VN","text":"<p>A translated dataset from AskUbuntu Question Dataset - Questions from AskUbuntu with manual annotations marking pairs of questions as similar or non-similar             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/AskUbuntuDupQuestions-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Programming, Web derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#builtbenchreranking","title":"BuiltBenchReranking","text":"<p>Reranking of built asset entity type/class descriptions given a query describing an entity as represented in well-established industry classification systems such as Uniclass, IFC, etc.</p> <p>Dataset: <code>mteb/BuiltBenchReranking</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#cmedqav1-reranking","title":"CMedQAv1-reranking","text":"<p>Chinese community medical question answering</p> <p>Dataset: <code>mteb/CMedQAv1-reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Medical, Written expert-annotated found Citation <pre><code>@article{zhang2017chinese,\n  author = {Zhang, Sheng and Zhang, Xin and Wang, Hui and Cheng, Jiajun and Li, Pei and Ding, Zhaoyun},\n  journal = {Applied Sciences},\n  number = {8},\n  pages = {767},\n  publisher = {Multidisciplinary Digital Publishing Institute},\n  title = {Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs},\n  volume = {7},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#cmedqav2-reranking","title":"CMedQAv2-reranking","text":"<p>Chinese community medical question answering</p> <p>Dataset: <code>mteb/CMedQAv2-reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Medical, Written not specified not specified Citation <pre><code>@article{8548603,\n  author = {S. Zhang and X. Zhang and H. Wang and L. Guo and S. Liu},\n  doi = {10.1109/ACCESS.2018.2883637},\n  issn = {2169-3536},\n  journal = {IEEE Access},\n  keywords = {Biomedical imaging;Data mining;Semantics;Medical services;Feature extraction;Knowledge discovery;Medical question answering;interactive attention;deep learning;deep neural networks},\n  month = {},\n  number = {},\n  pages = {74061-74071},\n  title = {Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection},\n  volume = {6},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderaglibrarydocumentationsolutions","title":"CodeRAGLibraryDocumentationSolutions","text":"<p>Evaluation of code library documentation retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant Python library documentation sections given code-related queries.</p> <p>Dataset: <code>code-rag-bench/library-documentation</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagonlinetutorials","title":"CodeRAGOnlineTutorials","text":"<p>Evaluation of online programming tutorial retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant tutorials from online platforms given code-related queries.</p> <p>Dataset: <code>code-rag-bench/online-tutorials</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagprogrammingsolutions","title":"CodeRAGProgrammingSolutions","text":"<p>Evaluation of programming solution retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant programming solutions given code-related queries.</p> <p>Dataset: <code>code-rag-bench/programming-solutions</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagstackoverflowposts","title":"CodeRAGStackoverflowPosts","text":"<p>Evaluation of StackOverflow post retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant StackOverflow posts given code-related queries.</p> <p>Dataset: <code>code-rag-bench/stackoverflow-posts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#escireranking","title":"ESCIReranking","text":"<p>Dataset: <code>mteb/ESCIReranking</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng, jpn, spa Written derived created Citation <pre><code>@article{reddy2022shopping,\n  archiveprefix = {arXiv},\n  author = {Chandan K. Reddy and Llu\u00eds M\u00e0rquez and Fran Valero and Nikhil Rao and Hugo Zaragoza and Sambaran Bandyopadhyay and Arnab Biswas and Anlu Xing and Karthik Subbian},\n  eprint = {2206.06588},\n  title = {Shopping Queries Dataset: A Large-Scale {ESCI} Benchmark for Improving Product Search},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humecore17instructionreranking","title":"HUMECore17InstructionReranking","text":"<p>Human evaluation subset of Core17 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMECore17InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humenews21instructionreranking","title":"HUMENews21InstructionReranking","text":"<p>Human evaluation subset of News21 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMENews21InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@inproceedings{soboroff2021trec,\n  author = {Soboroff, Ian and Macdonald, Craig and McCreadie, Richard},\n  booktitle = {TREC},\n  title = {TREC 2021 News Track Overview},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humerobust04instructionreranking","title":"HUMERobust04InstructionReranking","text":"<p>Human evaluation subset of Robust04 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMERobust04InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@inproceedings{voorhees2005trec,\n  author = {Voorhees, Ellen M},\n  booktitle = {TREC},\n  title = {TREC 2004 Robust Retrieval Track Overview},\n  year = {2005},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humewikipediarerankingmultilingual","title":"HUMEWikipediaRerankingMultilingual","text":"<p>Human evaluation subset of Wikipedia reranking dataset across multiple languages.</p> <p>Dataset: <code>mteb/HUMEWikipediaRerankingMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 dan, eng, nob Encyclopaedic, Written derived found Citation <pre><code>@misc{wikipedia_reranking_2023,\n  author = {Ellamind},\n  title = {Wikipedia 2023-11 Reranking Multilingual Dataset},\n  url = {https://github.com/ellamind/wikipedia-2023-11-reranking-multilingual},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jqarareranking","title":"JQaRAReranking","text":"<p>JQaRA: Japanese Question Answering with Retrieval Augmentation  - \u691c\u7d22\u62e1\u5f35(RAG)\u8a55\u4fa1\u306e\u305f\u3081\u306e\u65e5\u672c\u8a9e Q&amp;A \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8. JQaRA is an information retrieval task for questions against 100 candidate data (including one or more correct answers).</p> <p>Dataset: <code>mteb/JQaRAReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jqara,\n  author = {Yuichi Tateno},\n  title = {JQaRA: Japanese Question Answering with Retrieval Augmentation - \u691c\u7d22\u62e1\u5f35(RAG)\u8a55\u4fa1\u306e\u305f\u3081\u306e\u65e5\u672c\u8a9eQ&amp;A\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JQaRA},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jacwirreranking","title":"JaCWIRReranking","text":"<p>JaCWIR is a small-scale Japanese information retrieval evaluation dataset consisting of 5000 question texts and approximately 500k web page titles and web page introductions or summaries (meta descriptions, etc.). The question texts are created based on one of the 500k web pages, and that data is used as a positive example for the question text.</p> <p>Dataset: <code>mteb/JaCWIRReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Web, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#locbenchrr","title":"LocBenchRR","text":"<p>Software Issue Localization.</p> <p>Dataset: <code>mteb/LocBenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{chen2025locagentgraphguidedllmagents,\n  archiveprefix = {arXiv},\n  author = {Zhaoling Chen and Xiangru Tang and Gangda Deng and Fang Wu and Jialong Wu and Zhiwei Jiang and Viktor Prasanna and Arman Cohan and Xingyao Wang},\n  eprint = {2503.09089},\n  primaryclass = {cs.SE},\n  title = {LocAgent: Graph-Guided LLM Agents for Code Localization},\n  url = {https://arxiv.org/abs/2503.09089},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#miraclreranking","title":"MIRACLReranking","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages.</p> <p>Dataset: <code>mteb/MIRACLReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#mmarcoreranking","title":"MMarcoReranking","text":"<p>mMARCO is a multilingual version of the MS MARCO passage ranking dataset</p> <p>Dataset: <code>mteb/MMarcoReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn not specified not specified not specified Citation <pre><code>@misc{bonifacio2021mmarco,\n  archiveprefix = {arXiv},\n  author = {Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira},\n  eprint = {2108.13897},\n  primaryclass = {cs.CL},\n  title = {mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#mindsmallreranking","title":"MindSmallReranking","text":"<p>Microsoft News Dataset: A Large-Scale English Dataset for News Recommendation Research</p> <p>Dataset: <code>mteb/MindSmallReranking</code> \u2022 License: https://github.com/msnews/MIND/blob/master/MSR%20License_Data.pdf \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_over_subqueries_map_at_1000 eng News, Written expert-annotated found Citation <pre><code>@inproceedings{wu-etal-2020-mind,\n  address = {Online},\n  author = {Wu, Fangzhao  and Qiao, Ying  and Chen, Jiun-Hung  and Wu, Chuhan  and Qi,\nTao  and Lian, Jianxun  and Liu, Danyang  and Xie, Xing  and Gao, Jianfeng  and Wu, Winnie  and Zhou, Ming},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.331},\n  editor = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},\n  month = jul,\n  pages = {3597--3606},\n  publisher = {Association for Computational Linguistics},\n  title = {{MIND}: A Large-scale Dataset for News\nRecommendation},\n  url = {https://aclanthology.org/2020.acl-main.331},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#multiswebenchrr","title":"MultiSWEbenchRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/MultiSWEbenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{zan2025multiswebench,\n  archiveprefix = {arXiv},\n  author = {Daoguang Zan and Zhirong Huang and Wei Liu and Hanwu Chen and Linhao Zhang and Shulin Xin and Lu Chen and Qi Liu and Xiaojian Zhong and Aoyan Li and Siyao Liu and Yongsheng Xiao and Liangqiang Chen and Yuyu Zhang and Jing Su and Tianyu Liu and Rui Long and Kai Shen and Liang Xiang},\n  eprint = {2504.02605},\n  primaryclass = {cs.SE},\n  title = {Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving},\n  url = {https://arxiv.org/abs/2504.02605},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#namaamrtydireranking","title":"NamaaMrTydiReranking","text":"<p>Mr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations. This dataset adapts the arabic test split for Reranking evaluation purposes by the addition of multiple (Hard) Negatives to each query and positive</p> <p>Dataset: <code>mteb/NamaaMrTydiReranking</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 ara Encyclopaedic, Written human-annotated found Citation <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\\\"\\\\i}c and Reimers, Nils},\n  doi = {10.48550/ARXIV.2210.07316},\n  journal = {arXiv preprint arXiv:2210.07316},\n  publisher = {arXiv},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2210.07316},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#nevir","title":"NevIR","text":"<p>Paired evaluation of real world negation in retrieval, with questions and passages. Since models generally prefer one passage over the other always, there are two questions that the model must get right to understand the negation (hence the <code>paired_accuracy</code> metric).</p> <p>Dataset: <code>orionweller/NevIR-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) paired_accuracy eng Web human-annotated created Citation <pre><code>@inproceedings{Weller2023NevIRNI,\n  author = {{Orion Weller and Dawn J Lawrie and Benjamin Van Durme}},\n  booktitle = {{Conference of the European Chapter of the Association for Computational Linguistics}},\n  title = {{NevIR: Negation in Neural Information Retrieval}},\n  url = {{https://api.semanticscholar.org/CorpusID:258676146}},\n  year = {{2023}},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#rubqreranking","title":"RuBQReranking","text":"<p>Paragraph reranking based on RuBQ 2.0. Give paragraphs that answer the question higher scores.</p> <p>Dataset: <code>mteb/RuBQReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 rus Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{RuBQ2021,\n  author = {Ivan Rybin and Vladislav Korablinov and Pavel Efimov and Pavel Braslavski},\n  booktitle = {ESWC},\n  pages = {532--547},\n  title = {RuBQ 2.0: An Innovated Russian Question Answering Dataset},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swepolybenchrr","title":"SWEPolyBenchRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEPolyBenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{rashid2025swepolybenchmultilanguagebenchmarkrepository,\n  archiveprefix = {arXiv},\n  author = {Muhammad Shihab Rashid and Christian Bock and Yuan Zhuang and Alexander Buchholz and Tim Esler and Simon Valentin and Luca Franceschi and Martin Wistuba and Prabhu Teja Sivaprasad and Woo Jung Kim and Anoop Deoras and Giovanni Zappella and Laurent Callot},\n  eprint = {2504.08703},\n  primaryclass = {cs.SE},\n  title = {SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents},\n  url = {https://arxiv.org/abs/2504.08703},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchliterr","title":"SWEbenchLiteRR","text":"<p>Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEbenchLiteRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{jimenez2024swebenchlanguagemodelsresolve,\n  archiveprefix = {arXiv},\n  author = {Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},\n  eprint = {2310.06770},\n  primaryclass = {cs.CL},\n  title = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},\n  url = {https://arxiv.org/abs/2310.06770},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchmultilingualrr","title":"SWEbenchMultilingualRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEbenchMultilingualRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{yang2025swesmith,\n  archiveprefix = {arXiv},\n  author = {John Yang and Kilian Lieret and Carlos E. Jimenez and Alexander Wettig and Kabir Khandpur and Yanzhe Zhang and Binyuan Hui and Ofir Press and Ludwig Schmidt and Diyi Yang},\n  eprint = {2504.21798},\n  primaryclass = {cs.SE},\n  title = {SWE-smith: Scaling Data for Software Engineering Agents},\n  url = {https://arxiv.org/abs/2504.21798},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchverifiedrr","title":"SWEbenchVerifiedRR","text":"<p>Software Issue Localization for SWE-bench Verified</p> <p>Dataset: <code>mteb/SWEbenchVerifiedRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{openai2024swebenchverified,\n  author = {OpenAI},\n  title = {Introducing swe-bench verified},\n  url = {https://openai.com/index/introducing-swe-bench-verified/},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#scidocsrr","title":"SciDocsRR","text":"<p>Ranking of related scientific papers based on their title.</p> <p>Dataset: <code>mteb/SciDocsRR</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Academic, Non-fiction, Written not specified found Citation <pre><code>@inproceedings{cohan-etal-2020-specter,\n  address = {Online},\n  author = {Cohan, Arman  and\nFeldman, Sergey  and\nBeltagy, Iz  and\nDowney, Doug  and\nWeld, Daniel},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.207},\n  editor = {Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel},\n  month = jul,\n  pages = {2270--2282},\n  publisher = {Association for Computational Linguistics},\n  title = {{SPECTER}: Document-level Representation Learning using Citation-informed Transformers},\n  url = {https://aclanthology.org/2020.acl-main.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#scidocsrr-vn","title":"SciDocsRR-VN","text":"<p>A translated dataset from Ranking of related scientific papers based on their title.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/SciDocsRR-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#stackoverflowdupquestions","title":"StackOverflowDupQuestions","text":"<p>Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python</p> <p>Dataset: <code>mteb/StackOverflowDupQuestions</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Blog, Programming, Written derived found Citation <pre><code>@article{Liu2018LinkSOAD,\n  author = {Xueqing Liu and Chi Wang and Yue Leng and ChengXiang Zhai},\n  journal = {Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering},\n  title = {LinkSO: a dataset for learning to retrieve similar question answer pairs on software development forums},\n  url = {https://api.semanticscholar.org/CorpusID:53111679},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#stackoverflowdupquestions-vn","title":"StackOverflowDupQuestions-VN","text":"<p>A translated dataset from Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/StackOverflowDupQuestions-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#syntecreranking","title":"SyntecReranking","text":"<p>This dataset has been built from the Syntec Collective bargaining agreement.</p> <p>Dataset: <code>mteb/SyntecReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 fra Legal, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#t2reranking","title":"T2Reranking","text":"<p>T2Ranking: A large-scale Chinese Benchmark for Passage Ranking</p> <p>Dataset: <code>mteb/T2Reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn not specified not specified not specified Citation <pre><code>@misc{xie2023t2ranking,\n  archiveprefix = {arXiv},\n  author = {Xiaohui Xie and Qian Dong and Bingning Wang and Feiyang Lv and Ting Yao and Weinan Gan and Zhijing Wu and Xiangsheng Li and Haitao Li and Yiqun Liu and Jin Ma},\n  eprint = {2304.03679},\n  primaryclass = {cs.IR},\n  title = {T2Ranking: A large-scale Chinese Benchmark for Passage Ranking},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#voyagemmarcoreranking","title":"VoyageMMarcoReranking","text":"<p>a hard-negative augmented version of the Japanese MMARCO dataset as used in Voyage AI Evaluation Suite</p> <p>Dataset: <code>mteb/VoyageMMarcoReranking</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Academic, Non-fiction, Written derived found Citation <pre><code>@misc{clavi\u00e92023jacolbert,\n  archiveprefix = {arXiv},\n  author = {Benjamin Clavi\u00e9},\n  eprint = {2312.16144},\n  title = {JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings for Retrieval: Early Technical Report},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#weblinxcandidatesreranking","title":"WebLINXCandidatesReranking","text":"<p>WebLINX is a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. The reranking task focuses on finding relevant elements at every given step in the trajectory.</p> <p>Dataset: <code>mteb/WebLINXCandidatesReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) mrr_at_10 eng Academic, Web, Written expert-annotated created Citation <pre><code>@misc{l\u00f92024weblinx,\n  archiveprefix = {arXiv},\n  author = {Xing Han L\u00f9 and Zden\u011bk Kasner and Siva Reddy},\n  eprint = {2402.05930},\n  primaryclass = {cs.CL},\n  title = {WebLINX: Real-World Website Navigation with Multi-Turn Dialogue},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#wikipediarerankingmultilingual","title":"WikipediaRerankingMultilingual","text":"<p>The dataset is derived from Cohere's wikipedia-2023-11 dataset and contains synthetically generated queries.</p> <p>Dataset: <code>mteb/WikipediaRerankingMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 ben, bul, ces, dan, deu, ... (18) Encyclopaedic, Written LM-generated and reviewed LM-generated and verified Citation <pre><code>@online{wikidump,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#xgluewprreranking","title":"XGlueWPRReranking","text":"<p>XGLUE is a new benchmark dataset to evaluate the performance of cross-lingual pre-trained models         with respect to cross-lingual natural language understanding and generation. XGLUE is composed of 11 tasks spans 19 languages.</p> <p>Dataset: <code>mteb/XGlueWPRReranking</code> \u2022 License: http://hdl.handle.net/11234/1-3105 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 deu, eng, fra, ita, por, ... (7) Written human-annotated found Citation <pre><code>@misc{11234/1-3105,\n  author = {Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Aepli, No{\\\"e}mi and Agi{\\'c}, {\\v Z}eljko and Ahrenberg, Lars and Aleksandravi{\\v c}i{\\=u}t{\\.e}, Gabriel{\\.e} and Antonsen, Lene and Aplonova, Katya and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Batchelor, Colin and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\\.e}, Agn{\\.e} and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\\\"{\\i}}c and Borges V{\\\"o}lker, Emanuel and B{\\\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cavalcanti, Tatiana and Cebiro{\\u g}lu Eryi{\\u g}it, G{\\\"u}l{\\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\\v C}{\\'e}pl{\\\"o}, Slavom{\\'{\\i}}r and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cignarella, Alessandra T. and Cinkov{\\'a}, Silvie and Collomb, Aur{\\'e}lie and {\\c C}{\\\"o}ltekin, {\\c C}a{\\u g}r{\\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and de Souza, Elvis and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erina, Olga and Erjavec, Toma{\\v z} and Etienne, Aline and Evelyn, Wograine and Farkas, Rich{\\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\\'a}udia and Fujita, Kazunori and Gajdo{\\v s}ov{\\'a}, Katar{\\'{\\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\\\"a}rdenfors, Moa and Garza, Sebastian and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\\\"o}k{\\i}rmak, Memduh and Goldberg, Yoav and G{\\'o}mez Guinovart, Xavier and Gonz{\\'a}lez Saavedra, Berta and Grici{\\=u}t{\\.e}, Bernadeta and Grioni, Matias and Gr{\\=u}z{\\={\\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\\'e}line and Habash, Nizar and Haji{\\v c}, Jan and Haji{\\v c} jr., Jan and H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika and H{\\`a} M{\\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hennig, Felix and Hladk{\\'a}, Barbora and Hlav{\\'a}{\\v c}ov{\\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ikeda, Takumi and Ion, Radu and Irimia, Elena and Ishola, {\\d O}l{\\'a}j{\\'{\\i}}d{\\'e} and Jel{\\'{\\i}}nek, Tom{\\'a}{\\v s} and Johannsen, Anders and J{\\o}rgensen, Fredrik and Juutinen, Markus and Ka{\\c s}{\\i}kara, H{\\\"u}ner and Kaasen, Andre and Kabaeva, Nadezhda and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\\'a}, V{\\'a}clava and Kirchner, Jesse and Klementieva, Elena and K{\\\"o}hn, Arne and Kopacewicz, Kamil and Kotsyba, Natalia and Kovalevskait{\\.e}, Jolanta and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lam, Lucia and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\\^e} H{\\`{\\^o}}ng, Ph\u01b0\u01a1ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Liovina, Maria and Li, Yuan and Ljube{\\v s}i{\\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\\u a}r{\\u a}nduc, C{\\u a}t{\\u a}lina and Mare{\\v c}ek, David and Marheinecke, Katrin and Mart{\\'{\\i}}nez Alonso, H{\\'e}ctor and Martins, Andr{\\'e} and Ma{\\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendon{\\c c}a, Gustavo and Miekka, Niko and Misirpashayeva, Margarita and Missil{\\\"a}, Anna and Mititelu, C{\\u a}t{\\u a}lin and Mitrofan, Maria and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Morioka, Tomohiko and Mori, Shinsuke and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Munro, Robert and Murawaki, Yugo and M{\\\"u}{\\\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\\~n}iacek, Juan Ignacio and Nedoluzhko, Anna and Ne{\\v s}pore-B{\\=e}rzkalne, Gunta and Nguy{\\~{\\^e}}n Th{\\d i}, L\u01b0\u01a1ng and Nguy{\\~{\\^e}}n Th{\\d i} Minh, Huy{\\`{\\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ojha, Atul Kr. and Ol{\\'u}{\\`o}kun, Ad{\\'e}day{\\d o}\u0300 and Omura, Mai and Osenova, Petya and {\\\"O}stling, Robert and {\\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\\L}api{\\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrova, Daria and Petrov, Slav and Phelan, Jason and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Ponomareva, Larisa and Popel, Martin and Pretkalni{\\c n}a, Lauma and Pr{\\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and Qi, Peng and R{\\\"a}{\\\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Riabov, Ivan and Rie{\\ss}ler, Michael and Rimkut{\\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and Ro\u0219ca, Valentin and Rudina, Olga and Rueter, Jack and Sadde, Shoval and Sagot, Beno{\\^{\\i}}t and Saleh, Shadi and Salomoni, Alessio and Samard{\\v z}i{\\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\\\"a}rg, Dage and Saul{\\={\\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Aline and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\\'o}, Katalin and {\\v S}imkov{\\'a}, M{\\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Stella, Antonio and Straka, Milan and Strnadov{\\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Suzuki, Shingo and Sz{\\'a}nt{\\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tanaka, Takaaki and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\\v s}ov{\\'a}, Zde{\\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Utka, Andrius and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Williams, Seyi and Wir{\\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\\'o}blewska, Alina and Yako, Mary and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and {\\v Z}abokrtsk{\\'y}, Zden{\\v e}k and Zeldes, Amir and Zhang, Manying and Zhu, Hanzhi},\n  copyright = {Licence Universal Dependencies v2.5},\n  note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\\'U}FAL}), Faculty of Mathematics and Physics, Charles University},\n  title = {Universal Dependencies 2.5},\n  url = {http://hdl.handle.net/11234/1-3105},\n  year = {2019},\n}\n\n@inproceedings{Conneau2018XNLIEC,\n  author = {Alexis Conneau and Guillaume Lample and Ruty Rinott and Adina Williams and Samuel R. Bowman and Holger Schwenk and Veselin Stoyanov},\n  booktitle = {EMNLP},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n\n@article{Lewis2019MLQAEC,\n  author = {Patrick Lewis and Barlas Oguz and Ruty Rinott and Sebastian Riedel and Holger Schwenk},\n  journal = {ArXiv},\n  title = {MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  volume = {abs/1910.07475},\n  year = {2019},\n}\n\n@article{Liang2020XGLUEAN,\n  author = {Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},\n  journal = {arXiv},\n  title = {XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},\n  volume = {abs/2004.01401},\n  year = {2020},\n}\n\n@article{Sang2002IntroductionTT,\n  author = {Erik F. Tjong Kim Sang},\n  journal = {ArXiv},\n  title = {Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition},\n  volume = {cs.CL/0209010},\n  year = {2002},\n}\n\n@article{Sang2003IntroductionTT,\n  author = {Erik F. Tjong Kim Sang and Fien De Meulder},\n  journal = {ArXiv},\n  title = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},\n  volume = {cs.CL/0306050},\n  year = {2003},\n}\n\n@article{Yang2019PAWSXAC,\n  author = {Yinfei Yang and Yuan Zhang and Chris Tar and Jason Baldridge},\n  journal = {ArXiv},\n  title = {PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},\n  volume = {abs/1908.11828},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/","title":"Retrieval","text":"<ul> <li>Number of tasks: 369</li> </ul>"},{"location":"overview/available_tasks/retrieval/#ailacasedocs","title":"AILACasedocs","text":"<p>The task is to retrieve the case document that most closely matches or is most relevant to the scenario described in the provided query.</p> <p>Dataset: <code>mteb/AILA_casedocs</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@dataset{paheli_bhattacharya_2020_4063986,\n  author = {Paheli Bhattacharya and\nKripabandhu Ghosh and\nSaptarshi Ghosh and\nArindam Pal and\nParth Mehta and\nArnab Bhattacharya and\nPrasenjit Majumder},\n  doi = {10.5281/zenodo.4063986},\n  month = oct,\n  publisher = {Zenodo},\n  title = {AILA 2019 Precedent \\&amp; Statute Retrieval Task},\n  url = {https://doi.org/10.5281/zenodo.4063986},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ailastatutes","title":"AILAStatutes","text":"<p>This dataset is structured for the task of identifying the most relevant statutes for a given situation.</p> <p>Dataset: <code>mteb/AILA_statutes</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@dataset{paheli_bhattacharya_2020_4063986,\n  author = {Paheli Bhattacharya and\nKripabandhu Ghosh and\nSaptarshi Ghosh and\nArindam Pal and\nParth Mehta and\nArnab Bhattacharya and\nPrasenjit Majumder},\n  doi = {10.5281/zenodo.4063986},\n  month = oct,\n  publisher = {Zenodo},\n  title = {AILA 2019 Precedent \\&amp; Statute Retrieval Task},\n  url = {https://doi.org/10.5281/zenodo.4063986},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arcchallenge","title":"ARCChallenge","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on ARC-Challenge.</p> <p>Dataset: <code>mteb/ARCChallenge</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{clark2018think,\n  author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},\n  journal = {arXiv preprint arXiv:1803.05457},\n  title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},\n  year = {2018},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#alloprofretrieval","title":"AlloprofRetrieval","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>mteb/AlloprofRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#alphanli","title":"AlphaNLI","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on AlphaNLI.</p> <p>Dataset: <code>mteb/AlphaNLI</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{bhagavatula2019abductive,\n  author = {Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1908.05739},\n  title = {Abductive commonsense reasoning},\n  year = {2019},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#appsretrieval","title":"AppsRetrieval","text":"<p>The dataset is a collection of natural language queries and their corresponding code snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/apps</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming, Written derived found Citation <pre><code>@article{hendrycksapps2021,\n  author = {Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},\n  journal = {NeurIPS},\n  title = {Measuring Coding Challenge Competence With APPS},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana","title":"ArguAna","text":"<p>ArguAna: Retrieval of the Best Counterargument without Prior Topic Knowledge</p> <p>Dataset: <code>mteb/arguana</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social, Web, Written derived found Citation <pre><code>@inproceedings{wachsmuth2018retrieval,\n  author = {Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},\n  booktitle = {ACL},\n  title = {Retrieval of the Best Counterargument without Prior Topic Knowledge},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-fa","title":"ArguAna-Fa","text":"<p>ArguAna-Fa</p> <p>Dataset: <code>MCINext/arguana-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Blog derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-fav2","title":"ArguAna-Fa.v2","text":"<p>ArguAna-Fa.v2</p> <p>Dataset: <code>MCINext/arguana-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Blog derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-nl","title":"ArguAna-NL","text":"<p>ArguAna involves the task of retrieval of the best counterargument to an argument. ArguAna-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-arguana</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-pl","title":"ArguAna-PL","text":"<p>ArguAna-PL</p> <p>Dataset: <code>mteb/ArguAna-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Medical, Written not specified not specified Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-vn","title":"ArguAna-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/arguana-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#autoragretrieval","title":"AutoRAGRetrieval","text":"<p>This dataset enables the evaluation of Korean RAG performance across various domains\u2014finance, public sector, healthcare, legal, and commerce\u2014by providing publicly accessible documents, questions, and answers.</p> <p>Dataset: <code>yjoonjang/markers_bm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor Financial, Government, Legal, Medical, Social human-annotated created Citation <pre><code>@misc{kim2024autoragautomatedframeworkoptimization,\n  archiveprefix = {arXiv},\n  author = {Dongkyu Kim and Byoungwook Kim and Donggeon Han and Matou\u0161 Eibich},\n  eprint = {2410.20878},\n  primaryclass = {cs.CL},\n  title = {AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline},\n  url = {https://arxiv.org/abs/2410.20878},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-arguana","title":"BIRCO-ArguAna","text":"<p>Retrieval task using the ArguAna dataset from BIRCO. This dataset contains 100 queries where both queries and passages are complex one-paragraph arguments about current affairs. The objective is to retrieve the counter-argument that directly refutes the query\u2019s stance.</p> <p>Dataset: <code>mteb/BIRCO-ArguAna-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Written expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-clinicaltrial","title":"BIRCO-ClinicalTrial","text":"<p>Retrieval task using the Clinical-Trial dataset from BIRCO. This dataset contains 50 queries that are patient case reports. Each query has a candidate pool comprising 30-110 clinical trial descriptions. Relevance is graded (0, 1, 2), where 1 and 2 are considered relevant.</p> <p>Dataset: <code>mteb/BIRCO-ClinicalTrial-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-dorismae","title":"BIRCO-DorisMae","text":"<p>Retrieval task using the DORIS-MAE dataset from BIRCO. This dataset contains 60 queries that are complex research questions from computer scientists. Each query has a candidate pool of approximately 110 abstracts. Relevance is graded from 0 to 2 (scores of 1 and 2 are considered relevant).</p> <p>Dataset: <code>mteb/BIRCO-DorisMae-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-relic","title":"BIRCO-Relic","text":"<p>Retrieval task using the RELIC dataset from BIRCO. This dataset contains 100 queries which are excerpts from literary analyses with a missing quotation (indicated by [masked sentence(s)]). Each query has a candidate pool of 50 passages. The objective is to retrieve the passage that best completes the literary analysis.</p> <p>Dataset: <code>mteb/BIRCO-Relic-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-wtb","title":"BIRCO-WTB","text":"<p>Retrieval task using the WhatsThatBook dataset from BIRCO. This dataset contains 100 queries where each query is an ambiguous description of a book. Each query has a candidate pool of 50 book descriptions. The objective is to retrieve the correct book description.</p> <p>Dataset: <code>mteb/BIRCO-WTB-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#bsardretrieval","title":"BSARDRetrieval","text":"<p>The Belgian Statutory Article Retrieval Dataset (BSARD) is a French native dataset for studying legal information retrieval. BSARD consists of more than 22,600 statutory articles from Belgian law and about 1,100 legal questions posed by Belgian citizens and labeled by experienced jurists with relevant articles from the corpus.</p> <p>Dataset: <code>mteb/BSARDRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_100 fra Legal, Spoken expert-annotated found Citation <pre><code>@inproceedings{louis2022statutory,\n  address = {Dublin, Ireland},\n  author = {Louis, Antoine and Spanakis, Gerasimos},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2022.acl-long.468},\n  month = may,\n  pages = {6789\u20136803},\n  publisher = {Association for Computational Linguistics},\n  title = {A Statutory Article Retrieval Dataset in French},\n  url = {https://aclanthology.org/2022.acl-long.468/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#bsardretrievalv2","title":"BSARDRetrieval.v2","text":"<p>BSARD is a French native dataset for legal information retrieval. BSARDRetrieval.v2 covers multi-article queries, fixing issues (#2906) with the previous data loading. </p> <p>Dataset: <code>mteb/BSARDRetrieval.v2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_100 fra Legal, Spoken expert-annotated found Citation <pre><code>@inproceedings{louis2022statutory,\n  address = {Dublin, Ireland},\n  author = {Louis, Antoine and Spanakis, Gerasimos},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2022.acl-long.468},\n  month = may,\n  pages = {6789\u20136803},\n  publisher = {Association for Computational Linguistics},\n  title = {A Statutory Article Retrieval Dataset in French},\n  url = {https://aclanthology.org/2022.acl-long.468/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#barexamqa","title":"BarExamQA","text":"<p>A benchmark for retrieving legal provisions that answer US bar exam questions.</p> <p>Dataset: <code>isaacus/mteb-barexam-qa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Legal expert-annotated found Citation <pre><code>@inproceedings{Zheng_2025,\n  author = {Zheng, Lucia and Guha, Neel and Arifov, Javokhir and Zhang, Sarah and Skreta, Michal and Manning, Christopher D. and Henderson, Peter and Ho, Daniel E.},\n  booktitle = {Proceedings of the Symposium on Computer Science and Law on ZZZ},\n  collection = {CSLAW \u201925},\n  doi = {10.1145/3709025.3712219},\n  eprint = {2505.03970},\n  month = mar,\n  pages = {169\u2013193},\n  publisher = {ACM},\n  series = {CSLAW \u201925},\n  title = {A Reasoning-Focused Legal Retrieval Benchmark},\n  url = {http://dx.doi.org/10.1145/3709025.3712219},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#belebeleretrieval","title":"BelebeleRetrieval","text":"<p>Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants (including 115 distinct languages and their scripts)</p> <p>Dataset: <code>facebook/belebele</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 acm, afr, als, amh, apc, ... (115) News, Web, Written expert-annotated created Citation <pre><code>@article{bandarkar2023belebele,\n  author = {Lucas Bandarkar and Davis Liang and Benjamin Muller and Mikel Artetxe and Satya Narayan Shukla and Donald Husa and Naman Goyal and Abhinandan Krishnan and Luke Zettlemoyer and Madian Khabsa},\n  journal = {arXiv preprint arXiv:2308.16884},\n  title = {The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#billsumca","title":"BillSumCA","text":"<p>A benchmark for retrieving Californian bills based on their summaries.</p> <p>Dataset: <code>isaacus/mteb-BillSumCA</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{Eidelman_2019,\n  author = {Eidelman, Vladimir},\n  booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},\n  doi = {10.18653/v1/d19-5406},\n  pages = {48\u201356},\n  publisher = {Association for Computational Linguistics},\n  title = {BillSum: A Corpus for Automatic Summarization of US Legislation},\n  url = {http://dx.doi.org/10.18653/v1/D19-5406},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#billsumus","title":"BillSumUS","text":"<p>A benchmark for retrieving US federal bills based on their summaries.</p> <p>Dataset: <code>isaacus/mteb-BillSumUS</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{Eidelman_2019,\n  author = {Eidelman, Vladimir},\n  booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},\n  doi = {10.18653/v1/d19-5406},\n  pages = {48\u201356},\n  publisher = {Association for Computational Linguistics},\n  title = {BillSum: A Corpus for Automatic Summarization of US Legislation},\n  url = {http://dx.doi.org/10.18653/v1/D19-5406},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightlongretrieval","title":"BrightLongRetrieval","text":"<p>Bright retrieval dataset with long documents.</p> <p>Dataset: <code>xlangai/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@misc{su2024brightrealisticchallengingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Hongjin Su and Howard Yen and Mengzhou Xia and Weijia Shi and Niklas Muennighoff and Han-yu Wang and Haisu Liu and Quan Shi and Zachary S. Siegel and Michael Tang and Ruoxi Sun and Jinsung Yoon and Sercan O. Arik and Danqi Chen and Tao Yu},\n  eprint = {2407.12883},\n  primaryclass = {cs.CL},\n  title = {BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval},\n  url = {https://arxiv.org/abs/2407.12883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightretrieval","title":"BrightRetrieval","text":"<p>Bright retrieval dataset.</p> <p>Dataset: <code>xlangai/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@misc{su2024brightrealisticchallengingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Hongjin Su and Howard Yen and Mengzhou Xia and Weijia Shi and Niklas Muennighoff and Han-yu Wang and Haisu Liu and Quan Shi and Zachary S. Siegel and Michael Tang and Ruoxi Sun and Jinsung Yoon and Sercan O. Arik and Danqi Chen and Tao Yu},\n  eprint = {2407.12883},\n  primaryclass = {cs.CL},\n  title = {BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval},\n  url = {https://arxiv.org/abs/2407.12883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#builtbenchretrieval","title":"BuiltBenchRetrieval","text":"<p>Retrieval of built asset entity type/class descriptions given a query describing an entity as represented in well-established industry classification systems such as Uniclass, IFC, etc.</p> <p>Dataset: <code>mteb/BuiltBenchRetrieval</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#coircodesearchnetretrieval","title":"COIRCodeSearchNetRetrieval","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code summary given a code snippet.</p> <p>Dataset: <code>CoIR-Retrieval/CodeSearchNet</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-android-pl","title":"CQADupstack-Android-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Android-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-english-pl","title":"CQADupstack-English-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-English-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-gaming-pl","title":"CQADupstack-Gaming-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Gaming-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-gis-pl","title":"CQADupstack-Gis-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Gis-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-mathematica-pl","title":"CQADupstack-Mathematica-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Mathematica-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-physics-pl","title":"CQADupstack-Physics-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Physics-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-programmers-pl","title":"CQADupstack-Programmers-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Programmers-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Programming, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-stats-pl","title":"CQADupstack-Stats-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Stats-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-tex-pl","title":"CQADupstack-Tex-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Tex-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-unix-pl","title":"CQADupstack-Unix-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Unix-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-webmasters-pl","title":"CQADupstack-Webmasters-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Webmasters-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-wordpress-pl","title":"CQADupstack-Wordpress-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Wordpress-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroid-nl","title":"CQADupstackAndroid-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroid-vn","title":"CQADupstackAndroid-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-android-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroidretrieval","title":"CQADupstackAndroidRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/CQADupstackAndroidRetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroidretrieval-fa","title":"CQADupstackAndroidRetrieval-Fa","text":"<p>CQADupstackAndroidRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-android-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglish-nl","title":"CQADupstackEnglish-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglishretrieval","title":"CQADupstackEnglishRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-english</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglishretrieval-fa","title":"CQADupstackEnglishRetrieval-Fa","text":"<p>CQADupstackEnglishRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-english-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgaming-nl","title":"CQADupstackGaming-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgamingretrieval","title":"CQADupstackGamingRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-gaming</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgamingretrieval-fa","title":"CQADupstackGamingRetrieval-Fa","text":"<p>CQADupstackGamingRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-gaming-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgis-nl","title":"CQADupstackGis-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgis-vn","title":"CQADupstackGis-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-gis-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgisretrieval","title":"CQADupstackGisRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-gis</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgisretrieval-fa","title":"CQADupstackGisRetrieval-Fa","text":"<p>CQADupstackGisRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-gis-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematica-nl","title":"CQADupstackMathematica-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematica-vn","title":"CQADupstackMathematica-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-mathematica-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematicaretrieval","title":"CQADupstackMathematicaRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-mathematica</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematicaretrieval-fa","title":"CQADupstackMathematicaRetrieval-Fa","text":"<p>CQADupstackMathematicaRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-mathematica-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysics-nl","title":"CQADupstackPhysics-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysics-vn","title":"CQADupstackPhysics-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-physics-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysicsretrieval","title":"CQADupstackPhysicsRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-physics</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysicsretrieval-fa","title":"CQADupstackPhysicsRetrieval-Fa","text":"<p>CQADupstackPhysicsRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-physics-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammers-nl","title":"CQADupstackProgrammers-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammers-vn","title":"CQADupstackProgrammers-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-programmers-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Programming, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammersretrieval","title":"CQADupstackProgrammersRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-programmers</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Programming, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammersretrieval-fa","title":"CQADupstackProgrammersRetrieval-Fa","text":"<p>CQADupstackProgrammersRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-programmers-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstats-nl","title":"CQADupstackStats-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstats-vn","title":"CQADupstackStats-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-stats-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstatsretrieval","title":"CQADupstackStatsRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-stats</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstatsretrieval-fa","title":"CQADupstackStatsRetrieval-Fa","text":"<p>CQADupstackStatsRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-stats-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktex-nl","title":"CQADupstackTex-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktex-vn","title":"CQADupstackTex-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-tex-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktexretrieval","title":"CQADupstackTexRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-tex</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktexretrieval-fa","title":"CQADupstackTexRetrieval-Fa","text":"<p>CQADupstackTexRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-tex-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunix-nl","title":"CQADupstackUnix-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunix-vn","title":"CQADupstackUnix-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-unix-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunixretrieval","title":"CQADupstackUnixRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-unix</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunixretrieval-fa","title":"CQADupstackUnixRetrieval-Fa","text":"<p>CQADupstackUnixRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-unix-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmasters-nl","title":"CQADupstackWebmasters-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmasters-vn","title":"CQADupstackWebmasters-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-webmasters-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmastersretrieval","title":"CQADupstackWebmastersRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-webmasters</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmastersretrieval-fa","title":"CQADupstackWebmastersRetrieval-Fa","text":"<p>CQADupstackWebmastersRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-webmasters-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpress-nl","title":"CQADupstackWordpress-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpress-vn","title":"CQADupstackWordpress-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-wordpress-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpressretrieval","title":"CQADupstackWordpressRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-wordpress</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpressretrieval-fa","title":"CQADupstackWordpressRetrieval-Fa","text":"<p>CQADupstackWordpressRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-wordpress-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#curev1","title":"CUREv1","text":"<p>Collection of query-passage pairs curated by medical professionals, across 10 disciplines and 3 cross-lingual settings.</p> <p>Dataset: <code>clinia/CUREv1</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, fra, spa Academic, Medical, Written expert-annotated created"},{"location":"overview/available_tasks/retrieval/#chatdoctorretrieval","title":"ChatDoctorRetrieval","text":"<p>A medical retrieval task based on ChatDoctor_HealthCareMagic dataset containing 112,000 real-world medical question-and-answer pairs. Each query is a medical question from patients (e.g., 'What are the symptoms of diabetes?'), and the corpus contains medical responses and healthcare information. The task is to retrieve the correct medical information that answers the patient's question. The dataset includes grammatical inconsistencies which help separate strong healthcare retrieval models from weak ones. Queries are patient medical questions while the corpus contains relevant medical responses, diagnoses, and treatment information from healthcare professionals.</p> <p>Dataset: <code>embedding-benchmark/ChatDoctor_HealthCareMagic</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical expert-annotated found Citation <pre><code>@misc{chatdoctor_healthcaremagic,\n  title = {ChatDoctor HealthCareMagic: Medical Question-Answer Retrieval Dataset},\n  url = {https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#chemhotpotqaretrieval","title":"ChemHotpotQARetrieval","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/ChemHotpotQARetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Chemistry derived found Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#chemnqretrieval","title":"ChemNQRetrieval","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/ChemNQRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Chemistry derived found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational Linguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n\n@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever","title":"ClimateFEVER","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims (queries) regarding climate-change. The underlying corpus is the same as FEVER.</p> <p>Dataset: <code>mteb/climate-fever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-fa","title":"ClimateFEVER-Fa","text":"<p>ClimateFEVER-Fa</p> <p>Dataset: <code>MCINext/climate-fever-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-nl","title":"ClimateFEVER-NL","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. ClimateFEVER-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-climate-fever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-vn","title":"ClimateFEVER-VN","text":"<p>A translated dataset from CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/climate-fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverv2","title":"ClimateFEVER.v2","text":"<p>CLIMATE-FEVER is a dataset following the FEVER methodology, containing 1,535 real-world climate change claims. This updated version addresses corpus mismatches and qrel inconsistencies in MTEB, restoring labels while refining corpus-query alignment for better accuracy.</p> <p>Dataset: <code>mteb/climate-fever-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverhardnegatives","title":"ClimateFEVERHardNegatives","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/ClimateFEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverhardnegativesv2","title":"ClimateFEVERHardNegatives.v2","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/ClimateFEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cmedqaretrieval","title":"CmedqaRetrieval","text":"<p>Online medical consultation text. Used the CMedQAv2 as its underlying dataset.</p> <p>Dataset: <code>mteb/CmedqaRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Medical, Written not specified not specified Citation <pre><code>@misc{qiu2022dureaderretrievallargescalechinesebenchmark,\n  archiveprefix = {arXiv},\n  author = {Yifu Qiu and Hongyu Li and Yingqi Qu and Ying Chen and Qiaoqiao She and Jing Liu and Hua Wu and Haifeng Wang},\n  eprint = {2203.10232},\n  primaryclass = {cs.CL},\n  title = {DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine},\n  url = {https://arxiv.org/abs/2203.10232},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codeeditsearchretrieval","title":"CodeEditSearchRetrieval","text":"<p>The dataset is a collection of unified diffs of code changes, paired with a short instruction that describes the change. The dataset is derived from the CommitPackFT dataset.</p> <p>Dataset: <code>cassanof/CodeEditSearch</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 c, c++, go, java, javascript, ... (13) Programming, Written derived found Citation <pre><code>@article{muennighoff2023octopack,\n  author = {Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},\n  journal = {arXiv preprint arXiv:2308.07124},\n  title = {OctoPack: Instruction Tuning Code Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codefeedbackmt","title":"CodeFeedbackMT","text":"<p>The dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/codefeedback-mt</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{zheng2024opencodeinterpreterintegratingcodegeneration,\n  archiveprefix = {arXiv},\n  author = {Tianyu Zheng and Ge Zhang and Tianhao Shen and Xueling Liu and Bill Yuchen Lin and Jie Fu and Wenhu Chen and Xiang Yue},\n  eprint = {2402.14658},\n  primaryclass = {cs.SE},\n  title = {OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement},\n  url = {https://arxiv.org/abs/2402.14658},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codefeedbackst","title":"CodeFeedbackST","text":"<p>The dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/codefeedback-st</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codesearchnetccretrieval","title":"CodeSearchNetCCRetrieval","text":"<p>The dataset is a collection of code snippets. The task is to retrieve the most relevant code snippet for a given code snippet.</p> <p>Dataset: <code>CoIR-Retrieval/CodeSearchNet-ccr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codesearchnetretrieval","title":"CodeSearchNetRetrieval","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>mteb/CodeSearchNetRetrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codetransoceancontest","title":"CodeTransOceanContest","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code snippet</p> <p>Dataset: <code>CoIR-Retrieval/codetrans-contest</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 c++, python Programming, Written derived found Citation <pre><code>@misc{yan2023codetransoceancomprehensivemultilingualbenchmark,\n  archiveprefix = {arXiv},\n  author = {Weixiang Yan and Yuchen Tian and Yunzhe Li and Qian Chen and Wen Wang},\n  eprint = {2310.04951},\n  primaryclass = {cs.AI},\n  title = {CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation},\n  url = {https://arxiv.org/abs/2310.04951},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codetransoceandl","title":"CodeTransOceanDL","text":"<p>The dataset is a collection of equivalent Python Deep Learning code snippets written in different machine learning framework. The task is to retrieve the equivalent code snippet in another framework, given a query code snippet from one framework.</p> <p>Dataset: <code>CoIR-Retrieval/codetrans-dl</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming, Written derived found Citation <pre><code>@misc{yan2023codetransoceancomprehensivemultilingualbenchmark,\n  archiveprefix = {arXiv},\n  author = {Weixiang Yan and Yuchen Tian and Yunzhe Li and Qian Chen and Wen Wang},\n  eprint = {2310.04951},\n  primaryclass = {cs.AI},\n  title = {CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation},\n  url = {https://arxiv.org/abs/2310.04951},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cosqa","title":"CosQA","text":"<p>The dataset is a collection of natural language queries and their corresponding code snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/cosqa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{huang2021cosqa20000webqueries,\n  archiveprefix = {arXiv},\n  author = {Junjie Huang and Duyu Tang and Linjun Shou and Ming Gong and Ke Xu and Daxin Jiang and Ming Zhou and Nan Duan},\n  eprint = {2105.13239},\n  primaryclass = {cs.CL},\n  title = {CoSQA: 20,000+ Web Queries for Code Search and Question Answering},\n  url = {https://arxiv.org/abs/2105.13239},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#covidretrieval","title":"CovidRetrieval","text":"<p>COVID-19 news articles</p> <p>Dataset: <code>mteb/CovidRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Entertainment, Medical human-annotated not specified Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#crosslingualsemanticdiscriminationwmt19","title":"CrossLingualSemanticDiscriminationWMT19","text":"<p>Evaluate a multilingual embedding model based on its ability to discriminate against the original parallel pair against challenging distractors - spawning from WMT19 DE-FR test set</p> <p>Dataset: <code>Andrianos/clsd_wmt19_21</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 deu, fra News, Written derived LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#crosslingualsemanticdiscriminationwmt21","title":"CrossLingualSemanticDiscriminationWMT21","text":"<p>Evaluate a multilingual embedding model based on its ability to discriminate against the original parallel pair against challenging distractors - spawning from WMT21 DE-FR test set</p> <p>Dataset: <code>Andrianos/clsd_wmt19_21</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 deu, fra News, Written derived LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtofulltextretrieval","title":"DAPFAMAllTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtotitlabsclmretrieval","title":"DAPFAMAllTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtotitlabsretrieval","title":"DAPFAMAllTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstofulltextretrieval","title":"DAPFAMAllTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstotitlabsclmretrieval","title":"DAPFAMAllTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstotitlabsretrieval","title":"DAPFAMAllTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtofulltextretrieval","title":"DAPFAMInTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtotitlabsclmretrieval","title":"DAPFAMInTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtotitlabsretrieval","title":"DAPFAMInTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstofulltextretrieval","title":"DAPFAMInTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstotitlabsclmretrieval","title":"DAPFAMInTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstotitlabsretrieval","title":"DAPFAMInTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtofulltextretrieval","title":"DAPFAMOutTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtotitlabsclmretrieval","title":"DAPFAMOutTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtotitlabsretrieval","title":"DAPFAMOutTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstofulltextretrieval","title":"DAPFAMOutTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstotitlabsclmretrieval","title":"DAPFAMOutTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstotitlabsretrieval","title":"DAPFAMOutTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia","title":"DBPedia","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base</p> <p>Dataset: <code>mteb/dbpedia</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-fa","title":"DBPedia-Fa","text":"<p>DBPedia-Fa</p> <p>Dataset: <code>MCINext/dbpedia-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-nl","title":"DBPedia-NL","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. DBPedia-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-dbpedia-entity</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-pl","title":"DBPedia-PL","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base</p> <p>Dataset: <code>mteb/DBPedia-PL</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written derived machine-translated Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-plhardnegatives","title":"DBPedia-PLHardNegatives","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/DBPedia-PLHardNegatives</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written derived machine-translated Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-vn","title":"DBPedia-VN","text":"<p>A translated dataset from DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/dbpedia-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpediahardnegatives","title":"DBPediaHardNegatives","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/DBPedia_test_top_250_only_w_correct-v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpediahardnegativesv2","title":"DBPediaHardNegatives.v2","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/DBPedia_test_top_250_only_w_correct-v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ds1000retrieval","title":"DS1000Retrieval","text":"<p>A code retrieval task based on 1,000 data science programming problems from DS-1000. Each query is a natural language description of a data science task (e.g., 'Create a scatter plot of column A vs column B with matplotlib'), and the corpus contains Python code implementations using libraries like pandas, numpy, matplotlib, scikit-learn, and scipy. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations focused on data science workflows.</p> <p>Dataset: <code>embedding-benchmark/DS1000</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming expert-annotated found Citation <pre><code>@article{lai2022ds,\n  author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},\n  journal = {arXiv preprint arXiv:2211.11501},\n  title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#danfever","title":"DanFEVER","text":"<p>A Danish dataset intended for misinformation research. It follows the same format as the English FEVER dataset.</p> <p>Dataset: <code>mteb/DanFEVER</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Encyclopaedic, Non-fiction, Spoken human-annotated found Citation <pre><code>@inproceedings{norregaard-derczynski-2021-danfever,\n  address = {Reykjavik, Iceland (Online)},\n  author = {N{\\o}rregaard, Jeppe  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {422--428},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{D}an{FEVER}: claim verification dataset for {D}anish},\n  url = {https://aclanthology.org/2021.nodalida-main.47},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#danfeverretrieval","title":"DanFeverRetrieval","text":"<p>A Danish dataset intended for misinformation research. It follows the same format as the English FEVER dataset. DanFeverRetrieval fixed an issue in DanFever where some corpus entries were incorrectly removed.</p> <p>Dataset: <code>strombergnlp/danfever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Encyclopaedic, Non-fiction, Spoken human-annotated found Citation <pre><code>@inproceedings{norregaard-derczynski-2021-danfever,\n  address = {Reykjavik, Iceland (Online)},\n  author = {N{\\o}rregaard, Jeppe  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {422--428},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{D}an{FEVER}: claim verification dataset for {D}anish},\n  url = {https://aclanthology.org/2021.nodalida-main.47},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#duretrieval","title":"DuRetrieval","text":"<p>A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine</p> <p>Dataset: <code>mteb/DuRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn not specified not specified not specified Citation <pre><code>@misc{qiu2022dureaderretrieval,\n  archiveprefix = {arXiv},\n  author = {Yifu Qiu and Hongyu Li and Yingqi Qu and Ying Chen and Qiaoqiao She and Jing Liu and Hua Wu and Haifeng Wang},\n  eprint = {2203.10232},\n  primaryclass = {cs.CL},\n  title = {DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dutchnewsarticlesretrieval","title":"DutchNewsArticlesRetrieval","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld News, Written derived found"},{"location":"overview/available_tasks/retrieval/#ecomretrieval","title":"EcomRetrieval","text":"<p>EcomRetrieval</p> <p>Dataset: <code>mteb/EcomRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn not specified not specified not specified Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#estqa","title":"EstQA","text":"<p>EstQA is an Estonian question answering dataset based on Wikipedia.</p> <p>Dataset: <code>kardosdrur/estonian-qa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 est Encyclopaedic, Written human-annotated found Citation <pre><code>@mastersthesis{mastersthesis,\n  author = {Anu K\u00e4ver},\n  school = {Tallinn University of Technology (TalTech)},\n  title = {Extractive Question Answering for Estonian Language},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever","title":"FEVER","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.</p> <p>Dataset: <code>mteb/fever</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-fahardnegatives","title":"FEVER-FaHardNegatives","text":"<p>FEVER-FaHardNegatives</p> <p>Dataset: <code>MCINext/FEVER_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic, Written human-annotated found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-nl","title":"FEVER-NL","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. FEVER-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-fever</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-vn","title":"FEVER-VN","text":"<p>A translated dataset from FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences             extracted from Wikipedia and subsequently verified without knowledge of the sentence they were             derived from.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feverhardnegatives","title":"FEVERHardNegatives","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/FEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feverhardnegativesv2","title":"FEVERHardNegatives.v2","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/FEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fquadretrieval","title":"FQuADRetrieval","text":"<p>This dataset has been built from the French SQuad dataset.</p> <p>Dataset: <code>manu/fquad2_test</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{dhoffschmidt-etal-2020-fquad,\n  address = {Online},\n  author = {d{'}Hoffschmidt, Martin  and\nBelblidia, Wacim  and\nHeinrich, Quentin  and\nBrendl{\\'e}, Tom  and\nVidal, Maxime},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},\n  doi = {10.18653/v1/2020.findings-emnlp.107},\n  editor = {Cohn, Trevor  and\nHe, Yulan  and\nLiu, Yang},\n  month = nov,\n  pages = {1193--1208},\n  publisher = {Association for Computational Linguistics},\n  title = {{FQ}u{AD}: {F}rench Question Answering Dataset},\n  url = {https://aclanthology.org/2020.findings-emnlp.107},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#faithdial","title":"FaithDial","text":"<p>FaithDial is a faithful knowledge-grounded dialogue benchmark.It was curated by asking annotators to amend hallucinated utterances in Wizard of Wikipedia (WoW). It consists of conversation histories along with manually labelled relevant passage. For the purpose of retrieval, we only consider the instances marked as 'Edification' in the VRM field, as the gold passage associated with these instances is non-ambiguous.</p> <p>Dataset: <code>mteb/FaithDial</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@article{dziri2022faithdial,\n  author = {Dziri, Nouha and Kamalloo, Ehsan and Milton, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo M and Reddy, Siva},\n  doi = {10.1162/tacl_a_00529},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {12},\n  pages = {1473--1490},\n  publisher = {MIT Press},\n  title = {{FaithDial: A Faithful Benchmark for Information-Seeking Dialogue}},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feedbackqaretrieval","title":"FeedbackQARetrieval","text":"<p>Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment</p> <p>Dataset: <code>mteb/FeedbackQARetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) precision_at_1 eng Government, Medical, Web, Written human-annotated created Citation <pre><code>@inproceedings{li-etal-2022-using,\n  address = {Dublin, Ireland},\n  author = {Li, Zichao  and\nSharma, Prakhar  and\nLu, Xing Han  and\nCheung, Jackie  and\nReddy, Siva},\n  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},\n  doi = {10.18653/v1/2022.findings-acl.75},\n  editor = {Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline},\n  month = may,\n  pages = {926--937},\n  publisher = {Association for Computational Linguistics},\n  title = {Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment},\n  url = {https://aclanthology.org/2022.findings-acl.75},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa-pl","title":"FiQA-PL","text":"<p>Financial Opinion Mining and Question Answering</p> <p>Dataset: <code>mteb/FiQA-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Financial, Written human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018","title":"FiQA2018","text":"<p>Financial Opinion Mining and Question Answering</p> <p>Dataset: <code>mteb/fiqa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial, Written human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-fa","title":"FiQA2018-Fa","text":"<p>FiQA2018-Fa</p> <p>Dataset: <code>MCINext/fiqa-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-fav2","title":"FiQA2018-Fa.v2","text":"<p>FiQA2018-Fa.v2</p> <p>Dataset: <code>MCINext/fiqa-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-nl","title":"FiQA2018-NL","text":"<p>Financial Opinion Mining and Question Answering. FiQA2018-NL is a Dutch translation</p> <p>Dataset: <code>clips/beir-nl-fiqa</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-vn","title":"FiQA2018-VN","text":"<p>A translated dataset from Financial Opinion Mining and Question Answering             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/fiqa-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Financial, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#finqaretrieval","title":"FinQARetrieval","text":"<p>A financial retrieval task based on FinQA dataset containing numerical reasoning questions over financial documents. Each query is a financial question requiring numerical computation (e.g., 'What is the percentage change in operating expenses from 2019 to 2020?'), and the corpus contains financial document text with tables and numerical data. The task is to retrieve the correct financial information that enables answering the numerical question. Queries are numerical reasoning questions while the corpus contains financial text passages with embedded tables, figures, and quantitative financial data from earnings reports.</p> <p>Dataset: <code>embedding-benchmark/FinQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{chen2021finqa,\n  author = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},\n  journal = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  title = {FinQA: A Dataset of Numerical Reasoning over Financial Data},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#financebenchretrieval","title":"FinanceBenchRetrieval","text":"<p>A financial retrieval task based on FinanceBench dataset containing financial questions and answers. Each query is a financial question (e.g., 'What was the total revenue in Q3 2023?'), and the corpus contains financial document excerpts and annual reports. The task is to retrieve the correct financial information that answers the question. Queries are financial questions while the corpus contains relevant excerpts from financial documents, earnings reports, and SEC filings with detailed financial data and metrics.</p> <p>Dataset: <code>embedding-benchmark/FinanceBench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{islam2023financebench,\n  author = {Islam, Pranab and Kannappan, Anand and Kiela, Douwe and Fergus, Rob and Ott, Myle and Wang, Sam and Garimella, Aparna and Garcia, Nino},\n  journal = {arXiv preprint arXiv:2311.11944},\n  title = {FinanceBench: A New Benchmark for Financial Question Answering},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#freshstackretrieval","title":"FreshStackRetrieval","text":"<p>A code retrieval task based on FreshStack dataset containing programming problems across multiple languages. Each query is a natural language description of a programming task (e.g., 'Write a function to reverse a string using recursion'), and the corpus contains code implementations in Python, JavaScript, and Go. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains function implementations with proper syntax and logic across different programming languages.</p> <p>Dataset: <code>embedding-benchmark/FreshStack_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, go, javascript, python Programming expert-annotated found Citation <pre><code>@article{freshstack2023,\n  author = {FreshStack Authors},\n  journal = {arXiv preprint arXiv:2301.12345},\n  title = {FreshStack: A Multi-language Code Generation and Retrieval Benchmark},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#georgianfaqretrieval","title":"GeorgianFAQRetrieval","text":"<p>Frequently asked questions (FAQs) and answers mined from Georgian websites via Common Crawl.</p> <p>Dataset: <code>jupyterjazz/georgian-faq</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kat Web, Written derived created"},{"location":"overview/available_tasks/retrieval/#gerdalir","title":"GerDaLIR","text":"<p>GerDaLIR is a legal information retrieval dataset created from the Open Legal Data platform.</p> <p>Dataset: <code>mteb/GerDaLIR</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal not specified not specified Citation <pre><code>@inproceedings{wrzalik-krechel-2021-gerdalir,\n  address = {Punta Cana, Dominican Republic},\n  author = {Wrzalik, Marco  and\nKrechel, Dirk},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  month = nov,\n  pages = {123--128},\n  publisher = {Association for Computational Linguistics},\n  title = {{G}er{D}a{LIR}: A {G}erman Dataset for Legal Information Retrieval},\n  url = {https://aclanthology.org/2021.nllp-1.13},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#gerdalirsmall","title":"GerDaLIRSmall","text":"<p>The dataset consists of documents, passages and relevance labels in German. In contrast to the original dataset, only documents that have corresponding queries in the query set are chosen to create a smaller corpus for evaluation purposes.</p> <p>Dataset: <code>mteb/GerDaLIRSmall</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found Citation <pre><code>@inproceedings{wrzalik-krechel-2021-gerdalir,\n  address = {Punta Cana, Dominican Republic},\n  author = {Wrzalik, Marco  and\nKrechel, Dirk},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  month = nov,\n  pages = {123--128},\n  publisher = {Association for Computational Linguistics},\n  title = {{G}er{D}a{LIR}: A {G}erman Dataset for Legal Information Retrieval},\n  url = {https://aclanthology.org/2021.nllp-1.13},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#germandpr","title":"GermanDPR","text":"<p>GermanDPR is a German Question Answering dataset for open-domain QA. It associates questions with a textual context containing the answer</p> <p>Dataset: <code>mteb/GermanDPR</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Non-fiction, Web, Written human-annotated found Citation <pre><code>@misc{m\u00f6ller2021germanquad,\n  archiveprefix = {arXiv},\n  author = {Timo M\u00f6ller and Julian Risch and Malte Pietsch},\n  eprint = {2104.12741},\n  primaryclass = {cs.CL},\n  title = {GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#germangovserviceretrieval","title":"GermanGovServiceRetrieval","text":"<p>LHM-Dienstleistungen-QA is a German question answering dataset for government services of the Munich city administration. It associates questions with a textual context containing the answer</p> <p>Dataset: <code>it-at-m/LHM-Dienstleistungen-QA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_5 deu Government, Written derived found Citation <pre><code>@software{lhm-dienstleistungen-qa,\n  author = {Schr\u00f6der, Leon Marius and\nGutknecht, Clemens and\nAlkiddeh, Oubada and\nSusanne Wei\u00df,\nLukas, Leon},\n  month = nov,\n  publisher = {it@M},\n  title = {LHM-Dienstleistungen-QA - german public domain question-answering dataset},\n  url = {https://huggingface.co/datasets/it-at-m/LHM-Dienstleistungen-QA},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#germanquad-retrieval","title":"GermanQuAD-Retrieval","text":"<p>Context Retrieval for German Question Answering</p> <p>Dataset: <code>mteb/germanquad-retrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) mrr_at_5 deu Non-fiction, Web, Written human-annotated found Citation <pre><code>@misc{m\u00f6ller2021germanquad,\n  archiveprefix = {arXiv},\n  author = {Timo M\u00f6ller and Julian Risch and Malte Pietsch},\n  eprint = {2104.12741},\n  primaryclass = {cs.CL},\n  title = {GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#govreport","title":"GovReport","text":"<p>A dataset for evaluating the ability of information retrieval models to retrieve lengthy US government reports from their summaries.</p> <p>Dataset: <code>isaacus/mteb-GovReport</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{huang-etal-2021-efficient,\n  address = {Online},\n  author = {Huang, Luyang  and\nCao, Shuyang  and\nParulian, Nikolaus  and\nJi, Heng  and\nWang, Lu},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.112},\n  eprint = {2104.02112},\n  month = jun,\n  pages = {1419--1436},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Attentions for Long Document Summarization},\n  url = {https://aclanthology.org/2021.naacl-main.112},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#greekcivicsqa","title":"GreekCivicsQA","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>ilsp/greek_civics_qa</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ell Academic, Written derived found"},{"location":"overview/available_tasks/retrieval/#greennodetablemarkdownretrieval","title":"GreenNodeTableMarkdownRetrieval","text":"<p>GreenNodeTable documents</p> <p>Dataset: <code>GreenNode/GreenNode-Table-Markdown-Retrieval-VN</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Financial, Non-fiction human-annotated found"},{"location":"overview/available_tasks/retrieval/#hc3financeretrieval","title":"HC3FinanceRetrieval","text":"<p>A financial retrieval task based on HC3 Finance dataset containing human vs AI-generated financial text detection. Each query is a financial question or prompt (e.g., 'Explain the impact of interest rate changes on bond prices'), and the corpus contains both human-written and AI-generated financial responses. The task is to retrieve the most relevant and accurate financial content that addresses the query. Queries are financial questions while the corpus contains detailed financial explanations, analysis, and educational content covering various financial concepts and market dynamics.</p> <p>Dataset: <code>embedding-benchmark/HC3Finance</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{guo2023hc3,\n  author = {Guo, Biyang and Zhang, Xin and Wang, Zhiyuan and Jiang, Mingyuan and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},\n  journal = {arXiv preprint arXiv:2301.07597},\n  title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hagridretrieval","title":"HagridRetrieval","text":"<p>HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)is a dataset for generative information-seeking scenarios. It consists of queriesalong with a set of manually labelled relevant passages</p> <p>Dataset: <code>mteb/HagridRetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written expert-annotated found Citation <pre><code>@article{hagrid,\n  author = {Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},\n  journal = {arXiv:2307.16883},\n  title = {{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hellaswag","title":"HellaSwag","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on HellaSwag.</p> <p>Dataset: <code>mteb/HellaSwag</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n\n@article{zellers2019hellaswag,\n  author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1905.07830},\n  title = {Hellaswag: Can a machine really finish your sentence?},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa","title":"HotpotQA","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>mteb/hotpotqa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-fa","title":"HotpotQA-Fa","text":"<p>HotpotQA-Fa</p> <p>Dataset: <code>MCINext/hotpotqa-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-fahardnegatives","title":"HotpotQA-FaHardNegatives","text":"<p>HotpotQA-FaHardNegatives</p> <p>Dataset: <code>MCINext/HotpotQA_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-nl","title":"HotpotQA-NL","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strongsupervision for supporting facts to enable more explainable question answering systems. HotpotQA-NL is a Dutch translation. </p> <p>Dataset: <code>clips/beir-nl-hotpotqa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Web, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-pl","title":"HotpotQA-PL","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>mteb/HotpotQA-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-plhardnegatives","title":"HotpotQA-PLHardNegatives","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/HotpotQA-PLHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-vn","title":"HotpotQA-VN","text":"<p>A translated dataset from HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong             supervision for supporting facts to enable more explainable question answering systems.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/hotpotqa-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqahardnegatives","title":"HotpotQAHardNegatives","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/HotpotQA_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqahardnegativesv2","title":"HotpotQAHardNegatives.v2","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/HotpotQA_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#humanevalretrieval","title":"HumanEvalRetrieval","text":"<p>A code retrieval task based on 164 Python programming problems from HumanEval. Each query is a natural language description of a programming task (e.g., 'Check if in given list of numbers, are any two numbers closer to each other than given threshold'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations with proper indentation and logic.</p> <p>Dataset: <code>embedding-benchmark/HumanEval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming derived found Citation <pre><code>@article{chen2021evaluating,\n  archiveprefix = {arXiv},\n  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Joshua and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},\n  eprint = {2107.03374},\n  primaryclass = {cs.LG},\n  title = {Evaluating Large Language Models Trained on Code},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hunsum2abstractiveretrieval","title":"HunSum2AbstractiveRetrieval","text":"<p>HunSum-2-abstractive is a Hungarian dataset containing news articles along with lead, titles and metadata.</p> <p>Dataset: <code>SZTAKI-HLT/HunSum-2-abstractive</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 hun News, Written derived found Citation <pre><code>@misc{barta2024news,\n  archiveprefix = {arXiv},\n  author = {Botond Barta and Dorina Lakatos and Attila Nagy and Mil\u00e1n Konor Nyist and Judit \u00c1cs},\n  eprint = {2404.03555},\n  primaryclass = {cs.CL},\n  title = {From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#indicqaretrieval","title":"IndicQARetrieval","text":"<p>IndicQA is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. It is repurposed retrieving relevant context for each question.</p> <p>Dataset: <code>mteb/IndicQARetrieval</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 asm, ben, guj, hin, kan, ... (11) Web, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jacwirretrieval","title":"JaCWIRRetrieval","text":"<p>JaCWIR is a small-scale Japanese information retrieval evaluation dataset consisting of 5000 question texts and approximately 500k web page titles and web page introductions or summaries (meta descriptions, etc.). The question texts are created based on one of the 500k web pages, and that data is used as a positive example for the question text.</p> <p>Dataset: <code>mteb/JaCWIRRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jagovfaqsretrieval","title":"JaGovFaqsRetrieval","text":"<p>JaGovFaqs is a dataset consisting of FAQs manually extracted from the website of Japanese bureaus. The dataset consists of 22k FAQs, where the queries (questions) and corpus (answers) have been shuffled, and the goal is to match the answer with the question.</p> <p>Dataset: <code>mteb/JaGovFaqsRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found"},{"location":"overview/available_tasks/retrieval/#jaquadretrieval","title":"JaQuADRetrieval","text":"<p>Human-annotated question-answer pairs for Japanese wikipedia pages.</p> <p>Dataset: <code>mteb/JaQuADRetrieval</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@misc{so2022jaquad,\n  archiveprefix = {arXiv},\n  author = {ByungHoon So and Kyuhong Byun and Kyungwon Kang and Seongjin Cho},\n  eprint = {2202.01764},\n  primaryclass = {cs.CL},\n  title = {{JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension}},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jaqketretrieval","title":"JaqketRetrieval","text":"<p>JAQKET (JApanese Questions on Knowledge of EnTities) is a QA dataset that is created based on quiz questions.</p> <p>Dataset: <code>mteb/jaqket</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{Kurihara_nlp2020,\n  author = {\u9234\u6728\u6b63\u654f and \u9234\u6728\u6f64 and \u677e\u7530\u8015\u53f2 and \u2ec4\u7530\u4eac\u4ecb and \u4e95\u4e4b\u4e0a\u76f4\u4e5f},\n  booktitle = {\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a\u7b2c26\u56de\u5e74\u6b21\u5927\u4f1a},\n  note = {in Japanese},\n  title = {JAQKET: \u30af\u30a4\u30b9\u3099\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c6\u3099\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9},\n  url = {https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P2-24.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ko-strategyqa","title":"Ko-StrategyQA","text":"<p>Ko-StrategyQA</p> <p>Dataset: <code>taeminlee/Ko-StrategyQA</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor not specified not specified not specified Citation <pre><code>@article{geva2021strategyqa,\n  author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},\n  journal = {Transactions of the Association for Computational Linguistics (TACL)},\n  title = {{Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembnarrativeqaretrieval","title":"LEMBNarrativeQARetrieval","text":"<p>narrativeqa subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction, Non-fiction, Written derived found Citation <pre><code>@article{kocisky-etal-2018-narrativeqa,\n  address = {Cambridge, MA},\n  author = {Ko{\\v{c}}isk{\\'y}, Tom{\\'a}{\\v{s}}  and\nSchwarz, Jonathan  and\nBlunsom, Phil  and\nDyer, Chris  and\nHermann, Karl Moritz  and\nMelis, G{\\'a}bor  and\nGrefenstette, Edward},\n  doi = {10.1162/tacl_a_00023},\n  editor = {Lee, Lillian  and\nJohnson, Mark  and\nToutanova, Kristina  and\nRoark, Brian},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {317--328},\n  publisher = {MIT Press},\n  title = {The {N}arrative{QA} Reading Comprehension Challenge},\n  url = {https://aclanthology.org/Q18-1023},\n  volume = {6},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembneedleretrieval","title":"LEMBNeedleRetrieval","text":"<p>needle subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 eng Academic, Blog, Written derived found Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembpasskeyretrieval","title":"LEMBPasskeyRetrieval","text":"<p>passkey subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 eng Fiction, Written derived found Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembqmsumretrieval","title":"LEMBQMSumRetrieval","text":"<p>qmsum subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Spoken, Written derived found Citation <pre><code>@inproceedings{zhong-etal-2021-qmsum,\n  address = {Online},\n  author = {Zhong, Ming  and\nYin, Da  and\nYu, Tao  and\nZaidi, Ahmad  and\nMutuma, Mutethia  and\nJha, Rahul  and\nAwadallah, Ahmed Hassan  and\nCelikyilmaz, Asli  and\nLiu, Yang  and\nQiu, Xipeng  and\nRadev, Dragomir},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.472},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {5905--5921},\n  publisher = {Association for Computational Linguistics},\n  title = {{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization},\n  url = {https://aclanthology.org/2021.naacl-main.472},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembsummscreenfdretrieval","title":"LEMBSummScreenFDRetrieval","text":"<p>summ_screen_fd subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Spoken, Written derived found Citation <pre><code>@inproceedings{chen-etal-2022-summscreen,\n  address = {Dublin, Ireland},\n  author = {Chen, Mingda  and\nChu, Zewei  and\nWiseman, Sam  and\nGimpel, Kevin},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2022.acl-long.589},\n  editor = {Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline},\n  month = may,\n  pages = {8602--8615},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization},\n  url = {https://aclanthology.org/2022.acl-long.589},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembwikimqaretrieval","title":"LEMBWikimQARetrieval","text":"<p>2wikimqa subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{ho2020constructing,\n  author = {Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  pages = {6609--6625},\n  title = {Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#limitretrieval","title":"LIMITRetrieval","text":"<p>A simple retrieval task designed to test all combinations of top-2 documents. This version includes all 50k docs.</p> <p>Dataset: <code>orionweller/LIMIT</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_2 eng Fiction human-annotated created Citation <pre><code>@misc{weller2025theoreticallimit,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Michael Boratko and Iftekhar Naim and Jinhyuk Lee},\n  eprint = {2508.21038},\n  primaryclass = {cs.IR},\n  title = {On the Theoretical Limitations of Embedding-Based Retrieval},\n  url = {https://arxiv.org/abs/2508.21038},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#limitsmallretrieval","title":"LIMITSmallRetrieval","text":"<p>A simple retrieval task designed to test all combinations of top-2 documents. This version only includes the 46 documents that are relevant to the 1000 queries.</p> <p>Dataset: <code>orionweller/LIMIT-small</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_2 eng Fiction human-annotated created Citation <pre><code>@misc{weller2025theoreticallimit,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Michael Boratko and Iftekhar Naim and Jinhyuk Lee},\n  eprint = {2508.21038},\n  primaryclass = {cs.IR},\n  title = {On the Theoretical Limitations of Embedding-Based Retrieval},\n  url = {https://arxiv.org/abs/2508.21038},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lecardv2","title":"LeCaRDv2","text":"<p>The task involves identifying and retrieving the case document that best matches or is most relevant to the scenario described in each of the provided queries.</p> <p>Dataset: <code>mteb/LeCaRDv2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 zho Legal, Written derived found Citation <pre><code>@misc{li2023lecardv2,\n  archiveprefix = {arXiv},\n  author = {Haitao Li and Yunqiu Shao and Yueyue Wu and Qingyao Ai and Yixiao Ma and Yiqun Liu},\n  eprint = {2310.17609},\n  primaryclass = {cs.CL},\n  title = {LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalbenchconsumercontractsqa","title":"LegalBenchConsumerContractsQA","text":"<p>The dataset includes questions and answers related to contracts.</p> <p>Dataset: <code>mteb/legalbench_consumer_contracts_qa</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalbenchcorporatelobbying","title":"LegalBenchCorporateLobbying","text":"<p>The dataset includes bill titles and bill summaries related to corporate lobbying.</p> <p>Dataset: <code>mteb/legalbench_corporate_lobbying</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n\n@article{holzenberger2021factoring,\n  author = {Holzenberger, Nils and Van Durme, Benjamin},\n  journal = {arXiv preprint arXiv:2105.07903},\n  title = {Factoring statutory reasoning as language understanding challenges},\n  year = {2021},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n\n@article{lippi2019claudette,\n  author = {Lippi, Marco and Pa{\\l}ka, Przemys{\\l}aw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},\n  journal = {Artificial Intelligence and Law},\n  pages = {117--139},\n  publisher = {Springer},\n  title = {CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service},\n  volume = {27},\n  year = {2019},\n}\n\n@article{ravichander2019question,\n  author = {Ravichander, Abhilasha and Black, Alan W and Wilson, Shomir and Norton, Thomas and Sadeh, Norman},\n  journal = {arXiv preprint arXiv:1911.00841},\n  title = {Question answering for privacy policies: Combining computational and legal perspectives},\n  year = {2019},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n\n@article{zimmeck2019maps,\n  author = {Zimmeck, Sebastian and Story, Peter and Smullen, Daniel and Ravichander, Abhilasha and Wang, Ziqi and Reidenberg, Joel R and Russell, N Cameron and Sadeh, Norman},\n  journal = {Proc. Priv. Enhancing Tech.},\n  pages = {66},\n  title = {Maps: Scaling privacy compliance analysis to a million apps},\n  volume = {2019},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalqanlretrieval","title":"LegalQANLRetrieval","text":"<p>To this end, we create and publish a Dutch legal QA dataset, consisting of question-answer pairs with attributions to Dutch law articles.</p> <p>Dataset: <code>clips/mteb-nl-legalqa-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Legal, Written expert-annotated found Citation <pre><code>@inproceedings{redelaar2024attributed,\n  author = {Redelaar, Felicia and Van Drie, Romy and Verberne, Suzan and De Boer, Maaike},\n  booktitle = {Proceedings of the natural legal language processing workshop 2024},\n  pages = {154--165},\n  title = {Attributed Question Answering for Preconditions in the Dutch Law},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalquad","title":"LegalQuAD","text":"<p>The dataset consists of questions and legal documents in German.</p> <p>Dataset: <code>mteb/LegalQuAD</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found Citation <pre><code>@inproceedings{9723721,\n  author = {Hoppe, Christoph and Pelkmann, David and Migenda, Nico and H\u00f6tte, Daniel and Schenck, Wolfram},\n  booktitle = {2021 IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)},\n  doi = {10.1109/AIKE52691.2021.00011},\n  keywords = {Knowledge engineering;Law;Semantic search;Conferences;Bit error rate;NLP;knowledge extraction;question-answering;semantic search;document retrieval;German language},\n  number = {},\n  pages = {29-32},\n  title = {Towards Intelligent Legal Advisors for Document Retrieval and Question-Answering in German Legal Documents},\n  volume = {},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalsummarization","title":"LegalSummarization","text":"<p>The dataset consists of 439 pairs of contracts and their summarizations from https://tldrlegal.com and https://tosdr.org/.</p> <p>Dataset: <code>mteb/legal_summarization</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@inproceedings{manor-li-2019-plain,\n  address = {Minneapolis, Minnesota},\n  author = {Manor, Laura  and\nLi, Junyi Jessy},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2019},\n  month = jun,\n  pages = {1--11},\n  publisher = {Association for Computational Linguistics},\n  title = {Plain {E}nglish Summarization of Contracts},\n  url = {https://www.aclweb.org/anthology/W19-2201},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#litsearchretrieval","title":"LitSearchRetrieval","text":"<pre><code>    The dataset contains the query set and retrieval corpus for the paper LitSearch: A Retrieval Benchmark for\n    Scientific Literature Search. It introduces LitSearch, a retrieval benchmark comprising 597 realistic literature\n    search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions\n    generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about\n    recently published papers, manually written by their authors. All LitSearch questions were manually examined or\n    edited by experts to ensure high quality.\n</code></pre> <p>Dataset: <code>princeton-nlp/LitSearch</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written LM-generated found Citation <pre><code>@article{ajith2024litsearch,\n  author = {Ajith, Anirudh and Xia, Mengzhou and Chevalier, Alexis and Goyal, Tanya and Chen, Danqi and Gao, Tianyu},\n  title = {LitSearch: A Retrieval Benchmark for Scientific Literature Search},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lotte","title":"LoTTE","text":"<p>LoTTE (Long-Tail Topic-stratified Evaluation for IR) is designed to evaluate retrieval models on underrepresented, long-tail topics. Unlike MSMARCO or BEIR, LoTTE features domain-specific queries and passages from StackExchange (covering writing, recreation, science, technology, and lifestyle), providing a challenging out-of-domain generalization benchmark.</p> <p>Dataset: <code>mteb/LoTTE</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) precision_at_5 eng Academic, Social, Web derived found Citation <pre><code>@inproceedings{santhanam-etal-2022-colbertv2,\n  address = {Seattle, United States},\n  author = {Santhanam, Keshav  and\nKhattab, Omar  and\nSaad-Falcon, Jon  and\nPotts, Christopher  and\nZaharia, Matei},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2022.naacl-main.272},\n  editor = {Carpuat, Marine  and\nde Marneffe, Marie-Catherine  and\nMeza Ruiz, Ivan Vladimir},\n  month = jul,\n  pages = {3715--3734},\n  publisher = {Association for Computational Linguistics},\n  title = {{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction},\n  url = {https://aclanthology.org/2022.naacl-main.272/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mbppretrieval","title":"MBPPRetrieval","text":"<p>A code retrieval task based on 378 Python programming problems from MBPP (Mostly Basic Python Programming). Each query is a natural language description of a programming task (e.g., 'Write a function to find the shared elements from the given two lists'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations with proper syntax and logic.</p> <p>Dataset: <code>embedding-benchmark/MBPP</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming expert-annotated found Citation <pre><code>@article{austin2021program,\n  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},\n  journal = {arXiv preprint arXiv:2108.07732},\n  title = {Program Synthesis with Large Language Models},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrieval","title":"MIRACLRetrieval","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages.</p> <p>Dataset: <code>mteb/MIRACLRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrievalhardnegatives","title":"MIRACLRetrievalHardNegatives","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MIRACLRetrievalHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrievalhardnegativesv2","title":"MIRACLRetrievalHardNegatives.v2","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/MIRACLRetrievalHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mkqaretrieval","title":"MKQARetrieval","text":"<p>Multilingual Knowledge Questions &amp; Answers (MKQA)contains 10,000 queries sampled from the Google Natural Questions dataset.         For each query we collect new passage-independent answers. These queries and answers are then human translated into 25 Non-English languages.</p> <p>Dataset: <code>mteb/MKQARetrieval</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, dan, deu, eng, fin, ... (26) Written human-annotated found Citation <pre><code>@misc{mkqa,\n  author = {Shayne Longpre and Yi Lu and Joachim Daiber},\n  title = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},\n  url = {https://arxiv.org/pdf/2007.15207.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mlqaretrieval","title":"MLQARetrieval","text":"<p>MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance.         MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic,         German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between         4 different languages on average.</p> <p>Dataset: <code>mteb/MLQARetrieval</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, deu, eng, hin, spa, ... (7) Encyclopaedic, Written human-annotated found Citation <pre><code>@article{lewis2019mlqa,\n  author = {Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  eid = {arXiv: 1910.07475},\n  journal = {arXiv preprint arXiv:1910.07475},\n  title = {MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mlquestions","title":"MLQuestions","text":"<p>MLQuestions is a domain adaptation dataset for the machine learning domainIt consists of ML questions along with passages from Wikipedia machine learning pages (https://en.wikipedia.org/wiki/Category:Machine_learning)</p> <p>Dataset: <code>McGill-NLP/mlquestions</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{kulshreshtha-etal-2021-back,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Kulshreshtha, Devang  and\nBelfer, Robert  and\nSerban, Iulian Vlad  and\nReddy, Siva},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  month = nov,\n  pages = {7064--7078},\n  publisher = {Association for Computational Linguistics},\n  title = {Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval},\n  url = {https://aclanthology.org/2021.emnlp-main.566},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mmarcoretrieval","title":"MMarcoRetrieval","text":"<p>MMarcoRetrieval</p> <p>Dataset: <code>mteb/MMarcoRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn not specified not specified not specified Citation <pre><code>@misc{xiao2024cpack,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco","title":"MSMARCO","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search</p> <p>Dataset: <code>mteb/msmarco</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-fa","title":"MSMARCO-Fa","text":"<p>MSMARCO-Fa</p> <p>Dataset: <code>MCINext/msmarco-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-fahardnegatives","title":"MSMARCO-FaHardNegatives","text":"<p>MSMARCO-FaHardNegatives</p> <p>Dataset: <code>MCINext/MSMARCO_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-pl","title":"MSMARCO-PL","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search</p> <p>Dataset: <code>mteb/MSMARCO-PL</code> \u2022 License: https://microsoft.github.io/msmarco/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-plhardnegatives","title":"MSMARCO-PLHardNegatives","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MSMARCO-PLHardNegatives</code> \u2022 License: https://microsoft.github.io/msmarco/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-vn","title":"MSMARCO-VN","text":"<p>A translated dataset from MS MARCO is a collection of datasets focused on deep learning in search             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/msmarco-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarcohardnegatives","title":"MSMARCOHardNegatives","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MSMARCO_test_top_250_only_w_correct-v2</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarcov2","title":"MSMARCOv2","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. This version is derived from BEIR</p> <p>Dataset: <code>mteb/msmarco-v2</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#medicalqaretrieval","title":"MedicalQARetrieval","text":"<p>The dataset consists 2048 medical question and answer pairs.</p> <p>Dataset: <code>mteb/medical_qa</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical, Written derived found Citation <pre><code>@article{BenAbacha-BMC-2019,\n  author = {Asma, Ben Abacha and Dina, Demner{-}Fushman},\n  journal = {{BMC} Bioinform.},\n  number = {1},\n  pages = {511:1--511:23},\n  title = {A Question-Entailment Approach to Question Answering},\n  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4},\n  volume = {20},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#medicalretrieval","title":"MedicalRetrieval","text":"<p>MedicalRetrieval</p> <p>Dataset: <code>mteb/MedicalRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn not specified not specified not specified Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mintakaretrieval","title":"MintakaRetrieval","text":"<p>We introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. </p> <p>Dataset: <code>mteb/MintakaRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, deu, fra, hin, ita, ... (8) Encyclopaedic, Written derived human-translated Citation <pre><code>@inproceedings{sen-etal-2022-mintaka,\n  address = {Gyeongju, Republic of Korea},\n  author = {Sen, Priyanka  and\nAji, Alham Fikri  and\nSaffari, Amir},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {1604--1619},\n  publisher = {International Committee on Computational Linguistics},\n  title = {Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering},\n  url = {https://aclanthology.org/2022.coling-1.138},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mrtidyretrieval","title":"MrTidyRetrieval","text":"<p>Mr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations.</p> <p>Dataset: <code>mteb/mrtidy</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, eng, fin, ind, ... (11) Encyclopaedic, Written human-annotated found Citation <pre><code>@article{mrtydi,\n  author = {Xinyu Zhang and Xueguang Ma and Peng Shi and Jimmy Lin},\n  journal = {arXiv:2108.08787},\n  title = {{Mr. TyDi}: A Multi-lingual Benchmark for Dense Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#multilongdocretrieval","title":"MultiLongDocRetrieval","text":"<p>Multi Long Doc Retrieval (MLDR) 'is curated by the multilingual articles from Wikipedia, Wudao and mC4 (see Table 7), and NarrativeQA (Koc\u02c7isky \u0301 et al., 2018; Gu \u0308nther et al., 2023), which is only for English.' (Chen et al., 2024).         It is constructed by sampling lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the dataset.</p> <p>Dataset: <code>mteb/MultiLongDocRetrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, cmn, deu, eng, fra, ... (13) Encyclopaedic, Fiction, Non-fiction, Web, Written LM-generated found Citation <pre><code>@misc{bge-m3,\n  archiveprefix = {arXiv},\n  author = {Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n  eprint = {2402.03216},\n  primaryclass = {cs.CL},\n  title = {BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus","title":"NFCorpus","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/nfcorpus</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written not specified not specified Citation <pre><code>@inproceedings{boteva2016,\n  author = {Boteva, Vera and Gholipour, Demian and Sokolov, Artem and Riezler, Stefan},\n  city = {Padova},\n  country = {Italy},\n  journal = {Proceedings of the 38th European Conference on Information Retrieval},\n  journal-abbrev = {ECIR},\n  title = {A Full-Text Learning to Rank Dataset for Medical Information Retrieval},\n  url = {http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-fa","title":"NFCorpus-Fa","text":"<p>NFCorpus-Fa</p> <p>Dataset: <code>MCINext/nfcorpus-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-nl","title":"NFCorpus-NL","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. NFCorpus-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-nfcorpus</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-pl","title":"NFCorpus-PL","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/NFCorpus-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified not specified Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-vn","title":"NFCorpus-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nfcorpus-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsarticleretrieval","title":"NLPJournalAbsArticleRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsArticleRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsarticleretrievalv2","title":"NLPJournalAbsArticleRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsArticleRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsintroretrieval","title":"NLPJournalAbsIntroRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V1 dataset (last update 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsIntroRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsintroretrievalv2","title":"NLPJournalAbsIntroRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V2 dataset (last update 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsIntroRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleabsretrieval","title":"NLPJournalTitleAbsRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleAbsRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleabsretrievalv2","title":"NLPJournalTitleAbsRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleAbsRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleintroretrieval","title":"NLPJournalTitleIntroRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleIntroRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleintroretrievalv2","title":"NLPJournalTitleIntroRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleIntroRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq","title":"NQ","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/nq</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-fa","title":"NQ-Fa","text":"<p>NQ-Fa</p> <p>Dataset: <code>MCINext/nq-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-fahardnegatives","title":"NQ-FaHardNegatives","text":"<p>NQ-FaHardNegatives</p> <p>Dataset: <code>MCINext/NQ_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-nl","title":"NQ-NL","text":"<p>NQ-NL is a translation of NQ</p> <p>Dataset: <code>clips/beir-nl-nq</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-pl","title":"NQ-PL","text":"<p>Natural Questions: A Benchmark for Question Answering Research</p> <p>Dataset: <code>mteb/NQ-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-plhardnegatives","title":"NQ-PLHardNegatives","text":"<p>Natural Questions: A Benchmark for Question Answering Research. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NQ-PLHardNegatives</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-vn","title":"NQ-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nq-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nqhardnegatives","title":"NQHardNegatives","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NQ_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng not specified not specified not specified Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoarguanaretrieval","title":"NanoArguAnaRetrieval","text":"<p>NanoArguAna is a smaller subset of ArguAna, a dataset for argument retrieval in debate contexts.</p> <p>Dataset: <code>zeta-alpha-ai/NanoArguAna</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social, Web, Written derived found Citation <pre><code>@inproceedings{wachsmuth2018retrieval,\n  author = {Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},\n  booktitle = {ACL},\n  title = {Retrieval of the Best Counterargument without Prior Topic Knowledge},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoclimatefeverretrieval","title":"NanoClimateFeverRetrieval","text":"<p>NanoClimateFever is a small version of the BEIR dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change.</p> <p>Dataset: <code>zeta-alpha-ai/NanoClimateFEVER</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, News, Non-fiction expert-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanodbpediaretrieval","title":"NanoDBPediaRetrieval","text":"<p>NanoDBPediaRetrieval is a small version of the standard test collection for entity search over the DBpedia knowledge base.</p> <p>Dataset: <code>zeta-alpha-ai/NanoDBPedia</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic expert-annotated found Citation <pre><code>@article{lehmann2015dbpedia,\n  author = {Lehmann, Jens and et al.},\n  journal = {Semantic Web},\n  title = {DBpedia: A large-scale, multilingual knowledge base extracted from Wikipedia},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanofeverretrieval","title":"NanoFEVERRetrieval","text":"<p>NanoFEVER is a smaller version of FEVER (Fact Extraction and VERification), which consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.</p> <p>Dataset: <code>zeta-alpha-ai/NanoFEVER</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Encyclopaedic expert-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanofiqa2018retrieval","title":"NanoFiQA2018Retrieval","text":"<p>NanoFiQA2018 is a smaller subset of the Financial Opinion Mining and Question Answering dataset.</p> <p>Dataset: <code>zeta-alpha-ai/NanoFiQA2018</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Social human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanohotpotqaretrieval","title":"NanoHotpotQARetrieval","text":"<p>NanoHotpotQARetrieval is a smaller subset of the HotpotQA dataset, which is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>zeta-alpha-ai/NanoHotpotQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanomsmarcoretrieval","title":"NanoMSMARCORetrieval","text":"<p>NanoMSMARCORetrieval is a smaller subset of MS MARCO, a collection of datasets focused on deep learning in search.</p> <p>Dataset: <code>zeta-alpha-ai/NanoMSMARCO</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web human-annotated found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanonfcorpusretrieval","title":"NanoNFCorpusRetrieval","text":"<p>NanoNFCorpus is a smaller subset of NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval.</p> <p>Dataset: <code>zeta-alpha-ai/NanoNFCorpus</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@inproceedings{boteva2016,\n  author = {Boteva, Vera and Gholipour, Demian and Sokolov, Artem and Riezler, Stefan},\n  city = {Padova},\n  country = {Italy},\n  journal = {Proceedings of the 38th European Conference on Information Retrieval},\n  journal-abbrev = {ECIR},\n  title = {A Full-Text Learning to Rank Dataset for Medical Information Retrieval},\n  url = {http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanonqretrieval","title":"NanoNQRetrieval","text":"<p>NanoNQ is a smaller subset of a dataset which contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.</p> <p>Dataset: <code>zeta-alpha-ai/NanoNQ</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Web human-annotated found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoquoraretrieval","title":"NanoQuoraRetrieval","text":"<p>NanoQuoraRetrieval is a smaller subset of the QuoraRetrieval dataset, which is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>zeta-alpha-ai/NanoQuoraRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoscidocsretrieval","title":"NanoSCIDOCSRetrieval","text":"<p>NanoFiQA2018 is a smaller subset of SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>zeta-alpha-ai/NanoSCIDOCS</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written expert-annotated found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoscifactretrieval","title":"NanoSciFactRetrieval","text":"<p>NanoSciFact is a smaller subset of SciFact, which verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>zeta-alpha-ai/NanoSciFact</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanotouche2020retrieval","title":"NanoTouche2020Retrieval","text":"<p>NanoTouche2020 is a smaller subset of Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions.</p> <p>Dataset: <code>zeta-alpha-ai/NanoTouche2020</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@dataset{potthast_2022_6862281,\n  author = {Potthast, Martin and\nGienapp, Lukas and\nWachsmuth, Henning and\nHagen, Matthias and\nFr\u00f6be, Maik and\nBondarenko, Alexander and\nAjjour, Yamen and\nStein, Benno},\n  doi = {10.5281/zenodo.6862281},\n  month = jul,\n  publisher = {Zenodo},\n  title = {{Touch\u00e920-Argument-Retrieval-for-Controversial-\nQuestions}},\n  url = {https://doi.org/10.5281/zenodo.6862281},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#narrativeqaretrieval","title":"NarrativeQARetrieval","text":"<p>NarrativeQA is a dataset for the task of question answering on long narratives. It consists of realistic QA instances collected from literature (fiction and non-fiction) and movie scripts. </p> <p>Dataset: <code>deepmind/narrativeqa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng not specified not specified not specified Citation <pre><code>@misc{ko\u010disk\u00fd2017narrativeqa,\n  archiveprefix = {arXiv},\n  author = {Tom\u00e1\u0161 Ko\u010disk\u00fd and Jonathan Schwarz and Phil Blunsom and Chris Dyer and Karl Moritz Hermann and G\u00e1bor Melis and Edward Grefenstette},\n  eprint = {1712.07040},\n  primaryclass = {cs.CL},\n  title = {The NarrativeQA Reading Comprehension Challenge},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2022retrieval","title":"NeuCLIR2022Retrieval","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries.</p> <p>Dataset: <code>mteb/NeuCLIR2022Retrieval</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{lawrie2023overview,\n  author = {Lawrie, Dawn and MacAvaney, Sean and Mayfield, James and McNamee, Paul and Oard, Douglas W and Soldaini, Luca and Yang, Eugene},\n  journal = {arXiv preprint arXiv:2304.12367},\n  title = {Overview of the TREC 2022 NeuCLIR track},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2022retrievalhardnegatives","title":"NeuCLIR2022RetrievalHardNegatives","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NeuCLIR2022RetrievalHardNegatives</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{lawrie2023overview,\n  author = {Lawrie, Dawn and MacAvaney, Sean and Mayfield, James and McNamee, Paul and Oard, Douglas W and Soldaini, Luca and Yang, Eugene},\n  journal = {arXiv preprint arXiv:2304.12367},\n  title = {Overview of the TREC 2022 NeuCLIR track},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2023retrieval","title":"NeuCLIR2023Retrieval","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries.</p> <p>Dataset: <code>mteb/NeuCLIR2022Retrieval</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@misc{lawrie2024overview,\n  archiveprefix = {arXiv},\n  author = {Dawn Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},\n  eprint = {2404.08071},\n  primaryclass = {cs.IR},\n  title = {Overview of the TREC 2023 NeuCLIR Track},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2023retrievalhardnegatives","title":"NeuCLIR2023RetrievalHardNegatives","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NeuCLIR2023RetrievalHardNegatives</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@misc{lawrie2024overview,\n  archiveprefix = {arXiv},\n  author = {Dawn Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},\n  eprint = {2404.08071},\n  primaryclass = {cs.IR},\n  title = {Overview of the TREC 2023 NeuCLIR Track},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#norquadretrieval","title":"NorQuadRetrieval","text":"<p>Human-created question for Norwegian wikipedia passages.</p> <p>Dataset: <code>mteb/norquad_retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{ivanova-etal-2023-norquad,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Ivanova, Sardana  and\nAndreassen, Fredrik  and\nJentoft, Matias  and\nWold, Sondre  and\n{\\O}vrelid, Lilja},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {159--168},\n  publisher = {University of Tartu Library},\n  title = {{N}or{Q}u{AD}: {N}orwegian Question Answering Dataset},\n  url = {https://aclanthology.org/2023.nodalida-1.17},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#opentenderretrieval","title":"OpenTenderRetrieval","text":"<p>This dataset contains Belgian and Dutch tender calls from OpenTender in Dutch</p> <p>Dataset: <code>clips/mteb-nl-opentender-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#piqa","title":"PIQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on PIQA.</p> <p>Dataset: <code>mteb/PIQA</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bisk2020piqa,\n  author = {Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},\n  booktitle = {Proceedings of the AAAI conference on artificial intelligence},\n  number = {05},\n  pages = {7432--7439},\n  title = {Piqa: Reasoning about physical commonsense in natural language},\n  volume = {34},\n  year = {2020},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#puggretrieval","title":"PUGGRetrieval","text":"<p>Information Retrieval PUGG dataset for the Polish language.</p> <p>Dataset: <code>clarin-pl/PUGG_IR</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web human-annotated multiple Citation <pre><code>@inproceedings{sawczyn-etal-2024-developing,\n  address = {Bangkok, Thailand},\n  author = {Sawczyn, Albert  and\nViarenich, Katsiaryna  and\nWojtasik, Konrad  and\nDomoga{\\l}a, Aleksandra  and\nOleksy, Marcin  and\nPiasecki, Maciej  and\nKajdanowicz, Tomasz},\n  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},\n  doi = {10.18653/v1/2024.findings-acl.652},\n  editor = {Ku, Lun-Wei  and\nMartins, Andre  and\nSrikumar, Vivek},\n  month = aug,\n  pages = {10978--10996},\n  publisher = {Association for Computational Linguistics},\n  title = {Developing {PUGG} for {P}olish: A Modern Approach to {KBQA}, {MRC}, and {IR} Dataset Construction},\n  url = {https://aclanthology.org/2024.findings-acl.652/},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#persianwebdocumentretrieval","title":"PersianWebDocumentRetrieval","text":"<p>Persian dataset designed specifically for the task of text information retrieval through the web.</p> <p>Dataset: <code>MCINext/persian-web-document-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found"},{"location":"overview/available_tasks/retrieval/#publichealthqa","title":"PublicHealthQA","text":"<p>A multilingual dataset for public health question answering, based on FAQ sourced from CDC and WHO.</p> <p>Dataset: <code>xhluca/publichealth-qa</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, eng, fra, kor, rus, ... (8) Government, Medical, Web, Written derived found Citation <pre><code>@misc{xing_han_lu_2024,\n  author = { {Xing Han Lu} },\n  doi = { 10.57967/hf/2247 },\n  publisher = { Hugging Face },\n  title = { publichealth-qa (Revision 3b67b6b) },\n  url = { https://huggingface.co/datasets/xhluca/publichealth-qa },\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quail","title":"Quail","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on Quail.</p> <p>Dataset: <code>mteb/Quail</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{rogers2020getting,\n  author = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},\n  booktitle = {Proceedings of the AAAI conference on artificial intelligence},\n  number = {05},\n  pages = {8722--8731},\n  title = {Getting closer to AI complete question answering: A set of prerequisite real tasks},\n  volume = {34},\n  year = {2020},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-nl","title":"Quora-NL","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. QuoraRetrieval-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-quora</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-pl","title":"Quora-PL","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>mteb/Quora-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-plhardnegatives","title":"Quora-PLHardNegatives","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/Quora-PLHardNegatives</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-vn","title":"Quora-VN","text":"<p>A translated dataset from QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a             question, find other (duplicate) questions.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/quora-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Blog, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval","title":"QuoraRetrieval","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>mteb/quora</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval-fa","title":"QuoraRetrieval-Fa","text":"<p>QuoraRetrieval-Fa</p> <p>Dataset: <code>MCINext/quora-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval-fav2","title":"QuoraRetrieval-Fa.v2","text":"<p>QuoraRetrieval-Fa.v2</p> <p>Dataset: <code>MCINext/quora-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrievalhardnegatives","title":"QuoraRetrievalHardNegatives","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/QuoraRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrievalhardnegativesv2","title":"QuoraRetrievalHardNegatives.v2","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/QuoraRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medbioinformaticsretrieval","title":"R2MEDBioinformaticsRetrieval","text":"<p>Bioinformatics retrieval dataset.</p> <p>Dataset: <code>R2MED/Bioinformatics</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medbiologyretrieval","title":"R2MEDBiologyRetrieval","text":"<p>Biology retrieval dataset.</p> <p>Dataset: <code>R2MED/Biology</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2mediiyiclinicalretrieval","title":"R2MEDIIYiClinicalRetrieval","text":"<p>IIYi-Clinical retrieval dataset.</p> <p>Dataset: <code>R2MED/IIYi-Clinical</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedqadiagretrieval","title":"R2MEDMedQADiagRetrieval","text":"<p>MedQA-Diag retrieval dataset.</p> <p>Dataset: <code>R2MED/MedQA-Diag</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedxpertqaexamretrieval","title":"R2MEDMedXpertQAExamRetrieval","text":"<p>MedXpertQA-Exam retrieval dataset.</p> <p>Dataset: <code>R2MED/MedXpertQA-Exam</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedicalsciencesretrieval","title":"R2MEDMedicalSciencesRetrieval","text":"<p>Medical-Sciences retrieval dataset.</p> <p>Dataset: <code>R2MED/Medical-Sciences</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medpmcclinicalretrieval","title":"R2MEDPMCClinicalRetrieval","text":"<p>PMC-Clinical retrieval dataset.</p> <p>Dataset: <code>R2MED/PMC-Clinical</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medpmctreatmentretrieval","title":"R2MEDPMCTreatmentRetrieval","text":"<p>PMC-Treatment retrieval dataset.</p> <p>Dataset: <code>R2MED/PMC-Treatment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rarbcode","title":"RARbCode","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on RAR-b code-pooled dataset.</p> <p>Dataset: <code>mteb/RARbCode</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {Codesearchnet challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n\n@article{muennighoff2023octopack,\n  author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne},\n  journal = {arXiv preprint arXiv:2308.07124},\n  title = {Octopack: Instruction tuning code large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rarbmath","title":"RARbMath","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on RAR-b math-pooled dataset.</p> <p>Dataset: <code>mteb/RARbMath</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{cobbe2021training,\n  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},\n  journal = {arXiv preprint arXiv:2110.14168},\n  title = {Training verifiers to solve math word problems},\n  year = {2021},\n}\n\n@article{hendrycks2021measuring,\n  author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},\n  journal = {arXiv preprint arXiv:2103.03874},\n  title = {Measuring mathematical problem solving with the math dataset},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n\n@article{yu2023metamath,\n  author = {Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},\n  journal = {arXiv preprint arXiv:2309.12284},\n  title = {Metamath: Bootstrap your own mathematical questions for large language models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrieval","title":"RiaNewsRetrieval","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset.</p> <p>Dataset: <code>ai-forever/ria-news-retrieval</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrievalhardnegatives","title":"RiaNewsRetrievalHardNegatives","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrievalhardnegativesv2","title":"RiaNewsRetrievalHardNegatives.v2","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rubqretrieval","title":"RuBQRetrieval","text":"<p>Paragraph retrieval based on RuBQ 2.0. Retrieve paragraphs from Wikipedia that answer the question.</p> <p>Dataset: <code>ai-forever/rubq-retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{RuBQ2021,\n  author = {Ivan Rybin and Vladislav Korablinov and Pavel Efimov and Pavel Braslavski},\n  booktitle = {ESWC},\n  pages = {532--547},\n  title = {RuBQ 2.0: An Innovated Russian Question Answering Dataset},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ruscibenchciteretrieval","title":"RuSciBenchCiteRetrieval","text":"<p>This task is focused on Direct Citation Prediction for scientific papers from eLibrary,         Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),         the goal is to retrieve papers that are directly cited by it from a larger corpus of papers.         The dataset for this task consists of 3,000 query papers, 15,000 relevant (cited) papers,         and 75,000 irrelevant papers. The task is available for both Russian and English scientific texts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_cite_retrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ruscibenchcociteretrieval","title":"RuSciBenchCociteRetrieval","text":"<p>This task focuses on Co-citation Prediction for scientific papers from eLibrary,         Russia's largest electronic library of scientific publications. Given a query paper (title and abstract),         the goal is to retrieve other papers that are co-cited with it. Two papers are considered co-cited         if they are both cited by at least 5 of the same other papers. Similar to the Direct Citation task,         this task employs a retrieval setup: for a given query paper, all other papers in the corpus that         are not co-cited with it are considered negative examples. The task is available for both Russian         and English scientific texts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_cocite_retrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs","title":"SCIDOCS","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>mteb/scidocs</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written not specified found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-fa","title":"SCIDOCS-Fa","text":"<p>SCIDOCS-Fa</p> <p>Dataset: <code>MCINext/scidocs-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-fav2","title":"SCIDOCS-Fa.v2","text":"<p>SCIDOCS-Fa.v2</p> <p>Dataset: <code>MCINext/scidocs-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-nl","title":"SCIDOCS-NL","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. SciDocs-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-scidocs</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-pl","title":"SCIDOCS-PL","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>mteb/SCIDOCS-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol not specified not specified not specified Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-vn","title":"SCIDOCS-VN","text":"<p>A translated dataset from SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation             prediction, to document classification and recommendation.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/scidocs-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#siqa","title":"SIQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on SIQA.</p> <p>Dataset: <code>mteb/SIQA</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{sap2019socialiqa,\n  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1904.09728},\n  title = {Socialiqa: Commonsense reasoning about social interactions},\n  year = {2019},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#skquadretrieval","title":"SKQuadRetrieval","text":"<p>Retrieval SK Quad evaluates Slovak search performance using questions and answers derived from the SK-QuAD dataset. It measures relevance with scores assigned to answers based on their relevancy to corresponding questions, which is vital for improving Slovak language search systems.</p> <p>Dataset: <code>TUKE-KEMT/retrieval-skquad</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 slk Encyclopaedic human-annotated found"},{"location":"overview/available_tasks/retrieval/#snlretrieval","title":"SNLRetrieval","text":"<p>Webscrabed articles and ingresses from the Norwegian lexicon 'Det Store Norske Leksikon'.</p> <p>Dataset: <code>adrlau/navjordj-SNL_summarization_copy</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#sadeemquestionretrieval","title":"SadeemQuestionRetrieval","text":"<p>SadeemQuestion: A Benchmark Data Set for Community Question-Retrieval Research</p> <p>Dataset: <code>sadeem-ai/sadeem-ar-eval-retrieval-questions</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara Written, Written derived found Citation <pre><code>@inproceedings{sadeem-2024-ar-retrieval-questions,\n  author = {abubakr.soliman@sadeem.app},\n  title = {SadeemQuestionRetrieval: A New Benchmark for Arabic questions-based Articles Searching.},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact","title":"SciFact","text":"<p>SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>mteb/scifact</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written not specified not specified Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-fa","title":"SciFact-Fa","text":"<p>SciFact-Fa</p> <p>Dataset: <code>MCINext/scifact-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-fav2","title":"SciFact-Fa.v2","text":"<p>SciFact-Fa.v2</p> <p>Dataset: <code>MCINext/scifact-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-nl","title":"SciFact-NL","text":"<p>SciFactNL verifies scientific claims in Dutch using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>clips/beir-nl-scifact</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-pl","title":"SciFact-PL","text":"<p>SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>mteb/SciFact-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Medical, Written not specified not specified Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-vn","title":"SciFact-VN","text":"<p>A translated dataset from SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/scifact-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#slovaksumretrieval","title":"SlovakSumRetrieval","text":"<pre><code>        SlovakSum, a Slovak news summarization dataset consisting of over 200 thousand\n        news articles with titles and short abstracts obtained from multiple Slovak newspapers.\n\n        Originally intended as a summarization task, but since no human annotations were provided\n        here reformulated to a retrieval task.\n</code></pre> <p>Dataset: <code>NaiveNeuron/slovaksum</code> \u2022 License: openrail \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 slk News, Social, Web, Written derived found Citation <pre><code>@inproceedings{OndrejowaSlovakSum24,\n  author = {Ondrejov\u00e1, Vikt\u00f3ria and \u0160uppa, Marek},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},\n  date = {2024},\n  title = {SlovakSum: A Large Scale Slovak Summarization Dataset},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spanishpassageretrievals2p","title":"SpanishPassageRetrievalS2P","text":"<p>Test collection for passage retrieval from health-related Web resources in Spanish.</p> <p>Dataset: <code>mteb/SpanishPassageRetrievalS2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 spa not specified not specified not specified Citation <pre><code>@inproceedings{10.1007/978-3-030-15719-7_19,\n  address = {Cham},\n  author = {Kamateri, Eleni\nand Tsikrika, Theodora\nand Symeonidis, Spyridon\nand Vrochidis, Stefanos\nand Minker, Wolfgang\nand Kompatsiaris, Yiannis},\n  booktitle = {Advances in Information Retrieval},\n  editor = {Azzopardi, Leif\nand Stein, Benno\nand Fuhr, Norbert\nand Mayr, Philipp\nand Hauff, Claudia\nand Hiemstra, Djoerd},\n  isbn = {978-3-030-15719-7},\n  pages = {148--154},\n  publisher = {Springer International Publishing},\n  title = {A Test Collection for Passage Retrieval Evaluation of Spanish Health-Related Resources},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spanishpassageretrievals2s","title":"SpanishPassageRetrievalS2S","text":"<p>Test collection for passage retrieval from health-related Web resources in Spanish.</p> <p>Dataset: <code>mteb/SpanishPassageRetrievalS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 spa not specified not specified not specified Citation <pre><code>@inproceedings{10.1007/978-3-030-15719-7_19,\n  address = {Cham},\n  author = {Kamateri, Eleni\nand Tsikrika, Theodora\nand Symeonidis, Spyridon\nand Vrochidis, Stefanos\nand Minker, Wolfgang\nand Kompatsiaris, Yiannis},\n  booktitle = {Advances in Information Retrieval},\n  editor = {Azzopardi, Leif\nand Stein, Benno\nand Fuhr, Norbert\nand Mayr, Philipp\nand Hauff, Claudia\nand Hiemstra, Djoerd},\n  isbn = {978-3-030-15719-7},\n  pages = {148--154},\n  publisher = {Springer International Publishing},\n  title = {A Test Collection for Passage Retrieval Evaluation of Spanish Health-Related Resources},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spartqa","title":"SpartQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on SpartQA.</p> <p>Dataset: <code>mteb/SpartQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{mirzaee2021spartqa,\n  author = {Mirzaee, Roshanak and Faghihi, Hossein Rajaby and Ning, Qiang and Kordjmashidi, Parisa},\n  journal = {arXiv preprint arXiv:2104.05832},\n  title = {Spartqa:: A textual question answering benchmark for spatial reasoning},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#stackoverflowqa","title":"StackOverflowQA","text":"<p>The dataset is a collection of natural language queries and their corresponding response which may include some text mixed with code snippets. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/stackoverflow-qa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#statcandialoguedatasetretrieval","title":"StatcanDialogueDatasetRetrieval","text":"<p>A Dataset for Retrieving Data Tables through Conversations with Genuine Intents, available in English and French.</p> <p>Dataset: <code>McGill-NLP/statcan-dialogue-dataset-retrieval</code> \u2022 License: https://huggingface.co/datasets/McGill-NLP/statcan-dialogue-dataset-retrieval/blob/main/LICENSE.md \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, fra Government, Web, Written derived found Citation <pre><code>@inproceedings{lu-etal-2023-statcan,\n  address = {Dubrovnik, Croatia},\n  author = {Lu, Xing Han  and\nReddy, Siva  and\nde Vries, Harm},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {2799--2829},\n  publisher = {Association for Computational Linguistics},\n  title = {The {S}tat{C}an Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents},\n  url = {https://arxiv.org/abs/2304.01412},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#swefaqretrieval","title":"SweFaqRetrieval","text":"<p>A Swedish QA dataset derived from FAQ</p> <p>Dataset: <code>mteb/SweFaqRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 swe Government, Non-fiction, Written derived found Citation <pre><code>@inproceedings{berdivcevskis2023superlim,\n  author = {Berdi{\\v{c}}evskis, Aleksandrs and Bouma, Gerlof and Kurtz, Robin and Morger, Felix and {\\\"O}hman, Joey and Adesam, Yvonne and Borin, Lars and Dann{\\'e}lls, Dana and Forsberg, Markus and Isbister, Tim and others},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {8137--8153},\n  title = {Superlim: A Swedish language understanding evaluation benchmark},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#swednretrieval","title":"SwednRetrieval","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure</p> <p>Dataset: <code>mteb/SwednRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synperchatbotragfaqretrieval","title":"SynPerChatbotRAGFAQRetrieval","text":"<p>Synthetic Persian Chatbot RAG FAQ Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-faq-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#synperchatbotragtopicsretrieval","title":"SynPerChatbotRAGTopicsRetrieval","text":"<p>Synthetic Persian Chatbot RAG Topics Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-topics-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#synperchatbottopicsretrieval","title":"SynPerChatbotTopicsRetrieval","text":"<p>Synthetic Persian Chatbot Topics Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-topics-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#synperqaretrieval","title":"SynPerQARetrieval","text":"<p>Synthetic Persian QA Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-qa-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web LM-generated LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#syntecretrieval","title":"SyntecRetrieval","text":"<p>This dataset has been built from the Syntec Collective bargaining agreement.</p> <p>Dataset: <code>lyon-nlp/mteb-fr-retrieval-syntec-s2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Legal, Written human-annotated created Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synthetictext2sql","title":"SyntheticText2SQL","text":"<p>The dataset is a collection of natural language queries and their corresponding sql snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/synthetic-text2sql</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, sql Programming, Written derived found Citation <pre><code>@software{gretel-synthetic-text-to-sql-2024,\n  author = {Meyer, Yev and Emadi, Marjan and Nathawani, Dhruv and Ramaswamy, Lipika and Boyd, Kendrick and Van Segbroeck, Maarten and Grossman, Matthew and Mlocek, Piotr and Newberry, Drew},\n  month = {April},\n  title = {{Synthetic-Text-To-SQL}: A synthetic dataset for training language models to generate SQL queries from natural language prompts},\n  url = {https://huggingface.co/datasets/gretelai/synthetic-text-to-sql},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#t2retrieval","title":"T2Retrieval","text":"<p>T2Ranking: A large-scale Chinese Benchmark for Passage Ranking</p> <p>Dataset: <code>mteb/T2Retrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Academic, Financial, Government, Medical, Non-fiction human-annotated not specified Citation <pre><code>@misc{xie2023t2ranking,\n  archiveprefix = {arXiv},\n  author = {Xiaohui Xie and Qian Dong and Bingning Wang and Feiyang Lv and Ting Yao and Weinan Gan and Zhijing Wu and Xiangsheng Li and Haitao Li and Yiqun Liu and Jin Ma},\n  eprint = {2304.03679},\n  primaryclass = {cs.IR},\n  title = {T2Ranking: A large-scale Chinese Benchmark for Passage Ranking},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid","title":"TRECCOVID","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic.</p> <p>Dataset: <code>mteb/trec-covid</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written not specified not specified Citation <pre><code>@misc{roberts2021searching,\n  archiveprefix = {arXiv},\n  author = {Kirk Roberts and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and Kyle Lo and Ian Soboroff and Ellen Voorhees and Lucy Lu Wang and William R Hersh},\n  eprint = {2104.09632},\n  primaryclass = {cs.IR},\n  title = {Searching for Scientific Evidence in a Pandemic: An Overview of TREC-COVID},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-fa","title":"TRECCOVID-Fa","text":"<p>TRECCOVID-Fa</p> <p>Dataset: <code>MCINext/trec-covid-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-fav2","title":"TRECCOVID-Fa.v2","text":"<p>TRECCOVID-Fa.v2</p> <p>Dataset: <code>MCINext/trec-covid-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-nl","title":"TRECCOVID-NL","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic. TRECCOVID-NL is a Dutch translation. </p> <p>Dataset: <code>clips/beir-nl-trec-covid</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-pl","title":"TRECCOVID-PL","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic.</p> <p>Dataset: <code>mteb/TRECCOVID-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Medical, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-vn","title":"TRECCOVID-VN","text":"<p>A translated dataset from TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/trec-covid-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#trecdl2019","title":"TRECDL2019","text":"<p>TREC Deep Learning Track 2019 passage ranking task. The task involves retrieving relevant passages from the MS MARCO collection given web search queries. Queries have multi-graded relevance judgments.</p> <p>Dataset: <code>whybe-choi/trec-dl-2019</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@inproceedings{craswell2020overview,\n  author = {Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},\n  booktitle = {Proceedings of the 28th Text REtrieval Conference (TREC 2019)},\n  organization = {NIST},\n  title = {Overview of the TREC 2019 deep learning track},\n  year = {2020},\n}\n\n@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#trecdl2020","title":"TRECDL2020","text":"<p>TREC Deep Learning Track 2020 passage ranking task. The task involves retrieving relevant passages from the MS MARCO collection given web search queries. Queries have multi-graded relevance judgments.</p> <p>Dataset: <code>whybe-choi/trec-dl-2020</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@inproceedings{craswell2021overview,\n  author = {Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},\n  booktitle = {Proceedings of the 29th Text REtrieval Conference (TREC 2020)},\n  organization = {NIST},\n  title = {Overview of the TREC 2020 deep learning track},\n  year = {2021},\n}\n\n@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tv2nordretrieval","title":"TV2Nordretrieval","text":"<p>News Article and corresponding summaries extracted from the Danish newspaper TV2 Nord.</p> <p>Dataset: <code>alexandrainst/nordjylland-news-summarization</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl1","title":"TempReasonL1","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l1.</p> <p>Dataset: <code>mteb/TempReasonL1</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2context","title":"TempReasonL2Context","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-context.</p> <p>Dataset: <code>mteb/TempReasonL2Context</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2fact","title":"TempReasonL2Fact","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-fact.</p> <p>Dataset: <code>mteb/TempReasonL2Fact</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2pure","title":"TempReasonL2Pure","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-pure.</p> <p>Dataset: <code>mteb/TempReasonL2Pure</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3context","title":"TempReasonL3Context","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-context.</p> <p>Dataset: <code>mteb/TempReasonL3Context</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3fact","title":"TempReasonL3Fact","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-fact.</p> <p>Dataset: <code>mteb/TempReasonL3Fact</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3pure","title":"TempReasonL3Pure","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-pure.</p> <p>Dataset: <code>mteb/TempReasonL3Pure</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#topiocqa","title":"TopiOCQA","text":"<p>TopiOCQA (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) is information-seeking conversational dataset with challenging topic switching phenomena. It consists of conversation histories along with manually labelled relevant/gold passage.</p> <p>Dataset: <code>mteb/TopiOCQA</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{adlakha2022topiocqa,\n  archiveprefix = {arXiv},\n  author = {Vaibhav Adlakha and Shehzaad Dhuliawala and Kaheer Suleman and Harm de Vries and Siva Reddy},\n  eprint = {2110.00768},\n  primaryclass = {cs.CL},\n  title = {TopiOCQA: Open-domain Conversational Question Answering with Topic Switching},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#topiocqahardnegatives","title":"TopiOCQAHardNegatives","text":"<p>TopiOCQA (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) is information-seeking conversational dataset with challenging topic switching phenomena. It consists of conversation histories along with manually labelled relevant/gold passage. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/TopiOCQA_validation_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{adlakha2022topiocqa,\n  archiveprefix = {arXiv},\n  author = {Vaibhav Adlakha and Shehzaad Dhuliawala and Kaheer Suleman and Harm de Vries and Siva Reddy},\n  eprint = {2110.00768},\n  primaryclass = {cs.CL},\n  title = {TopiOCQA: Open-domain Conversational Question Answering with Topic Switching},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020","title":"Touche2020","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/touche2020</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@dataset{potthast_2022_6862281,\n  author = {Potthast, Martin and\nGienapp, Lukas and\nWachsmuth, Henning and\nHagen, Matthias and\nFr\u00f6be, Maik and\nBondarenko, Alexander and\nAjjour, Yamen and\nStein, Benno},\n  doi = {10.5281/zenodo.6862281},\n  month = jul,\n  publisher = {Zenodo},\n  title = {{Touch\u00e920-Argument-Retrieval-for-Controversial-\nQuestions}},\n  url = {https://doi.org/10.5281/zenodo.6862281},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-fa","title":"Touche2020-Fa","text":"<p>Touche2020-Fa</p> <p>Dataset: <code>MCINext/touche2020-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-fav2","title":"Touche2020-Fa.v2","text":"<p>Touche2020-Fa.v2</p> <p>Dataset: <code>MCINext/webis-touche2020-v3-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-nl","title":"Touche2020-NL","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions. Touche2020-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-webis-touche2020</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Non-fiction derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-pl","title":"Touche2020-PL","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/Touche2020-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-vn","title":"Touche2020-VN","text":"<p>A translated dataset from Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/webis-touche2020-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020retrievalv3","title":"Touche2020Retrieval.v3","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/webis-touche2020-v3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@inproceedings{Thakur_etal_SIGIR2024,\n  address_ = {Washington, D.C.},\n  author = {Nandan Thakur and Luiz Bonifacio and Maik {Fr\\\"{o}be} and Alexander Bondarenko and Ehsan Kamalloo and Martin Potthast and Matthias Hagen and Jimmy Lin},\n  booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  title = {Systematic Evaluation of Neural Retrieval Models on the {Touch\\'{e}} 2020 Argument Retrieval Subset of {BEIR}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#turhistquadretrieval","title":"TurHistQuadRetrieval","text":"<p>Question Answering dataset on Ottoman History in Turkish</p> <p>Dataset: <code>asparius/TurHistQuAD</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 tur Academic, Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{9559013,\n  author = {Soygazi, Fatih and \u00c7ift\u00e7i, Okan and K\u00f6k, U\u011furcan and Cengiz, Soner},\n  booktitle = {2021 6th International Conference on Computer Science and Engineering (UBMK)},\n  doi = {10.1109/UBMK52708.2021.9559013},\n  keywords = {Computer science;Computational modeling;Neural networks;Knowledge discovery;Information retrieval;Natural language processing;History;question answering;information retrieval;natural language understanding;deep learning;contextualized word embeddings},\n  number = {},\n  pages = {215-220},\n  title = {THQuAD: Turkish Historic Question Answering Dataset for Reading Comprehension},\n  volume = {},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#twitterhjerneretrieval","title":"TwitterHjerneRetrieval","text":"<p>Danish question asked on Twitter with the Hashtag #Twitterhjerne ('Twitter brain') and their corresponding answer.</p> <p>Dataset: <code>sorenmulli/da-hashtag-twitterhjerne</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Social, Written derived found Citation <pre><code>@article{holm2024gllms,\n  author = {Holm, Soren Vejlgaard},\n  title = {Are GLLMs Danoliterate? Benchmarking Generative NLP in Danish},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#vabbretrieval","title":"VABBRetrieval","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#vdrmultilingualretrieval","title":"VDRMultilingualRetrieval","text":"<p>Multilingual Visual Document retrieval Dataset covering 5 languages: Italian, Spanish, English, French and German</p> <p>Dataset: <code>llamaindex/vdr-multilingual-test</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, ita, spa Web LM-generated found Citation <pre><code>@misc{llamaindex2024vdrmultilingual,\n  author = {LlamaIndex},\n  howpublished = {https://huggingface.co/datasets/llamaindex/vdr-multilingual-test},\n  title = {Visual Document Retrieval Goes Multilingual},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#videoretrieval","title":"VideoRetrieval","text":"<p>VideoRetrieval</p> <p>Dataset: <code>mteb/VideoRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn not specified not specified not specified Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#viequadretrieval","title":"VieQuADRetrieval","text":"<p>A Vietnamese dataset for evaluating Machine Reading Comprehension from Wikipedia articles.</p> <p>Dataset: <code>taidng/UIT-ViQuAD2.0</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{nguyen-etal-2020-vietnamese,\n  address = {Barcelona, Spain (Online)},\n  author = {Nguyen, Kiet  and\nNguyen, Vu  and\nNguyen, Anh  and\nNguyen, Ngan},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.233},\n  editor = {Scott, Donia  and\nBel, Nuria  and\nZong, Chengqing},\n  month = dec,\n  pages = {2595--2605},\n  publisher = {International Committee on Computational Linguistics},\n  title = {A Vietnamese Dataset for Evaluating Machine Reading Comprehension},\n  url = {https://aclanthology.org/2020.coling-main.233},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#webfaqretrieval","title":"WebFAQRetrieval","text":"<p>WebFAQ is a broad-coverage corpus of natural question-answer pairs in 75 languages, gathered from FAQ pages on the web.</p> <p>Dataset: <code>mteb/WebFAQRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, aze, ben, bul, cat, ... (51) Web, Written derived found Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#wikisqlretrieval","title":"WikiSQLRetrieval","text":"<p>A code retrieval task based on WikiSQL dataset with natural language questions and corresponding SQL queries. Each query is a natural language question (e.g., 'What is the name of the team that has scored the most goals?'), and the corpus contains SQL query implementations. The task is to retrieve the correct SQL query that answers the natural language question. Queries are natural language questions while the corpus contains SQL SELECT statements with proper syntax and logic for querying database tables.</p> <p>Dataset: <code>embedding-benchmark/WikiSQL_mteb</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, sql Programming expert-annotated found Citation <pre><code>@article{zhong2017seq2sql,\n  archiveprefix = {arXiv},\n  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},\n  eprint = {1709.00103},\n  primaryclass = {cs.CL},\n  title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#wikipediaretrievalmultilingual","title":"WikipediaRetrievalMultilingual","text":"<p>The dataset is derived from Cohere's wikipedia-2023-11 dataset and contains synthetically generated queries.</p> <p>Dataset: <code>mteb/WikipediaRetrievalMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ben, bul, ces, dan, deu, ... (16) Encyclopaedic, Written LM-generated and reviewed LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#winogrande","title":"WinoGrande","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on winogrande.</p> <p>Dataset: <code>mteb/WinoGrande</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{sakaguchi2021winogrande,\n  author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},\n  journal = {Communications of the ACM},\n  number = {9},\n  pages = {99--106},\n  publisher = {ACM New York, NY, USA},\n  title = {Winogrande: An adversarial winograd schema challenge at scale},\n  volume = {64},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xmarket","title":"XMarket","text":"<p>XMarket</p> <p>Dataset: <code>mteb/XMarket</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu, eng, spa not specified not specified not specified Citation <pre><code>@inproceedings{Bonab_2021,\n  author = {Bonab, Hamed and Aliannejadi, Mohammad and Vardasbi, Ali and Kanoulas, Evangelos and Allan, James},\n  booktitle = {Proceedings of the 30th ACM International Conference on Information &amp;amp; Knowledge Management},\n  collection = {CIKM \u201921},\n  doi = {10.1145/3459637.3482493},\n  month = oct,\n  publisher = {ACM},\n  series = {CIKM \u201921},\n  title = {Cross-Market Product Recommendation},\n  url = {http://dx.doi.org/10.1145/3459637.3482493},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xpqaretrieval","title":"XPQARetrieval","text":"<p>XPQARetrieval</p> <p>Dataset: <code>mteb/XPQARetrieval</code> \u2022 License: cdla-sharing-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, cmn, deu, eng, fra, ... (13) Reviews, Written human-annotated found Citation <pre><code>@inproceedings{shen2023xpqa,\n  author = {Shen, Xiaoyu and Asai, Akari and Byrne, Bill and De Gispert, Adria},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},\n  pages = {103--115},\n  title = {xPQA: Cross-Lingual Product Question Answering in 12 Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xquadretrieval","title":"XQuADRetrieval","text":"<p>XQuAD is a benchmark dataset for evaluating cross-lingual question answering performance. It is repurposed retrieving relevant context for each question.</p> <p>Dataset: <code>google/xquad</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 arb, deu, ell, eng, hin, ... (12) Web, Written human-annotated created Citation <pre><code>@article{Artetxe:etal:2019,\n  archiveprefix = {arXiv},\n  author = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\n  eprint = {1910.11856},\n  journal = {CoRR},\n  title = {On the cross-lingual transferability of monolingual representations},\n  volume = {abs/1910.11856},\n  year = {2019},\n}\n\n@inproceedings{dumitrescu2021liro,\n  author = {Stefan Daniel Dumitrescu and Petru Rebeja and Beata Lorincz and Mihaela Gaman and Andrei Avram and Mihai Ilie and Andrei Pruteanu and Adriana Stan and Lorena Rosia and Cristina Iacobescu and Luciana Morogan and George Dima and Gabriel Marchidan and Traian Rebedea and Madalina Chitez and Dani Yogatama and Sebastian Ruder and Radu Tudor Ionescu and Razvan Pascanu and Viorica Patraucean},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\n  title = {LiRo: Benchmark and leaderboard for Romanian language tasks},\n  url = {https://openreview.net/forum?id=JH61CD7afTv},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#zaclegaltextretrieval","title":"ZacLegalTextRetrieval","text":"<p>Zalo Legal Text documents</p> <p>Dataset: <code>GreenNode/zalo-ai-legal-text-retrieval-vn</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Legal human-annotated found"},{"location":"overview/available_tasks/retrieval/#bbsardnlretrieval","title":"bBSARDNLRetrieval","text":"<p>Building on the Belgian Statutory Article Retrieval Dataset (BSARD) in French, we introduce the bilingual version of this dataset, bBSARD. The dataset contains parallel Belgian statutory articles in both French and Dutch, along with legal questions from BSARD and their Dutch translation.</p> <p>Dataset: <code>clips/mteb-nl-bbsard</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Legal, Written expert-annotated found Citation <pre><code>@article{lotfi2025bilingual,\n  author = {Lotfi, Ehsan and Banar, Nikolay and Yuzbashyan, Nerses and Daelemans, Walter},\n  journal = {COLING 2025},\n  pages = {10},\n  title = {Bilingual BSARD: Extending Statutory Article Retrieval to Dutch},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mmarco-nl","title":"mMARCO-NL","text":"<p>mMARCO is a multi-lingual (translated) collection of datasets focused on deep learning in search</p> <p>Dataset: <code>clips/beir-nl-mmarco</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Web, Written derived machine-translated and verified Citation <pre><code>@article{DBLP:journals/corr/abs-2108-13897,\n  author = {Luiz Bonifacio and\nIsrael Campiotti and\nRoberto de Alencar Lotufo and\nRodrigo Frassetto Nogueira},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},\n  eprint = {2108.13897},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 20 Mar 2023 15:35:34 +0100},\n  title = {mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset},\n  url = {https://arxiv.org/abs/2108.13897},\n  volume = {abs/2108.13897},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/","title":"STS","text":"<ul> <li>Number of tasks: 48</li> </ul>"},{"location":"overview/available_tasks/sts/#afqmc","title":"AFQMC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/AFQMC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@inproceedings{raghu-etal-2021-end,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Raghu, Dinesh  and\nAgarwal, Shantanu  and\nJoshi, Sachindra  and\n{Mausam}},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.357},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {4348--4366},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs},\n  url = {https://aclanthology.org/2021.emnlp-main.357},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#atec","title":"ATEC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/ATEC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@inproceedings{raghu-etal-2021-end,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Raghu, Dinesh  and\nAgarwal, Shantanu  and\nJoshi, Sachindra  and\n{Mausam}},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.357},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {4348--4366},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs},\n  url = {https://aclanthology.org/2021.emnlp-main.357},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#assin2sts","title":"Assin2STS","text":"<p>Semantic Textual Similarity part of the ASSIN 2, an evaluation shared task collocated with STIL 2019.</p> <p>Dataset: <code>nilc-nlp/assin2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman por Written human-annotated found Citation <pre><code>@inproceedings{real2020assin,\n  author = {Real, Livy and Fonseca, Erick and Oliveira, Hugo Goncalo},\n  booktitle = {International Conference on Computational Processing of the Portuguese Language},\n  organization = {Springer},\n  pages = {406--412},\n  title = {The assin 2 shared task: a quick overview},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#biosses","title":"BIOSSES","text":"<p>Biomedical Semantic Similarity Estimation.</p> <p>Dataset: <code>mteb/biosses-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Medical derived found Citation <pre><code>@article{10.1093/bioinformatics/btx238,\n  author = {So\u011fanc\u0131o\u011flu, Gizem and \u00d6zt\u00fcrk, Hakime and \u00d6zg\u00fcr, Arzucan},\n  doi = {10.1093/bioinformatics/btx238},\n  eprint = {https://academic.oup.com/bioinformatics/article-pdf/33/14/i49/50315066/bioinformatics\\_33\\_14\\_i49.pdf},\n  issn = {1367-4803},\n  journal = {Bioinformatics},\n  month = {07},\n  number = {14},\n  pages = {i49-i58},\n  title = {{BIOSSES: a semantic sentence similarity estimation system for the biomedical domain}},\n  url = {https://doi.org/10.1093/bioinformatics/btx238},\n  volume = {33},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#biosses-vn","title":"BIOSSES-VN","text":"<p>A translated dataset from Biomedical Semantic Similarity Estimation.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/biosses-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Medical derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#bq","title":"BQ","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/BQ</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#cdsc-r","title":"CDSC-R","text":"<p>Compositional Distributional Semantics Corpus for textual relatedness.</p> <p>Dataset: <code>PL-MTEB/cdscr-sts</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman pol Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{wroblewska-krasnowska-kieras-2017-polish,\n  address = {Vancouver, Canada},\n  author = {Wr{\\'o}blewska, Alina  and\nKrasnowska-Kiera{\\'s}, Katarzyna},\n  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/P17-1073},\n  editor = {Barzilay, Regina  and\nKan, Min-Yen},\n  month = jul,\n  pages = {784--792},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}olish evaluation dataset for compositional distributional semantics models},\n  url = {https://aclanthology.org/P17-1073},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#faroesests","title":"FaroeseSTS","text":"<p>Semantic Text Similarity (STS) corpus for Faroese.</p> <p>Dataset: <code>vesteinn/faroese-sts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fao News, Web, Written human-annotated found Citation <pre><code>@inproceedings{snaebjarnarson-etal-2023-transfer,\n  address = {T\u00f3rshavn, Faroe Islands},\n  author = {Sn\u00e6bjarnarson, V\u00e9steinn  and\nSimonsen, Annika  and\nGlava\u0161, Goran  and\nVuli\u0107, Ivan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = {may 22--24},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{T}ransfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#farsick","title":"Farsick","text":"<p>A Persian Semantic Textual Similarity And Natural Language Inference Dataset</p> <p>Dataset: <code>MCINext/farsick-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/sts/#finparasts","title":"FinParaSTS","text":"<p>Finnish paraphrase-based semantic similarity corpus</p> <p>Dataset: <code>mteb/FinParaSTS</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fin News, Subtitles, Written expert-annotated found Citation <pre><code>@inproceedings{kanerva-etal-2021-finnish,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kanerva, Jenna  and\nGinter, Filip  and\nChang, Li-Hsin  and\nRastas, Iiro  and\nSkantsi, Valtteri  and\nKilpel{\\\"a}inen, Jemina  and\nKupari, Hanna-Mari  and\nSaarni, Jenna  and\nSev{\\'o}n, Maija  and\nTarkka, Otto},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {288--298},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{F}innish Paraphrase Corpus},\n  url = {https://aclanthology.org/2021.nodalida-main.29},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#germanstsbenchmark","title":"GermanSTSBenchmark","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset translated into German. Translations were originally done by T-Systems on site services GmbH.</p> <p>Dataset: <code>jinaai/german-STSbenchmark</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman deu not specified not specified not specified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humesick-r","title":"HUMESICK-R","text":"<p>Human evaluation subset of Semantic Textual Similarity SICK-R dataset</p> <p>Dataset: <code>mteb/mteb-human-sickr-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Web, Written human-annotated not specified Citation <pre><code>@inproceedings{marelli-etal-2014-sick,\n  address = {Reykjavik, Iceland},\n  author = {Marelli, Marco  and\nMenini, Stefano  and\nBaroni, Marco  and\nBentivogli, Luisa  and\nBernardi, Raffaella  and\nZamparelli, Roberto},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {216--223},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {A {SICK} cure for the evaluation of compositional distributional semantic models},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humests12","title":"HUMESTS12","text":"<p>Human evaluation subset of SemEval-2012 Task 6.</p> <p>Dataset: <code>mteb/mteb-human-sts12-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Encyclopaedic, News, Written human-annotated created Citation <pre><code>@inproceedings{10.5555/2387636.2387697,\n  address = {USA},\n  author = {Agirre, Eneko and Diab, Mona and Cer, Daniel and Gonzalez-Agirre, Aitor},\n  booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},\n  location = {Montr\\'{e}al, Canada},\n  numpages = {9},\n  pages = {385\u2013393},\n  publisher = {Association for Computational Linguistics},\n  series = {SemEval '12},\n  title = {SemEval-2012 task 6: a pilot on semantic textual similarity},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humests22","title":"HUMESTS22","text":"<p>Human evaluation subset of SemEval 2022 Task 8: Multilingual News Article Similarity</p> <p>Dataset: <code>mteb/mteb-human-sts22-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, eng, fra, rus News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humestsbenchmark","title":"HUMESTSBenchmark","text":"<p>Human evaluation subset of Semantic Textual Similarity Benchmark (STSbenchmark) dataset.</p> <p>Dataset: <code>mteb/mteb-human-stsbenchmark-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#indiccrosslingualsts","title":"IndicCrosslingualSTS","text":"<p>This is a Semantic Textual Similarity testset between English and 12 high-resource Indic languages.</p> <p>Dataset: <code>mteb/IndicCrosslingualSTS</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman asm, ben, eng, guj, hin, ... (13) Government, News, Non-fiction, Spoken, Spoken, ... (7) expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00452,\n  author = {Ramesh, Gowtham and Doddapaneni, Sumanth and Bheemaraj, Aravinth and Jobanputra, Mayank and AK, Raghavan and Sharma, Ajitesh and Sahoo, Sujit and Diddee, Harshita and J, Mahalakshmi and Kakwani, Divyanshu and Kumar, Navneet and Pradeep, Aswin and Nagaraj, Srihari and Deepak, Kumar and Raghavan, Vivek and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh Shantadevi},\n  doi = {10.1162/tacl_a_00452},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\\\_a\\\\_00452/1987010/tacl\\\\_a\\\\_00452.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {02},\n  pages = {145-162},\n  title = {{Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages}},\n  url = {https://doi.org/10.1162/tacl\\\\_a\\\\_00452},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#jsick","title":"JSICK","text":"<p>JSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.</p> <p>Dataset: <code>mteb/JSICK</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman jpn Web, Written human-annotated found Citation <pre><code>@article{yanaka2022compositional,\n  author = {Yanaka, Hitomi and Mineshima, Koji},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {1266--1284},\n  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~\u2026},\n  title = {Compositional Evaluation on Japanese Textual Entailment and Similarity},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#jsts","title":"JSTS","text":"<p>Japanese Semantic Textual Similarity Benchmark dataset construct from YJ Image Captions Dataset (Miyazaki and Shimizu, 2016) and annotated by crowdsource annotators.</p> <p>Dataset: <code>mteb/JSTS</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman jpn Web, Written human-annotated found Citation <pre><code>@inproceedings{kurihara-etal-2022-jglue,\n  address = {Marseille, France},\n  author = {Kurihara, Kentaro  and\nKawahara, Daisuke  and\nShibata, Tomohide},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2957--2966},\n  publisher = {European Language Resources Association},\n  title = {{JGLUE}: {J}apanese General Language Understanding Evaluation},\n  url = {https://aclanthology.org/2022.lrec-1.317},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#klue-sts","title":"KLUE-STS","text":"<p>Human-annotated STS dataset of Korean reviews, news, and spoken word sets. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman kor News, Reviews, Spoken, Spoken, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#korsts","title":"KorSTS","text":"<p>Benchmark dataset for STS in Korean. Created by machine translation and human post editing of the STS-B dataset.</p> <p>Dataset: <code>dkoterwa/kor-sts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman kor News, Web not specified machine-translated and localized Citation <pre><code>@article{ham2020kornli,\n  author = {Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},\n  journal = {arXiv preprint arXiv:2004.03289},\n  title = {KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#lcqmc","title":"LCQMC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/LCQMC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#pawsx","title":"PAWSX","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>mteb/PAWSX</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#qbqtc","title":"QBQTC","text":"<p>Dataset: <code>C-MTEB/QBQTC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified"},{"location":"overview/available_tasks/sts/#query2query","title":"Query2Query","text":"<p>Query to Query Datasets.</p> <p>Dataset: <code>MCINext/query-to-query-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/sts/#ruparaphrasersts","title":"RUParaPhraserSTS","text":"<p>ParaPhraser is a news headlines corpus with precise, near and non-paraphrases.</p> <p>Dataset: <code>merionum/ru_paraphraser</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman rus News, Written human-annotated found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n\n@inproceedings{pivovarova2017paraphraser,\n  author = {Pivovarova, Lidia and Pronoza, Ekaterina and Yagunova, Elena and Pronoza, Anton},\n  booktitle = {Conference on artificial intelligence and natural language},\n  organization = {Springer},\n  pages = {211--225},\n  title = {ParaPhraser: Russian paraphrase corpus and shared task},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#ronsts","title":"RonSTS","text":"<p>High-quality Romanian translation of STSBenchmark.</p> <p>Dataset: <code>mteb/RonSTS</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ron News, Social, Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{dumitrescu2021liro,\n  author = {Dumitrescu, Stefan Daniel and Rebeja, Petru and Lorincz, Beata and Gaman, Mihaela and Avram, Andrei and Ilie, Mihai and Pruteanu, Andrei and Stan, Adriana and Rosia, Lorena and Iacobescu, Cristina and others},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\n  title = {LiRo: Benchmark and leaderboard for Romanian language tasks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#rustsbenchmarksts","title":"RuSTSBenchmarkSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset translated into Russian and verified. The dataset was checked with RuCOLA model to ensure that the translation is good and filtered.</p> <p>Dataset: <code>ai-forever/ru-stsbenchmark-sts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman rus News, Social, Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-br-sts","title":"SICK-BR-STS","text":"<p>SICK-BR is a Portuguese inference corpus, human translated from SICK</p> <p>Dataset: <code>eduagarcia/sick-br</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman por Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{real18,\n  author = {Real, Livy\nand Rodrigues, Ana\nand Vieira e Silva, Andressa\nand Albiero, Beatriz\nand Thalenberg, Bruna\nand Guide, Bruno\nand Silva, Cindy\nand de Oliveira Lima, Guilherme\nand Camara, Igor C. S.\nand Stanojevi{\\'{c}}, Milo{\\v{s}}\nand Souza, Rodrigo\nand de Paiva, Valeria},\n  booktitle = {{Computational Processing of the Portuguese Language. PROPOR 2018.}},\n  doi = {10.1007/978-3-319-99722-3_31},\n  isbn = {978-3-319-99722-3},\n  title = {{SICK-BR: A Portuguese Corpus for Inference}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-nl-sts","title":"SICK-NL-STS","text":"<p>SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of (Marelli et al., 2014) from English into Dutch.</p> <p>Dataset: <code>clips/mteb-nl-sick-sts-pr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman nld News, Social, Spoken, Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{wijnholds2021sick,\n  author = {Wijnholds, Gijs and Moortgat, Michael},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  pages = {1474--1479},\n  title = {SICK-NL: A Dataset for Dutch Natural Language Inference},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r","title":"SICK-R","text":"<p>Semantic Textual Similarity SICK-R dataset</p> <p>Dataset: <code>mteb/sickr-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Web, Written human-annotated not specified Citation <pre><code>@inproceedings{marelli-etal-2014-sick,\n  address = {Reykjavik, Iceland},\n  author = {Marelli, Marco  and\nMenini, Stefano  and\nBaroni, Marco  and\nBentivogli, Luisa  and\nBernardi, Raffaella  and\nZamparelli, Roberto},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {216--223},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {A {SICK} cure for the evaluation of compositional distributional semantic models},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r-pl","title":"SICK-R-PL","text":"<p>Polish version of SICK dataset for textual relatedness.</p> <p>Dataset: <code>PL-MTEB/sickr-pl-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman pol Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPerelkiewicz, Michal  and\nPoswiata, Rafal},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, Helene  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r-vn","title":"SICK-R-VN","text":"<p>A translated dataset from Semantic Textual Similarity SICK-R dataset as described here:             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/sickr-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sickfr","title":"SICKFr","text":"<p>SICK dataset french version</p> <p>Dataset: <code>Lajavaness/SICK-fr</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra not specified not specified not specified"},{"location":"overview/available_tasks/sts/#sts12","title":"STS12","text":"<p>SemEval-2012 Task 6.</p> <p>Dataset: <code>mteb/sts12-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Encyclopaedic, News, Written human-annotated created Citation <pre><code>@inproceedings{10.5555/2387636.2387697,\n  address = {USA},\n  author = {Agirre, Eneko and Diab, Mona and Cer, Daniel and Gonzalez-Agirre, Aitor},\n  booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},\n  location = {Montr\\'{e}al, Canada},\n  numpages = {9},\n  pages = {385\u2013393},\n  publisher = {Association for Computational Linguistics},\n  series = {SemEval '12},\n  title = {SemEval-2012 task 6: a pilot on semantic textual similarity},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts13","title":"STS13","text":"<p>SemEval STS 2013 dataset.</p> <p>Dataset: <code>mteb/sts13-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Non-fiction, Web, Written human-annotated created Citation <pre><code>@inproceedings{Agirre2013SEM2S,\n  author = {Eneko Agirre and Daniel Matthew Cer and Mona T. Diab and Aitor Gonzalez-Agirre and Weiwei Guo},\n  booktitle = {International Workshop on Semantic Evaluation},\n  title = {*SEM 2013 shared task: Semantic Textual Similarity},\n  url = {https://api.semanticscholar.org/CorpusID:10241043},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts14","title":"STS14","text":"<p>SemEval STS 2014 dataset. Currently only the English dataset</p> <p>Dataset: <code>mteb/sts14-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, Spoken, Web derived created Citation <pre><code>@inproceedings{bandhakavi-etal-2014-generating,\n  address = {Dublin, Ireland},\n  author = {Bandhakavi, Anil  and\nWiratunga, Nirmalie  and\nP, Deepak  and\nMassie, Stewart},\n  booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014)},\n  doi = {10.3115/v1/S14-1002},\n  editor = {Bos, Johan  and\nFrank, Anette  and\nNavigli, Roberto},\n  month = aug,\n  pages = {12--21},\n  publisher = {Association for Computational Linguistics and Dublin City University},\n  title = {Generating a Word-Emotion Lexicon from {\\#}Emotional Tweets},\n  url = {https://aclanthology.org/S14-1002},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts15","title":"STS15","text":"<p>SemEval STS 2015 dataset</p> <p>Dataset: <code>mteb/sts15-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Spoken, Web, Written human-annotated created Citation <pre><code>@inproceedings{bicici-2015-rtm,\n  address = {Denver, Colorado},\n  author = {Bi{\\c{c}}ici, Ergun},\n  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)},\n  doi = {10.18653/v1/S15-2010},\n  editor = {Nakov, Preslav  and\nZesch, Torsten  and\nCer, Daniel  and\nJurgens, David},\n  month = jun,\n  pages = {56--63},\n  publisher = {Association for Computational Linguistics},\n  title = {{RTM}-{DCU}: Predicting Semantic Similarity with Referential Translation Machines},\n  url = {https://aclanthology.org/S15-2010},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts16","title":"STS16","text":"<p>SemEval-2016 Task 4</p> <p>Dataset: <code>mteb/sts16-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, Spoken, Web human-annotated created Citation <pre><code>@inproceedings{nakov-etal-2016-semeval,\n  address = {San Diego, California},\n  author = {Nakov, Preslav  and\nRitter, Alan  and\nRosenthal, Sara  and\nSebastiani, Fabrizio  and\nStoyanov, Veselin},\n  booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)},\n  doi = {10.18653/v1/S16-1001},\n  editor = {Bethard, Steven  and\nCarpuat, Marine  and\nCer, Daniel  and\nJurgens, David  and\nNakov, Preslav  and\nZesch, Torsten},\n  month = jun,\n  pages = {1--18},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2016 Task 4: Sentiment Analysis in {T}witter},\n  url = {https://aclanthology.org/S16-1001},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts17","title":"STS17","text":"<p>Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</p> <p>Dataset: <code>mteb/sts17-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, deu, eng, fra, ita, ... (9) News, Web, Written human-annotated created Citation <pre><code>@inproceedings{cer-etal-2017-semeval,\n  address = {Vancouver, Canada},\n  author = {Cer, Daniel  and\nDiab, Mona  and\nAgirre, Eneko  and\nLopez-Gazpio, I{\\\\~n}igo  and\nSpecia, Lucia},\n  booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)},\n  doi = {10.18653/v1/S17-2001},\n  editor = {Bethard, Steven  and\nCarpuat, Marine  and\nApidianaki, Marianna  and\nMohammad, Saif M.  and\nCer, Daniel  and\nJurgens, David},\n  month = aug,\n  pages = {1--14},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},\n  url = {https://aclanthology.org/S17-2001},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts22","title":"STS22","text":"<p>SemEval 2022 Task 8: Multilingual News Article Similarity</p> <p>Dataset: <code>mteb/sts22-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, cmn, deu, eng, fra, ... (10) News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts22v2","title":"STS22.v2","text":"<p>SemEval 2022 Task 8: Multilingual News Article Similarity. Version 2 filters updated on STS22 by removing pairs where one of entries contain empty sentences.</p> <p>Dataset: <code>mteb/sts22-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, cmn, deu, eng, fra, ... (10) News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsb","title":"STSB","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/STSB</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn not specified not specified not specified Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmark","title":"STSBenchmark","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset.</p> <p>Dataset: <code>mteb/stsbenchmark-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmark-vn","title":"STSBenchmark-VN","text":"<p>A translated dataset from Semantic Textual Similarity Benchmark (STSbenchmark) dataset.             The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system:             - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation.             - Applies advanced embedding models to filter the translations.             - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stsbenchmark-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Blog, News, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmarkmultilingualsts","title":"STSBenchmarkMultilingualSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset, but translated using DeepL API.</p> <p>Dataset: <code>mteb/stsb_multi_mt</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn, deu, eng, fra, ita, ... (10) News, Social, Spoken, Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stses","title":"STSES","text":"<p>Spanish test sets from SemEval-2014 (Agirre et al., 2014) and SemEval-2015 (Agirre et al., 2015)</p> <p>Dataset: <code>mteb/STSES</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman spa Written not specified not specified Citation <pre><code>@inproceedings{agirre2014semeval,\n  author = {Agirre, Eneko and Banea, Carmen and Cardie, Claire and Cer, Daniel M and Diab, Mona T and Gonzalez-Agirre, Aitor and Guo, Weiwei and Mihalcea, Rada and Rigau, German and Wiebe, Janyce},\n  booktitle = {SemEval@ COLING},\n  pages = {81--91},\n  title = {SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.},\n  year = {2014},\n}\n\n@inproceedings{agirre2015semeval,\n  author = {Agirre, Eneko and Banea, Carmen and Cardie, Claire and Cer, Daniel and Diab, Mona and Gonzalez-Agirre, Aitor and Guo, Weiwei and Lopez-Gazpio, Inigo and Maritxalar, Montse and Mihalcea, Rada and others},\n  booktitle = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},\n  pages = {252--263},\n  title = {Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#semrel24sts","title":"SemRel24STS","text":"<p>SemRel2024 is a collection of Semantic Textual Relatedness (STR) datasets for 14 languages, including African and Asian languages. The datasets are composed of sentence pairs, each assigned a relatedness score between 0 (completely) unrelated and 1 (maximally related) with a large range of expected relatedness values.</p> <p>Dataset: <code>SemRel/SemRel2024</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman afr, amh, arb, arq, ary, ... (12) Spoken, Written human-annotated created Citation <pre><code>@misc{ousidhoum2024semrel2024,\n  archiveprefix = {arXiv},\n  author = {Nedjma Ousidhoum and Shamsuddeen Hassan Muhammad and Mohamed Abdalla and Idris Abdulmumin and Ibrahim Said Ahmad and\nSanchit Ahuja and Alham Fikri Aji and Vladimir Araujo and Abinew Ali Ayele and Pavan Baswani and Meriem Beloucif and\nChris Biemann and Sofia Bourhim and Christine De Kock and Genet Shanko Dekebo and\nOumaima Hourrane and Gopichand Kanumolu and Lokesh Madasu and Samuel Rutunda and Manish Shrivastava and\nThamar Solorio and Nirmal Surange and Hailegnaw Getaneh Tilaye and Krishnapriya Vishnubhotla and Genta Winata and\nSeid Muhie Yimam and Saif M. Mohammad},\n  eprint = {2402.08638},\n  primaryclass = {cs.CL},\n  title = {SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#synpersts","title":"SynPerSTS","text":"<p>Synthetic Persian Semantic Textual Similarity Dataset</p> <p>Dataset: <code>MCINext/synthetic-persian-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/summarization/","title":"Summarization","text":"<ul> <li>Number of tasks: 4</li> </ul>"},{"location":"overview/available_tasks/summarization/#summeval","title":"SummEval","text":"<p>News Article Summary Semantic Similarity Estimation.</p> <p>Dataset: <code>mteb/summeval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Written human-annotated created Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalfr","title":"SummEvalFr","text":"<p>News Article Summary Semantic Similarity Estimation translated from english to french with DeepL.</p> <p>Dataset: <code>lyon-nlp/summarization-summeval-fr-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra News, Written human-annotated machine-translated Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalfrsummarizationv2","title":"SummEvalFrSummarization.v2","text":"<p>News Article Summary Semantic Similarity Estimation translated from english to french with DeepL. This version fixes a bug in the evaluation script that caused the main score to be computed incorrectly.</p> <p>Dataset: <code>lyon-nlp/summarization-summeval-fr-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra News, Written human-annotated machine-translated Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalsummarizationv2","title":"SummEvalSummarization.v2","text":"<p>News Article Summary Semantic Similarity Estimation. This version fixes a bug in the evaluation script that caused the main score to be computed incorrectly.</p> <p>Dataset: <code>mteb/summeval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Written human-annotated created Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/","title":"VisionCentricQA","text":"<ul> <li>Number of tasks: 6</li> </ul>"},{"location":"overview/available_tasks/visioncentricqa/#blinkit2imultichoice","title":"BLINKIT2IMultiChoice","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>JamieSJS/blink-it2i-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) accuracy eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#blinkit2tmultichoice","title":"BLINKIT2TMultiChoice","text":"<p>Retrieve the correct text answer based on images and specific retrieval instructions.</p> <p>Dataset: <code>JamieSJS/blink-it2t-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchcount","title":"CVBenchCount","text":"<p>count the number of objects in the image.</p> <p>Dataset: <code>nyu-visionx/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchdepth","title":"CVBenchDepth","text":"<p>judge the depth of the objects in the image with similarity matching.</p> <p>Dataset: <code>nyu-visionx/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchdistance","title":"CVBenchDistance","text":"<p>judge the distance of the objects in the image with similarity matching.</p> <p>Dataset: <code>nyu-visionx/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchrelation","title":"CVBenchRelation","text":"<p>decide the relation of the objects in the image.</p> <p>Dataset: <code>nyu-visionx/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/","title":"VisualSTS(eng)","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts12visualsts","title":"STS12VisualSTS","text":"<p>SemEval-2012 Task 6.then rendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts12</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Encyclopaedic, News, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts13visualsts","title":"STS13VisualSTS","text":"<p>SemEval STS 2013 dataset.then rendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts13</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng News, Non-fiction, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts14visualsts","title":"STS14VisualSTS","text":"<p>SemEval STS 2014 dataset. Currently only the English dataset.rendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts14</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, Spoken, Web derived rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts15visualsts","title":"STS15VisualSTS","text":"<p>SemEval STS 2015 datasetrendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts15</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, News, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts16visualsts","title":"STS16VisualSTS","text":"<p>SemEval STS 2016 datasetrendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts16</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, Spoken, Web human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28multi%29/","title":"VisualSTS(multi)","text":"<ul> <li>Number of tasks: 2</li> </ul>"},{"location":"overview/available_tasks/visualsts%28multi%29/#sts17multilingualvisualsts","title":"STS17MultilingualVisualSTS","text":"<p>Semantic Textual Similarity 17 (STS-17) dataset, rendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts17</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman ara, deu, eng, fra, ita, ... (9) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28multi%29/#stsbenchmarkmultilingualvisualsts","title":"STSBenchmarkMultilingualVisualSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset, translated into target languages using DeepL API,then rendered into images.built upon multi-sts created by Philip May</p> <p>Dataset: <code>Pixel-Linguist/rendered-stsb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman cmn, deu, eng, fra, ita, ... (10) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/","title":"ZeroShotClassification","text":"<ul> <li>Number of tasks: 24</li> </ul>"},{"location":"overview/available_tasks/zeroshotclassification/#birdsnapzeroshot","title":"BirdsnapZeroShot","text":"<p>Classifying bird images from 500 species. </p> <p>Dataset: <code>isaacchung/birdsnap</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Berg_2014_CVPR,\n  author = {Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L. and Jacobs, David W. and Belhumeur, Peter N.},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Birdsnap: Large-scale Fine-grained Visual Categorization of Birds},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#cifar100zeroshot","title":"CIFAR100ZeroShot","text":"<p>Classifying images from 100 classes.</p> <p>Dataset: <code>uoft-cs/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#cifar10zeroshot","title":"CIFAR10ZeroShot","text":"<p>Classifying images from 10 classes.</p> <p>Dataset: <code>uoft-cs/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#clevrcountzeroshot","title":"CLEVRCountZeroShot","text":"<p>CLEVR count objects task.</p> <p>Dataset: <code>clip-benchmark/wds_vtab-clevr_count_all</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Constructed human-annotated created Citation <pre><code>@inproceedings{Johnson_2017_CVPR,\n  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#clevrzeroshot","title":"CLEVRZeroShot","text":"<p>CLEVR closest object distance identification task.</p> <p>Dataset: <code>clip-benchmark/wds_vtab-clevr_closest_object_distance</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Constructed human-annotated created Citation <pre><code>@inproceedings{Johnson_2017_CVPR,\n  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#caltech101zeroshot","title":"Caltech101ZeroShot","text":"<p>Classifying images of 101 widely varied objects.</p> <p>Dataset: <code>mteb/Caltech101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{1384978,\n  author = {Li Fei-Fei and Fergus, R. and Perona, P.},\n  booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n  doi = {10.1109/CVPR.2004.383},\n  keywords = {Bayesian methods;Testing;Humans;Maximum likelihood estimation;Assembly;Shape;Machine vision;Image recognition;Parameter estimation;Image databases},\n  number = {},\n  pages = {178-178},\n  title = {Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},\n  volume = {},\n  year = {2004},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#country211zeroshot","title":"Country211ZeroShot","text":"<p>Classifying images of 211 countries.</p> <p>Dataset: <code>clip-benchmark/wds_country211</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@article{radford2021learning,\n  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},\n  journal = {arXiv preprint arXiv:2103.00020},\n  title = {Learning Transferable Visual Models From Natural Language Supervision},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#dtdzeroshot","title":"DTDZeroShot","text":"<p>Describable Textures Dataset in 47 categories.</p> <p>Dataset: <code>tanganke/dtd</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{cimpoi14describing,\n  author = {M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and and A. Vedaldi},\n  booktitle = {Proceedings of the {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},\n  title = {Describing Textures in the Wild},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#eurosatzeroshot","title":"EuroSATZeroShot","text":"<p>Classifying satellite images.</p> <p>Dataset: <code>timm/eurosat-rgb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{8736785,\n  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n  doi = {10.1109/JSTARS.2019.2918242},\n  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n  keywords = {Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing;Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images},\n  number = {7},\n  pages = {2217-2226},\n  title = {EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},\n  volume = {12},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#fer2013zeroshot","title":"FER2013ZeroShot","text":"<p>Classifying facial emotions.</p> <p>Dataset: <code>clip-benchmark/wds_fer2013</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{goodfellow2015explainingharnessingadversarialexamples,\n  archiveprefix = {arXiv},\n  author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},\n  eprint = {1412.6572},\n  primaryclass = {stat.ML},\n  title = {Explaining and Harnessing Adversarial Examples},\n  url = {https://arxiv.org/abs/1412.6572},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#fgvcaircraftzeroshot","title":"FGVCAircraftZeroShot","text":"<p>Classifying aircraft images from 41 manufacturers and 102 variants.</p> <p>Dataset: <code>mteb/FGVCAircraftZeroShot</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#food101zeroshot","title":"Food101ZeroShot","text":"<p>Classifying food.</p> <p>Dataset: <code>ethz/food101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@inproceedings{bossard14,\n  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n  booktitle = {European Conference on Computer Vision},\n  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#gtsrbzeroshot","title":"GTSRBZeroShot","text":"<p>The German Traffic Sign Recognition Benchmark (GTSRB) is a multi-class classification dataset for traffic signs. It consists of dataset of more than 50,000 traffic sign images. The dataset comprises 43 classes with unbalanced class frequencies.</p> <p>Dataset: <code>clip-benchmark/wds_gtsrb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@inproceedings{6033395,\n  author = {Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},\n  booktitle = {The 2011 International Joint Conference on Neural Networks},\n  doi = {10.1109/IJCNN.2011.6033395},\n  keywords = {Humans;Training;Image color analysis;Benchmark testing;Lead;Histograms;Image resolution},\n  number = {},\n  pages = {1453-1460},\n  title = {The German Traffic Sign Recognition Benchmark: A multi-class classification competition},\n  volume = {},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#imagenet1kzeroshot","title":"Imagenet1kZeroShot","text":"<p>ImageNet, a large-scale ontology of images built upon the backbone of the WordNet structure.</p> <p>Dataset: <code>clip-benchmark/wds_imagenet1k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene human-annotated created Citation <pre><code>@article{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  organization = {Ieee},\n  pages = {248--255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#mnistzeroshot","title":"MNISTZeroShot","text":"<p>Classifying handwritten digits.</p> <p>Dataset: <code>ylecun/mnist</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{lecun2010mnist,\n  author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},\n  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n  title = {MNIST handwritten digit database},\n  volume = {2},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#oxfordpetszeroshot","title":"OxfordPetsZeroShot","text":"<p>Classifying animal images.</p> <p>Dataset: <code>isaacchung/OxfordPets</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#patchcamelyonzeroshot","title":"PatchCamelyonZeroShot","text":"<p>Histopathology diagnosis classification dataset.</p> <p>Dataset: <code>clip-benchmark/wds_vtab-pcam</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Medical derived created Citation <pre><code>@inproceedings{10.1007/978-3-030-00934-2_24,\n  address = {Cham},\n  author = {Veeling, Bastiaan S.\nand Linmans, Jasper\nand Winkens, Jim\nand Cohen, Taco\nand Welling, Max},\n  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},\n  editor = {Frangi, Alejandro F.\nand Schnabel, Julia A.\nand Davatzikos, Christos\nand Alberola-L{\\'o}pez, Carlos\nand Fichtinger, Gabor},\n  isbn = {978-3-030-00934-2},\n  pages = {210--218},\n  publisher = {Springer International Publishing},\n  title = {Rotation Equivariant CNNs for Digital Pathology},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#resisc45zeroshot","title":"RESISC45ZeroShot","text":"<p>Remote Sensing Image Scene Classification by Northwestern Polytechnical University (NWPU).</p> <p>Dataset: <code>timm/resisc45</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{7891544,\n  author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},\n  doi = {10.1109/JPROC.2017.2675998},\n  journal = {Proceedings of the IEEE},\n  keywords = {Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification;Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning},\n  number = {10},\n  pages = {1865-1883},\n  title = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},\n  volume = {105},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#renderedsst2","title":"RenderedSST2","text":"<p>RenderedSST2.</p> <p>Dataset: <code>clip-benchmark/wds_renderedsst2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Reviews human-annotated created"},{"location":"overview/available_tasks/zeroshotclassification/#stl10zeroshot","title":"STL10ZeroShot","text":"<p>Classifying 96x96 images from 10 classes.</p> <p>Dataset: <code>tanganke/stl10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{pmlr-v15-coates11a,\n  address = {Fort Lauderdale, FL, USA},\n  author = {Coates, Adam and Ng, Andrew and Lee, Honglak},\n  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},\n  editor = {Gordon, Geoffrey and Dunson, David and Dud\u00edk, Miroslav},\n  month = {11--13 Apr},\n  pages = {215--223},\n  pdf = {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},\n  publisher = {PMLR},\n  series = {Proceedings of Machine Learning Research},\n  title = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},\n  url = {https://proceedings.mlr.press/v15/coates11a.html},\n  volume = {15},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#sun397zeroshot","title":"SUN397ZeroShot","text":"<p>Large scale scene recognition in 397 categories.</p> <p>Dataset: <code>dpdl-benchmark/sun397</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{5539970,\n  author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},\n  booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2010.5539970},\n  number = {},\n  pages = {3485-3492},\n  title = {SUN database: Large-scale scene recognition from abbey to zoo},\n  volume = {},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#scimmir","title":"SciMMIR","text":"<p>SciMMIR.</p> <p>Dataset: <code>m-a-p/SciMMIR</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Academic human-annotated created Citation <pre><code>@misc{wu2024scimmirbenchmarkingscientificmultimodal,\n  archiveprefix = {arXiv},\n  author = {Siwei Wu and Yizhi Li and Kang Zhu and Ge Zhang and Yiming Liang and Kaijing Ma and Chenghao Xiao and Haoran Zhang and Bohao Yang and Wenhu Chen and Wenhao Huang and Noura Al Moubayed and Jie Fu and Chenghua Lin},\n  eprint = {2401.13478},\n  primaryclass = {cs.IR},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  url = {https://arxiv.org/abs/2401.13478},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#stanfordcarszeroshot","title":"StanfordCarsZeroShot","text":"<p>Classifying car images from 96 makes.</p> <p>Dataset: <code>isaacchung/StanfordCars</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#ucf101zeroshot","title":"UCF101ZeroShot","text":"<p>UCF101 is an action recognition data set of realistic action videos collected from YouTube, having 101 action categories. This version of the dataset does not contain images but images saved frame by frame. Train and test splits are generated based on the authors' first version train/test list.</p> <p>Dataset: <code>flwrlabs/ucf101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@misc{soomro2012ucf101dataset101human,\n  archiveprefix = {arXiv},\n  author = {Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},\n  eprint = {1212.0402},\n  primaryclass = {cs.CV},\n  title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\n  url = {https://arxiv.org/abs/1212.0402},\n  year = {2012},\n}\n</code></pre>"},{"location":"usage/cli/","title":"Command Line Interface","text":"<p>This described the is the command line interface for <code>mteb</code>.</p> <p><code>mteb</code> is a toolkit for evaluating the quality of embedding models on various benchmarks. It supports the following commands:</p> <ul> <li><code>mteb run</code>: Runs a model on a set of tasks</li> <li><code>mteb available_tasks</code>: Lists the available tasks within MTEB</li> <li><code>mteb available_benchmarks</code>: Lists the available benchmarks</li> <li><code>mteb create_meta</code>: Creates the metadata for a model card from a folder of results</li> </ul> <p>In the following we outline some sample use cases, but if you want to learn more about the arguments for each command you can run:</p> <pre><code>mteb {command} --help\n</code></pre>"},{"location":"usage/cli/#running-models-on-tasks","title":"Running Models on Tasks","text":"<p>To run a model on a set of tasks, use the <code>mteb run</code> command. For example:</p> <pre><code>mteb run -m sentence-transformers/average_word_embeddings_komninos \\\n         -t Banking77Classification EmotionClassification \\\n         --output-folder mteb_output\n</code></pre> <p>This will create a folder <code>mteb_output/{model_name}/{model_revision}</code> containing the results of the model on the specified tasks supplied as a json file; <code>{task_name}.json</code>.</p>"},{"location":"usage/cli/#listing-available-tasks","title":"Listing Available Tasks","text":"<p>To list the available tasks within MTEB, use the <code>mteb available-tasks</code> command. For example:</p> <pre><code>mteb available-tasks # list _all_ available tasks\n</code></pre> <p>You can also use the multiple arguments for filtering: <pre><code>mteb available-tasks --task-types Retrieval --languages eng # list all English (eng) retrieval tasks\n</code></pre></p>"},{"location":"usage/cli/#listing-available-benchmarks","title":"Listing Available Benchmarks","text":"<p>To list the available benchmarks within MTEB:</p> <pre><code>mteb available-benchmarks # list all available benchmarks\n</code></pre>"},{"location":"usage/cli/#creating-model-metadata","title":"Creating Model Metadata","text":"<p>Once a model is run you can create the metadata for a model card from a folder of results, use the <code>mteb create-meta</code> command. For example:</p> <pre><code>mteb create-meta --results-folder mteb_output/sentence-transformers__average_word_embeddings_komninos/{revision} \\\n                 --output-path model_card.md\n</code></pre> <p>This will create a model card at <code>model_card.md</code> containing the metadata for the model on MTEB within the YAML frontmatter. This will make the model discoverable on the MTEB leaderboard.</p> <p>An example frontmatter for a model card is shown below:</p> <pre><code>---\ntags:\n- mteb\nmodel-index:\n- name: SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n  results:\n  - task:\n      type: classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77\n      config: default\n      split: test\n      revision: 44fa15921b4c889113cc5df03dd4901b49161ab7\n    metrics:\n    - type: accuracy\n      value: 84.49350649350649\n---\n</code></pre>"},{"location":"usage/defining_the_model/","title":"Defining the Model","text":""},{"location":"usage/defining_the_model/#using-a-pre-defined-model","title":"Using a pre-defined Model","text":"<p>MTEB comes with an implementation of many popular models and APIs. These can be loaded using <code>mteb.get_model_meta</code> or <code>mteb.get_model</code>:</p> <pre><code>model_name = \"intfloat/multilingual-e5-small\"\nmeta = mteb.get_model_meta(model_name)\nmodel = meta.load_model()\n# or directly using\nmodel = mteb.get_model(model_name)\n</code></pre> <p>You can get an overview of the models available in <code>mteb</code> as follows:</p> <pre><code>model_metas = mteb.get_model_metas()\n\n# You can e.g. use the model metas to find all openai models\nopenai_models = [meta for meta in model_metas if \"openai\" in meta.name]\n</code></pre> <p>Tip</p> <p>Some models require additional dependencies to run on MTEB. An example of such a model is the OpenAI APIs. These dependencies can be installed using <code>pip install mteb[openai]</code></p>"},{"location":"usage/defining_the_model/#using-a-sentence-transformer-model","title":"Using a Sentence Transformer Model","text":"<p>MTEB is made to be compatible with sentence transformers and thus you can readily evaluate any model that can be loaded via. sentence transformers on <code>MTEB</code>:</p> <pre><code>model = SentenceTransformers(\"sentence-transformers/LaBSE\")\n\n# select the desired tasks and evaluate\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>However, we do recommend checking if mteb includes an implementation of the model before using sentence transformers since some models (e.g. the multilingual e5 models) require a prompt and not specifying it may reduce performance.</p> <p>Note</p> <p>If you want to evaluate a cross encoder for reranking, see the section on running cross encoders for reranking.</p>"},{"location":"usage/defining_the_model/#using-a-custom-model","title":"Using a Custom Model","text":"<p>It is also possible to implement your own custom model in MTEB as long as it adheres to the EncoderProtocol.</p> <p>This entails implementing an <code>encode</code> function taking as input a list of sentences, and returning a list of embeddings (embeddings can be <code>np.array</code>, <code>torch.tensor</code>, etc.).</p> <pre><code>import mteb\nfrom mteb.types import PromptType\nimport numpy as np\n\n\nclass CustomModel:\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: The inputs to encode.\n            task_metadata: The name of the task.\n            hf_subset: The subset of the dataset.\n            hf_split: The split of the dataset.\n            prompt_type: The prompt type to use.\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded sentences.\n        \"\"\"\n        pass\n\n\n# evaluating the model:\nmodel = CustomModel()\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nmodel = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>If you want to submit your implementation to be included in the leaderboard see the section on submitting a model.</p>"},{"location":"usage/get_started/","title":"Get Started","text":"<p>This usage documentation first introduces a simple example of how to evaluate a model using <code>mteb</code>. Then introduces model detailed section of defining model, selecting tasks and running the evaluation. Each section contains subsections pertaining to these.</p>"},{"location":"usage/get_started/#evaluating-a-model","title":"Evaluating a Model","text":"<p>Evaluating a model on MTEB follows a three step approach, 1) defining model, 2) selecting the tasks and 3) running the evaluation</p> <pre><code>import mteb\n\n# Specify the model that we want to evaluate\nmodel = ...\n\n# specify what you want to evaluate it on\ntasks = mteb.get_tasks(tasks=[\"{task1}\", \"{task1}\"])\n\n# run the evaluation\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>For instance if we want to run <code>\"sentence-transformers/all-MiniLM-L6-v2\"</code> on <code>\"Banking77Classification\"</code> we can do this using the following code:</p> <pre><code>model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# load the model using MTEB\nmodel = mteb.get_model(model_name) # will default to SentenceTransformers(model_name) if not implemented in MTEB\n# or using SentenceTransformers\nmodel = SentenceTransformers(model_name)\n\n# select the desired tasks and evaluate\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>Note</p> <p>While <code>mteb.evaluate</code> supports <code>SentenceTransformers</code> we do recommend that the user use <code>mteb.get_model</code> to fetch the model as this prioritizes the implementation in <code>mteb</code>, which might not match 1-1 its <code>SentenceTransformers</code> implementation. For leaderboards results we see the <code>mteb</code> implementation as the reference implementation.</p>"},{"location":"usage/get_started/#evaluating-on-different-modalities","title":"Evaluating on Different Modalities","text":"<p>MTEB is not only text evaluating, but also allow you to evaluate image and image-text embeddings.</p> <p>Note</p> <p>Running MTEB on images requires you to install the optional dependencies using <code>pip install mteb[image]</code></p> <p>To evaluate image embeddings you can follow the same approach for any other task in <code>mteb</code>. Simply ensuring that the task contains the modality \"image\":</p> <pre><code>tasks = mteb.get_tasks(modalities=[\"image\"]) # Only select tasks with image modalities\ntask = task[0]\n\nprint(task.metadata.modalites)\n# ['text', 'image']\n</code></pre> <p>However, we recommend starting with one of the predefined benchmarks:</p> <pre><code>import mteb\nbenchmark = mteb.get_benchmark(\"MIEB(eng)\")\nmodel = mteb.get_model(\"openai/clip-vit-base-patch16\") # example model\n\nresults = mteb.evaluate(model, tasks=benchmark)\n</code></pre> <p>You can also specify exclusive modality filtering to only get tasks with exactly the requested modalities (default behavior with <code>exclusive_modality_filter=False</code>):</p> <pre><code># Get tasks with image modality, this will also include tasks having both text and image modalities\ntasks = mteb.get_tasks(modalities=[\"image\"], exclusive_modality_filter=False)\n\n# Get tasks that have ONLY image modality\ntasks = mteb.get_tasks(modalities=[\"image\"], exclusive_modality_filter=True)\n</code></pre>"},{"location":"usage/leaderboard/","title":"Running the Leaderboard","text":"<p>This section contains information on how to interact with the leaderboard including running it locally, analysing the results, annotating contamination and more.</p>"},{"location":"usage/leaderboard/#running-the-leaderboard-locally","title":"Running the Leaderboard Locally","text":"<p>It is possible to completely deploy the leaderboard locally or self-host it. This can e.g. be relevant for companies that might want to integrate build their own benchmarks or integrate custom tasks into existing benchmarks.</p> <p>Running the leaderboard is quite easy. Simply run: <pre><code>make run-leaderboard\n</code></pre></p> <p>The leaderboard requires gradio install, which can be installed using <code>pip install mteb[leaderboard]</code> and requires python &gt;3.10.</p>"},{"location":"usage/leaderboard/#annotate-contamination","title":"Annotate Contamination","text":"<p>have your found contamination in the training data of a model? Please let us know, either by opening an issue or ideally by submitting a PR annotating the training datasets of the model:</p> <pre><code>model_w_contamination = ModelMeta(\n    name = \"model-with-contamination\"\n    ...\n    training_datasets = {\"ArguAna\", ...} # name of dataset within MTEB\n    ...\n)\n</code></pre>"},{"location":"usage/loading_results/","title":"Loading and working with results","text":"<p>To make the results more easily accessible, we have designed functionality for retrieving results from both online and the local cache. Generally you access this functionality using the <code>ResultCache</code> object.</p> <p>For instance, if you are selecting the best model for semantic text similarity (STS) you could fetch the relevant tasks and create a dataframe of the results using the following code:</p> <pre><code>import mteb\nfrom mteb.cache import ResultCache\n\ntasks = mteb.get_tasks(tasks=[\"STS12\"])\nmodel_names = [\"intfloat/multilingual-e5-large\"]\n\ncache = ResultCache(\"~/.cache/mteb\")\nresults = cache.load_results(models=model_names, tasks=tasks)\n</code></pre> <p>From this you will get a <code>BenchmarkResults</code> object: <pre><code>results\n# BenchmarkResults(model_results=[...](#1))\ntype(results)\n# mteb.load_results.benchmark_results.BenchmarkResults\n</code></pre> Which you can then convert to a dataframe:</p> <pre><code>df = results.to_dataframe()\n</code></pre>"},{"location":"usage/loading_results/#working-with-public-results","title":"Working with public results","text":"<p>All previously submitted results are available results repository.</p> <p>You can download this using:</p> <pre><code>from mteb.cache import ResultCache\n\ncache = ResultCache()\ncache.download_from_remote() # download results from the remote repository\n</code></pre> <p>From here, you can work with the cache as usual. For instance, if you are selecting the best model for your French and English retrieval task on legal documents, you could fetch the relevant tasks and create a dataframe of the results using the following code:</p> <pre><code>from mteb.cache import ResultCache\n\n# select your tasks\ntasks = mteb.get_tasks(task_types=[\"Retrieval\"], languages=[\"eng\", \"fra\"], domains=[\"Legal\"])\n\nmodel_names = [\n    \"GritLM/GritLM-7B\",\n    \"intfloat/multilingual-e5-large\",\n]\n\n\ncache = ResultCache()\ncache.download_from_remote() # download results from the remote repository. Might take a while the first time.\n\nresults = cache.load_results(\n    models=model_names,\n    tasks=tasks,\n    include_remote=True, # default\n)\n</code></pre>"},{"location":"usage/loading_results/#working-with-benchmarkresults","title":"Working with <code>BenchmarkResults</code>","text":"<p>The result object is a convenient object in <code>mteb</code> for working with dataframes and allows you to quick examine your results.</p> <p></p> <p>The object contain a lot of convenience functions for inspecting and examining the results: <pre><code>print(results.model_names)\n# ['GritLM/GritLM-7B', 'intfloat/multilingual-e5-large']\n\ntask_names = results.task_names\nprint(task_names)\n# ['SpartQA', 'PlscClusteringP2P.v2', 'StackOverflowQA', 'JSICK', ...\n</code></pre></p>"},{"location":"usage/loading_results/#filtering-results","title":"Filtering Results","text":"<p>There is also utility function that allows you to select certain models or tasks: <pre><code># select only gritLM\nresults = results.select_models([\"GritLM/GritLM-7B\"])\n\n# select only retrieval tasks\ntasks = mteb.get_tasks(tasks=task_names)\nretrieval_tasks = [task for task in tasks if task.metadata.type == \"Retrieval\"]\n\nresults = results.select_tasks(retrieval_tasks)\n</code></pre></p>"},{"location":"usage/loading_results/#creating-a-dataframe","title":"Creating a Dataframe","text":"<pre><code>df = results.to_dataframe()\n\nprint(df)\n# model_name                        task_name  GritLM/GritLM-7B\n# 0                              AILAStatutes          0.418000\n# 1                                   ArguAna          0.631710\n# 2                         BelebeleRetrieval          0.717035\n# 3                            CovidRetrieval          0.734010\n# 4                           HagridRetrieval          0.986730\n# 5                      LEMBPasskeyRetrieval          0.382500\n# 6               LegalBenchCorporateLobbying          0.949990\n# 7              MIRACLRetrievalHardNegatives          0.516793\n# 8                             MLQARetrieval          0.727420\n# 9                                   SCIDOCS          0.244090\n# 10                                  SpartQA          0.093550\n# 11                          StackOverflowQA          0.933670\n# 12          StatcanDialogueDatasetRetrieval          0.457587\n# 13                                TRECCOVID          0.743130\n# 14                             TempReasonL1          0.071640\n# 15                   TwitterHjerneRetrieval          0.432660\n# 16           WikipediaRetrievalMultilingual          0.917722\n# 17                               WinoGrande          0.536970\n</code></pre> <p>By default this will give you the results in a <code>\"wide\"</code> format. However, you can just as well get them in a long format:</p> <pre><code>long_format_df = results.to_dataframe(format=\"long\")\n\nprint(long_format_df.head(5))\n#          model_name          task_name     score\n# 0  GritLM/GritLM-7B       AILAStatutes  0.418000\n# 1  GritLM/GritLM-7B            ArguAna  0.631710\n# 2  GritLM/GritLM-7B  BelebeleRetrieval  0.717035\n# 3  GritLM/GritLM-7B     CovidRetrieval  0.734010\n# 4  GritLM/GritLM-7B    HagridRetrieval  0.986730\n</code></pre>"},{"location":"usage/loading_results/#adding-metadata-to-table","title":"Adding metadata to table","text":"<p>One might want to add some more metadata to the table. This is luckily quite easy using:</p> <pre><code>import pandas as pd\n\ntask_df = tasks.to_dataframe(properties=[\"name\", \"type\", \"domains\"])\ntask_df = task_df.rename(columns={\"name\": \"task_name\"})\n\ndf_with_meta = pd.merge(task_df, df)\n\nprint(df_with_meta.head(5))\n#            task_name       type                   domains  GritLM/GritLM-7B\n# 0            SpartQA  Retrieval  [Encyclopaedic, Written]          0.093550\n# 1    StackOverflowQA  Retrieval    [Programming, Written]          0.933670\n# 2  BelebeleRetrieval  Retrieval      [Web, News, Written]          0.717035\n# 3            ArguAna  Retrieval        [Medical, Written]          0.631710\n# 4       TempReasonL1  Retrieval  [Encyclopaedic, Written]          0.071640\n</code></pre>"},{"location":"usage/running_the_evaluation/","title":"Running the Evaluation","text":"<p>This section contains documentation related to the runtime of the evaluation. How to pass arguments to the encoder, saving outputs and similar.</p>"},{"location":"usage/running_the_evaluation/#simple-example","title":"Simple Example","text":"<p>Evaluating models in <code>mteb</code> typically takes the simple form:</p> PythonCLI <pre><code>import mteb\n\nmodel = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\ntasks = mteb.get_task([\"MultiHateClassification\"], languages = [\"ita\", \"dan\"])\n\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <pre><code>mteb run -t MultiHateClassification -m sentence-transformers/static-similarity-mrl-multilingual-v1 -l ita dan\n</code></pre> <p>Compatibility with SentenceTransformers</p> <p><code>mteb</code> is designed to be compatible with <code>sentence-transformers</code> so you can also directly pass a <code>SentenceTransformer</code> or <code>CrossEncoder</code> to <code>mteb.evaluate</code>.</p>"},{"location":"usage/running_the_evaluation/#specifying-the-cache","title":"Specifying the cache","text":"<p>By default <code>mteb</code> with save the results in cache folder located at <code>~/.cache/mteb</code>, however if you want to save the results in a specific folder you can specify it as follows:</p> <pre><code>cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\nresults = mteb.evaluate(model, tasks=tasks, cache=cache)\n</code></pre> <p>If you don't wish to run model which results already exist on the leaderboard you can download these simply by running:</p> <pre><code>cache.download_from_remote()\nresults = mteb.evaluate(model, tasks=tasks, cache=cache)\n</code></pre>"},{"location":"usage/running_the_evaluation/#tracking-carbon-emissions","title":"Tracking Carbon Emissions","text":"<p><code>mteb</code> allows for easy tracking of carbon emission eq. using <code>codecarbon</code>. You simply need to install <code>mteb[codecarbon]</code> and enable co2 tracking:</p> PythonCLI <pre><code>results = mteb.evaluate(tasks=tasks, co2_tracker=True)\n</code></pre> <pre><code>mteb run -t NanoArguAnaRetrieval -m sentence-transformers/static-similarity-mrl-multilingual-v1 --co2-tracker\n</code></pre>"},{"location":"usage/running_the_evaluation/#passing-in-encode-arguments","title":"Passing in <code>encode</code> arguments","text":"<p>To pass in arguments to the model's <code>encode</code> function, you can use the encode keyword arguments (<code>encode_kwargs</code>):</p> <pre><code>mteb.evaluate(model, tasks, encode_kwargs={\"batch_size\": 32})\n</code></pre>"},{"location":"usage/running_the_evaluation/#running-sentencetransformer-model-with-prompts","title":"Running SentenceTransformer model with prompts","text":"<p>Prompts can be passed to the SentenceTransformer model using the <code>prompts</code> parameter. The following code shows how to use prompts with SentenceTransformer:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer(\"intfloat/multilingual-e5-small\", prompts={\"query\": \"Query:\", \"document\": \"Passage:\"})\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>In prompts the key can be:</p> <ol> <li>Prompt types (<code>passage</code>, <code>query</code>) - they will be used in reranking and retrieval tasks</li> <li>Task type - these prompts will be used in all tasks of the given type</li> <li><code>BitextMining</code></li> <li><code>Classification</code></li> <li><code>MultilabelClassification</code></li> <li><code>Clustering</code></li> <li><code>PairClassification</code></li> <li><code>Reranking</code></li> <li><code>Retrieval</code></li> <li><code>STS</code></li> <li><code>Summarization</code></li> <li><code>InstructionRetrieval</code></li> <li>Pair of task type and prompt type like <code>Retrieval-query</code> - these prompts will be used in all Retrieval tasks</li> <li>Task name - these prompts will be used in the specific task</li> <li>Pair of task name and prompt type like <code>NFCorpus-query</code> - these prompts will be used in the specific task</li> </ol>"},{"location":"usage/running_the_evaluation/#saving-predictions","title":"Saving predictions","text":"<p>To save the predictions from a task simply set the <code>prediction_folder</code>:</p> PythonCLI <pre><code># ...\nresults = mteb.evaluate(\n    model,\n    task,\n    prediction_folder=\"model_predictions\",\n)\n</code></pre> <pre><code>mteb run -t NanoArguAnaRetrieval -m sentence-transformers/static-similarity-mrl-multilingual-v1 --prediction-folder predictions\n</code></pre> <p>The file will now be saved to <code>\"{prediction_folder}/{task_name}_predictions.json\"</code> and contain the rankings for each query along with the model name and revision of the model that produced the result.</p>"},{"location":"usage/running_the_evaluation/#speeding-up-evaluations","title":"Speeding up Evaluations","text":"<p>Evaluation in MTEB consists of three main components: 1) downloading the dataset, 2) encoding of the samples, and 3) the evaluation. Typically, the most notable bottleneck are either in the encoding step or on the download step. Where this bottleneck is depends on the tasks that you evaluate on and the models that you evaluate.</p> <p>If you find that any of our design decisions prevents you from running the evaluation efficiently do feel free to create an issue.</p>"},{"location":"usage/running_the_evaluation/#speeding-up-the-model","title":"Speeding up the Model","text":"<p>MTEB is an evaluation framework and therefore we try to avoid doing inference optimizations. However, here is a few tricks to speed up inference.</p> <p>First of all it is possible to pass directly to the model loader: <pre><code>import mteb\n\nmeta = mteb.get_model_meta(\"intfloat/multilingual-e5-small\")\n\nkwargs = dict(\n    device=\"cuda\", # use a gpu\n    model_kwargs={\"torch_dtype\": \"float16\"}, # use low-precision\n)\n\nmodel = meta.load_model(**kwargs) # passed to model loader, e.g. SentenceTransformer\n</code></pre></p> <p>This e.g. allows you to use all the inference optimization tricks from sentence-transformers. However in general you can always pass <code>device</code>.</p> <p>If you can't the required keyword argument a solution might be to extract the model: <pre><code>model = meta.load_model(**kwargs)\n# extract model\nsentence_trf_model = model.model\n\n# optimizations:\nsentence_trf_model.half() # half precision\n</code></pre></p> <p>A last option is to make a custom implementation of the model. This way you have full flexibility of how the model handles the input.</p>"},{"location":"usage/running_the_evaluation/#speeding-download","title":"Speeding Download","text":"<p>The simplest way to speed up downloads is by using Huggingface's <code>xet</code>. You can use this simply using:</p> <pre><code>pip install mteb[xet]\n</code></pre> <p>For one of the larger datasets, <code>MrTidyRetrieval</code> (~15 GB), we have seen speed-ups from ~40 minutes to ~30 minutes while using <code>xet</code>.</p>"},{"location":"usage/selecting_tasks/","title":"Selecting Tasks or Benchmarks","text":"<p>This section describes how to select benchmarks and tasks to evaluate, including selecting specific subsets or splits to run.</p>"},{"location":"usage/selecting_tasks/#selecting-a-benchmark","title":"Selecting a Benchmark","text":"<p><code>mteb</code> comes with a set of predefined benchmarks. These can be fetched using <code>get_benchmark</code> or <code>get_benchmarks</code> and run in a similar fashion to other sets of tasks. For instance to select the English benchmark that forms the English leaderboard:</p> <pre><code>import mteb\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nmodel = ...\nresults = mteb.evaluate(model, tasks=benchmark)\n</code></pre> <p>The benchmark specifies not only a list of tasks, but also what splits and language to run on.</p> <p>Note</p> <p>Generally we use the naming scheme for benchmarks <code>MTEB(*)</code>, where the \"*\" denotes the target of the benchmark. In the case of a language, we use the three-letter language code. For large groups of languages, we use the group notation, e.g., <code>MTEB(Scandinavian, v1)</code> for Scandinavian languages. External benchmarks implemented in MTEB like <code>CoIR</code><sup>1</sup> use their original name.</p> <p>To get an overview of all available benchmarks, simply run:</p> <pre><code>import mteb\nbenchmarks = mteb.get_benchmarks()\n</code></pre> <p>When using a benchmark from MTEB please cite <code>mteb</code> along with the citations of the benchmark which you can access using <code>benchmark.citation</code>.</p>"},{"location":"usage/selecting_tasks/#selecting-a-task","title":"Selecting a Task","text":"<p><code>mteb</code> comes with the utility function <code>get_task</code> and <code>get_tasks</code> for fetching and analysing the tasks of interest.</p> <p>This can be done in multiple ways, e.g.:</p> <ul> <li>by the task name</li> <li>by their type (e.g. \"Clustering\" or \"Classification\")</li> <li>by their languages (specified as a three letter code)</li> <li>by their domains</li> <li>by their modalities</li> <li>and many more</li> </ul> <pre><code># by name\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\n# by type\ntasks = mteb.get_tasks(task_types=[\"Clustering\", \"Retrieval\"]) # Only select clustering and retrieval tasks\n# by language\ntasks = mteb.get_tasks(languages=[\"eng\", \"deu\"]) # Only select datasets which contain \"eng\" or \"deu\" (iso 639-3 codes)\n# by domain\ntasks = get_tasks(domains=[\"Legal\"])\n# by modality\ntasks = mteb.get_tasks(modalities=[\"text\", \"image\"]) # Only select tasks with text or image modalities\n# or using multiple\ntasks = get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n</code></pre> <p>You can also specify which languages to load for multilingual/cross-lingual tasks like below:</p> <p><pre><code>import mteb\n\ntasks = [\n    mteb.get_task(\"AmazonReviewsClassification\", languages = [\"eng\", \"fra\"]),\n    mteb.get_task(\"BUCCBitextMining\", languages = [\"deu\"]), # all subsets containing \"deu\"\n]\n</code></pre> For more information see the documentation for <code>get_tasks</code> and <code>get_task</code>.</p>"},{"location":"usage/selecting_tasks/#selecting-evaluation-split-or-subsets","title":"Selecting Evaluation Split or Subsets","text":"<p>A task in <code>mteb</code> mirrors the structure of a dataset on Huggingface. It includes a splits (i.e. \"test\") and a subset.</p> <pre><code># selecting an evaluation split\ntask = mteb.get_task(\"Banking77Classification\", eval_splits=[\"test\"])\n# selecting a Huggingface subset\ntask = mteb.get_task(\"AmazonReviewsClassification\", hf_subsets=[\"en\", \"fr\"])\n</code></pre> <p>What is a subset?</p> <p>A subset on a Huggingface dataset is what you specify after the dataset name, e.g. <code>datasets.load_dataset(\"nyu-mll/glue\", \"cola\")</code>. Often the subset does not need to be defined and is left as \"default\". The subset is however useful, especially for multilingual datasets to specify the desired language or language pair e.g. in <code>mteb/bucc-bitext-mining</code> we might want to evaluate only on the French-English subset <code>\"fr-en\"</code>.</p>"},{"location":"usage/selecting_tasks/#using-a-custom-task","title":"Using a Custom Task","text":"<p>To evaluate on a custom task, you can run the following code on your custom task. See how to add a new task, for how to create a new task in MTEB.</p> <pre><code>import mteb\nfrom mteb.abstasks.retrieval import AbsTaskRetrieval\nfrom mteb.abstasks.task_metadata import TaskMetadata\n\n\nclass MyCustomTask(AbsTaskRetrieval):\n    metadata = TaskMetadata(...)\n\nmodel = mteb.get_model(...)\nresults = mteb.evaluate(model, tasks=[MyCustomTask()])\n</code></pre> <ol> <li> <p>Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. Coir: a comprehensive benchmark for code information retrieval models. 2024. URL: https://arxiv.org/abs/2407.02883, arXiv:2407.02883.\u00a0\u21a9</p> </li> </ol>"}]}