{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"MTEB Documentation","text":"<p>Info</p> <p>We recently released <code>mteb</code> version 2.0.0, to see what is new check of what is new and see how to upgrade your existing code.</p> <p>Welcome documentation of MTEB. <code>mteb</code> a package for benchmark and evaluating the quality of embeddings.</p> <p>MTEB is the go-to documentation for evaluating embeddings models across a variety of tasks, modalities and domains. MTEB covers more than a 1000 different tasks from covering a diverse set of tasks from historic Swedish patent classification to documentation retrieval for Python. These tasks spread across more than 1000 languages and cover both image and text tasks.</p> <p>This package was initially introduced as a package for evaluating text embeddings predominantly for English<sup>1</sup>, but have since been extended for broad languages coverage<sup>2</sup> and to support multiple modalities<sup>3</sup>.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is as simple as:</p> pipuv <pre><code>pip install mteb\n</code></pre> <pre><code>uv add mteb\n</code></pre> <p>To see more check out the installation guide.</p>"},{"location":"#quickstart","title":"Quickstart","text":"Using ScriptUsing the CLI <p>To evaluating a model simply select a model, select tasks and evaluate:</p> <pre><code>import mteb\nfrom sentence_transformers import SentenceTransformer\n\n# Select model\nmodel_name = \"sentence-transformers/all-MiniLM-L6-v2\"\nmodel = mteb.get_model(model_name) # (1)\n\n# Select tasks\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\n\n# evaluate\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <ol> <li>If the model is not implemented in MTEB, it will be loaded with <code>SentenceTransformer(model_name)</code> or <code>CrossEncoder(model_name)</code> if applicable.</li> </ol> <p>To see more check out the usage documentation</p> <p>To run a model from the cli simply specify the <code>--model/-m</code> and the <code>--tasks/-t</code> <pre><code>mteb run \\\n    -m sentence-transformers/all-MiniLM-L6-v2 \\\n    -t Banking77Classification \\\n    --output-folder results\n</code></pre></p> <p>To read more about what you can do with the command line interface check out its documentation</p>"},{"location":"#citing","title":"Citing","text":"<p>MTEB was introduced in the paper \"MTEB: Massive Text Embedding Benchmark\"<sup>1</sup>, and heavily expanded in \"MMTEB: Massive Multilingual Text Embedding Benchmark\"<sup>2</sup>. When using <code>mteb</code>, we recommend that you cite both articles.</p> <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo\u00efc and Reimers, Nils},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2210.07316},\n  year = {2022}\n  url = {https://arxiv.org/abs/2210.07316},\n  doi = {10.48550/ARXIV.2210.07316},\n}\n\n@article{enevoldsen2025mmtebmassivemultilingualtext,\n  title={MMTEB: Massive Multilingual Text Embedding Benchmark},\n  author={Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2502.13595},\n  year={2025},\n  url={https://arxiv.org/abs/2502.13595},\n  doi = {10.48550/arXiv.2502.13595},\n}\n</code></pre> <p>If you use any of the specific benchmarks, we also recommend that you cite the paper, which you can obtain using:</p> <pre><code>benchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nbenchmark.citation # get citation for a specific benchmark\n\n# you can also create a table of the task for the appendix using:\nbenchmark.tasks.to_latex()\n</code></pre> <ol> <li> <p>Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2014\u20132037. Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL: https://aclanthology.org/2023.eacl-main.148, doi:10.18653/v1/2023.eacl-main.148.\u00a0\u21a9\u21a9</p> </li> <li> <p>Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. URL: https://arxiv.org/abs/2502.13595, doi:10.48550/arXiv.2502.13595.\u00a0\u21a9\u21a9</p> </li> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"CONTRIBUTING/","title":"Contributing to <code>mteb</code>","text":"<p>We welcome contributions. Please see the current open issues or open an issue yourself. Once you have decided on what you'd like to contribute, this document describes how to set up the repository for development.</p>"},{"location":"CONTRIBUTING/#development-installation","title":"Development Installation","text":"<p>MTEB now uses uv for fast dependency management. If you want to contribute to MTEB:</p> <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Clone and setup\ngit clone https://github.com/embeddings-benchmark/mteb\ncd mteb\nmake install  # Uses uv sync with dev dependencies\n</code></pre> <p>The <code>make install</code> command now uses uv under the hood for faster dependency resolution. You can also install manually with uv:</p> <pre><code>uv sync --extra image --group dev\n</code></pre> <p>This uses make to define the install command. You can see what each command does in the makefile. All commands now use <code>uv run</code> and <code>uv sync</code> for better performance and reliability.</p>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<p>To run the tests, you can use the following command:</p> <pre><code>make test\n</code></pre> <p>This is also run by the CI pipeline, so you can be sure that your changes do not break the package. We recommend running the tests in the lowest version of Python supported by the package (see the pyproject.toml) to ensure compatibility.</p>"},{"location":"CONTRIBUTING/#running-linting","title":"Running linting","text":"<p>To run the linting before a PR, you can use the following command:</p> <pre><code>make lint\n</code></pre> <p>This command is equivalent to the command run during CI. It will check for code style and formatting issues.</p>"},{"location":"CONTRIBUTING/#semantic-versioning-and-releases","title":"Semantic Versioning and Releases","text":"<p>MTEB follows semantic versioning. This means that the version number of the package is composed of three numbers: <code>MAJOR.MINOR.PATCH</code>. This allows us to use existing tools to manage the versioning of the package automatically. For maintainers (and contributors), this means that commits with the following prefixes will automatically trigger a version bump:</p> <ul> <li><code>fix:</code> for patches</li> <li><code>model:</code> for new models</li> <li><code>dataset:</code> for new datasets and benchmarks</li> <li><code>feat:</code> for minor versions</li> <li><code>breaking:</code> for major versions</li> </ul> <p>Any commit with one of these prefixes will trigger a version bump upon merging to the main branch, as long as the tests pass. A version bump will then trigger a new release on PyPI as well as a new release on GitHub.</p> <p>Other prefixes will not trigger a version bump. For example, <code>docs:</code>, <code>chore:</code>, <code>refactor:</code>, etc., however they will structure the commit history and the changelog. You can find more information about this in the python-semantic-release documentation. If you do not intend to trigger a version bump, you're not required to follow this convention when contributing to MTEB.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> pipuv <pre><code>pip install mteb\n</code></pre> <pre><code>uv add mteb\n</code></pre>"},{"location":"installation/#model-specific-installations","title":"Model Specific Installations","text":"<p>If you want to run certain models implemented within mteb you will often need some additional dependencies. These can be installed using:</p> pipuv <pre><code>pip install mteb[cohere]\n</code></pre> <pre><code>uv add \"mteb[cohere]\"\n</code></pre> <p>If a specific model requires a dependency it will raise an error with the recommended installation. To see full list of available models you can look at the models overview.</p>"},{"location":"installation/#audio-tasks","title":"Audio Tasks","text":"<p>If you want to run audio tasks, install the audio dependencies:</p> pipuv <pre><code>pip install mteb[audio]\n</code></pre> <pre><code>uv add \"mteb[audio]\"\n</code></pre>"},{"location":"installation/#additional-requirements-for-datasets4","title":"Additional Requirements for <code>datasets&gt;=4</code>","text":"<p>If you are using <code>datasets&gt;=4</code>, you will need to:</p> <ol> <li> <p>Install FFmpeg: The <code>datasets</code> library version 4+ uses <code>torchcodec</code> for audio processing, which requires FFmpeg to be installed on your system.</p> macOSUbuntu/DebianWindows <pre><code>brew install ffmpeg\n</code></pre> <pre><code>sudo apt-get install ffmpeg\n</code></pre> <p>Download from ffmpeg.org and add to your PATH.</p> </li> <li> <p>Use <code>transformers&gt;=4.57.6</code>: Due to compatibility issues with <code>datasets&gt;=4</code>, you need a recent version of transformers:     <pre><code>pip install \"transformers&gt;=4.57.6\"\n</code></pre></p> </li> </ol> <p>If you are using <code>datasets&lt;4</code>, no additional requirements are needed beyond the <code>mteb[audio]</code> installation.</p>"},{"location":"installation/#migrating-to-uv-for-contributors","title":"Migrating to uv (for Contributors)","text":"<p>If you're a contributor currently using pip, here's how to migrate to uv for faster dependency management:</p>"},{"location":"installation/#why-uv","title":"Why uv?","text":"<ul> <li>Faster: 10-100x faster dependency resolution</li> <li>Reliable: Deterministic builds with uv.lock</li> <li>Simpler: One tool for virtual environments and packages</li> </ul>"},{"location":"installation/#migration-steps","title":"Migration Steps","text":"<ol> <li> <p>Install uv:    <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> </li> <li> <p>Replace your workflow:</p> </li> <li><code>pip install -e .</code> \u2192 <code>uv sync</code></li> <li><code>pip install mteb[extra]</code> \u2192 <code>uv sync --extra extra</code></li> <li><code>python -m pytest</code> \u2192 <code>uv run pytest</code></li> </ol>"},{"location":"installation/#development-groups","title":"Development Groups","text":"<p>For contributors, uv provides organized dependency groups:</p> <ul> <li><code>uv sync --group test</code> - Install test dependencies</li> <li><code>uv sync --group docs</code> - Install documentation dependencies</li> <li><code>uv sync --group typing</code> - Install type checking dependencies</li> <li><code>uv sync --group lint</code> - Install linting dependencies</li> <li><code>uv sync --group dev</code> - Install all development dependencies (recommended)</li> </ul>"},{"location":"whats_new/","title":"What's New","text":"<p>This section is an overview of releases for more information check out the autogenerated changelog.</p>"},{"location":"whats_new/#new-in-v210","title":"New in v2.10","text":""},{"location":"whats_new/#experiments","title":"Experiments","text":"<p>MTEB support running experiments out of the box! PR #4071</p> <p><pre><code>import mteb\n\n# base run\nmodel = mteb.get_model(\"mteb/baseline-random-encoder\")\ntask = mteb.get_task(\"HUMEEmotionClassification\")\nmteb.evaluate(model, task)\n\n# experiment with different hyperparameters\nmodel = mteb.get_model(\"mteb/baseline-random-encoder\", embed_dim=128)\nmteb.evaluate(model, task)\n</code></pre> Then in results folder will be <pre><code>results\n\u2514\u2500\u2500 results\n    \u2514\u2500\u2500 baseline__random-encoder-baseline\n        \u2514\u2500\u2500 1\n            \u251c\u2500\u2500 HUMEEmotionClassification.json\n            \u251c\u2500\u2500 experiments\n            \u2502 \u2514\u2500\u2500 embed_dim_128\n            \u2502     \u251c\u2500\u2500 HUMEEmotionClassification.json\n            \u2502     \u2514\u2500\u2500 model_meta.json\n            \u2514\u2500\u2500 model_meta.json\n</code></pre> And you can load results with</p> <pre><code>import mteb\n\ncache = mteb.ResultCache()\n\n# load default results\nresults = cache.load_results()\nprint(results.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, task_results=[...](#1))]\n\n# getting results with specific experiment\nmodel_meta = mteb.get_model_meta(\"mteb/baseline-random-encoder\", experiment_kwargs={\"a\": \"test\"})\nresult_with_experiment = cache.load_results(models=[model_meta])\n# equal to\nresult_with_experiment = cache.load_results(models=[\"mteb/baseline-random-encoder\"], experiment_kwargs={\"a\": \"test\"})\nprint(result_with_experiment.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, experiment_name=a_test, task_results=[...](#1))]\n\n# don't load experiment results\nresults_without_experiment = cache.load_results(models=[\"mteb/baseline-random-encoder\"], load_experiments=\"no_experiments\")\nprint(results_without_experiment.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, task_results=[...](#1))]\n\n# load all results for specific model\nresults_without_experiment = cache.load_results(models=[\"mteb/baseline-random-encoder\"], load_experiments=\"match_name\")\nprint(results_without_experiment.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, task_results=[...](#1)), ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, experiment_name=a_test, task_results=[...](#1))]\n\n# only one result will be loaded as experiment name doesn't match, so it will load the default one\nresults_without_experiment = cache.load_results(models=[\"mteb/baseline-random-encoder\"], load_experiments=\"match_kwargs\")\nprint(results_without_experiment.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, task_results=[...](#1))]\n\n# load all results with matching experiment params\nall_specific_result = cache.load_results(experiment_params={\"a\": \"test\"})\nprint(all_specific_result.model_results)\n# [ModelResult(model_name=mteb/baseline-random-encoder, model_revision=1, experiment_name=a_test, task_results=[...](#1))]\n</code></pre>"},{"location":"whats_new/#new-in-v29","title":"New in v2.9","text":""},{"location":"whats_new/#integration-with-huggingface-benchmarks","title":"Integration with HuggingFace Benchmarks","text":"<p>Added integration with HuggingFace Benchmarks (more info in blog). This allows for users easily see scores of task directly on dataset card.</p> <p></p> <p>To convert your task into a HuggingFace benchmark, you can simply run:</p> <pre><code>import mteb\ntask = mteb.get_task(\"MyTask\")\ntask.push_eval_to_hub(\"myorg/mytask\")\n</code></pre> <p>Or if you want to add a results of your model to an existing benchmark:</p> <pre><code>import mteb\n\nmodel_meta = mteb.get_model_meta(\"model_name\")\nmodel_meta.push_eval_results(\n    tasks=[\n        \"Task1\",\n        \"Task2\",\n    ],\n    cache=mteb.ResultCache(),\n    create_pr=True,\n)\n</code></pre> <p>In coming time we will be adding more \"benchmark\" tasks and submit scores to existing models.</p>"},{"location":"whats_new/#new-in-v28","title":"New in v2.8","text":""},{"location":"whats_new/#added-audio-support","title":"Added Audio Support","text":"<p>Added audio support to MTEB \ud83c\udf89. This includes support for loading and processing audio data in tasks. Overall this includes</p> <pre><code>import mteb\n\ntasks= mteb.get_tasks()\n\naudio_task = mteb.get_tasks(modalities=[\"audio\"])\nlen(audio_task) # 108 tasks\n\nmodels = mteb.get_model_metas()\naudio_models = [model for model in models if \"audio\" in model.modalities]\nlen(audio_models) # 56 models\n\n# and as easy as always to evaluate on these tasks:\naudio_task = audio_task[0]\nprint(audio_task) # CREMAD(name='CREMA_D', languages=['eng'])\n\naudio_model = audio_models[0]\nprint(audio_model.name) # google/vggish\n\nmteb.evaluate(audio_model, audio_task)\n</code></pre> <p>To run audio tasks you will need to have the audio extension installed, you can do this using <code>pip install mteb[audio]</code>. For more information on installation check out the extended installation guide in the documentation here.</p>"},{"location":"whats_new/#added-event-logging-support","title":"Added event logging support","text":"<p>Added event logging support. This change introduces a new <code>event_logger</code> module for tracking key user interactions within MTEB\u2019s leaderboard UI and backend. Logged events include actions such as <code>page loads</code>, <code>benchmark switches</code>, and <code>filter changes</code>, along with associated metadata. This enables better insight into how users interact with the leaderboard and provides groundwork for analytics and future improvements.</p>"},{"location":"whats_new/#new-in-v27","title":"New in v2.7","text":""},{"location":"whats_new/#added-vllm-support","title":"Added vLLM support","text":"<p>Added vLLM support. While it is currently not the reference implementation for any models it allows you to run comparisons on performance and throughput on a single model. This can inform whether it is worth switching your local setup over to vLLM. While you can read more about it here</p>"},{"location":"whats_new/#new-in-v26","title":"New in v2.6","text":""},{"location":"whats_new/#added-leaderboard-cli-command","title":"Added leaderboard CLI command","text":"<p>While the mteb leaderboard before could be run locally, we have now added an official CLI to run the leaderboard, which comes with additional arguments, e.g. for changing which results cache to use, so that e.g. companies can host it with their internal results.</p> <pre><code># Launch leaderboard with custom results directory\nmteb leaderboard --cache-path results\n\n# Launch with specific host and port\nmteb leaderboard --cache-path ./my_results --host 0.0.0.0 --port 8080\n\n# Create public shareable link\nmteb leaderboard --share\n\n# View all options\nmteb leaderboard --help\n</code></pre>"},{"location":"whats_new/#improved-typing-throughout","title":"Improved typing throughout","text":"<p><code>mteb</code> has now added a type checks, which both improved our typing going forward. This also come with a lot of additional typing information.</p>"},{"location":"whats_new/#new-in-v25","title":"New in v2.5","text":""},{"location":"whats_new/#work-with-leaderboard-tables-locally","title":"Work with leaderboard tables locally","text":"<p>If you loaded results for a specific benchmark, you can get the aggregated benchmark scores for each model using the <code>get_benchmark_result()</code> method:</p> <pre><code>import mteb\nfrom mteb.cache import ResultCache\n\n# Load results for a specific benchmark\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\ncache = ResultCache()\ncache.download_from_remote()  # download results from the remote repository\nresults = cache.load_results(\n    models=[\"intfloat/e5-small\", \"intfloat/multilingual-e5-small\"],\n    tasks=benchmark,\n)\n\nbenchmark_scores_df = results.get_benchmark_result()\nprint(benchmark_scores_df)\n#    Rank (Borda)                                              Model  Zero-shot  Memory Usage (MB)  Number of Parameters (B)  Embedding Dimensions  Max Tokens  ...  Classification  Clustering  Pair Classification  Reranking  Retrieval       STS  Summarization\n# 0             1  [e5-small](https://huggingface.co/intfloat/e5-...        100                127                     0.033                   384       512.0  ...        0.599545    0.422085             0.850895   0.444613   0.450684  0.790284       0.310609\n# 1             2  [multilingual-e5-small](https://huggingface.co...         95                449                     0.118                   384       512.0  ...        0.673919    0.413591             0.840878   0.431942   0.464342  0.800185       0.292190\n</code></pre>"},{"location":"whats_new/#new-in-v24","title":"New in v2.4","text":""},{"location":"whats_new/#added-utilities-for-autogenerating-modelmeta","title":"Added utilities for autogenerating <code>ModelMeta</code>","text":"<p>To make it easier to generate high quality metadata from models we created <code>.from_hf_hub</code>, <code>.from_sentence_transformer_model</code> and <code>.from_cross_encoder</code>.</p> <p>This does not fill out everything, but it fills out everything that can be automated. <pre><code>from sentence_transformers import SentenceTransformer\n\nfrom mteb.models import ModelMeta\n\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=\"cpu\")\nmeta = ModelMeta.from_sentence_transformer_model(model)\nprint(meta.to_dict())\n# {'loader_kwargs': {}, 'name': 'Qwen/Qwen3-Embedding-0.6B', 'revision': 'c54f2e6e80b2d7b7de06f51cec4959f6b3e03418', 'release_date': None, 'languages': None, 'n_parameters': 595776512, 'memory_usage_mb': 1136, 'max_tokens': 32768, 'embed_dim': 1024, 'license': 'apache-2.0', 'open_weights': True, 'public_training_code': None, 'public_training_data': None, 'framework': ['Sentence Transformers'], 'reference': None, 'similarity_fn_name': &lt;ScoringFunction.COSINE: 'cosine'&gt;, 'use_instructions': None, 'training_datasets': None, 'adapted_from': None, 'superseded_by': None, 'modalities': ['text'], 'is_cross_encoder': None, 'citation': None, 'contacts': None, 'loader': 'sentence_transformers_loader'}\n</code></pre></p>"},{"location":"whats_new/#new-in-v23","title":"New in v2.3","text":""},{"location":"whats_new/#support-for-custom-search-backends","title":"Support for custom search backends","text":"<p>MTEB v2.3 adds support for custom search encoder IndexEncoderSearchProtocol and adds the FaissSearchIndex.</p> <pre><code>import mteb\nfrom mteb.models import SearchEncoderWrapper\nfrom mteb.models.search_encoder_index import FaissSearchIndex\n\nmodel = mteb.get_model(...)\nindex_backend = FaissSearchIndex(model)\nmodel = SearchEncoderWrapper(\n    model,\n    index_backend=index_backend\n)\n...\n</code></pre> <p>This leads to a slight increase in performance, for example running <code>minishlab/potion-base-2M</code> on <code>SWEbenchVerifiedRR</code> took 694 seconds instead of 769. It, however, does not change the default behaviour.</p>"},{"location":"whats_new/#new-in-v22","title":"New in v2.2","text":""},{"location":"whats_new/#support-for-asymmetric-embeddings-in-sts-and-pairclassification","title":"Support for Asymmetric embeddings in STS and <code>PairClassification</code>","text":"<p>MTEB v2.2 adds support for <code>prompt_type</code> for <code>STS</code> and <code>PairClassification</code> thus allowing for asymmetric embeddings.</p> <p>E.g. for <code>TERRa</code>, this allow us to add <code>TERRa.v2</code>, <pre><code>class TERRaV2(AbsTaskPairClassification):\n    input1_prompt_type = PromptType.document\n    input2_prompt_type = PromptType.query\n\n    metadata = TaskMetadata(\n        name=\"TERRa.V2\", ...\n    )\n</code></pre></p> <p>This is not backward compatible in scores for models with <code>query</code>/<code>document</code> separation, which is why we introduce the v2, but it better reflect the actual performance of these models.</p> <p>Example for <code>intfloat/multilingual-e5-small</code>:</p> Task main PR TERRa.v2 0.575105 0.589083"},{"location":"whats_new/#new-benchmark-vidore-v3","title":"New Benchmark Vidore v3","text":"<p>Added Vidore V3 to the leaderboard (#3542), thanks QuentinJGMace et al for working on this!</p>"},{"location":"whats_new/#added-support-for-python-314","title":"Added support for python 3.14","text":"<p>Support for python 3.14 was added in #3450.</p>"},{"location":"whats_new/#new-in-v21","title":"New in v2.1","text":""},{"location":"whats_new/#new-benchmark-for-dutch","title":"New benchmark for Dutch","text":"<p>MTEB v2.1 introduces a new benchmark for dutch <code>MTEB(nld, v1)</code> (#3464). Thanks to nikolay-banar for the PR.</p>"},{"location":"whats_new/#new-in-v20","title":"New in v2.0","text":"<p>This section goes through new features added in v2. Below we give an overview of changes following by detailed examples.</p> <p>What are the reasons for the changes? Generally the many inconsistencies in the library made it hard to maintain without introducing breaking changes and we do think that there are multiple important areas to expand in, e.g. [adding new benchmark for image embeddings]<sup>1</sup>, support new model types in general making the library more accessible. We have already been able to add many new feature in v2.0, but hope that this new version allow us to keep doing so without breaking backward compatibility. See upgrading from v1 for specific deprecations and how to fix them.</p>"},{"location":"whats_new/#easier-evaluation","title":"Easier evaluation","text":"<p>Evaluations are now a lot easier using <code>mteb.evaluate</code>,</p> <pre><code>results = mteb.evaluate(model, tasks)\n</code></pre>"},{"location":"whats_new/#better-local-and-online-caching","title":"Better local and online caching","text":"<p>The new <code>mteb.ResultCache</code> makes managing the cache notably easier: <pre><code>import mteb\n\nmodel = ...\ntasks = ...\n\ncache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")  # default\n\n# simple evaluate with cache\nresults = mteb.evaluate(model, tasks, cache=cache)  # only runs if results not in cache\n</code></pre></p> <p>It allow you to access the online cache so you don't have to rerun existing models.</p> <pre><code># no need to rerun already public results\ncache.download_from_remote() # download the latest results from the remote repository\nresults = mteb.evaluate(model, tasks, cache=cache)\n</code></pre>"},{"location":"whats_new/#multimodal-input-format","title":"Multimodal Input format","text":"<p>Models in mteb who implements the <code>Encoder</code> protocol now supports multimodal input With the model protocol roughly looking like so:</p> <p><pre><code>class EncoderProtocol(Protocol):  # simplified\n    \"\"\"The interface for an encoder in MTEB.\"\"\"\n\n    def encode(self, inputs: DataLoader[BatchedInput], ...) -&gt; Array: ...\n</code></pre> Not only does this allow more efficient loading using the torch dataloader, but it also allows keys for multiple modalities:</p> <pre><code>batch_input: BatchedInput = {\n    \"text\": list[str],\n    \"images\": list[PIL.Image],\n    \"audio\": list[list[audio]], # upcoming\n    # + optional fields such as document title\n}\n</code></pre> <p>Where <code>text</code> is a batch of texts and <code>list[images]</code> is a batch for that texts. This e.g. allows markdown documents with multiple figures like so:</p> <pre><code>&gt; As you see in the following figure [figure 1](image_1) there is a correlation between A and B.\n</code></pre> <p>Note</p> <p>More examples of new multimodal inputs you can find in BatchedInput documentation.</p> <p>However, this also allows no text, multi-image inputs (e.g. for PDFs). Overall this greatly expands the possible tasks that can now be evaluated in MTEB. To see how to convert a legacy model see the converting model section.</p>"},{"location":"whats_new/#better-support-for-crossencoders","title":"Better support for CrossEncoders","text":"<p>Also, we've introduced a new <code>CrossEncoderProtocol</code> for cross-encoders and now all cross-encoders have better support for evaluation:</p> <pre><code>class CrossEncoderProtocol(Protocol):\n    def predict(\n        self,\n        inputs1: DataLoader[BatchedInput],\n        inputs2: DataLoader[BatchedInput],\n        ...\n    ) -&gt; Array:\n</code></pre>"},{"location":"whats_new/#unified-retrieval-reranking-and-instruction-variants","title":"Unified Retrieval, Reranking and instruction variants","text":"<p>The retrieval tasks in MTEB now supports both retrieval and reranking using the same base task. The main difference now that Reranking tasks should have <code>top_ranked</code> subset to be evaluated on. New structure of retrieval tasks: <code>dataset[subset][split]</code> = RetrievalSplitData. On HF this dataset should these subsets:</p> <ol> <li><code>Corpus</code> - the corpus to retrieve from. Monolingual name: <code>corpus</code>, multilingual name: <code>{subset}-corpus</code>. Can contain columns:</li> <li><code>id</code>, <code>text</code>, <code>title</code> for text corpus</li> <li><code>id</code>, <code>image</code>, (<code>text</code> optionally) for image or multimodal corpus</li> <li><code>Queries</code> - the queries to retrieve with. Monolingual name: <code>queries</code>, multilingual name: <code>{subset}-queries</code>.</li> <li><code>id</code>, <code>text</code> for text queries. Where text can be str for single query or <code>list[str]</code> or <code>Conversation</code> for multi-turn dialogs queries.</li> <li><code>id</code>, <code>text</code>, <code>instructions</code> for instruction retrieval/reranking tasks</li> <li><code>id</code>, <code>image</code>, (<code>text</code> optionally) for image or multimodal queries</li> <li><code>Qrels</code> - the relevance judgements. Monolingual name: <code>qrels</code>, multilingual name: <code>{subset}-qrels</code>.       <code>query-id</code>, <code>corpus-id</code>, <code>score</code> (int or float) for relevance judgements.</li> <li><code>Top Ranked</code> - the top ranked documents to rerank. Only for reranking tasks. Monolingual name: <code>top_ranked</code>, multilingual name: <code>{subset}-top_ranked</code>.       <code>query-id</code>, <code>corpus-ids</code> (<code>list[str]</code>) - the top ranked documents for each query.</li> </ol>"},{"location":"whats_new/#search-interface","title":"Search Interface","text":"<p>To make it easier to use MTEB for search, we have added a simple search interface using the new <code>SearchProtocol</code>:</p> <pre><code>class SearchProtocol(Protocol):\n    \"\"\"Interface for searching models.\"\"\"\n\n    def index(\n        self,\n        corpus: CorpusDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: dict[str, Any],\n    ) -&gt; None:\n        ...\n\n    def search(\n        self,\n        queries: QueryDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        top_k: int,\n        encode_kwargs: dict[str, Any],\n        top_ranked: TopRankedDocumentsType | None = None,\n    ) -&gt; RetrievalOutputType:\n        ...\n</code></pre> <p>We're automatically wrapping <code>Encoder</code> and <code>CrossEncoder</code> models support <code>SearchProtocol</code>. However, if your model needs a custom index you can implement this protocol directly, like was done for colbert-like models.</p>"},{"location":"whats_new/#new-documentation","title":"New Documentation","text":"<p>We've added a lot of new documentation to make it easier to get started with MTEB.</p> <ul> <li>You can see api of our models in tasks in API documentation.</li> <li>We've added a getting started guide to help you get started with MTEB.</li> <li>You can see implemented tasks and models in MTEB.</li> </ul>"},{"location":"whats_new/#better-support-for-loading-and-comparing-results","title":"Better support for loading and comparing results","text":"<p>The new <code>ResultCache</code> also makes it easier to load, inspect and compare both local and online results:</p> <pre><code>import mteb\n\ncache = mteb.ResultCache(cache_path=\"~/.cache/mteb\") # default\ncache.download_from_remote() # download the latest results from the remote repository\n\n# load both local and online results\nresults = cache.load_results(models=[\"sentence-transformers/all-MiniLM-L6-v2\", ...], tasks=[\"STS12\"])\ndf = results.to_dataframe()\n</code></pre>"},{"location":"whats_new/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>Descriptive statistics isn't a new thing in MTEB, however, now it is there for every task, to extract it simply run:</p> <pre><code>import mteb\ntask = mteb.get_task(\"MIRACLRetrievalHardNegatives\")\n\ntask.metadata.descriptive_stats\n</code></pre> <p>And you will get a highly detailed set of descriptive statistics covering everything from number of samples query lengths, duplicates, etc. These not only make it easier for you to examine tasks, but it also makes it easier for us to make quality checks on future tasks.</p> <p>Example for reranking task: <pre><code>{\n    \"test\": {\n        \"num_samples\": 160,\n        \"number_of_characters\": 310133,\n        \"documents_text_statistics\": {\n            \"total_text_length\": 307938,\n            \"min_text_length\": 0,\n            \"average_text_length\": 2199.557142857143,\n            \"max_text_length\": 2710,\n            \"unique_texts\": 140\n        },\n        \"documents_image_statistics\": null,\n        \"queries_text_statistics\": {\n            \"total_text_length\": 2195,\n            \"min_text_length\": 55,\n            \"average_text_length\": 109.75,\n            \"max_text_length\": 278,\n            \"unique_texts\": 20\n        },\n        \"queries_image_statistics\": null,\n        \"relevant_docs_statistics\": {\n            \"num_relevant_docs\": 60,\n            \"min_relevant_docs_per_query\": 7,\n            \"average_relevant_docs_per_query\": 3.0,\n            \"max_relevant_docs_per_query\": 7,\n            \"unique_relevant_docs\": 140\n        },\n        \"top_ranked_statistics\": {\n            \"num_top_ranked\": 140,\n            \"min_top_ranked_per_query\": 7,\n            \"average_top_ranked_per_query\": 7.0,\n            \"max_top_ranked_per_query\": 7\n        }\n    }\n}\n</code></pre></p> <p>Documentation for the descriptive statistics types.</p>"},{"location":"whats_new/#saving-predictions","title":"Saving Predictions","text":"<p>To support error analysis it is now possible to save the model prediction on a given task. You can do this simply as follows: <pre><code>import mteb\n\n# using a small model and small dataset\nencoder = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\ntask = mteb.get_task(\"NanoArguAnaRetrieval\")\n\nprediction_folder = \"path/to/model_predictions\"\n\nres = mteb.evaluate(\n    encoder,\n    task,\n    prediction_folder=prediction_folder,\n)\n</code></pre></p> <p>Result of prediction will be saved in <code>path/to/model_predictions/{task_name}_predictions.json</code> and will look like so for retrieval tasks:</p> <pre><code>{\n  \"test\": {\n        \"query1\": {\"document1\": 0.77, \"document2\": 0.12, ...},\n        \"query2\": {\"document2\": 0.87, \"document1\": 0.32, ...},\n        ...\n    }\n}\n</code></pre>"},{"location":"whats_new/#support-datasets-v4","title":"Support datasets v4","text":"<p>With the new functionality for reuploading datasets to the standard datasets Parquet format, we\u2019ve reuploaded all tasks with <code>trust_remote_code</code>, and MTEB now fully supports Datasets v4.</p>"},{"location":"whats_new/#upgrading-from-v1","title":"Upgrading from v1","text":"<p>This section gives an introduction of how to upgrade from v1 to v2.</p>"},{"location":"whats_new/#replacing-mtebmteb","title":"Replacing <code>mteb.MTEB</code>","text":"<p>The previous approach to evaluate would require you to first create <code>MTEB</code> object and then call <code>.run</code> on that object. The <code>MTEB</code> object was initially a sort of catch all object intended for both filtering tasks, selecting tasks, evaluating and few other cases.</p> <p>This overload of functionality made it hard to change. We have already for a while made it easier to filter and select tasks using <code>get_tasks</code> and <code>mteb.evaluate</code> now superseded <code>MTEB</code> as the method for evaluation.</p> <pre><code># Approach before 2.0.0:\neval = mteb.MTEB(tasks=tasks) # now throw a deprecation warning\nresults = eval.run(\n    model,\n    overwrite=True,\n    encode_kwargs={},\n    ...\n)\n\n# Recommended:\nmteb.evaluate(\n    model,\n    tasks,\n    overwrite_strategy=\"only-missing\", # only rerun missing splits\n    encode_kwargs={},\n    ...\n)\n</code></pre>"},{"location":"whats_new/#replacing-mtebload_results","title":"Replacing <code>mteb.load_results()</code>","text":"<p>Given the new <code>ResultCache</code> makes dealing with a results from both local and online caches a lot easier, it can now replace <code>mteb.load_results</code> it</p> <pre><code>tasks = mteb.get_tasks(tasks=[\"STS12\"])\nmodel_names = [\"intfloat/multilingual-e5-large\"]\n\n# Approach before 2.0.0:\nresults = mteb.load_results(models=model_names, tasks=tasks, download_latest=True)\n\n# Recommended:\ncache = ResultCache(\"~/.cache/mteb\")  # default\ncache.download_from_remote()  # downloads remote results\n\nresults = cache.load_results(models=model_names, tasks=tasks)\n</code></pre>"},{"location":"whats_new/#converting-model-to-new-format","title":"Converting model to new format","text":"<p>As mentioned in the above section MTEB v2, now supports multimodal input as the default. Luckily for you all models implemented in MTEB already supports this new format! However, if you have a local model that you would like to evaluate Here is a quick conversion guide. If you previous implementation looks like so:</p> <pre><code># v1.X.X\nclass MyDummyEncoder:\n    def __init__(self, **kwargs):\n        self.model = ...\n\n    def encode(self, sentences: list[str], **kwargs) -&gt; Array:\n        embeddings = self.model.encode(sentences)\n        return embeddings\n</code></pre> <p>You can simply unpack it to its text input like so:</p> <pre><code># v2.0.0\nclass MyDummyEncoder:\n    def __init__(self, **kwargs):\n        self.model = ...\n\n    def encode(self, input: DataLoader[BatchedInput], **kwargs) -&gt; Array:\n        # unpack to v1 format:\n        sentences = [text for batch in inputs for text in batch[\"text\"]]\n        # do as you did beforehand:\n        embeddings = self.model.encode(sentences)\n        return embeddings\n</code></pre> <p>Of course, it will be more efficient if you work directly with the dataloader.</p>"},{"location":"whats_new/#reuploading-datasets","title":"Reuploading datasets","text":"<p>If your dataset is in old format, or you want to reupload it to the new Parquet format, you can do so using the new <code>push_dataset_to_hub</code> method:</p> <pre><code>import mteb\n\ntask = mteb.get_task(\"MyOldTask\")\ntask.push_dataset_to_hub(\"my-username/my-new-task\")\n</code></pre>"},{"location":"whats_new/#converting-reranking-datasets-to-new-format","title":"Converting Reranking datasets to new format","text":"<p>If you have a reranking dataset, you can convert it to the retrieval format. To do this you need to add your task name to the <code>mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS</code> and after this it would be converted to the new format automatically. To reupload them in new reranking format you refer to the reuploading datasets section.</p> <pre><code>import mteb\nfrom mteb.abstasks.text.reranking import OLD_FORMAT_RERANKING_TASKS\n\nOLD_FORMAT_RERANKING_TASKS.append(\"MyOldRerankingTask\")\n\ntask = mteb.get_task(\"MyOldRerankingTask\")\nmodel = ...\nmteb.evaluate(model, task)\n</code></pre> <ol> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"advanced_usage/cache_embeddings/","title":"Cache embeddings","text":""},{"location":"advanced_usage/cache_embeddings/#caching-embeddings-to-re-use-them","title":"Caching Embeddings To Re-Use Them","text":"<p>There are times you may want to cache the embeddings so you can re-use them. This may be true if you have multiple query sets for the same corpus (e.g. Wikipedia) or are doing some optimization over the queries (e.g. prompting, other experiments). You can setup a cache by using a simple wrapper, which will save the cache per task in the <code>&lt;path_to_cache_dir&gt;/&lt;task_name&gt;</code> folder:</p> <pre><code>import mteb\n\n# define your task(s) and model above as normal\ntask = mteb.get_task(\"LccSentimentClassification\")\nmodel = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\n\n# wrap the model with the cache wrapper\nfrom mteb.models.cache_wrappers import CachedEmbeddingWrapper\nmodel_with_cached_emb = CachedEmbeddingWrapper(model, cache_path='path_to_cache_dir')\n# run as normal\nresults = mteb.evaluate(model_with_cached_emb, tasks=[task])\n</code></pre> <p>If you want to directly access the cached embeddings (e.g. for subsequent analyses) follow this example:</p> <pre><code>import numpy as np\nfrom mteb.models.cache_wrappers.cache_backends import NumpyCache\n\n# Access the memory-mapped file and convert to array\nvector_map = NumpyCache(\"path_to_cache_dir/LccSentimentClassification\")\nvector_map.load()\nvectors = np.asarray(vector_map.vectors)\n\n# Remove all \"placeholders\" in the embedding cache\nzero_mask = (vectors == 0).all(axis=1)\nvectors = vectors[~zero_mask]\n</code></pre>"},{"location":"advanced_usage/cache_embeddings/#different-cache-backends","title":"Different Cache backends","text":"<p>By default, the <code>CachedEmbeddingWrapper</code> uses a NumPy memmap backend (<code>NumpyCache</code>) to store embeddings. However, you can also use other backends. Currently, only <code>FAISS</code> is implemented, but you can provide your own custom backend that implements the <code>CacheBackendProtocol</code> by passing it as the <code>cache_backend</code> parameter when initializing the <code>CachedEmbeddingWrapper</code>. For example:</p> <pre><code>import mteb\nfrom mteb.models.cache_wrappers.cache_backends import FaissCache\nfrom mteb.models import CachedEmbeddingWrapper\n\nmodel = mteb.get_model(...)\ncachedmodel = CachedEmbeddingWrapper(model, \"cache_dir\", cache_backend=FaissCache)\n</code></pre>"},{"location":"advanced_usage/retrieval_backend/","title":"Retrieval Search backend","text":"<p>Available since 2.3.0</p> <p>This feature was introduced in version 2.3.0.</p> <p>For some large dataset search can take a lot of time and memory. To reduce this you can use <code>FaissSearchIndex</code>. To work with it install:</p> <pre><code># pip\npip install mteb[faiss-cpu]\n\n# or uv\nuv add \"mteb[faiss-cpu]\"\n</code></pre> <p>Usage example: <pre><code>import mteb\nfrom mteb.models import SearchEncoderWrapper\nfrom mteb.models.search_encoder_index import FaissSearchIndex\n\nmodel = mteb.get_model(...)\nindex_backend = FaissSearchIndex(model)\nmodel = SearchEncoderWrapper(\n    model,\n    index_backend=index_backend\n)\n...\n</code></pre></p> <p>For example running <code>minishlab/potion-base-2M</code> on <code>SWEbenchVerifiedRR</code> took 694 seconds instead of 769.</p>"},{"location":"advanced_usage/two_stage_reranking/","title":"Two stage reranking","text":""},{"location":"advanced_usage/two_stage_reranking/#two-stage-reranking","title":"Two stage reranking","text":"<p>To use a cross encoder for reranking on a retrieval task you first need a task, a stage 1 model and our cross-encoder.</p> <pre><code>import mteb\n\ntask = mteb.get_task(\"NanoArguAnaRetrieval\")\n# stage 1 model:\nencoder = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\n# stage 2 model:\ncross_encoder = mteb.get_model(\"cross-encoder/ms-marco-TinyBERT-L-2-v2\") # (1)\n</code></pre> <ol> <li>You can also directly use <code>CrossEncoder</code> from sentence transformers.</li> </ol> <p>Once we have that we can perform stage 1 retrieval, followed by a stage 2 reranking (call <code>convert_to_reranking</code> to convert the task to a reranking task, which will use the predictions from stage 1 as input for stage 2):</p> <pre><code>prediction_folder = \"model_predictions\"\n\n# stage 1: retrieval\nres = mteb.evaluate(\n    encoder,\n    task,\n    prediction_folder=prediction_folder,\n)\n\n# convert task to retrieval\ntask = task.convert_to_reranking(prediction_folder, top_k=100)\n\n# stage 2: reranking\ncross_enc_results = mteb.evaluate(cross_encoder, task)\n\nprint(task.metadata.main_score) # NDCG@10\nres[0].get_score()  # 0.286\ncross_enc_results[0].get_score() # 0.338\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/","title":"vLLM Wrapper","text":""},{"location":"advanced_usage/vllm_wrapper/#vllm","title":"vLLM","text":"<p>Note</p> <p>vLLM currently supports only a limited number of models, and many implementations have subtle differences compared to the default implementations in mteb. For the full list of supported models, refer to the vllm documentation.</p>"},{"location":"advanced_usage/vllm_wrapper/#installation","title":"Installation","text":"<p>If you're using cuda you can run</p> pipuv <pre><code>pip install \"mteb[vllm]\"\n</code></pre> <pre><code>uv pip install \"mteb[vllm]\"\n</code></pre> <p>For other architectures, please refer to the vllm installation guide.</p>"},{"location":"advanced_usage/vllm_wrapper/#usage","title":"Usage","text":"<p>To use vLLM with MTEB you have to wrap the model with its respective wrapper.</p> <p>Note</p> <p>you must update your Python code to guard usage of vllm behind a if name == 'main': block. For example, instead of this:</p> <p><pre><code>import vllm\n\nllm = vllm.LLM(...)\n</code></pre> try this instead: <pre><code>if __name__ == '__main__':\n    import vllm\n\n    llm = vllm.LLM(...)\n</code></pre></p> <p>See more troubleshooting</p> Embedding modelsReranking models <pre><code>import mteb\nfrom mteb.models.vllm_wrapper import VllmEncoderWrapper\n\ndef run_vllm_encoder():\n    \"\"\"Evaluate a model on specified MTEB tasks using vLLM for inference.\"\"\"\n    encoder = VllmEncoderWrapper(model=\"intfloat/e5-small\")\n    return mteb.evaluate(\n        encoder,\n        mteb.get_task(\"STS12\"),\n    )\n\nif __name__ == \"__main__\":\n    results = run_vllm_encoder()\n    print(results)\n</code></pre> <pre><code>import mteb\nfrom mteb.models.vllm_wrapper import VllmCrossEncoderWrapper\n\ndef run_vllm_crossencoder():\n    \"\"\"Evaluate a model on specified MTEB tasks using vLLM for inference.\"\"\"\n    cross_encoder = VllmCrossEncoderWrapper(model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n    return mteb.evaluate(\n        cross_encoder,\n        mteb.get_task(\"AskUbuntuDupQuestions\"),\n    )\n\n\nif __name__ == \"__main__\":\n    results = run_vllm_crossencoder()\n    print(results)\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#why-is-vllm-fast","title":"Why is vLLM fast?","text":""},{"location":"advanced_usage/vllm_wrapper/#half-precision-inference","title":"Half-Precision Inference","text":"<p>By default, vLLM uses Flash Attention, which only supports float16 and bfloat16 but not float32. vLLM does not optimize inference performance for float32.</p> The throughput using float16 is approximately four times that of float32. ST: using sentence transformers backend vLLM: using vLLM backend X-axis: Throughput (request/s) Y-axis: Latency, Time needed for one step (ms) &lt;- logarithmic scale The curve lower right is better \u2198  <p>Note</p> Format Bits Exponent Fraction float32 32 8 23 float16 16 5 10 bfloat16 16 8 7 <p>If the model weights are stored in float32:</p> <ul> <li>VLLM uses float16 for inference by default to inference a float32 model, it will keep numerical precision in most cases, for it have retains relatively more Fraction bits. However, due to the smaller Exponent part (only 5 bits), some models (e.g., the Gemma family) may risk producing NaN. VLLM maintains a list models that may cause NaN values and uses bfloat16 for inference by default.</li> <li>Using bfloat16 for inference avoids NaN risks because its Exponent part matches float32 with 8 bits. However, with only 7 Fraction bits, numerical precision decreases noticeably.</li> <li>Using float32 for inference incurs no precision loss but is about four times slower than float16/bfloat16.</li> </ul> <p>If model weights are stored in float16 or bfloat16, vLLM defaults to using the original dtype for inference.</p> <p>Quantization: With the advancement of open-source large models, fine-tuning of larger models for tasks like embedding and reranking is increasing. Exploring quantization methods to accelerate inference and reduce GPU memory usage may become necessary.</p>"},{"location":"advanced_usage/vllm_wrapper/#unpadding","title":"Unpadding","text":"<p>By default, Sentence Transformers (st) pads all inputs in a batch to the length of the longest one, which is undoubtedly very inefficient. VLLM avoids padding entirely during inference.</p> X-axis: Throughput (request/s) ST: using sentence transformers vLLM: using vLLM Y-axis: Latency, Time needed for one step (ms) &lt;- logarithmic scale The curve lower right is better \u2198  <p>Sentence Transformers (st) suffers a noticeable drop in speed when handling requests with varied input lengths, whereas vLLM does not.</p>"},{"location":"advanced_usage/vllm_wrapper/#others","title":"Others","text":"<p>For models using bidirectional attention, such as BERT, VLLM offers a range of performance optimizations:</p> <ul> <li>Optimized CUDA kernels, including FlashAttention and FlashInfer integration</li> <li>CUDA Graphs and <code>torch.compile</code> support to reduce overhead and accelerate execution</li> <li>Support for tensor, pipeline, data, and expert parallelism for distributed inference</li> <li>Multiple quantization schemes\u2014GPTQ, AWQ, AutoRound, INT4, INT8, and FP8\u2014for efficient deployment</li> <li>Continuous batching of incoming requests to maximize throughput</li> </ul> <p>For causal attention models, such as the Qwen3 reranker, the following optimizations are also applicable:</p> <ul> <li>Efficient KV cache memory management via PagedAttention</li> <li>Chunked prefill for improved memory handling during long-context processing</li> <li>Prefix caching to accelerate repeated prompt processing</li> </ul> <p>vLLM\u2019s optimizations are primarily designed for and most effective with causal language models (generative models). For the full list of features, refer to the vllm documentation.</p>"},{"location":"advanced_usage/vllm_wrapper/#api-reference","title":"API Reference","text":"<p>Info</p> <p>For all vLLM parameters, please refer to https://docs.vllm.ai/en/latest/configuration/engine_args/.</p>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmWrapperBase","title":"<code>mteb.models.vllm_wrapper.VllmWrapperBase</code>","text":"<p>Wrapper for vllm serving engine.</p> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>class VllmWrapperBase:\n    \"\"\"Wrapper for vllm serving engine.\"\"\"\n\n    convert = \"auto\"\n    mteb_model_meta: ModelMeta | None = None\n\n    def __init__(\n        self,\n        model: str | ModelMeta,\n        revision: str | None = None,\n        *,\n        trust_remote_code: bool = True,\n        dtype: Dtype = \"auto\",\n        head_dtype: Literal[\"model\"] | Dtype | None = None,\n        max_model_len: int | None = None,\n        max_num_batched_tokens: int | None = None,\n        max_num_seqs: int = 128,\n        tensor_parallel_size: int = 1,\n        enable_prefix_caching: bool | None = None,\n        gpu_memory_utilization: float = 0.9,\n        hf_overrides: dict[str, Any] | None = None,\n        pooler_config: PoolerConfig | None = None,\n        enforce_eager: bool = False,\n        **kwargs: Any,\n    ):\n        \"\"\"Wrapper for vllm serving engine.\n\n        Args:\n            model: model name string.\n            revision: The revision of the model to use.\n            trust_remote_code: Whether to trust remote code execution when loading the model.\n                Should be True for models with custom code.\n            dtype: Data type for model weights. \"auto\" will automatically select appropriate\n                dtype based on hardware and model capabilities. vllm uses flash attention by\n                default, which does not support fp32. Therefore, it defaults to using fp16 for\n                inference on fp32 models. Testing has shown a relatively small drop in accuracy.\n                You can manually opt for fp32, but inference speed will be very slow.\n            head_dtype: \"head\" refers to the last Linear layer(s) of an LLMs, such as the score\n                or classifier in a classification model. Uses fp32 for the head by default to\n                gain extra precision.\n            max_model_len: Maximum sequence length (context window) supported by the model.\n                If None, uses the model's default maximum length.\n            max_num_batched_tokens: Maximum number of tokens to process in a single batch.\n                If None, automatically determined.\n            max_num_seqs: Maximum number of sequences to process concurrently.\n            tensor_parallel_size: Number of GPUs for tensor parallelism.\n            enable_prefix_caching: Whether to enable KV cache sharing for common prompt prefixes.\n                If None, uses the model's default setting.\n            gpu_memory_utilization: Target GPU memory utilization ratio (0.0 to 1.0).\n            hf_overrides: Dictionary mapping Hugging Face configuration keys to override values.\n            pooler_config: Controls the behavior of output pooling in pooling models.\n            enforce_eager: Whether to disable CUDA graph optimization and use eager execution.\n            **kwargs: Additional arguments to pass to the vllm serving engine model.\n        \"\"\"\n        requires_package(\n            self,\n            \"vllm\",\n            \"Wrapper for vllm serving engine\",\n            install_instruction=\"pip install mteb[vllm]\",\n        )\n\n        os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n\n        from vllm import LLM, EngineArgs\n\n        hf_overrides = {} if hf_overrides is None else hf_overrides\n\n        if head_dtype is not None:\n            hf_overrides[\"head_dtype\"] = head_dtype\n\n        model_name = model if isinstance(model, str) else model.name\n\n        if isinstance(model, ModelMeta):\n            logger.info(\n                \"Using revision from model meta. Passed revision will be ignored\"\n            )\n            revision = model.revision\n\n        args = EngineArgs(\n            model=model_name,\n            revision=revision,\n            runner=\"pooling\",\n            convert=self.convert,  # type: ignore[arg-type]\n            max_model_len=max_model_len,\n            max_num_batched_tokens=max_num_batched_tokens,\n            max_num_seqs=max_num_seqs,\n            tensor_parallel_size=tensor_parallel_size,\n            enable_prefix_caching=enable_prefix_caching,\n            gpu_memory_utilization=gpu_memory_utilization,\n            hf_overrides=hf_overrides,\n            pooler_config=pooler_config,\n            enforce_eager=enforce_eager,\n            trust_remote_code=trust_remote_code,\n            dtype=dtype,\n            **kwargs,\n        )\n        self.llm = LLM(**vars(args))\n\n        if isinstance(model, str):\n            self.mteb_model_meta = ModelMeta.from_hub(model=model, revision=revision)\n        else:\n            self.mteb_model_meta = model\n\n        atexit.register(self.cleanup)\n\n    def cleanup(self):\n        \"\"\"Clean up the VLLM distributed runtime environment and release GPU resources.\"\"\"\n        if self.llm is None:\n            return\n\n        from vllm.distributed import (  # type: ignore[import-not-found]\n            cleanup_dist_env_and_memory,\n        )\n\n        self.llm = None\n        gc.collect()\n        cleanup_dist_env_and_memory()\n\n    def __del__(self):\n        try:\n            self.cleanup()\n        except Exception:\n            pass\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmWrapperBase.__init__","title":"<code>__init__(model, revision=None, *, trust_remote_code=True, dtype='auto', head_dtype=None, max_model_len=None, max_num_batched_tokens=None, max_num_seqs=128, tensor_parallel_size=1, enable_prefix_caching=None, gpu_memory_utilization=0.9, hf_overrides=None, pooler_config=None, enforce_eager=False, **kwargs)</code>","text":"<p>Wrapper for vllm serving engine.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | ModelMeta</code> <p>model name string.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model to use.</p> <code>None</code> <code>trust_remote_code</code> <code>bool</code> <p>Whether to trust remote code execution when loading the model. Should be True for models with custom code.</p> <code>True</code> <code>dtype</code> <code>Dtype</code> <p>Data type for model weights. \"auto\" will automatically select appropriate dtype based on hardware and model capabilities. vllm uses flash attention by default, which does not support fp32. Therefore, it defaults to using fp16 for inference on fp32 models. Testing has shown a relatively small drop in accuracy. You can manually opt for fp32, but inference speed will be very slow.</p> <code>'auto'</code> <code>head_dtype</code> <code>Literal['model'] | Dtype | None</code> <p>\"head\" refers to the last Linear layer(s) of an LLMs, such as the score or classifier in a classification model. Uses fp32 for the head by default to gain extra precision.</p> <code>None</code> <code>max_model_len</code> <code>int | None</code> <p>Maximum sequence length (context window) supported by the model. If None, uses the model's default maximum length.</p> <code>None</code> <code>max_num_batched_tokens</code> <code>int | None</code> <p>Maximum number of tokens to process in a single batch. If None, automatically determined.</p> <code>None</code> <code>max_num_seqs</code> <code>int</code> <p>Maximum number of sequences to process concurrently.</p> <code>128</code> <code>tensor_parallel_size</code> <code>int</code> <p>Number of GPUs for tensor parallelism.</p> <code>1</code> <code>enable_prefix_caching</code> <code>bool | None</code> <p>Whether to enable KV cache sharing for common prompt prefixes. If None, uses the model's default setting.</p> <code>None</code> <code>gpu_memory_utilization</code> <code>float</code> <p>Target GPU memory utilization ratio (0.0 to 1.0).</p> <code>0.9</code> <code>hf_overrides</code> <code>dict[str, Any] | None</code> <p>Dictionary mapping Hugging Face configuration keys to override values.</p> <code>None</code> <code>pooler_config</code> <code>PoolerConfig | None</code> <p>Controls the behavior of output pooling in pooling models.</p> <code>None</code> <code>enforce_eager</code> <code>bool</code> <p>Whether to disable CUDA graph optimization and use eager execution.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the vllm serving engine model.</p> <code>{}</code> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>def __init__(\n    self,\n    model: str | ModelMeta,\n    revision: str | None = None,\n    *,\n    trust_remote_code: bool = True,\n    dtype: Dtype = \"auto\",\n    head_dtype: Literal[\"model\"] | Dtype | None = None,\n    max_model_len: int | None = None,\n    max_num_batched_tokens: int | None = None,\n    max_num_seqs: int = 128,\n    tensor_parallel_size: int = 1,\n    enable_prefix_caching: bool | None = None,\n    gpu_memory_utilization: float = 0.9,\n    hf_overrides: dict[str, Any] | None = None,\n    pooler_config: PoolerConfig | None = None,\n    enforce_eager: bool = False,\n    **kwargs: Any,\n):\n    \"\"\"Wrapper for vllm serving engine.\n\n    Args:\n        model: model name string.\n        revision: The revision of the model to use.\n        trust_remote_code: Whether to trust remote code execution when loading the model.\n            Should be True for models with custom code.\n        dtype: Data type for model weights. \"auto\" will automatically select appropriate\n            dtype based on hardware and model capabilities. vllm uses flash attention by\n            default, which does not support fp32. Therefore, it defaults to using fp16 for\n            inference on fp32 models. Testing has shown a relatively small drop in accuracy.\n            You can manually opt for fp32, but inference speed will be very slow.\n        head_dtype: \"head\" refers to the last Linear layer(s) of an LLMs, such as the score\n            or classifier in a classification model. Uses fp32 for the head by default to\n            gain extra precision.\n        max_model_len: Maximum sequence length (context window) supported by the model.\n            If None, uses the model's default maximum length.\n        max_num_batched_tokens: Maximum number of tokens to process in a single batch.\n            If None, automatically determined.\n        max_num_seqs: Maximum number of sequences to process concurrently.\n        tensor_parallel_size: Number of GPUs for tensor parallelism.\n        enable_prefix_caching: Whether to enable KV cache sharing for common prompt prefixes.\n            If None, uses the model's default setting.\n        gpu_memory_utilization: Target GPU memory utilization ratio (0.0 to 1.0).\n        hf_overrides: Dictionary mapping Hugging Face configuration keys to override values.\n        pooler_config: Controls the behavior of output pooling in pooling models.\n        enforce_eager: Whether to disable CUDA graph optimization and use eager execution.\n        **kwargs: Additional arguments to pass to the vllm serving engine model.\n    \"\"\"\n    requires_package(\n        self,\n        \"vllm\",\n        \"Wrapper for vllm serving engine\",\n        install_instruction=\"pip install mteb[vllm]\",\n    )\n\n    os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n\n    from vllm import LLM, EngineArgs\n\n    hf_overrides = {} if hf_overrides is None else hf_overrides\n\n    if head_dtype is not None:\n        hf_overrides[\"head_dtype\"] = head_dtype\n\n    model_name = model if isinstance(model, str) else model.name\n\n    if isinstance(model, ModelMeta):\n        logger.info(\n            \"Using revision from model meta. Passed revision will be ignored\"\n        )\n        revision = model.revision\n\n    args = EngineArgs(\n        model=model_name,\n        revision=revision,\n        runner=\"pooling\",\n        convert=self.convert,  # type: ignore[arg-type]\n        max_model_len=max_model_len,\n        max_num_batched_tokens=max_num_batched_tokens,\n        max_num_seqs=max_num_seqs,\n        tensor_parallel_size=tensor_parallel_size,\n        enable_prefix_caching=enable_prefix_caching,\n        gpu_memory_utilization=gpu_memory_utilization,\n        hf_overrides=hf_overrides,\n        pooler_config=pooler_config,\n        enforce_eager=enforce_eager,\n        trust_remote_code=trust_remote_code,\n        dtype=dtype,\n        **kwargs,\n    )\n    self.llm = LLM(**vars(args))\n\n    if isinstance(model, str):\n        self.mteb_model_meta = ModelMeta.from_hub(model=model, revision=revision)\n    else:\n        self.mteb_model_meta = model\n\n    atexit.register(self.cleanup)\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmWrapperBase.cleanup","title":"<code>cleanup()</code>","text":"<p>Clean up the VLLM distributed runtime environment and release GPU resources.</p> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>def cleanup(self):\n    \"\"\"Clean up the VLLM distributed runtime environment and release GPU resources.\"\"\"\n    if self.llm is None:\n        return\n\n    from vllm.distributed import (  # type: ignore[import-not-found]\n        cleanup_dist_env_and_memory,\n    )\n\n    self.llm = None\n    gc.collect()\n    cleanup_dist_env_and_memory()\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmEncoderWrapper","title":"<code>mteb.models.vllm_wrapper.VllmEncoderWrapper</code>","text":"<p>               Bases: <code>AbsEncoder</code>, <code>VllmWrapperBase</code></p> <p>vLLM wrapper for Encoder models.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str | ModelMeta</code> <p>model name string or ModelMeta.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model to use.</p> <code>None</code> <code>prompt_dict</code> <code>dict[str, str] | None</code> <p>A dictionary mapping task names to prompt strings.</p> <code>None</code> <code>use_instructions</code> <code>bool</code> <p>Whether to use instructions from the prompt_dict. When False, values from prompt_dict are used as static prompts (prefixes). When True, values from prompt_dict are used as instructions to be formatted using the instruction_template.</p> <code>False</code> <code>instruction_template</code> <code>str | Callable[[str, PromptType | None], str] | None</code> <p>A template or callable to format instructions. Can be a string with '{instruction}' placeholder or a callable that takes the instruction and prompt type and returns a formatted string.</p> <code>None</code> <code>apply_instruction_to_documents</code> <code>bool</code> <p>Whether to apply instructions to documents prompts.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the vllm serving engine model.</p> <code>{}</code> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>class VllmEncoderWrapper(AbsEncoder, VllmWrapperBase):\n    \"\"\"vLLM wrapper for Encoder models.\n\n    Args:\n        model: model name string or ModelMeta.\n        revision: The revision of the model to use.\n        prompt_dict: A dictionary mapping task names to prompt strings.\n        use_instructions: Whether to use instructions from the prompt_dict.\n            When False, values from prompt_dict are used as static prompts (prefixes).\n            When True, values from prompt_dict are used as instructions to be formatted\n            using the instruction_template.\n        instruction_template: A template or callable to format instructions.\n            Can be a string with '{instruction}' placeholder or a callable that takes\n            the instruction and prompt type and returns a formatted string.\n        apply_instruction_to_documents: Whether to apply instructions to documents prompts.\n        **kwargs: Additional arguments to pass to the vllm serving engine model.\n    \"\"\"\n\n    convert = \"embed\"\n\n    def __init__(\n        self,\n        model: str | ModelMeta,\n        revision: str | None = None,\n        prompt_dict: dict[str, str] | None = None,\n        use_instructions: bool = False,\n        instruction_template: (\n            str | Callable[[str, PromptType | None], str] | None\n        ) = None,\n        apply_instruction_to_documents: bool = True,\n        **kwargs: Any,\n    ):\n        if use_instructions and instruction_template is None:\n            raise ValueError(\n                \"To use instructions, an instruction_template must be provided. \"\n                \"For example, `Instruction: {instruction}`\"\n            )\n\n        if (\n            isinstance(instruction_template, str)\n            and \"{instruction}\" not in instruction_template\n        ):\n            raise ValueError(\n                \"Instruction template must contain the string '{instruction}'.\"\n            )\n\n        self.prompts_dict = prompt_dict\n        self.use_instructions = use_instructions\n        self.instruction_template = instruction_template\n        self.apply_instruction_to_passages = apply_instruction_to_documents\n        super().__init__(\n            model,\n            revision,\n            **kwargs,\n        )\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Any,\n    ) -&gt; Array:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: The sentences to encode.\n            task_metadata: The metadata of the task. Sentence-transformers uses this to\n                determine which prompt to use from a specified dictionary.\n            prompt_type: The name type of prompt. (query or passage)\n            hf_split: Split of current task\n            hf_subset: Subset of current task\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded sentences.\n        \"\"\"\n        prompt = \"\"\n        if self.use_instructions and self.prompts_dict is not None:\n            prompt = self.get_task_instruction(task_metadata, prompt_type)\n        elif self.prompts_dict is not None:\n            prompt_name = self.get_prompt_name(task_metadata, prompt_type)\n            if prompt_name is not None:\n                prompt = self.prompts_dict.get(prompt_name, \"\")\n\n        if (\n            self.use_instructions\n            and self.apply_instruction_to_passages is False\n            and prompt_type == PromptType.document\n        ):\n            logger.info(\n                f\"No instruction used, because prompt type = {prompt_type.document}\"\n            )\n            prompt = \"\"\n        else:\n            logger.info(\n                f\"Using instruction: '{prompt}' for task: '{task_metadata.name}' prompt type: '{prompt_type}'\"\n            )\n\n        prompts = [prompt + text for batch in inputs for text in batch[\"text\"]]\n        outputs = self.llm.encode(\n            prompts, pooling_task=\"embed\", truncate_prompt_tokens=-1\n        )\n        embeddings = torch.stack([output.outputs.data for output in outputs])\n        return embeddings\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmEncoderWrapper.encode","title":"<code>encode(inputs, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Encodes the given sentences using the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader[BatchedInput]</code> <p>The sentences to encode.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>The metadata of the task. Sentence-transformers uses this to determine which prompt to use from a specified dictionary.</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>hf_split</code> <code>str</code> <p>Split of current task</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The encoded sentences.</p> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>def encode(\n    self,\n    inputs: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Any,\n) -&gt; Array:\n    \"\"\"Encodes the given sentences using the encoder.\n\n    Args:\n        inputs: The sentences to encode.\n        task_metadata: The metadata of the task. Sentence-transformers uses this to\n            determine which prompt to use from a specified dictionary.\n        prompt_type: The name type of prompt. (query or passage)\n        hf_split: Split of current task\n        hf_subset: Subset of current task\n        **kwargs: Additional arguments to pass to the encoder.\n\n    Returns:\n        The encoded sentences.\n    \"\"\"\n    prompt = \"\"\n    if self.use_instructions and self.prompts_dict is not None:\n        prompt = self.get_task_instruction(task_metadata, prompt_type)\n    elif self.prompts_dict is not None:\n        prompt_name = self.get_prompt_name(task_metadata, prompt_type)\n        if prompt_name is not None:\n            prompt = self.prompts_dict.get(prompt_name, \"\")\n\n    if (\n        self.use_instructions\n        and self.apply_instruction_to_passages is False\n        and prompt_type == PromptType.document\n    ):\n        logger.info(\n            f\"No instruction used, because prompt type = {prompt_type.document}\"\n        )\n        prompt = \"\"\n    else:\n        logger.info(\n            f\"Using instruction: '{prompt}' for task: '{task_metadata.name}' prompt type: '{prompt_type}'\"\n        )\n\n    prompts = [prompt + text for batch in inputs for text in batch[\"text\"]]\n    outputs = self.llm.encode(\n        prompts, pooling_task=\"embed\", truncate_prompt_tokens=-1\n    )\n    embeddings = torch.stack([output.outputs.data for output in outputs])\n    return embeddings\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmCrossEncoderWrapper","title":"<code>mteb.models.vllm_wrapper.VllmCrossEncoderWrapper</code>","text":"<p>               Bases: <code>VllmWrapperBase</code></p> <p>vLLM wrapper for CrossEncoder models.</p> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>class VllmCrossEncoderWrapper(VllmWrapperBase):\n    \"\"\"vLLM wrapper for CrossEncoder models.\"\"\"\n\n    convert = \"classify\"\n\n    def __init__(\n        self,\n        model: str | ModelMeta,\n        revision: str | None = None,\n        query_prefix: str = \"\",\n        document_prefix: str = \"\",\n        **kwargs: Any,\n    ):\n        super().__init__(\n            model,\n            revision,\n            **kwargs,\n        )\n        self.query_prefix = query_prefix\n        self.document_prefix = document_prefix\n\n    def predict(\n        self,\n        inputs1: DataLoader[BatchedInput],\n        inputs2: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Any,\n    ) -&gt; Array:\n        \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n        Args:\n            inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n            inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n            task_metadata: Metadata of the current task.\n            hf_split: Split of current task, allows to know some additional information about current split.\n                E.g. Current language\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            prompt_type: The name type of prompt. (query or passage)\n            **kwargs: Additional arguments to pass to the cross-encoder.\n\n        Returns:\n            The predicted relevance scores for each inputs pair.\n        \"\"\"\n        queries = [\n            self.query_prefix + text for batch in inputs1 for text in batch[\"text\"]\n        ]\n        corpus = [\n            self.document_prefix + text for batch in inputs2 for text in batch[\"text\"]\n        ]\n        # TODO: support score prompt\n\n        outputs = self.llm.score(\n            queries,\n            corpus,\n            truncate_prompt_tokens=-1,\n            use_tqdm=False,\n        )\n        scores = np.array([output.outputs.score for output in outputs])\n        return scores\n</code></pre>"},{"location":"advanced_usage/vllm_wrapper/#mteb.models.vllm_wrapper.VllmCrossEncoderWrapper.predict","title":"<code>predict(inputs1, inputs2, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs1</code> <code>DataLoader[BatchedInput]</code> <p>First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks <code>QueryDatasetType</code>).</p> required <code>inputs2</code> <code>DataLoader[BatchedInput]</code> <p>Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks <code>RetrievalOutputType</code>).</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Metadata of the current task.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split. E.g. Current language</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the cross-encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The predicted relevance scores for each inputs pair.</p> Source code in <code>mteb/models/vllm_wrapper.py</code> <pre><code>def predict(\n    self,\n    inputs1: DataLoader[BatchedInput],\n    inputs2: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Any,\n) -&gt; Array:\n    \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n    Args:\n        inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n        inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n        task_metadata: Metadata of the current task.\n        hf_split: Split of current task, allows to know some additional information about current split.\n            E.g. Current language\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        prompt_type: The name type of prompt. (query or passage)\n        **kwargs: Additional arguments to pass to the cross-encoder.\n\n    Returns:\n        The predicted relevance scores for each inputs pair.\n    \"\"\"\n    queries = [\n        self.query_prefix + text for batch in inputs1 for text in batch[\"text\"]\n    ]\n    corpus = [\n        self.document_prefix + text for batch in inputs2 for text in batch[\"text\"]\n    ]\n    # TODO: support score prompt\n\n    outputs = self.llm.score(\n        queries,\n        corpus,\n        truncate_prompt_tokens=-1,\n        use_tqdm=False,\n    )\n    scores = np.array([output.outputs.score for output in outputs])\n    return scores\n</code></pre>"},{"location":"api/","title":"Overview","text":"<p>This is the API documentation for <code>mteb</code> a package for benchmark and evaluating the quality of embeddings. This package was initially introduced as a package for evaluating text embeddings for English<sup>1</sup>, but have since been extended cover multiple languages<sup>2</sup> and multiple modalities<sup>3</sup>.</p>"},{"location":"api/#package-overview","title":"Package Overview","text":"<p>This package generally consists of three main concepts benchmarks, tasks and model implementations.</p>"},{"location":"api/#benchmarks","title":"Benchmarks","text":"<p>A benchmark is a tool to evaluate an embedding model for a given use case. For instance, <code>mteb(eng)</code> is intended to evaluate the quality of text embedding models for broad range of English use-cases such retrieval, classification, and reranking. A benchmark consist of a collection of tasks. When a model is run on a benchmark it is run on each task individually.</p> An overview of the benchmark within <code>mteb</code>"},{"location":"api/#task","title":"Task","text":"<p>A task is an implementation of a dataset for evaluation. It could for instance be the MIRACL dataset consisting of queries, a corpus of documents as well as the correct documents to retrieve for a given query. In addition to the dataset a task includes specification for how a model should be run on the dataset and how its output should be evaluation. We implement a variety of different tasks e.g. for evaluating classification, retrieval etc., We denote these task categories. Each task also come with extensive metadata including the license, who annotated the data and so on.</p> An overview of the tasks within <code>mteb</code>"},{"location":"api/#model-implementation","title":"Model Implementation","text":"<p>A model implementation is simply an implementation of an embedding model or API to ensure that others can reproduce the exact results on a given task. For instance, when running the OpenAI embedding API on a document larger than the maximum amount of tokens a user will have to decide how they want to deal with this limitations (e.g. by truncating the sequence). Having a shared implementation allow us to examine these implementation assumptions and allow for reproducible workflow. To ensure consistency we define a standard interface/protocol that models should follow to be implemented. These implementations additionally come with metadata, that for example include license, compatible frameworks, and whether the weight are public or not.</p> An overview of the model and its metadata within <code>mteb</code> <ol> <li> <p>Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2014\u20132037. Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. URL: https://aclanthology.org/2023.eacl-main.148, doi:10.18653/v1/2023.eacl-main.148.\u00a0\u21a9</p> </li> <li> <p>Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M\u00e1rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzemi\u0144ski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan Rystr\u00f8m, Roman Solomatin, \u00d6mer \u00c7a\u011fatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa\u0142 Po\u015bwiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj\u00f6rn Pl\u00fcster, Jan Philipp Harries, Lo\u00efc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek \u0160uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G\u00fcnther, Mengzhou Xia, Weijia Shi, Xing Han L\u00f9, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. Mmteb: massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. URL: https://arxiv.org/abs/2502.13595, doi:10.48550/arXiv.2502.13595.\u00a0\u21a9</p> </li> <li> <p>Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M\u00e1rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: massive image embedding benchmark. arXiv preprint arXiv:2504.10471, 2025. URL: https://arxiv.org/abs/2504.10471, doi:10.48550/ARXIV.2504.10471.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/benchmark/","title":"Benchmark","text":"<p>A benchmark within <code>mteb</code> is essentially just a list of tasks along with some metadata about the benchmark.</p> An overview of the benchmark within <code>mteb</code> <p>This metadata includes a short description of the benchmark's intention, the reference, and the citation. If you use a benchmark from <code>mteb</code>, we recommend that you cite it along with <code>mteb</code>.</p>"},{"location":"api/benchmark/#utilities","title":"Utilities","text":""},{"location":"api/benchmark/#mteb.get_benchmarks","title":"<code>mteb.get_benchmarks(names=None, display_on_leaderboard=None)</code>","text":"<p>Get a list of benchmarks by name.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str] | None</code> <p>A list of benchmark names to retrieve. If None, all benchmarks are returned.</p> <code>None</code> <code>display_on_leaderboard</code> <code>bool | None</code> <p>If specified, filters benchmarks by whether they are displayed on the leaderboard.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Benchmark]</code> <p>A list of Benchmark instances.</p> Source code in <code>mteb/benchmarks/get_benchmark.py</code> <pre><code>def get_benchmarks(\n    names: list[str] | None = None, display_on_leaderboard: bool | None = None\n) -&gt; list[Benchmark]:\n    \"\"\"Get a list of benchmarks by name.\n\n    Args:\n        names: A list of benchmark names to retrieve. If None, all benchmarks are returned.\n        display_on_leaderboard: If specified, filters benchmarks by whether they are displayed on the leaderboard.\n\n    Returns:\n        A list of Benchmark instances.\n    \"\"\"\n    benchmark_registry = _build_registry()\n\n    if names is None:\n        names = list(benchmark_registry.keys())\n    benchmarks = [get_benchmark(name) for name in names]\n    if display_on_leaderboard is not None:\n        benchmarks = [\n            b for b in benchmarks if b.display_on_leaderboard is display_on_leaderboard\n        ]\n    return benchmarks\n</code></pre>"},{"location":"api/benchmark/#mteb.get_benchmark","title":"<code>mteb.get_benchmark(benchmark_name)</code>","text":"<p>Get a benchmark by name.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_name</code> <code>str</code> <p>The name of the benchmark to retrieve.</p> required <p>Returns:</p> Type Description <code>Benchmark</code> <p>The Benchmark instance corresponding to the given name.</p> Source code in <code>mteb/benchmarks/get_benchmark.py</code> <pre><code>def get_benchmark(\n    benchmark_name: str,\n) -&gt; Benchmark:\n    \"\"\"Get a benchmark by name.\n\n    Args:\n        benchmark_name: The name of the benchmark to retrieve.\n\n    Returns:\n        The Benchmark instance corresponding to the given name.\n    \"\"\"\n    benchmark_registry = _build_registry()\n    aliases_registry = _build_aliases_registry()\n\n    if benchmark_name in aliases_registry:\n        return aliases_registry[benchmark_name]\n    if benchmark_name not in benchmark_registry:\n        close_matches = difflib.get_close_matches(\n            benchmark_name, benchmark_registry.keys()\n        )\n        if close_matches:\n            suggestion = (\n                f\"'{benchmark_name}' not found. Did you mean: {close_matches[0]}?\"\n            )\n        else:\n            suggestion = f\"'{benchmark_name}' not found and no similar keys were found.\"\n        raise KeyError(suggestion)\n    return benchmark_registry[benchmark_name]\n</code></pre>"},{"location":"api/benchmark/#the-benchmark-object","title":"The Benchmark Object","text":""},{"location":"api/benchmark/#mteb.Benchmark","title":"<code>mteb.Benchmark</code>  <code>dataclass</code>","text":"<p>A benchmark object intended to run a certain benchmark within MTEB.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the benchmark</p> required <code>aliases</code> <code>Sequence[str]</code> <p>Alternative names for the benchmark</p> <code>tuple()</code> <code>tasks</code> <code>Sequence[AbsTask]</code> <p>The tasks within the benchmark.</p> required <code>description</code> <code>str | None</code> <p>A description of the benchmark, should include its intended goal and potentially a description of its construction</p> <code>None</code> <code>reference</code> <code>StrURL | None</code> <p>A link reference, to a source containing additional information typically to a paper, leaderboard or github.</p> <code>None</code> <code>citation</code> <code>str | None</code> <p>A bibtex citation</p> <code>None</code> <code>contacts</code> <code>list[str] | None</code> <p>The people to contact in case of a problem in the benchmark, preferably a GitHub handle.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Benchmark(\n...     name=\"MTEB(custom)\",\n...     tasks=mteb.get_tasks(\n...         tasks=[\"AmazonCounterfactualClassification\", \"AmazonPolarityClassification\"],\n...         languages=[\"eng\"],\n...     ),\n...     description=\"A custom benchmark\"\n... )\n</code></pre> Source code in <code>mteb/benchmarks/benchmark.py</code> <pre><code>@dataclass\nclass Benchmark:\n    \"\"\"A benchmark object intended to run a certain benchmark within MTEB.\n\n    Args:\n        name: The name of the benchmark\n        aliases: Alternative names for the benchmark\n        tasks: The tasks within the benchmark.\n        description: A description of the benchmark, should include its intended goal and potentially a description of its construction\n        reference: A link reference, to a source containing additional information typically to a paper, leaderboard or github.\n        citation: A bibtex citation\n        contacts: The people to contact in case of a problem in the benchmark, preferably a GitHub handle.\n\n    Examples:\n        &gt;&gt;&gt; Benchmark(\n        ...     name=\"MTEB(custom)\",\n        ...     tasks=mteb.get_tasks(\n        ...         tasks=[\"AmazonCounterfactualClassification\", \"AmazonPolarityClassification\"],\n        ...         languages=[\"eng\"],\n        ...     ),\n        ...     description=\"A custom benchmark\"\n        ... )\n    \"\"\"\n\n    name: str\n    tasks: Sequence[AbsTask]\n    aliases: Sequence[str] = field(default_factory=tuple)\n    description: str | None = None\n    reference: StrURL | None = None\n    citation: str | None = None\n    contacts: list[str] | None = None\n    display_on_leaderboard: bool = True\n    icon: str | None = None\n    display_name: str | None = None\n    language_view: list[str] | Literal[\"all\"] = field(default_factory=list)\n\n    def __iter__(self) -&gt; Iterator[AbsTask]:\n        return iter(self.tasks)\n\n    def __len__(self) -&gt; int:\n        return len(self.tasks)\n\n    def __getitem__(self, index: int) -&gt; AbsTask:\n        return self.tasks[index]\n\n    def _create_summary_table(\n        self, benchmark_results: BenchmarkResults\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create summary table. Called by the leaderboard app.\n\n        Returns:\n            A pandas DataFrame representing the summary results.\n        \"\"\"\n        from mteb.benchmarks._create_table import (\n            _create_summary_table_from_benchmark_results,\n        )\n\n        return _create_summary_table_from_benchmark_results(benchmark_results)\n\n    def _create_per_task_table(\n        self, benchmark_results: BenchmarkResults\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create per-task table. Called by the leaderboard app.\n\n        Returns:\n            A pandas DataFrame representing the per-task results.\n        \"\"\"\n        from mteb.benchmarks._create_table import (\n            _create_per_task_table_from_benchmark_results,\n        )\n\n        return _create_per_task_table_from_benchmark_results(benchmark_results)\n\n    def _create_per_language_table(\n        self, benchmark_results: BenchmarkResults\n    ) -&gt; pd.DataFrame:\n        \"\"\"Create per-language table. Called by the leaderboard app.\n\n        Returns:\n            A pandas DataFrame representing the per-language results.\n        \"\"\"\n        from mteb.benchmarks._create_table import (\n            _create_per_language_table_from_benchmark_results,\n        )\n\n        if self.language_view == \"all\" or len(self.language_view) &gt; 0:\n            return _create_per_language_table_from_benchmark_results(\n                benchmark_results, self.language_view\n            )\n        else:\n            no_results_frame = pd.DataFrame(\n                {\n                    \"No results\": [\n                        \"The per-language table is not available for this benchmark.\"\n                    ]\n                }\n            )\n            return no_results_frame\n\n    def push_collection_to_hub(\n        self,\n        hf_username: str,\n        collection_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Push the benchmark collection to Hugging Face Hub.\n\n        Args:\n            hf_username: Hugging Face username or organization name\n            collection_name: Name for the collection on Hugging Face Hub. If not provided, the benchmark name will be used.\n        \"\"\"\n        collections = huggingface_hub.list_collections(owner=hf_username)\n        collection_name = collection_name or self.name\n        existing_collection = None\n        for collection in collections:\n            if collection.title == collection_name:\n                existing_collection = collection\n                break\n\n        if existing_collection is None:\n            description = self.description\n            if description and len(description) &gt; 150:\n                description = description[:147] + \"...\"\n            collection = huggingface_hub.create_collection(\n                title=collection_name,\n                namespace=hf_username,\n                # hf collections have a 150 character limit for description, so we truncate it if it's too long\n                description=description if description else None,\n            )\n        else:\n            # list collections would output only 4 items\n            collection = huggingface_hub.get_collection(\n                collection_slug=existing_collection.slug\n            )\n\n        existing_items = {item.item_id for item in collection.items}\n\n        for task in self.tasks:\n            tasks = (\n                cast(\"AbsTaskAggregate\", task).tasks if task.is_aggregate else [task]\n            )\n            for benchmark_task in tasks:\n                task_path = benchmark_task.metadata.dataset[\"path\"]\n                if task_path in existing_items:\n                    continue\n                huggingface_hub.add_collection_item(\n                    collection_slug=collection.slug,\n                    item_id=task_path,\n                    item_type=\"dataset\",\n                )\n                existing_items.add(task_path)\n\n    def __repr__(self) -&gt; str:\n        n_tasks = len(self.tasks)\n        max_len = 50\n        desc = self.description if self.description else \"\"\n        desc = f\"'{desc[:max_len]}...\" if len(desc) &gt; max_len else f\"'{desc}'\"\n        return f\"{self.__class__.__name__}(name='{self.name}', description={desc}, tasks=[...] (#{n_tasks}), ...)\"\n</code></pre>"},{"location":"api/benchmark/#mteb.Benchmark.push_collection_to_hub","title":"<code>push_collection_to_hub(hf_username, collection_name=None)</code>","text":"<p>Push the benchmark collection to Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>hf_username</code> <code>str</code> <p>Hugging Face username or organization name</p> required <code>collection_name</code> <code>str | None</code> <p>Name for the collection on Hugging Face Hub. If not provided, the benchmark name will be used.</p> <code>None</code> Source code in <code>mteb/benchmarks/benchmark.py</code> <pre><code>def push_collection_to_hub(\n    self,\n    hf_username: str,\n    collection_name: str | None = None,\n) -&gt; None:\n    \"\"\"Push the benchmark collection to Hugging Face Hub.\n\n    Args:\n        hf_username: Hugging Face username or organization name\n        collection_name: Name for the collection on Hugging Face Hub. If not provided, the benchmark name will be used.\n    \"\"\"\n    collections = huggingface_hub.list_collections(owner=hf_username)\n    collection_name = collection_name or self.name\n    existing_collection = None\n    for collection in collections:\n        if collection.title == collection_name:\n            existing_collection = collection\n            break\n\n    if existing_collection is None:\n        description = self.description\n        if description and len(description) &gt; 150:\n            description = description[:147] + \"...\"\n        collection = huggingface_hub.create_collection(\n            title=collection_name,\n            namespace=hf_username,\n            # hf collections have a 150 character limit for description, so we truncate it if it's too long\n            description=description if description else None,\n        )\n    else:\n        # list collections would output only 4 items\n        collection = huggingface_hub.get_collection(\n            collection_slug=existing_collection.slug\n        )\n\n    existing_items = {item.item_id for item in collection.items}\n\n    for task in self.tasks:\n        tasks = (\n            cast(\"AbsTaskAggregate\", task).tasks if task.is_aggregate else [task]\n        )\n        for benchmark_task in tasks:\n            task_path = benchmark_task.metadata.dataset[\"path\"]\n            if task_path in existing_items:\n                continue\n            huggingface_hub.add_collection_item(\n                collection_slug=collection.slug,\n                item_id=task_path,\n                item_type=\"dataset\",\n            )\n            existing_items.add(task_path)\n</code></pre>"},{"location":"api/evaluation/","title":"Evaluation","text":""},{"location":"api/evaluation/#mteb.evaluate","title":"<code>mteb.evaluate</code>","text":""},{"location":"api/evaluation/#mteb.evaluate.OverwriteStrategy","title":"<code>OverwriteStrategy</code>","text":"<p>               Bases: <code>HelpfulStrEnum</code></p> <p>Enum for the overwrite strategy when running a task.</p> <ul> <li>\"always\": Always run the task, overwriting the results</li> <li>\"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.</li> <li>\"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has     changed.</li> <li>\"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the     cache.</li> </ul> Source code in <code>mteb/evaluate.py</code> <pre><code>class OverwriteStrategy(HelpfulStrEnum):\n    \"\"\"Enum for the overwrite strategy when running a task.\n\n    - \"always\": Always run the task, overwriting the results\n    - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.\n    - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has\n        changed.\n    - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the\n        cache.\n    \"\"\"\n\n    ALWAYS = \"always\"\n    NEVER = \"never\"\n    ONLY_MISSING = \"only-missing\"\n    ONLY_CACHE = \"only-cache\"\n</code></pre>"},{"location":"api/evaluation/#mteb.evaluate.evaluate","title":"<code>evaluate(model, tasks, *, co2_tracker=None, raise_error=True, encode_kwargs=None, cache=ResultCache(), overwrite_strategy='only-missing', prediction_folder=None, show_progress_bar=True, public_only=None, num_proc=None)</code>","text":"<p>This function runs a model on a given task and returns the results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelMeta | MTEBModels | SentenceTransformer | CrossEncoder</code> <p>The model to use for encoding.</p> required <code>tasks</code> <code>AbsTask | Iterable[AbsTask]</code> <p>A task to run.</p> required <code>co2_tracker</code> <code>bool | None</code> <p>If True, track the CO\u2082 emissions of the evaluation, required codecarbon to be installed, which can be installed using <code>pip install mteb[codecarbon]</code>. If none is passed co2 tracking will only be run if codecarbon is installed.</p> <code>None</code> <code>encode_kwargs</code> <code>EncodeKwargs | None</code> <p>Additional keyword arguments passed to the models <code>encode</code> and <code>load_data</code> methods;</p> <code>None</code> <code>raise_error</code> <code>bool</code> <p>If True, raise an error if the task fails. If False, return an empty list.</p> <code>True</code> <code>cache</code> <code>ResultCache | None</code> <p>The cache to use for loading the results. If None, then no cache will be used. The default cache saved the cache in the <code>~/.cache/mteb</code> directory. It can be overridden by setting the <code>MTEB_CACHE</code> environment variable to a different directory or by directly passing a <code>ResultCache</code> object.</p> <code>ResultCache()</code> <code>overwrite_strategy</code> <code>str | OverwriteStrategy</code> <p>The strategy to use for run a task and overwrite the results. Can be: - \"always\": Always run the task, overwriting the results - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task. - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has     changed. - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the     cache.</p> <code>'only-missing'</code> <code>prediction_folder</code> <code>Path | str | None</code> <p>Optional folder in which to save model predictions for the task. Predictions of the tasks will be saved in <code>prediction_folder/{task_name}_predictions.json</code></p> <code>None</code> <code>show_progress_bar</code> <code>bool</code> <p>Whether to show a progress bar when running the evaluation. Default is True. Setting this to False will also set the <code>encode_kwargs['show_progress_bar']</code> to False if encode_kwargs is unspecified.</p> <code>True</code> <code>public_only</code> <code>bool | None</code> <p>Run only public tasks. If None, it will attempt to run the private task.</p> <code>None</code> <code>num_proc</code> <code>int | None</code> <p>Number of processes to use during data loading and transformation. Defaults to 1.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModelResult</code> <p>The results of the evaluation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n&gt;&gt;&gt; task = mteb.get_task(\"STS12\")\n&gt;&gt;&gt; result = mteb.evaluate(ModelMeta, task)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with CO2 tracking\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, co2_tracker=True)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with encode kwargs\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, encode_kwargs={\"batch_size\": 16})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # with online cache\n&gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; cache.download_from_remote()\n&gt;&gt;&gt; result = mteb.evaluate(model_meta, task, cache=cache)\n</code></pre> Source code in <code>mteb/evaluate.py</code> <pre><code>def evaluate(\n    model: ModelMeta | MTEBModels | SentenceTransformer | CrossEncoder,\n    tasks: AbsTask | Iterable[AbsTask],\n    *,\n    co2_tracker: bool | None = None,\n    raise_error: bool = True,\n    encode_kwargs: EncodeKwargs | None = None,\n    cache: ResultCache | None = ResultCache(),\n    overwrite_strategy: str | OverwriteStrategy = \"only-missing\",\n    prediction_folder: Path | str | None = None,\n    show_progress_bar: bool = True,\n    public_only: bool | None = None,\n    num_proc: int | None = None,\n) -&gt; ModelResult:\n    \"\"\"This function runs a model on a given task and returns the results.\n\n    Args:\n        model: The model to use for encoding.\n        tasks: A task to run.\n        co2_tracker: If True, track the CO\u2082 emissions of the evaluation, required codecarbon to be installed, which can be installed using\n            `pip install mteb[codecarbon]`. If none is passed co2 tracking will only be run if codecarbon is installed.\n        encode_kwargs: Additional keyword arguments passed to the models `encode` and `load_data` methods;\n        raise_error: If True, raise an error if the task fails. If False, return an empty list.\n        cache: The cache to use for loading the results. If None, then no cache will be used. The default cache saved the cache in the\n            `~/.cache/mteb` directory. It can be overridden by setting the `MTEB_CACHE` environment variable to a different directory or by directly\n            passing a `ResultCache` object.\n        overwrite_strategy: The strategy to use for run a task and overwrite the results. Can be:\n            - \"always\": Always run the task, overwriting the results\n            - \"never\": Run the task only if the results are not found in the cache. If the results are found, it will not run the task.\n            - \"only-missing\": Only rerun the missing splits of a task. It will not rerun the splits if the dataset revision or mteb version has\n                changed.\n            - \"only-cache\": Only load the results from the cache folder and do not run the task. Useful if you just want to load the results from the\n                cache.\n        prediction_folder: Optional folder in which to save model predictions for the task. Predictions of the tasks will be saved in `prediction_folder/{task_name}_predictions.json`\n        show_progress_bar: Whether to show a progress bar when running the evaluation. Default is True. Setting this to False will also set the\n            `encode_kwargs['show_progress_bar']` to False if encode_kwargs is unspecified.\n        public_only: Run only public tasks. If None, it will attempt to run the private task.\n        num_proc: Number of processes to use during data loading and transformation. Defaults to 1.\n\n    Returns:\n        The results of the evaluation.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n        &gt;&gt;&gt; task = mteb.get_task(\"STS12\")\n        &gt;&gt;&gt; result = mteb.evaluate(ModelMeta, task)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with CO2 tracking\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, co2_tracker=True)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with encode kwargs\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, encode_kwargs={\"batch_size\": 16})\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # with online cache\n        &gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; cache.download_from_remote()\n        &gt;&gt;&gt; result = mteb.evaluate(model_meta, task, cache=cache)\n    \"\"\"\n    if isinstance(prediction_folder, str):\n        prediction_folder = Path(prediction_folder)\n\n    if encode_kwargs is None:\n        encode_kwargs = (\n            {\"show_progress_bar\": False} if show_progress_bar is False else {}\n        )\n    if \"batch_size\" not in encode_kwargs:\n        encode_kwargs[\"batch_size\"] = 32\n        logger.info(\n            \"No batch size defined in encode_kwargs. Setting `encode_kwargs['batch_size'] = 32`. Explicitly set the batch size to silence this message.\"\n        )\n\n    model, meta, model_name, model_revision = _sanitize_model(model)\n    _check_model_modalities(meta, tasks)\n\n    # AbsTaskAggregate is a special case where we have to run multiple tasks and combine the results\n    if isinstance(tasks, AbsTaskAggregate):\n        aggregated_task = cast(\"AbsTaskAggregate\", tasks)\n        results = evaluate(\n            model,\n            aggregated_task.metadata.tasks,\n            co2_tracker=co2_tracker,\n            raise_error=raise_error,\n            encode_kwargs=encode_kwargs,\n            cache=cache,\n            overwrite_strategy=overwrite_strategy,\n            prediction_folder=prediction_folder,\n            show_progress_bar=show_progress_bar,\n            public_only=public_only,\n            num_proc=num_proc,\n        )\n        combined_results = aggregated_task.combine_task_results(results.task_results)\n        if cache:\n            cache.save_to_cache(combined_results, meta)\n\n        return ModelResult(\n            model_name=results.model_name,\n            model_revision=results.model_revision,\n            task_results=[combined_results],\n        )\n\n    if isinstance(tasks, AbsTask):\n        task = tasks\n    else:\n        tasks = cast(\"Iterable[AbsTask]\", tasks)\n        evaluate_results = []\n        exceptions = []\n        tasks_tqdm = tqdm(\n            tasks,\n            desc=\"Evaluating tasks\",\n            disable=not show_progress_bar,\n        )\n        for i, task in enumerate(tasks_tqdm):\n            tasks_tqdm.set_description(f\"Evaluating task {task.metadata.name}\")\n            _res = evaluate(\n                model,\n                task,\n                co2_tracker=co2_tracker,\n                raise_error=raise_error,\n                encode_kwargs=encode_kwargs,\n                cache=cache,\n                overwrite_strategy=overwrite_strategy,\n                prediction_folder=prediction_folder,\n                show_progress_bar=False,\n                public_only=public_only,\n                num_proc=num_proc,\n            )\n            evaluate_results.extend(_res.task_results)\n            if _res.exceptions:\n                exceptions.extend(_res.exceptions)\n        return ModelResult(\n            model_name=_res.model_name,\n            model_revision=_res.model_revision,\n            task_results=evaluate_results,\n            exceptions=exceptions,\n        )\n\n    overwrite_strategy = OverwriteStrategy.from_str(overwrite_strategy)\n\n    existing_results: TaskResult | None = None\n    if cache and overwrite_strategy != OverwriteStrategy.ALWAYS:\n        cache_results = cache.load_task_result(task.metadata.name, meta)\n        if cache_results:\n            existing_results = cache_results\n\n    if (\n        existing_results\n        and overwrite_strategy\n        not in (OverwriteStrategy.ALWAYS, OverwriteStrategy.NEVER)\n        and (\n            not _requires_merge(task, existing_results)\n            or existing_results.is_mergeable(task)\n        )\n    ):\n        missing_eval = existing_results.get_missing_evaluations(task)\n    else:\n        missing_eval = dict.fromkeys(task.eval_splits, task.hf_subsets)\n        # Will be fully recomputed so we set it to None to avoid merging:\n        existing_results = None\n\n    if (\n        existing_results\n        and not missing_eval\n        and overwrite_strategy != OverwriteStrategy.ALWAYS\n    ):\n        # if there are no missing evals we can just return the results\n        logger.info(\n            f\"Results for {task.metadata.name} already exist in cache. Skipping evaluation and loading results.\"\n        )\n        return ModelResult(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_results=[existing_results],\n        )\n    if missing_eval and overwrite_strategy in [\n        OverwriteStrategy.NEVER,\n        OverwriteStrategy.ONLY_CACHE,\n    ]:\n        raise ValueError(\n            f\"overwrite_strategy is set to '{overwrite_strategy.value}' and the results file exists for task {task.metadata.name}. \"\n            + f\"However there are the following missing splits (and subsets): {missing_eval}. To rerun these set overwrite_strategy to 'only-missing'.\"\n        )\n\n    if existing_results:\n        logger.info(\n            f\"Found existing results for {task.metadata.name}, only running missing splits (subsets): {missing_eval}\"\n        )\n\n    if isinstance(model, ModelMeta):\n        logger.info(\n            f\"Loading model {model_name} with revision {model_revision} from ModelMeta.\"\n        )\n        model = model.load_model()\n        logger.info(\"\u2713 Model loaded\")\n\n    if raise_error is False:\n        try:\n            result = _evaluate_task(\n                model=model,\n                splits=missing_eval,\n                task=task,\n                co2_tracker=co2_tracker,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                public_only=public_only,\n                num_proc=num_proc,\n            )\n        except Exception as e:\n            logger.error(\n                f\"Error while running task {task.metadata.name} on splits {list(missing_eval.keys())}: {e}\"\n            )\n            result = TaskError(task_name=task.metadata.name, exception=str(e))\n    else:\n        result = _evaluate_task(\n            model=model,\n            splits=missing_eval,\n            task=task,\n            co2_tracker=False,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            public_only=public_only,\n            num_proc=num_proc,\n        )\n    logger.info(f\"\u2713 Finished evaluation for {task.metadata.name}\")\n\n    if isinstance(result, TaskError):\n        return ModelResult(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_results=[],\n            exceptions=[result],\n        )\n\n    if existing_results:\n        result = result.merge(existing_results)\n\n    if cache:\n        cache.save_to_cache(result, meta)\n\n    return ModelResult(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_results=[result],\n    )\n</code></pre>"},{"location":"api/model/","title":"Models","text":"<p>A model in <code>mteb</code> covers two concepts: metadata and implementation. - Metadata contains information about the model such as maximum input length, valid frameworks, license, and degree of openness. - Implementation is a reproducible workflow, which allows others to run the same model again, using the same prompts, hyperparameters, aggregation strategies, etc.</p> An overview of the model and its metadata within <code>mteb</code>"},{"location":"api/model/#utilities","title":"Utilities","text":""},{"location":"api/model/#mteb.get_model_metas","title":"<code>mteb.get_model_metas(model_names=None, languages=None, open_weights=None, frameworks=None, n_parameters_range=(None, None), use_instructions=None, zero_shot_on=None, model_types=None)</code>","text":"<p>Load all models' metadata that fit the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>Iterable[str] | None</code> <p>A list of model names to filter by. If None, all models are included.</p> <code>None</code> <code>languages</code> <code>Iterable[str] | None</code> <p>A list of languages to filter by. If None, all languages are included.</p> <code>None</code> <code>open_weights</code> <code>bool | None</code> <p>Whether to filter by models with open weights. If None this filter is ignored.</p> <code>None</code> <code>frameworks</code> <code>Iterable[str] | None</code> <p>A list of frameworks to filter by. If None, all frameworks are included.</p> <code>None</code> <code>n_parameters_range</code> <code>tuple[int | None, int | None]</code> <p>A tuple of lower and upper bounds of the number of parameters to filter by. If (None, None), this filter is ignored.</p> <code>(None, None)</code> <code>use_instructions</code> <code>bool | None</code> <p>Whether to filter by models that use instructions. If None, all models are included.</p> <code>None</code> <code>zero_shot_on</code> <code>list[AbsTask] | None</code> <p>A list of tasks on which the model is zero-shot. If None this filter is ignored.</p> <code>None</code> <code>model_types</code> <code>Iterable[str] | None</code> <p>A list of model types to filter by. If None, all model types are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ModelMeta]</code> <p>A list of model metadata objects that fit the specified criteria.</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model_metas(\n    model_names: Iterable[str] | None = None,\n    languages: Iterable[str] | None = None,\n    open_weights: bool | None = None,\n    frameworks: Iterable[str] | None = None,\n    n_parameters_range: tuple[int | None, int | None] = (None, None),\n    use_instructions: bool | None = None,\n    zero_shot_on: list[AbsTask] | None = None,\n    model_types: Iterable[str] | None = None,\n) -&gt; list[ModelMeta]:\n    \"\"\"Load all models' metadata that fit the specified criteria.\n\n    Args:\n        model_names: A list of model names to filter by. If None, all models are included.\n        languages: A list of languages to filter by. If None, all languages are included.\n        open_weights: Whether to filter by models with open weights. If None this filter is ignored.\n        frameworks: A list of frameworks to filter by. If None, all frameworks are included.\n        n_parameters_range: A tuple of lower and upper bounds of the number of parameters to filter by.\n            If (None, None), this filter is ignored.\n        use_instructions: Whether to filter by models that use instructions. If None, all models are included.\n        zero_shot_on: A list of tasks on which the model is zero-shot. If None this filter is ignored.\n        model_types: A list of model types to filter by. If None, all model types are included.\n\n    Returns:\n        A list of model metadata objects that fit the specified criteria.\n    \"\"\"\n    res = []\n    model_names = set(model_names) if model_names is not None else None\n    languages = set(languages) if languages is not None else None\n    frameworks = set(frameworks) if frameworks is not None else None\n    model_types_set = set(model_types) if model_types is not None else None\n    for model_meta in MODEL_REGISTRY.values():\n        if (model_names is not None) and (model_meta.name not in model_names):\n            continue\n        if languages is not None:\n            if (model_meta.languages is None) or not (\n                languages &lt;= set(model_meta.languages)\n            ):\n                continue\n        if (open_weights is not None) and (model_meta.open_weights != open_weights):\n            continue\n        if (frameworks is not None) and not (frameworks &lt;= set(model_meta.framework)):\n            continue\n        if (use_instructions is not None) and (\n            model_meta.use_instructions != use_instructions\n        ):\n            continue\n        if model_types_set is not None and not model_types_set.intersection(\n            model_meta.model_type\n        ):\n            continue\n\n        lower, upper = n_parameters_range\n        n_parameters = model_meta.n_parameters\n\n        if upper is not None:\n            if (n_parameters is None) or (n_parameters &gt; upper):\n                continue\n            if lower is not None and n_parameters &lt; lower:\n                continue\n\n        if zero_shot_on is not None:\n            if not model_meta.is_zero_shot_on(zero_shot_on):\n                continue\n        res.append(model_meta)\n    return res\n</code></pre>"},{"location":"api/model/#mteb.get_model_meta","title":"<code>mteb.get_model_meta(model_name, revision=None, fetch_from_hf=True, fill_missing=False, experiment_kwargs=None)</code>","text":"<p>A function to fetch a model metadata object by name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to fetch</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model to fetch</p> <code>None</code> <code>fetch_from_hf</code> <code>bool</code> <p>Whether to fetch the model from HuggingFace Hub if not found in the registry</p> <code>True</code> <code>fill_missing</code> <code>bool</code> <p>Fill missing attributes from the metadata including number of parameters and memory usage.</p> <code>False</code> <code>experiment_kwargs</code> <code>Mapping[str, Any] | None</code> <p>Optional dictionary of parameters to fill in the metadata for experimental models.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModelMeta</code> <p>A model metadata object</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model_meta(\n    model_name: str,\n    revision: str | None = None,\n    fetch_from_hf: bool = True,\n    fill_missing: bool = False,\n    experiment_kwargs: Mapping[str, Any] | None = None,\n) -&gt; ModelMeta:\n    \"\"\"A function to fetch a model metadata object by name.\n\n    Args:\n        model_name: Name of the model to fetch\n        revision: Revision of the model to fetch\n        fetch_from_hf: Whether to fetch the model from HuggingFace Hub if not found in the registry\n        fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n        experiment_kwargs: Optional dictionary of parameters to fill in the metadata for experimental models.\n\n    Returns:\n        A model metadata object\n    \"\"\"\n    if model_name in _MODEL_RENAMES:\n        new_name = _MODEL_RENAMES[model_name]\n        msg = f\"The model '{model_name}' has been renamed to '{new_name}'. To prevent this warning use the new name.\"\n        warnings.warn(msg, DeprecationWarning, stacklevel=2)\n        model_name = new_name\n\n    if model_name in MODEL_REGISTRY:\n        model_meta = MODEL_REGISTRY[model_name]\n\n        if revision and (not model_meta.revision == revision):\n            raise ValueError(\n                f\"Model revision {revision} not found for model {model_name}. Expected {model_meta.revision}.\"\n            )\n\n        if experiment_kwargs is not None:\n            model_meta = model_meta.model_copy(\n                update={\"experiment_kwargs\": experiment_kwargs}\n            )\n\n        if fill_missing and fetch_from_hf:\n            original_meta_dict = model_meta.model_dump()\n            new_meta = ModelMeta.from_hub(model_name, fill_missing=fill_missing)\n            new_meta_dict = new_meta.model_dump(exclude_none=True)\n\n            updates = {\n                k: v\n                for k, v in new_meta_dict.items()\n                if original_meta_dict.get(k) is None\n            }\n\n            if updates:\n                return model_meta.model_copy(update=updates)\n        return model_meta\n\n    if fetch_from_hf:\n        logger.info(\n            f\"Model not found in model registry. Attempting to extract metadata by loading the model ({model_name}) using HuggingFace.\"\n        )\n        meta = ModelMeta.from_hub(model_name, revision)\n        return meta\n\n    not_found_msg = f\"Model '{model_name}' not found in MTEB registry\"\n    not_found_msg += \" nor on the Huggingface Hub.\" if fetch_from_hf else \".\"\n\n    close_matches = difflib.get_close_matches(model_name, MODEL_REGISTRY.keys())\n    model_names_no_org = {mdl: mdl.split(\"/\")[-1] for mdl in MODEL_REGISTRY.keys()}\n    if model_name in model_names_no_org:\n        close_matches = [model_names_no_org[model_name]] + close_matches\n\n    suggestion = \"\"\n    if close_matches:\n        if len(close_matches) &gt; 1:\n            suggestion = f\" Did you mean: '{close_matches[0]}' or {close_matches[1]}?\"\n        else:\n            suggestion = f\" Did you mean: '{close_matches[0]}'?\"\n\n    raise KeyError(not_found_msg + suggestion)\n</code></pre>"},{"location":"api/model/#mteb.get_model","title":"<code>mteb.get_model(model_name, revision=None, device=None, **kwargs)</code>","text":"<p>A function to fetch and load model object by name.</p> <p>Note</p> <p>This function loads the model into memory. If you only want to fetch the metadata, use <code>get_model_meta</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to fetch</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model to fetch</p> <code>None</code> <code>device</code> <code>str | None</code> <p>Device used to load the model</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model loader</p> <code>{}</code> <p>Returns:</p> Type Description <code>MTEBModels</code> <p>A model object</p> Source code in <code>mteb/models/get_model_meta.py</code> <pre><code>def get_model(\n    model_name: str,\n    revision: str | None = None,\n    device: str | None = None,\n    **kwargs: Any,\n) -&gt; MTEBModels:\n    \"\"\"A function to fetch and load model object by name.\n\n    !!! note\n        This function loads the model into memory. If you only want to fetch the metadata, use [`get_model_meta`](#mteb.get_model_meta) instead.\n\n    Args:\n        model_name: Name of the model to fetch\n        revision: Revision of the model to fetch\n        device: Device used to load the model\n        **kwargs: Additional keyword arguments to pass to the model loader\n\n    Returns:\n        A model object\n    \"\"\"\n    meta = get_model_meta(model_name, revision).model_copy(deep=True)\n    model = meta.load_model(device=device, **kwargs)\n\n    if kwargs:\n        logger.info(\n            f\"Model '{model_name}' loaded with additional arguments: {list(kwargs.keys())}\"\n        )\n        meta.loader_kwargs |= kwargs\n\n    model.mteb_model_meta = meta  # type: ignore[misc]\n    return model\n</code></pre>"},{"location":"api/model/#metadata","title":"Metadata","text":""},{"location":"api/model/#mteb.models.model_meta.ModelMeta","title":"<code>mteb.models.model_meta.ModelMeta</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The model metadata object.</p> <p>Attributes:</p> Name Type Description <code>loader</code> <code>Callable[..., MTEBModels] | None</code> <p>The function that loads the model. If None it assumes that the model is not implemented.</p> <code>loader_kwargs</code> <code>dict[str, Any]</code> <p>The keyword arguments to pass to the loader function.</p> <code>name</code> <code>str | None</code> <p>The name of the model, ideally the name on huggingface. It should be in the format \"organization/model_name\".</p> <code>n_parameters</code> <code>int | None</code> <p>The total number of parameters in the model, e.g. <code>7_000_000</code> for a 7M parameter model. Can be none in case the number of parameters is unknown.</p> <code>n_embedding_parameters</code> <code>int | None</code> <p>The number of parameters used for the embedding layer. Can be None if the number of embedding parameters is not known (e.g. for proprietary models).</p> <code>n_active_parameters_override</code> <code>int | None</code> <p>The number of active parameters used bu model. Should be used only for Mixture of Experts models.</p> <code>memory_usage_mb</code> <code>float | None</code> <p>The memory usage of the model in MB. Can be None if the memory usage is not known (e.g. for proprietary models). To calculate it use the <code>calculate_memory_usage_mb</code> method.</p> <code>max_tokens</code> <code>float | None</code> <p>The maximum number of tokens the model can handle. Can be None if the maximum number of tokens is not known (e.g. for proprietary models).</p> <code>embed_dim</code> <code>int | None</code> <p>The dimension of the embeddings produced by the model. Currently all models are assumed to produce fixed-size embeddings.</p> <code>revision</code> <code>str | None</code> <p>The revision number of the model. If None, it is assumed that the metadata (including the loader) is valid for all revisions of the model.</p> <code>release_date</code> <code>StrDate | None</code> <p>The date the model's revision was released. If None, then release date will be added based on 1st commit in hf repository of model.</p> <code>license</code> <code>Licenses | StrURL | None</code> <p>The license under which the model is released. Required if open_weights is True.</p> <code>open_weights</code> <code>bool | None</code> <p>Whether the model is open source or proprietary.</p> <code>public_training_code</code> <code>str | None</code> <p>A link to the publicly available training code. If None, it is assumed that the training code is not publicly available.</p> <code>public_training_data</code> <code>str | bool | None</code> <p>A link to the publicly available training data. If None, it is assumed that the training data is not publicly available.</p> <code>similarity_fn_name</code> <code>ScoringFunction | None</code> <p>The distance metric used by the model.</p> <code>framework</code> <code>list[FRAMEWORKS]</code> <p>The framework the model is implemented in, can be a list of frameworks e.g. <code>[\"Sentence Transformers\", \"PyTorch\"]</code>.</p> <code>reference</code> <code>StrURL | None</code> <p>A URL to the model's page on huggingface or another source.</p> <code>languages</code> <code>list[ISOLanguageScript] | None</code> <p>The languages the model is intended to be specified as a 3-letter language code followed by a script code e.g., \"eng-Latn\" for English in the Latin script.</p> <code>use_instructions</code> <code>bool | None</code> <p>Whether the model uses instructions E.g. for prompt-based models. This also includes models that require a specific format for input, such as \"query: {document}\" or \"passage: {document}\".</p> <code>citation</code> <code>str | None</code> <p>The citation for the model. This is a bibtex string.</p> <code>training_datasets</code> <code>set[str] | None</code> <p>A dictionary of datasets that the model was trained on. Names should be names as their appear in <code>mteb</code> for example {\"ArguAna\"} if the model is trained on the ArguAna test set. This field is used to determine if a model generalizes zero-shot to a benchmark as well as mark dataset contaminations.</p> <code>adapted_from</code> <code>str | None</code> <p>Name of the model from which this model is adapted. For quantizations, fine-tunes, long doc extensions, etc.</p> <code>superseded_by</code> <code>str | None</code> <p>Name of the model that supersedes this model, e.g., nvidia/NV-Embed-v2 supersedes v1.</p> <code>model_type</code> <code>list[MODEL_TYPES]</code> <p>A list of strings representing the type of model.</p> <code>modalities</code> <code>list[Modalities]</code> <p>A list of strings representing the modalities the model supports. Default is [\"text\"].</p> <code>contacts</code> <code>list[str] | None</code> <p>The people to contact in case of a problem in the model, preferably a GitHub handle.</p> <code>experiment_kwargs</code> <code>Mapping[str, Any] | None</code> <p>A dictionary of parameters used in the experiment that are not covered by other fields. This is used to create experiment names for ablation studies and similar experiments.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>class ModelMeta(BaseModel):\n    \"\"\"The model metadata object.\n\n    Attributes:\n        loader: The function that loads the model. If None it assumes that the model is not implemented.\n        loader_kwargs: The keyword arguments to pass to the loader function.\n        name: The name of the model, ideally the name on huggingface. It should be in the format \"organization/model_name\".\n        n_parameters: The total number of parameters in the model, e.g. `7_000_000` for a 7M parameter model. Can be none in case the number of parameters is unknown.\n        n_embedding_parameters: The number of parameters used for the embedding layer. Can be None if the number of embedding parameters is not known (e.g. for proprietary models).\n        n_active_parameters_override: The number of active parameters used bu model. Should be used **only** for Mixture of Experts models.\n        memory_usage_mb: The memory usage of the model in MB. Can be None if the memory usage is not known (e.g. for proprietary models). To calculate it use the `calculate_memory_usage_mb` method.\n        max_tokens: The maximum number of tokens the model can handle. Can be None if the maximum number of tokens is not known (e.g. for proprietary\n            models).\n        embed_dim: The dimension of the embeddings produced by the model. Currently all models are assumed to produce fixed-size embeddings.\n        revision: The revision number of the model. If None, it is assumed that the metadata (including the loader) is valid for all revisions of the model.\n        release_date: The date the model's revision was released. If None, then release date will be added based on 1st commit in hf repository of model.\n        license: The license under which the model is released. Required if open_weights is True.\n        open_weights: Whether the model is open source or proprietary.\n        public_training_code: A link to the publicly available training code. If None, it is assumed that the training code is not publicly available.\n        public_training_data: A link to the publicly available training data. If None, it is assumed that the training data is not publicly available.\n        similarity_fn_name: The distance metric used by the model.\n        framework: The framework the model is implemented in, can be a list of frameworks e.g. `[\"Sentence Transformers\", \"PyTorch\"]`.\n        reference: A URL to the model's page on huggingface or another source.\n        languages: The languages the model is intended to be specified as a 3-letter language code followed by a script code e.g., \"eng-Latn\" for English\n            in the Latin script.\n        use_instructions: Whether the model uses instructions E.g. for prompt-based models. This also includes models that require a specific format for\n            input, such as \"query: {document}\" or \"passage: {document}\".\n        citation: The citation for the model. This is a bibtex string.\n        training_datasets: A dictionary of datasets that the model was trained on. Names should be names as their appear in `mteb` for example\n            {\"ArguAna\"} if the model is trained on the ArguAna test set. This field is used to determine if a model generalizes zero-shot to\n            a benchmark as well as mark dataset contaminations.\n        adapted_from: Name of the model from which this model is adapted. For quantizations, fine-tunes, long doc extensions, etc.\n        superseded_by: Name of the model that supersedes this model, e.g., nvidia/NV-Embed-v2 supersedes v1.\n        model_type: A list of strings representing the type of model.\n        modalities: A list of strings representing the modalities the model supports. Default is [\"text\"].\n        contacts: The people to contact in case of a problem in the model, preferably a GitHub handle.\n        experiment_kwargs: A dictionary of parameters used in the experiment that are not covered by other fields. This is used to create experiment names for ablation studies and similar experiments.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    # loaders\n    loader: Callable[..., MTEBModels] | None\n    loader_kwargs: dict[str, Any] = field(default_factory=dict)\n    name: str | None\n    revision: str | None\n    release_date: StrDate | None\n    languages: list[ISOLanguageScript] | None\n    n_parameters: int | None\n    n_active_parameters_override: int | None = None\n    n_embedding_parameters: int | None = None\n    memory_usage_mb: float | None\n    max_tokens: float | None\n    embed_dim: int | None\n    license: Licenses | StrURL | None\n    open_weights: bool | None\n    public_training_code: str | None\n    public_training_data: str | bool | None\n    framework: list[FRAMEWORKS]\n    reference: StrURL | None = None\n    similarity_fn_name: ScoringFunction | None\n    use_instructions: bool | None\n    training_datasets: set[str] | None\n    adapted_from: str | None = None\n    superseded_by: str | None = None\n    modalities: list[Modalities] = [\"text\"]\n    model_type: list[MODEL_TYPES] = [\"dense\"]\n    citation: str | None = None\n    contacts: list[str] | None = None\n    experiment_kwargs: Mapping[str, Any] | None = None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def _handle_legacy_is_cross_encoder(cls, data: Any) -&gt; Any:\n        \"\"\"Handle legacy is_cross_encoder field by converting it to model_type.\n\n        This validator handles backward compatibility for the deprecated is_cross_encoder field.\n        If is_cross_encoder=True is provided, it adds \"cross_encoder\" to model_type.\n        \"\"\"\n        if isinstance(data, dict) and \"is_cross_encoder\" in data:\n            is_cross_encoder_value = data.pop(\"is_cross_encoder\")\n\n            if is_cross_encoder_value is not None:\n                warnings.warn(\n                    \"is_cross_encoder is deprecated and will be removed in a future version. \"\n                    \"Use model_type=['cross-encoder'] instead.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                model_type = data.get(\"model_type\", [\"dense\"])\n\n                if is_cross_encoder_value:\n                    if \"cross-encoder\" not in model_type:\n                        data[\"model_type\"] = [\"cross-encoder\"]\n                else:\n                    if \"cross-encoder\" in model_type:\n                        model_type = [t for t in model_type if t != \"cross-encoder\"]\n                        data[\"model_type\"] = model_type if model_type else [\"dense\"]\n\n        return data\n\n    @property\n    def is_cross_encoder(self) -&gt; bool:\n        \"\"\"Returns True if the model is a cross-encoder.\n\n        Derived from model_type field. A model is considered a cross-encoder if \"cross-encoder\" is in its model_type list.\n        \"\"\"\n        return \"cross-encoder\" in self.model_type\n\n    @property\n    def n_active_parameters(self):\n        \"\"\"Number of active parameters. Assumed to be `n_parameters - n_embedding_parameters`. Can be overwritten using `n_active_parameters_override` e.g. for MoE models.\"\"\"\n        if self.n_active_parameters_override is not None:\n            return self.n_active_parameters_override\n\n        if self.n_parameters is not None and self.n_embedding_parameters is not None:\n            return self.n_parameters - self.n_embedding_parameters\n        return None\n\n    @field_validator(\"similarity_fn_name\", mode=\"before\")\n    @classmethod\n    def _validate_similarity_fn_name(cls, value: str) -&gt; ScoringFunction | None:\n        \"\"\"Converts the similarity function name to the corresponding enum value.\n\n        Sentence_transformers uses Literal['cosine', 'dot', 'euclidean', 'manhattan'],\n        and pylate uses Literal['MaxSim']\n\n        Args:\n            value: The similarity function name as a string.\n\n        Returns:\n            The corresponding ScoringFunction enum value.\n        \"\"\"\n        if type(value) is ScoringFunction or value is None:\n            return value\n        mapping = {\n            \"cosine\": ScoringFunction.COSINE,\n            \"dot\": ScoringFunction.DOT_PRODUCT,\n            \"MaxSim\": ScoringFunction.MAX_SIM,\n        }\n        if value in mapping:\n            return mapping[value]\n        raise ValueError(f\"Invalid similarity function name: {value}\")\n\n    def to_dict(self):\n        \"\"\"Returns a dictionary representation of the model metadata.\"\"\"\n        dict_repr = self.model_dump()\n        loader = dict_repr.pop(\"loader\", None)\n        dict_repr[\"training_datasets\"] = (\n            list(dict_repr[\"training_datasets\"])\n            if isinstance(dict_repr[\"training_datasets\"], set)\n            else dict_repr[\"training_datasets\"]\n        )\n        dict_repr[\"loader\"] = _get_loader_name(loader)\n        dict_repr[\"is_cross_encoder\"] = self.is_cross_encoder\n        return dict_repr\n\n    @field_validator(\"languages\")\n    @classmethod\n    def _languages_are_valid(\n        cls, languages: list[ISOLanguageScript] | None\n    ) -&gt; list[ISOLanguageScript] | None:\n        if languages is None:\n            return None\n\n        for code in languages:\n            check_language_code(code)\n        return languages\n\n    @field_validator(\"name\")\n    @classmethod\n    def _check_name(cls, v: str | None) -&gt; str | None:\n        if v is None:\n            return v\n        if \"/\" not in v:\n            raise ValueError(\n                \"Model name must be in the format 'organization/model_name'\"\n            )\n        return v\n\n    def load_model(self, device: str | None = None, **kwargs: Any) -&gt; MTEBModels:\n        \"\"\"Loads the model using the specified loader function.\"\"\"\n        if self.loader is None:\n            raise NotImplementedError(\n                \"No model implementation is available for this model.\"\n            )\n        if self.name is None:\n            raise ValueError(\"name is not set for ModelMeta. Cannot load model.\")\n\n        if self.experiment_kwargs is None:\n            self.experiment_kwargs = kwargs if len(kwargs) &gt; 0 else None\n        elif len(kwargs) &gt; 0 and self.experiment_kwargs is not None:\n            kwargs |= self.experiment_kwargs\n            self.experiment_kwargs = kwargs\n\n        # Allow overwrites\n        _kwargs = self.loader_kwargs.copy()\n        _kwargs.update(kwargs)\n        if device is not None:\n            _kwargs[\"device\"] = device\n\n        model: MTEBModels = self.loader(self.name, revision=self.revision, **_kwargs)\n        model.mteb_model_meta = self  # type: ignore[misc]\n        return model\n\n    def model_name_as_path(self) -&gt; str:\n        \"\"\"Returns the model name in a format that can be used as a file path.\n\n        Replaces \"/\" with \"__\" and spaces with \"_\".\n        \"\"\"\n        if self.name is None:\n            raise ValueError(\"Model name is not set\")\n        return self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n    @property\n    def experiment_name(self) -&gt; str | None:\n        \"\"\"Create a filesystem-safe string representation of the experiment parameters.\n\n        Uses deterministic serialization and hashing to ensure stable, bounded output.\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; model = mteb.get_model(\"mteb/baseline-random-encoder\", param1=\"test\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; print(model.mteb_model_meta.experiment_name)\n            &gt;&gt;&gt; # param1_test\n        \"\"\"\n        return _serialize_experiment_kwargs_to_name(\n            experiment_kwargs=self.experiment_kwargs\n        )\n\n    @property\n    def model_name_with_experiment(self) -&gt; str | None:\n        \"\"\"Combines the model name with the experiment parameters for a more descriptive name.\"\"\"\n        if self.name is None:\n            return None\n        experiment_str = _serialize_experiment_kwargs_to_name(\n            experiment_kwargs=self.experiment_kwargs,\n            value_field_separator=\"=\",\n            kwargs_separator=\", \",\n        )\n        return f\"{self.name} ({experiment_str})\" if experiment_str else self.name\n\n    @classmethod\n    def _detect_cross_encoder_or_dense(\n        cls,\n        model_name: str,\n        revision: str | None,\n        config: dict[str, Any] | None,\n        sentence_transformers_loader: Callable[..., MTEBModels],\n        cross_encoder_loader: Callable[..., MTEBModels],\n    ) -&gt; tuple[Callable[..., MTEBModels] | None, MODEL_TYPES]:\n        \"\"\"Detect if model is CrossEncoder or default to dense.\"\"\"\n        if not config:\n            logger.warning(\n                f\"Could not load config.json for {model_name}. \"\n                \"Defaulting to SentenceTransformer loader.\"\n            )\n            return sentence_transformers_loader, \"dense\"\n\n        architectures = config.get(\"architectures\", [])\n\n        is_cross_encoder = any(\n            arch.endswith(\"ForSequenceClassification\") for arch in architectures\n        )\n        if is_cross_encoder:\n            return cross_encoder_loader, \"cross-encoder\"\n\n        if cls._is_causal_lm_reranker(architectures, config, model_name):\n            return cross_encoder_loader, \"cross-encoder\"\n\n        logger.info(\n            f\"Model {model_name} does not have modules.json or recognized architecture. \"\n            \"Defaulting to SentenceTransformer loader.\"\n        )\n        return sentence_transformers_loader, \"dense\"\n\n    @staticmethod\n    def _is_causal_lm_reranker(\n        architectures: list[str], config: dict[str, Any], model_name: str\n    ) -&gt; bool:\n        \"\"\"Check if model is a CausalLM-style reranker.\"\"\"\n        is_causal_lm = any(arch.endswith(\"ForCausalLM\") for arch in architectures)\n\n        if not is_causal_lm:\n            return False\n\n        num_labels = config.get(\"num_labels\", 0)\n        model_name_lower = model_name.lower()\n\n        return (\n            num_labels &gt; 0\n            or \"rerank\" in model_name_lower\n            or \"cross-encoder\" in model_name_lower\n        )\n\n    @classmethod\n    def _detect_model_type_and_loader(\n        cls,\n        model_name: str,\n        revision: str | None = None,\n        config: dict[str, Any] | None = None,\n    ) -&gt; tuple[Callable[..., MTEBModels] | None, MODEL_TYPES]:\n        \"\"\"Detect the model type and appropriate loader based on HuggingFace Hub configuration files.\n\n        This follows the Sentence Transformers architecture detection logic:\n        1. Check for modules.json - If present, model is a SentenceTransformer (dense encoder)\n        2. If no modules.json, check config.json for architecture:\n            - ForSequenceClassification \u2192 CrossEncoder\n            - CausalLM with reranking indicators \u2192 CrossEncoder\n        3. Default to dense (SentenceTransformer) if no clear indicators are found\n\n        Detection for CausalLM-style rerankers:\n        - Model has ForCausalLM architecture AND\n        - Has num_labels &gt; 0 in config, OR\n        - Model name contains \"rerank\" or \"cross-encoder\"\n\n        Args:\n            model_name: The HuggingFace model name\n            revision: The model revision\n            config: The loaded config.json from the HuggingFace model repository. If not provided, it will be fetched from the hub.\n\n\n        Returns:\n            A tuple of (loader_function, model_type) where:\n            - loader_function: A callable that returns MTEBModels, or None if model doesn't exist\n            - model_type: One of \"dense\", \"cross-encoder\", or \"late-interaction\"\n        \"\"\"\n        from mteb.models import CrossEncoderWrapper, sentence_transformers_loader\n\n        try:\n            modules_config = _get_json_from_hub(\n                model_name, \"modules.json\", \"model\", revision=revision\n            )\n\n            if (\n                modules_config\n            ):  # SentenceTransformer/SparseEncoder (Not support for now)\n                return sentence_transformers_loader, \"dense\"\n            else:\n                return cls._detect_cross_encoder_or_dense(\n                    model_name,\n                    revision,\n                    config,\n                    sentence_transformers_loader,\n                    cross_encoder_loader=CrossEncoderWrapper,\n                )\n\n        except Exception as e:\n            logger.warning(\n                f\"Error detecting model type for {model_name}: {e}. \"\n                \"Defaulting to SentenceTransformer loader.\"\n            )\n\n        return sentence_transformers_loader, \"dense\"\n\n    @classmethod\n    def create_empty(cls, overwrites: dict[str, Any] | None = None) -&gt; Self:\n        \"\"\"Creates an empty ModelMeta with all fields set to None or empty.\"\"\"\n        empty_model = cls(\n            loader=None,\n            name=None,\n            revision=None,\n            release_date=None,\n            languages=None,\n            n_parameters=None,\n            n_embedding_parameters=None,\n            memory_usage_mb=None,\n            max_tokens=None,\n            embed_dim=None,\n            license=None,\n            open_weights=None,\n            public_training_code=None,\n            public_training_data=None,\n            framework=[],\n            reference=None,\n            similarity_fn_name=None,\n            use_instructions=None,\n            training_datasets=None,\n            adapted_from=None,\n            superseded_by=None,\n            citation=None,\n            contacts=None,\n        )\n        if overwrites:\n            empty_model = empty_model.model_copy(update=overwrites)\n\n        if empty_model.name is None:\n            empty_model.name = \"no_model_name/available\"\n        if empty_model.revision is None:\n            empty_model.revision = \"no_revision_available\"\n\n        return empty_model\n\n    def merge(self, overwrite: Self) -&gt; Self:\n        \"\"\"Merges another this ModelMeta with another ModelMeta.\n\n        Args:\n            overwrite: The ModelMeta to merge into this one. Non-None fields in this ModelMeta will overwrite the corresponding fields in this\n                ModelMeta. the `framework` and `model_type` fields with combined.\n\n        Returns:\n            A new ModelMeta with the merged fields.\n        \"\"\"\n        merged_data = self.model_dump()\n        overwrite_data = overwrite.model_dump()\n\n        for key, value in overwrite_data.items():\n            if (\n                key == \"name\"\n                and value == \"no_model_name/available\"\n                and self.name != \"no_model_name/available\"\n            ):\n                continue  # skip overwriting name if overwrite has no name available\n            if (\n                key == \"revision\"\n                and value == \"no_revision_available\"\n                and self.revision != \"no_revision_available\"\n            ):\n                continue  # skip overwriting revision if overwrite has no revision available\n            if key in [\"framework\", \"model_type\"]:\n                # Combine lists and remove duplicates\n                merged_list = set(merged_data.get(key, [])) | set(value or [])\n                merged_data[key] = list(merged_list)\n            if value is not None:\n                merged_data[key] = value\n\n        return self.model_copy(update=merged_data)\n\n    @classmethod\n    def _from_sentence_transformer_model(cls, model: SentenceTransformer) -&gt; Self:\n        \"\"\"Generates a ModelMeta from only a SentenceTransformer model, without fetching any additional metadata from HuggingFace Hub.\"\"\"\n        from mteb.models import sentence_transformers_loader\n\n        name: str | None = (\n            model.model_card_data.model_name\n            if model.model_card_data.model_name\n            else model.model_card_data.base_model\n        )\n        n_embedding_parameters = (\n            cls._get_n_embedding_parameters_from_sentence_transformers(model)\n        )\n        return cls.create_empty(\n            overwrites=dict(\n                name=name,\n                revision=model.model_card_data.base_model_revision,\n                loader=sentence_transformers_loader,\n                max_tokens=model.max_seq_length,\n                embed_dim=model.get_sentence_embedding_dimension(),\n                similarity_fn_name=ScoringFunction.from_str(model.similarity_fn_name),\n                framework=[\"Sentence Transformers\", \"PyTorch\"],\n                n_embedding_parameters=n_embedding_parameters,\n            )\n        )\n\n    @staticmethod\n    def _get_n_embedding_parameters_from_sentence_transformers(\n        model: SentenceTransformer | CrossEncoder,\n    ) -&gt; int | None:\n        \"\"\"Calculates the number of embedding parameters in a SentenceTransformer model\n\n        This is based on the heuristic: `vocab_size * embedding_dim` where vocab_size and embedding_dim are extracted from the model's first\n        Transformer module.\n        \"\"\"\n        logger.info(\n            \"Calculating number of embedding parameters for SentenceTransformer model.\"\n        )\n\n        emb = None\n        if isinstance(model, CrossEncoder) and hasattr(\n            model.model, \"get_input_embeddings\"\n        ):\n            emb = model.model.get_input_embeddings()\n            return int(np.prod(emb.weight.shape))\n        elif isinstance(model, SentenceTransformer):\n            vocab = None\n            try:\n                vocab = len(model.tokenizer.vocab)\n            except Exception as e:\n                msg = f\"Could not determine vocab size for model {model.model_card_data.model_name} and therefore cannot calculate number of embedding parameters. \\nError: \\n{e}\"\n                logger.warning(msg)\n            embedding_dimensions = model.get_sentence_embedding_dimension()\n            if embedding_dimensions is not None and vocab is not None:\n                return vocab * embedding_dimensions\n\n        logger.warning(\n            f\"Model does not have a recognized architecture for calculating embedding parameters (model={model.model_card_data.model_name}).\"\n        )\n        return None\n\n    @classmethod\n    def _from_cross_encoder_model(cls, model: CrossEncoder) -&gt; Self:\n        \"\"\"Generates a ModelMeta from only a CrossEncoder model, without fetching any additional metadata from HuggingFace Hub.\"\"\"\n        from mteb.models import CrossEncoderWrapper\n\n        return cls.create_empty(\n            overwrites=dict(\n                loader=CrossEncoderWrapper,\n                name=model.model.name_or_path,\n                revision=model.config._commit_hash,\n                framework=[\"Sentence Transformers\", \"PyTorch\"],\n                model_type=[\"cross-encoder\"],\n                n_embedding_parameters=cls._get_n_embedding_parameters_from_sentence_transformers(\n                    model\n                ),\n            )\n        )\n\n    @classmethod\n    def _from_hub(\n        cls,\n        model_name: str,\n        revision: str | None = None,\n    ) -&gt; Self:\n        \"\"\"Generates a ModelMeta from a HuggingFace model name.\n\n        Args:\n            model_name: The HuggingFace model name.\n            revision: Revision of the model\n            fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n\n        Returns:\n            The generated ModelMeta.\n        \"\"\"\n        loader: Callable[..., MTEBModels] | None\n        model_type: MODEL_TYPES\n\n        reference = \"https://huggingface.co/\" + model_name\n\n        if not _repo_exists(model_name):\n            warnings.warn(\n                f\"Could not find model {model_name} on HuggingFace Hub repository ({reference}). Metadata will be limited.\"\n            )\n            return cls.create_empty(\n                overwrites=dict(\n                    name=model_name,\n                    revision=revision,\n                )\n            )\n        config = _get_json_from_hub(\n            model_name, \"config.json\", \"model\", revision=revision\n        )\n        loader, model_type = cls._detect_model_type_and_loader(\n            model_name, revision, config=config\n        )\n        card = ModelCard.load(model_name)\n        card_data = card.data\n        card_data = cast(\"ModelCardData\", card_data)\n        try:\n            model_config = AutoConfig.from_pretrained(model_name)\n        except Exception as e:\n            # some models can't load AutoConfig (e.g. `average_word_embeddings_levy_dependency`)\n            model_config = None\n            logger.warning(\n                f\"Can't get model configuration for {model_name}. Error: {e}\"\n            )\n\n        frameworks = cls._get_frameworks_from_hf_tags(model_name) if model_name else []\n\n        if revision is None:\n            revisions = _get_repo_commits(model_name, \"model\")\n            revision = revisions[0].commit_id if revisions else None\n\n        model_license = card_data.license if card_data.license != \"other\" else None\n        n_parameters = cls._calculate_num_parameters_from_hub(model_name)\n        n_embedding_parameters = cls._estimate_embedding_parameters_from_hub(\n            model_name, revision=revision, config=config\n        )\n        memory_usage_mb = cls._calculate_memory_usage_mb(\n            model_name, n_parameters, fetch_from_hf=True\n        )\n\n        embedding_dim = getattr(model_config, \"hidden_size\", None)\n        max_tokens = getattr(model_config, \"max_position_embeddings\", None)\n\n        sbert_config = _get_json_from_hub(\n            model_name, \"sentence_bert_config.json\", \"model\", revision=revision\n        )\n        if sbert_config:\n            if max_tokens is None:\n                max_tokens = sbert_config.get(\"max_seq_length\", None)\n        # have model type, similarity function fields\n        config_sbert = _get_json_from_hub(\n            model_name, \"config_sentence_transformers.json\", \"model\", revision=revision\n        )\n        similarity_fn_name = (\n            ScoringFunction.from_str(config_sbert[\"similarity_fn_name\"])\n            if config_sbert is not None\n            and config_sbert.get(\"similarity_fn_name\") is not None\n            else ScoringFunction.COSINE\n        )\n\n        return cls.create_empty(\n            overwrites=dict(\n                loader=loader,\n                name=model_name,\n                model_type=[model_type],\n                revision=revision,\n                reference=reference,\n                release_date=cls.fetch_release_date(model_name),\n                license=model_license,\n                framework=frameworks,\n                n_parameters=n_parameters,\n                n_embedding_parameters=n_embedding_parameters,\n                memory_usage_mb=memory_usage_mb,\n                max_tokens=max_tokens,\n                embed_dim=embedding_dim,\n                similarity_fn_name=similarity_fn_name,\n            )\n        )\n\n    @classmethod\n    def from_sentence_transformer_model(\n        cls,\n        model: SentenceTransformer,\n        revision: str | None = None,\n        fill_missing: bool = False,\n        compute_metadata: bool | None = None,\n        fetch_from_hf: bool = False,\n    ) -&gt; Self:\n        \"\"\"Generates a ModelMeta from a SentenceTransformer model.\n\n        Args:\n            model: SentenceTransformer model.\n            revision: Revision of the model\n            fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n            compute_metadata: Deprecated. Use fill_missing instead.\n            fetch_from_hf: Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be\n                extracted from the SentenceTransformer model will be used.\n\n        Returns:\n            The generated ModelMeta.\n        \"\"\"\n        if compute_metadata is not None:\n            warnings.warn(\n                \"The compute_metadata parameter is deprecated and will be removed in a future version. \"\n                f\"Use fetch_from_hf instead. Setting `fetch_from_hf={compute_metadata}`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fetch_from_hf = compute_metadata\n\n        if fill_missing is not None:\n            warnings.warn(\n                \"The fill_missing parameter is deprecated and will be removed in a future version. \"\n                f\"Use fetch_from_hf instead. Setting `fetch_from_hf={fill_missing}`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fetch_from_hf = fill_missing\n\n        meta = cls._from_sentence_transformer_model(model)\n        if fetch_from_hf:\n            if meta.name is None:\n                logger.warning(\n                    \"Model name is not set in metadata extracted from SentenceTransformer model. Cannot fetch additional metadata from HuggingFace Hub.\"\n                )\n            else:\n                name = cast(\"str\", meta.name)\n                meta_hub = cls._from_hub(name, revision)\n                # prioritize metadata from the model card but fill missing fields from the hub\n                meta = meta_hub.merge(meta)\n\n        return meta\n\n    @classmethod\n    def from_hub(\n        cls,\n        model: str,\n        revision: str | None = None,\n        fill_missing: bool | None = None,\n        compute_metadata: bool | None = None,\n    ) -&gt; Self:\n        \"\"\"Generates a ModelMeta for model from HuggingFace hub.\n\n        Args:\n            model: Name of the model from HuggingFace hub. For example, `intfloat/multilingual-e5-large`\n            revision: Revision of the model\n            fill_missing: Deprecated. The fill missing did not add any functionality for this function, but was added for compatibility with\n                'from_sentence_transformer_model' and `from_cross_encoder`. It will be removed in a future version.\n            compute_metadata: Deprecated. Was superseded by fill_missing.\n\n        Returns:\n            The generated ModelMeta.\n        \"\"\"\n        if compute_metadata is not None:\n            warnings.warn(\n                \"The compute_metadata parameter is deprecated and will be removed in a future version. It will be ignored.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if fill_missing is not None:\n            warnings.warn(\n                \"The fill_missing parameter is deprecated and will be removed in a future version. It will be ignored.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        meta = cls._from_hub(\n            model,\n            revision,\n        )\n\n        return meta\n\n    @classmethod\n    def from_cross_encoder(\n        cls,\n        model: CrossEncoder,\n        revision: str | None = None,\n        fill_missing: bool | None = None,\n        compute_metadata: bool | None = None,\n        fetch_from_hf: bool = False,\n    ) -&gt; Self:\n        \"\"\"Generates a ModelMeta from a CrossEncoder.\n\n        Args:\n            model: The CrossEncoder model\n            revision: Revision of the model\n            fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n            compute_metadata: Deprecated. Use fill_missing instead.\n            fetch_from_hf: Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be\n                extracted from the CrossEncoder model will be used.\n\n        Returns:\n            The generated ModelMeta\n        \"\"\"\n        if compute_metadata is not None:\n            warnings.warn(\n                \"The compute_metadata parameter is deprecated and will be removed in a future version. \"\n                f\"Use fetch_from_hf instead. Setting `fetch_from_hf={compute_metadata}`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fetch_from_hf = compute_metadata\n        if fill_missing is not None:\n            warnings.warn(\n                \"The fill_missing parameter is deprecated and will be removed in a future version. \"\n                f\"Use fill_missing instead. Setting `fill_missing={fill_missing}`.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            fetch_from_hf = fill_missing\n\n        meta = cls._from_cross_encoder_model(model)\n        if fetch_from_hf:\n            name = cast(\"str\", meta.name)\n            meta_hub = cls._from_hub(name, revision)\n            # prioritize metadata from the model card but fill missing fields from the hub\n            meta = meta_hub.merge(meta)\n\n        return meta\n\n    def is_zero_shot_on(self, tasks: Sequence[AbsTask] | Sequence[str]) -&gt; bool | None:\n        \"\"\"Indicates whether the given model can be considered zero-shot or not on the given tasks.\n\n        Returns:\n             None if no training data is specified on the model.\n        \"\"\"\n        # If no tasks were specified, we're obviously zero-shot\n        if not tasks:\n            return True\n        training_datasets = self.get_training_datasets()\n        # If no tasks were specified, we're obviously zero-shot\n        if training_datasets is None:\n            return None\n\n        if isinstance(tasks[0], str):\n            benchmark_datasets = set(tasks)\n        else:\n            tasks = cast(\"Sequence[AbsTask]\", tasks)\n            benchmark_datasets = set()\n            for task in tasks:\n                benchmark_datasets.add(task.metadata.name)\n        intersection = training_datasets &amp; benchmark_datasets\n        return len(intersection) == 0\n\n    def get_training_datasets(self) -&gt; set[str] | None:\n        \"\"\"Returns all training datasets of the model including similar tasks.\"\"\"\n        import mteb\n\n        if self.training_datasets is None:\n            return None\n\n        training_datasets = self.training_datasets.copy()\n        if self.adapted_from is not None:\n            try:\n                adapted_from_model = mteb.get_model_meta(\n                    self.adapted_from, fetch_from_hf=False\n                )\n                adapted_training_datasets = adapted_from_model.get_training_datasets()\n                if adapted_training_datasets is not None:\n                    training_datasets |= adapted_training_datasets\n            except (ValueError, KeyError) as e:\n                msg = f\"Could not get source model: {e} in MTEB\"\n                logger.warning(msg)\n                warnings.warn(msg)\n\n        return_dataset = training_datasets.copy()\n        visited: set[str] = set()\n\n        for dataset in training_datasets:\n            similar_tasks = _collect_similar_tasks(dataset, visited)\n            return_dataset |= similar_tasks\n\n        return return_dataset\n\n    def zero_shot_percentage(\n        self, tasks: Sequence[AbsTask] | Sequence[str]\n    ) -&gt; int | None:\n        \"\"\"Indicates how out-of-domain the selected tasks are for the given model.\n\n        Args:\n            tasks: A sequence of tasks or dataset names to evaluate against.\n\n        Returns:\n            An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.\n            Returns None if no training data is specified on the model or if no tasks are provided.\n        \"\"\"\n        training_datasets = self.get_training_datasets()\n        if (training_datasets is None) or (not tasks):\n            return None\n        if isinstance(tasks[0], str):\n            benchmark_datasets = set(tasks)\n        else:\n            tasks = cast(\"Sequence[AbsTask]\", tasks)\n            benchmark_datasets = {task.metadata.name for task in tasks}\n        overlap = training_datasets &amp; benchmark_datasets\n        perc_overlap = 100 * (len(overlap) / len(benchmark_datasets))\n        return int(100 - perc_overlap)\n\n    @staticmethod\n    def _calculate_num_parameters_from_hub(model_name: str | None = None) -&gt; int | None:\n        if not model_name:\n            return None\n        try:\n            safetensors_metadata = get_safetensors_metadata(model_name)\n            if len(safetensors_metadata.parameter_count) &gt;= 0:\n                return sum(safetensors_metadata.parameter_count.values())\n        except (\n            NotASafetensorsRepoError,\n            SafetensorsParsingError,\n            GatedRepoError,\n            RepositoryNotFoundError,\n        ) as e:\n            logger.warning(\n                f\"Can't calculate number of parameters for {model_name}. Got error {e}\"\n            )\n        return None\n\n    def calculate_num_parameters_from_hub(self) -&gt; int | None:\n        \"\"\"Calculates the number of parameters in the model.\n\n        Returns:\n            Number of parameters in the model.\n        \"\"\"\n        return self._calculate_num_parameters_from_hub(self.name)\n\n    @staticmethod\n    def _estimate_embedding_parameters_from_hub(\n        model_name: str | None = None,\n        revision: str | None = None,\n        config: dict[str, Any] | None = None,\n    ) -&gt; int | None:\n        \"\"\"Calculate the number of embedding parameters from the model config (vocab_size * hidden_size).  Note that this is an heuristic that works for many models, but might be incorrect.\n\n        Returns:\n            Number of embedding parameters in the model.\n        \"\"\"\n        if not model_name:\n            return None\n\n        if not config:\n            logger.warning(\n                f\"Could not calculate embedding parameters for {model_name} as config.json could not be loaded\"\n            )\n            return None\n\n        vocab_size = config.get(\"vocab_size\")\n        if vocab_size is None and \"text_config\" in config:\n            vocab_size = config[\"text_config\"].get(\"vocab_size\")\n\n        if vocab_size is None:\n            logger.warning(\n                f\"Could not calculate embedding parameters for {model_name} as vocab_size is missing from config\"\n            )\n            return None\n\n        hidden_size = config.get(\"hidden_size\") or config.get(\"hidden_dim\")\n        if hidden_size is None and \"text_config\" in config:\n            hidden_size = config[\"text_config\"].get(\"hidden_size\") or config[\n                \"text_config\"\n            ].get(\"hidden_dim\")\n\n        if hidden_size is None:\n            logger.warning(\n                f\"Could not calculate embedding parameters for {model_name} as hidden_size/hidden_dim is missing from config\"\n            )\n            return None\n        return vocab_size * hidden_size\n\n    @staticmethod\n    def _calculate_memory_usage_mb(\n        model_name: str,\n        n_parameters: int | None,\n        *,\n        fetch_from_hf: bool = False,\n    ) -&gt; int | None:\n        MB = 1024**2  # noqa: N806\n\n        if fetch_from_hf:\n            try:\n                safetensors_metadata = get_safetensors_metadata(model_name)\n                if safetensors_metadata.parameter_count:\n                    dtype_size_map = {\n                        \"F64\": 8,  # 64-bit float\n                        \"F32\": 4,  # 32-bit float (FP32)\n                        \"F16\": 2,  # 16-bit float (FP16)\n                        \"BF16\": 2,  # BFloat16\n                        \"I64\": 8,  # 64-bit integer\n                        \"I32\": 4,  # 32-bit integer\n                        \"I16\": 2,  # 16-bit integer\n                        \"I8\": 1,  # 8-bit integer\n                        \"U8\": 1,  # Unsigned 8-bit integer\n                        \"BOOL\": 1,  # Boolean (assuming 1 byte per value)\n                    }\n                    total_memory_bytes = sum(\n                        parameters * dtype_size_map.get(dtype, 4)\n                        for dtype, parameters in safetensors_metadata.parameter_count.items()\n                    )\n                    return round(total_memory_bytes / MB)  # Convert to MB\n            except (\n                NotASafetensorsRepoError,\n                SafetensorsParsingError,\n                GatedRepoError,\n                RepositoryNotFoundError,\n            ) as e:\n                logger.warning(\n                    f\"Can't calculate memory usage for {model_name}. Got error {e}\"\n                )\n\n        if n_parameters is None:\n            return None\n        # Model memory in bytes. For FP32 each parameter is 4 bytes.\n        model_memory_bytes = n_parameters * 4\n\n        # Convert to MB\n        model_memory_mb = model_memory_bytes / MB\n        return round(model_memory_mb)\n\n    def calculate_memory_usage_mb(self, fetch_from_hf: bool = False) -&gt; int | None:\n        \"\"\"Calculates the memory usage of the model in MB.\n\n        Args:\n            fetch_from_hf: If True, fetch safetensors metadata from HuggingFace Hub\n                to get precise dtype-aware memory usage. If False (default), estimate\n                from n_parameters assuming FP32 (4 bytes per parameter).\n\n        Returns:\n            The memory usage of the model in MB, or None if it cannot be determined.\n        \"\"\"\n        if \"API\" in self.framework or self.name is None:\n            return None\n\n        return self._calculate_memory_usage_mb(\n            self.name, self.n_parameters, fetch_from_hf=fetch_from_hf\n        )\n\n    @staticmethod\n    def fetch_release_date(model_name: str) -&gt; StrDate | None:\n        \"\"\"Fetches the release date from HuggingFace Hub based on the first commit.\n\n        Returns:\n            The release date in YYYY-MM-DD format, or None if it cannot be determined.\n        \"\"\"\n        commits = _get_repo_commits(repo_id=model_name, repo_type=\"model\")\n        if commits:\n            initial_commit = commits[-1]\n            release_date = initial_commit.created_at.strftime(\"%Y-%m-%d\")\n            return release_date\n        return None\n\n    @staticmethod\n    def _get_frameworks_from_hf_tags(model_name: str) -&gt; list[FRAMEWORKS]:\n        \"\"\"Extract frameworks supported by the model from HuggingFace model tags.\n\n        Args:\n            model_name: HuggingFace model name\n\n        Returns:\n            List of framework names found in tags. Defaults to empty list if no frameworks found.\n        \"\"\"\n        try:\n            info = model_info(model_name)\n            if not info.tags:\n                return []\n        except Exception as e:\n            logger.warning(\n                f\"Failed to fetch frameworks from HuggingFace tags for {model_name}: {e}\"\n            )\n            return []\n\n        # Mapping from HuggingFace tags to MTEB framework names\n        tag_to_framework: dict[str, FRAMEWORKS] = {\n            \"sentence-transformers\": \"Sentence Transformers\",\n            \"transformers\": \"Transformers\",\n            \"onnx\": \"ONNX\",\n            \"safetensors\": \"safetensors\",\n            \"gguf\": \"GGUF\",\n        }\n\n        # Assume PyTorch support by default\n        # TODO: could be detected from repo as well: https://github.com/embeddings-benchmark/mteb/issues/4104\n        frameworks: list[FRAMEWORKS] = [\"PyTorch\"]\n\n        for framework_tag in tag_to_framework.keys():\n            if framework_tag in info.tags:\n                frameworks.append(tag_to_framework[framework_tag])\n\n        return frameworks\n\n    def to_python(self) -&gt; str:\n        \"\"\"Returns a string representation of the model.\"\"\"\n        return _pydantic_instance_to_code(self, exclude_fields=[\"experiment_kwargs\"])\n\n    def push_eval_results(\n        self,\n        user: str | None = None,\n        *,\n        tasks: Sequence[AbsTask] | Sequence[str] | None = None,\n        cache: ResultCache | None = None,\n        create_pr: bool = False,\n    ) -&gt; None:\n        \"\"\"Pushes the evaluation results of the model to the HuggingFace Hub.\n\n        Args:\n            user: The user or organization of results source.\n            tasks: The tasks to push results for. If None, results for all tasks will be pushed.\n            cache: The ResultCache containing the evaluation results to push.\n            create_pr: Whether to create a pull request for the model card update if the model card already exists on the HuggingFace Hub. If False, the model card will be updated directly without a pull request.\n        \"\"\"\n        from mteb.cache import ResultCache\n\n        if cache is None:\n            cache = ResultCache()\n\n        benchmark_result = cache.load_results(\n            models=[self],\n            tasks=tasks,\n        )\n        model_result = benchmark_result.model_results[0]\n        model_result.push_model_results(\n            user=user,\n            create_pr=create_pr,\n        )\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.experiment_name","title":"<code>experiment_name</code>  <code>property</code>","text":"<p>Create a filesystem-safe string representation of the experiment parameters.</p> <p>Uses deterministic serialization and hashing to ensure stable, bounded output.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; model = mteb.get_model(\"mteb/baseline-random-encoder\", param1=\"test\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(model.mteb_model_meta.experiment_name)\n&gt;&gt;&gt; # param1_test\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.is_cross_encoder","title":"<code>is_cross_encoder</code>  <code>property</code>","text":"<p>Returns True if the model is a cross-encoder.</p> <p>Derived from model_type field. A model is considered a cross-encoder if \"cross-encoder\" is in its model_type list.</p>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.model_name_with_experiment","title":"<code>model_name_with_experiment</code>  <code>property</code>","text":"<p>Combines the model name with the experiment parameters for a more descriptive name.</p>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.n_active_parameters","title":"<code>n_active_parameters</code>  <code>property</code>","text":"<p>Number of active parameters. Assumed to be <code>n_parameters - n_embedding_parameters</code>. Can be overwritten using <code>n_active_parameters_override</code> e.g. for MoE models.</p>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.calculate_memory_usage_mb","title":"<code>calculate_memory_usage_mb(fetch_from_hf=False)</code>","text":"<p>Calculates the memory usage of the model in MB.</p> <p>Parameters:</p> Name Type Description Default <code>fetch_from_hf</code> <code>bool</code> <p>If True, fetch safetensors metadata from HuggingFace Hub to get precise dtype-aware memory usage. If False (default), estimate from n_parameters assuming FP32 (4 bytes per parameter).</p> <code>False</code> <p>Returns:</p> Type Description <code>int | None</code> <p>The memory usage of the model in MB, or None if it cannot be determined.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def calculate_memory_usage_mb(self, fetch_from_hf: bool = False) -&gt; int | None:\n    \"\"\"Calculates the memory usage of the model in MB.\n\n    Args:\n        fetch_from_hf: If True, fetch safetensors metadata from HuggingFace Hub\n            to get precise dtype-aware memory usage. If False (default), estimate\n            from n_parameters assuming FP32 (4 bytes per parameter).\n\n    Returns:\n        The memory usage of the model in MB, or None if it cannot be determined.\n    \"\"\"\n    if \"API\" in self.framework or self.name is None:\n        return None\n\n    return self._calculate_memory_usage_mb(\n        self.name, self.n_parameters, fetch_from_hf=fetch_from_hf\n    )\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.calculate_num_parameters_from_hub","title":"<code>calculate_num_parameters_from_hub()</code>","text":"<p>Calculates the number of parameters in the model.</p> <p>Returns:</p> Type Description <code>int | None</code> <p>Number of parameters in the model.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def calculate_num_parameters_from_hub(self) -&gt; int | None:\n    \"\"\"Calculates the number of parameters in the model.\n\n    Returns:\n        Number of parameters in the model.\n    \"\"\"\n    return self._calculate_num_parameters_from_hub(self.name)\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.create_empty","title":"<code>create_empty(overwrites=None)</code>  <code>classmethod</code>","text":"<p>Creates an empty ModelMeta with all fields set to None or empty.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>@classmethod\ndef create_empty(cls, overwrites: dict[str, Any] | None = None) -&gt; Self:\n    \"\"\"Creates an empty ModelMeta with all fields set to None or empty.\"\"\"\n    empty_model = cls(\n        loader=None,\n        name=None,\n        revision=None,\n        release_date=None,\n        languages=None,\n        n_parameters=None,\n        n_embedding_parameters=None,\n        memory_usage_mb=None,\n        max_tokens=None,\n        embed_dim=None,\n        license=None,\n        open_weights=None,\n        public_training_code=None,\n        public_training_data=None,\n        framework=[],\n        reference=None,\n        similarity_fn_name=None,\n        use_instructions=None,\n        training_datasets=None,\n        adapted_from=None,\n        superseded_by=None,\n        citation=None,\n        contacts=None,\n    )\n    if overwrites:\n        empty_model = empty_model.model_copy(update=overwrites)\n\n    if empty_model.name is None:\n        empty_model.name = \"no_model_name/available\"\n    if empty_model.revision is None:\n        empty_model.revision = \"no_revision_available\"\n\n    return empty_model\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.fetch_release_date","title":"<code>fetch_release_date(model_name)</code>  <code>staticmethod</code>","text":"<p>Fetches the release date from HuggingFace Hub based on the first commit.</p> <p>Returns:</p> Type Description <code>StrDate | None</code> <p>The release date in YYYY-MM-DD format, or None if it cannot be determined.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>@staticmethod\ndef fetch_release_date(model_name: str) -&gt; StrDate | None:\n    \"\"\"Fetches the release date from HuggingFace Hub based on the first commit.\n\n    Returns:\n        The release date in YYYY-MM-DD format, or None if it cannot be determined.\n    \"\"\"\n    commits = _get_repo_commits(repo_id=model_name, repo_type=\"model\")\n    if commits:\n        initial_commit = commits[-1]\n        release_date = initial_commit.created_at.strftime(\"%Y-%m-%d\")\n        return release_date\n    return None\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.from_cross_encoder","title":"<code>from_cross_encoder(model, revision=None, fill_missing=None, compute_metadata=None, fetch_from_hf=False)</code>  <code>classmethod</code>","text":"<p>Generates a ModelMeta from a CrossEncoder.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>CrossEncoder</code> <p>The CrossEncoder model</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model</p> <code>None</code> <code>fill_missing</code> <code>bool | None</code> <p>Fill missing attributes from the metadata including number of parameters and memory usage.</p> <code>None</code> <code>compute_metadata</code> <code>bool | None</code> <p>Deprecated. Use fill_missing instead.</p> <code>None</code> <code>fetch_from_hf</code> <code>bool</code> <p>Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be extracted from the CrossEncoder model will be used.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The generated ModelMeta</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>@classmethod\ndef from_cross_encoder(\n    cls,\n    model: CrossEncoder,\n    revision: str | None = None,\n    fill_missing: bool | None = None,\n    compute_metadata: bool | None = None,\n    fetch_from_hf: bool = False,\n) -&gt; Self:\n    \"\"\"Generates a ModelMeta from a CrossEncoder.\n\n    Args:\n        model: The CrossEncoder model\n        revision: Revision of the model\n        fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n        compute_metadata: Deprecated. Use fill_missing instead.\n        fetch_from_hf: Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be\n            extracted from the CrossEncoder model will be used.\n\n    Returns:\n        The generated ModelMeta\n    \"\"\"\n    if compute_metadata is not None:\n        warnings.warn(\n            \"The compute_metadata parameter is deprecated and will be removed in a future version. \"\n            f\"Use fetch_from_hf instead. Setting `fetch_from_hf={compute_metadata}`.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fetch_from_hf = compute_metadata\n    if fill_missing is not None:\n        warnings.warn(\n            \"The fill_missing parameter is deprecated and will be removed in a future version. \"\n            f\"Use fill_missing instead. Setting `fill_missing={fill_missing}`.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fetch_from_hf = fill_missing\n\n    meta = cls._from_cross_encoder_model(model)\n    if fetch_from_hf:\n        name = cast(\"str\", meta.name)\n        meta_hub = cls._from_hub(name, revision)\n        # prioritize metadata from the model card but fill missing fields from the hub\n        meta = meta_hub.merge(meta)\n\n    return meta\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.from_hub","title":"<code>from_hub(model, revision=None, fill_missing=None, compute_metadata=None)</code>  <code>classmethod</code>","text":"<p>Generates a ModelMeta for model from HuggingFace hub.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model from HuggingFace hub. For example, <code>intfloat/multilingual-e5-large</code></p> required <code>revision</code> <code>str | None</code> <p>Revision of the model</p> <code>None</code> <code>fill_missing</code> <code>bool | None</code> <p>Deprecated. The fill missing did not add any functionality for this function, but was added for compatibility with 'from_sentence_transformer_model' and <code>from_cross_encoder</code>. It will be removed in a future version.</p> <code>None</code> <code>compute_metadata</code> <code>bool | None</code> <p>Deprecated. Was superseded by fill_missing.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>The generated ModelMeta.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>@classmethod\ndef from_hub(\n    cls,\n    model: str,\n    revision: str | None = None,\n    fill_missing: bool | None = None,\n    compute_metadata: bool | None = None,\n) -&gt; Self:\n    \"\"\"Generates a ModelMeta for model from HuggingFace hub.\n\n    Args:\n        model: Name of the model from HuggingFace hub. For example, `intfloat/multilingual-e5-large`\n        revision: Revision of the model\n        fill_missing: Deprecated. The fill missing did not add any functionality for this function, but was added for compatibility with\n            'from_sentence_transformer_model' and `from_cross_encoder`. It will be removed in a future version.\n        compute_metadata: Deprecated. Was superseded by fill_missing.\n\n    Returns:\n        The generated ModelMeta.\n    \"\"\"\n    if compute_metadata is not None:\n        warnings.warn(\n            \"The compute_metadata parameter is deprecated and will be removed in a future version. It will be ignored.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    if fill_missing is not None:\n        warnings.warn(\n            \"The fill_missing parameter is deprecated and will be removed in a future version. It will be ignored.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    meta = cls._from_hub(\n        model,\n        revision,\n    )\n\n    return meta\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.from_sentence_transformer_model","title":"<code>from_sentence_transformer_model(model, revision=None, fill_missing=False, compute_metadata=None, fetch_from_hf=False)</code>  <code>classmethod</code>","text":"<p>Generates a ModelMeta from a SentenceTransformer model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SentenceTransformer</code> <p>SentenceTransformer model.</p> required <code>revision</code> <code>str | None</code> <p>Revision of the model</p> <code>None</code> <code>fill_missing</code> <code>bool</code> <p>Fill missing attributes from the metadata including number of parameters and memory usage.</p> <code>False</code> <code>compute_metadata</code> <code>bool | None</code> <p>Deprecated. Use fill_missing instead.</p> <code>None</code> <code>fetch_from_hf</code> <code>bool</code> <p>Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be extracted from the SentenceTransformer model will be used.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The generated ModelMeta.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>@classmethod\ndef from_sentence_transformer_model(\n    cls,\n    model: SentenceTransformer,\n    revision: str | None = None,\n    fill_missing: bool = False,\n    compute_metadata: bool | None = None,\n    fetch_from_hf: bool = False,\n) -&gt; Self:\n    \"\"\"Generates a ModelMeta from a SentenceTransformer model.\n\n    Args:\n        model: SentenceTransformer model.\n        revision: Revision of the model\n        fill_missing: Fill missing attributes from the metadata including number of parameters and memory usage.\n        compute_metadata: Deprecated. Use fill_missing instead.\n        fetch_from_hf: Whether to fetch additional metadata from HuggingFace Hub based on the model name. If False, only metadata that can be\n            extracted from the SentenceTransformer model will be used.\n\n    Returns:\n        The generated ModelMeta.\n    \"\"\"\n    if compute_metadata is not None:\n        warnings.warn(\n            \"The compute_metadata parameter is deprecated and will be removed in a future version. \"\n            f\"Use fetch_from_hf instead. Setting `fetch_from_hf={compute_metadata}`.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fetch_from_hf = compute_metadata\n\n    if fill_missing is not None:\n        warnings.warn(\n            \"The fill_missing parameter is deprecated and will be removed in a future version. \"\n            f\"Use fetch_from_hf instead. Setting `fetch_from_hf={fill_missing}`.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        fetch_from_hf = fill_missing\n\n    meta = cls._from_sentence_transformer_model(model)\n    if fetch_from_hf:\n        if meta.name is None:\n            logger.warning(\n                \"Model name is not set in metadata extracted from SentenceTransformer model. Cannot fetch additional metadata from HuggingFace Hub.\"\n            )\n        else:\n            name = cast(\"str\", meta.name)\n            meta_hub = cls._from_hub(name, revision)\n            # prioritize metadata from the model card but fill missing fields from the hub\n            meta = meta_hub.merge(meta)\n\n    return meta\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.get_training_datasets","title":"<code>get_training_datasets()</code>","text":"<p>Returns all training datasets of the model including similar tasks.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def get_training_datasets(self) -&gt; set[str] | None:\n    \"\"\"Returns all training datasets of the model including similar tasks.\"\"\"\n    import mteb\n\n    if self.training_datasets is None:\n        return None\n\n    training_datasets = self.training_datasets.copy()\n    if self.adapted_from is not None:\n        try:\n            adapted_from_model = mteb.get_model_meta(\n                self.adapted_from, fetch_from_hf=False\n            )\n            adapted_training_datasets = adapted_from_model.get_training_datasets()\n            if adapted_training_datasets is not None:\n                training_datasets |= adapted_training_datasets\n        except (ValueError, KeyError) as e:\n            msg = f\"Could not get source model: {e} in MTEB\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n    return_dataset = training_datasets.copy()\n    visited: set[str] = set()\n\n    for dataset in training_datasets:\n        similar_tasks = _collect_similar_tasks(dataset, visited)\n        return_dataset |= similar_tasks\n\n    return return_dataset\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.is_zero_shot_on","title":"<code>is_zero_shot_on(tasks)</code>","text":"<p>Indicates whether the given model can be considered zero-shot or not on the given tasks.</p> <p>Returns:</p> Type Description <code>bool | None</code> <p>None if no training data is specified on the model.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def is_zero_shot_on(self, tasks: Sequence[AbsTask] | Sequence[str]) -&gt; bool | None:\n    \"\"\"Indicates whether the given model can be considered zero-shot or not on the given tasks.\n\n    Returns:\n         None if no training data is specified on the model.\n    \"\"\"\n    # If no tasks were specified, we're obviously zero-shot\n    if not tasks:\n        return True\n    training_datasets = self.get_training_datasets()\n    # If no tasks were specified, we're obviously zero-shot\n    if training_datasets is None:\n        return None\n\n    if isinstance(tasks[0], str):\n        benchmark_datasets = set(tasks)\n    else:\n        tasks = cast(\"Sequence[AbsTask]\", tasks)\n        benchmark_datasets = set()\n        for task in tasks:\n            benchmark_datasets.add(task.metadata.name)\n    intersection = training_datasets &amp; benchmark_datasets\n    return len(intersection) == 0\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.load_model","title":"<code>load_model(device=None, **kwargs)</code>","text":"<p>Loads the model using the specified loader function.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def load_model(self, device: str | None = None, **kwargs: Any) -&gt; MTEBModels:\n    \"\"\"Loads the model using the specified loader function.\"\"\"\n    if self.loader is None:\n        raise NotImplementedError(\n            \"No model implementation is available for this model.\"\n        )\n    if self.name is None:\n        raise ValueError(\"name is not set for ModelMeta. Cannot load model.\")\n\n    if self.experiment_kwargs is None:\n        self.experiment_kwargs = kwargs if len(kwargs) &gt; 0 else None\n    elif len(kwargs) &gt; 0 and self.experiment_kwargs is not None:\n        kwargs |= self.experiment_kwargs\n        self.experiment_kwargs = kwargs\n\n    # Allow overwrites\n    _kwargs = self.loader_kwargs.copy()\n    _kwargs.update(kwargs)\n    if device is not None:\n        _kwargs[\"device\"] = device\n\n    model: MTEBModels = self.loader(self.name, revision=self.revision, **_kwargs)\n    model.mteb_model_meta = self  # type: ignore[misc]\n    return model\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.merge","title":"<code>merge(overwrite)</code>","text":"<p>Merges another this ModelMeta with another ModelMeta.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>Self</code> <p>The ModelMeta to merge into this one. Non-None fields in this ModelMeta will overwrite the corresponding fields in this ModelMeta. the <code>framework</code> and <code>model_type</code> fields with combined.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>A new ModelMeta with the merged fields.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def merge(self, overwrite: Self) -&gt; Self:\n    \"\"\"Merges another this ModelMeta with another ModelMeta.\n\n    Args:\n        overwrite: The ModelMeta to merge into this one. Non-None fields in this ModelMeta will overwrite the corresponding fields in this\n            ModelMeta. the `framework` and `model_type` fields with combined.\n\n    Returns:\n        A new ModelMeta with the merged fields.\n    \"\"\"\n    merged_data = self.model_dump()\n    overwrite_data = overwrite.model_dump()\n\n    for key, value in overwrite_data.items():\n        if (\n            key == \"name\"\n            and value == \"no_model_name/available\"\n            and self.name != \"no_model_name/available\"\n        ):\n            continue  # skip overwriting name if overwrite has no name available\n        if (\n            key == \"revision\"\n            and value == \"no_revision_available\"\n            and self.revision != \"no_revision_available\"\n        ):\n            continue  # skip overwriting revision if overwrite has no revision available\n        if key in [\"framework\", \"model_type\"]:\n            # Combine lists and remove duplicates\n            merged_list = set(merged_data.get(key, [])) | set(value or [])\n            merged_data[key] = list(merged_list)\n        if value is not None:\n            merged_data[key] = value\n\n    return self.model_copy(update=merged_data)\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.model_name_as_path","title":"<code>model_name_as_path()</code>","text":"<p>Returns the model name in a format that can be used as a file path.</p> <p>Replaces \"/\" with \"__\" and spaces with \"_\".</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def model_name_as_path(self) -&gt; str:\n    \"\"\"Returns the model name in a format that can be used as a file path.\n\n    Replaces \"/\" with \"__\" and spaces with \"_\".\n    \"\"\"\n    if self.name is None:\n        raise ValueError(\"Model name is not set\")\n    return self.name.replace(\"/\", \"__\").replace(\" \", \"_\")\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.push_eval_results","title":"<code>push_eval_results(user=None, *, tasks=None, cache=None, create_pr=False)</code>","text":"<p>Pushes the evaluation results of the model to the HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str | None</code> <p>The user or organization of results source.</p> <code>None</code> <code>tasks</code> <code>Sequence[AbsTask] | Sequence[str] | None</code> <p>The tasks to push results for. If None, results for all tasks will be pushed.</p> <code>None</code> <code>cache</code> <code>ResultCache | None</code> <p>The ResultCache containing the evaluation results to push.</p> <code>None</code> <code>create_pr</code> <code>bool</code> <p>Whether to create a pull request for the model card update if the model card already exists on the HuggingFace Hub. If False, the model card will be updated directly without a pull request.</p> <code>False</code> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def push_eval_results(\n    self,\n    user: str | None = None,\n    *,\n    tasks: Sequence[AbsTask] | Sequence[str] | None = None,\n    cache: ResultCache | None = None,\n    create_pr: bool = False,\n) -&gt; None:\n    \"\"\"Pushes the evaluation results of the model to the HuggingFace Hub.\n\n    Args:\n        user: The user or organization of results source.\n        tasks: The tasks to push results for. If None, results for all tasks will be pushed.\n        cache: The ResultCache containing the evaluation results to push.\n        create_pr: Whether to create a pull request for the model card update if the model card already exists on the HuggingFace Hub. If False, the model card will be updated directly without a pull request.\n    \"\"\"\n    from mteb.cache import ResultCache\n\n    if cache is None:\n        cache = ResultCache()\n\n    benchmark_result = cache.load_results(\n        models=[self],\n        tasks=tasks,\n    )\n    model_result = benchmark_result.model_results[0]\n    model_result.push_model_results(\n        user=user,\n        create_pr=create_pr,\n    )\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns a dictionary representation of the model metadata.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def to_dict(self):\n    \"\"\"Returns a dictionary representation of the model metadata.\"\"\"\n    dict_repr = self.model_dump()\n    loader = dict_repr.pop(\"loader\", None)\n    dict_repr[\"training_datasets\"] = (\n        list(dict_repr[\"training_datasets\"])\n        if isinstance(dict_repr[\"training_datasets\"], set)\n        else dict_repr[\"training_datasets\"]\n    )\n    dict_repr[\"loader\"] = _get_loader_name(loader)\n    dict_repr[\"is_cross_encoder\"] = self.is_cross_encoder\n    return dict_repr\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.to_python","title":"<code>to_python()</code>","text":"<p>Returns a string representation of the model.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def to_python(self) -&gt; str:\n    \"\"\"Returns a string representation of the model.\"\"\"\n    return _pydantic_instance_to_code(self, exclude_fields=[\"experiment_kwargs\"])\n</code></pre>"},{"location":"api/model/#mteb.models.model_meta.ModelMeta.zero_shot_percentage","title":"<code>zero_shot_percentage(tasks)</code>","text":"<p>Indicates how out-of-domain the selected tasks are for the given model.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[AbsTask] | Sequence[str]</code> <p>A sequence of tasks or dataset names to evaluate against.</p> required <p>Returns:</p> Type Description <code>int | None</code> <p>An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.</p> <code>int | None</code> <p>Returns None if no training data is specified on the model or if no tasks are provided.</p> Source code in <code>mteb/models/model_meta.py</code> <pre><code>def zero_shot_percentage(\n    self, tasks: Sequence[AbsTask] | Sequence[str]\n) -&gt; int | None:\n    \"\"\"Indicates how out-of-domain the selected tasks are for the given model.\n\n    Args:\n        tasks: A sequence of tasks or dataset names to evaluate against.\n\n    Returns:\n        An integer percentage (0-100) indicating how out-of-domain the tasks are for the model.\n        Returns None if no training data is specified on the model or if no tasks are provided.\n    \"\"\"\n    training_datasets = self.get_training_datasets()\n    if (training_datasets is None) or (not tasks):\n        return None\n    if isinstance(tasks[0], str):\n        benchmark_datasets = set(tasks)\n    else:\n        tasks = cast(\"Sequence[AbsTask]\", tasks)\n        benchmark_datasets = {task.metadata.name for task in tasks}\n    overlap = training_datasets &amp; benchmark_datasets\n    perc_overlap = 100 * (len(overlap) / len(benchmark_datasets))\n    return int(100 - perc_overlap)\n</code></pre>"},{"location":"api/model/#model-protocols","title":"Model Protocols","text":""},{"location":"api/model/#mteb.models.EncoderProtocol","title":"<code>mteb.models.EncoderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for an encoder in MTEB.</p> <p>Besides the required functions specified below, the encoder can additionally specify the following signatures seen below. In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass EncoderProtocol(Protocol):\n    \"\"\"The interface for an encoder in MTEB.\n\n    Besides the required functions specified below, the encoder can additionally specify the following signatures seen below.\n    In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        revision: str | None,\n        device: str | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n        Args:\n            model_name: Name of the model\n            revision: revision of the model\n            device: Device used to load the model\n            kwargs: Any additional kwargs\n        \"\"\"\n        ...\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Unpack[EncodeKwargs],\n    ) -&gt; Array:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: Batch of inputs to encode.\n            task_metadata: The metadata of the task. Encoders (e.g. SentenceTransformers) use to\n                select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.\n\n                The order of priorities for prompt selection are:\n                    1. Composed prompt of task name + prompt type (query or passage)\n                    2. Specific task prompt\n                    3. Composed prompt of task type + prompt type (query or passage)\n                    4. Specific task type prompt\n                    5. Specific prompt type (query or passage)\n            hf_split: Split of current task, allows to know some additional information about current split.\n                E.g. Current language\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            prompt_type: The name type of prompt. (query or passage)\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n        \"\"\"\n        ...\n\n    def similarity(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Compute the similarity between two collections of embeddings.\n\n        The output will be a matrix with the similarity scores between all embeddings from the first parameter and all\n        embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity\n        between corresponding pairs of embeddings.\n\n        Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity\n\n        Args:\n            embeddings1: [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n            embeddings2: [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n        Returns:\n            A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.\n        \"\"\"\n        ...\n\n    def similarity_pairwise(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.\n\n        Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise\n\n        Args:\n            embeddings1: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n            embeddings2: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n        Returns:\n            A [num_embeddings]-shaped torch tensor with pairwise similarity scores.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; ModelMeta:\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.EncoderProtocol.__init__","title":"<code>__init__(model_name, revision, device=None, **kwargs)</code>","text":"<p>The initialization function for the encoder. Used when calling it from the mteb run CLI.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>revision</code> <code>str | None</code> <p>revision of the model</p> required <code>device</code> <code>str | None</code> <p>Device used to load the model</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Any additional kwargs</p> <code>{}</code> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    revision: str | None,\n    device: str | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n    Args:\n        model_name: Name of the model\n        revision: revision of the model\n        device: Device used to load the model\n        kwargs: Any additional kwargs\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.encode","title":"<code>encode(inputs, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Encodes the given sentences using the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader[BatchedInput]</code> <p>Batch of inputs to encode.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>The metadata of the task. Encoders (e.g. SentenceTransformers) use to select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.</p> <p>The order of priorities for prompt selection are:     1. Composed prompt of task name + prompt type (query or passage)     2. Specific task prompt     3. Composed prompt of task type + prompt type (query or passage)     4. Specific task type prompt     5. Specific prompt type (query or passage)</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split. E.g. Current language</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>**kwargs</code> <code>Unpack[EncodeKwargs]</code> <p>Additional arguments to pass to the encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def encode(\n    self,\n    inputs: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Unpack[EncodeKwargs],\n) -&gt; Array:\n    \"\"\"Encodes the given sentences using the encoder.\n\n    Args:\n        inputs: Batch of inputs to encode.\n        task_metadata: The metadata of the task. Encoders (e.g. SentenceTransformers) use to\n            select the appropriate prompts, with priority given to more specific task/prompt combinations over general ones.\n\n            The order of priorities for prompt selection are:\n                1. Composed prompt of task name + prompt type (query or passage)\n                2. Specific task prompt\n                3. Composed prompt of task type + prompt type (query or passage)\n                4. Specific task type prompt\n                5. Specific prompt type (query or passage)\n        hf_split: Split of current task, allows to know some additional information about current split.\n            E.g. Current language\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        prompt_type: The name type of prompt. (query or passage)\n        **kwargs: Additional arguments to pass to the encoder.\n\n    Returns:\n        The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.similarity","title":"<code>similarity(embeddings1, embeddings2)</code>","text":"<p>Compute the similarity between two collections of embeddings.</p> <p>The output will be a matrix with the similarity scores between all embeddings from the first parameter and all embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity between corresponding pairs of embeddings.</p> <p>Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity</p> <p>Parameters:</p> Name Type Description Default <code>embeddings1</code> <code>Array</code> <p>[num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <code>embeddings2</code> <code>Array</code> <p>[num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def similarity(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Compute the similarity between two collections of embeddings.\n\n    The output will be a matrix with the similarity scores between all embeddings from the first parameter and all\n    embeddings from the second parameter. This differs from similarity_pairwise which computes the similarity\n    between corresponding pairs of embeddings.\n\n    Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity\n\n    Args:\n        embeddings1: [num_embeddings_1, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n        embeddings2: [num_embeddings_2, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n    Returns:\n        A [num_embeddings_1, num_embeddings_2]-shaped torch tensor with similarity scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.EncoderProtocol.similarity_pairwise","title":"<code>similarity_pairwise(embeddings1, embeddings2)</code>","text":"<p>Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.</p> <p>Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise</p> <p>Parameters:</p> Name Type Description Default <code>embeddings1</code> <code>Array</code> <p>[num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <code>embeddings2</code> <code>Array</code> <p>[num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A [num_embeddings]-shaped torch tensor with pairwise similarity scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def similarity_pairwise(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Compute the similarity between two collections of embeddings. The output will be a vector with the similarity scores between each pair of embeddings.\n\n    Read more at: https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html#sentence_transformers.SentenceTransformer.similarity_pairwise\n\n    Args:\n        embeddings1: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n        embeddings2: [num_embeddings, embedding_dim] or [embedding_dim]-shaped numpy array or torch tensor.\n\n    Returns:\n        A [num_embeddings]-shaped torch tensor with pairwise similarity scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol","title":"<code>mteb.models.SearchProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Interface for searching models.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass SearchProtocol(Protocol):\n    \"\"\"Interface for searching models.\"\"\"\n\n    def index(\n        self,\n        corpus: CorpusDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        num_proc: int | None,\n    ) -&gt; None:\n        \"\"\"Index the corpus for retrieval.\n\n        Args:\n            corpus: Corpus dataset to index.\n            task_metadata: Metadata of the task, used to determine how to index the corpus.\n            hf_split: Split of current task, allows to know some additional information about current split.\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            encode_kwargs: Additional arguments to pass to the encoder during indexing.\n            num_proc: Number of processes to use for dataloading.\n        \"\"\"\n        ...\n\n    def search(\n        self,\n        queries: QueryDatasetType,\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        top_k: int,\n        encode_kwargs: EncodeKwargs,\n        top_ranked: TopRankedDocumentsType | None = None,\n        num_proc: int | None,\n    ) -&gt; RetrievalOutputType:\n        \"\"\"Search the corpus using the given queries.\n\n        Args:\n            queries: Queries to find\n            task_metadata: Task metadata\n            hf_split: split of the dataset\n            hf_subset: subset of the dataset\n            top_ranked: Top-ranked documents for each query, mapping query IDs to a list of document IDs.\n                Passed only from Reranking tasks.\n            top_k: Number of top documents to return for each query.\n            encode_kwargs: Additional arguments to pass to the encoder during indexing.\n            num_proc: Number of processes to use for dataloading.\n\n        Returns:\n            Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; ModelMeta:\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.SearchProtocol.index","title":"<code>index(corpus, *, task_metadata, hf_split, hf_subset, encode_kwargs, num_proc)</code>","text":"<p>Index the corpus for retrieval.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>CorpusDatasetType</code> <p>Corpus dataset to index.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Metadata of the task, used to determine how to index the corpus.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split.</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>encode_kwargs</code> <code>EncodeKwargs</code> <p>Additional arguments to pass to the encoder during indexing.</p> required <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for dataloading.</p> required Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def index(\n    self,\n    corpus: CorpusDatasetType,\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    encode_kwargs: EncodeKwargs,\n    num_proc: int | None,\n) -&gt; None:\n    \"\"\"Index the corpus for retrieval.\n\n    Args:\n        corpus: Corpus dataset to index.\n        task_metadata: Metadata of the task, used to determine how to index the corpus.\n        hf_split: Split of current task, allows to know some additional information about current split.\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        encode_kwargs: Additional arguments to pass to the encoder during indexing.\n        num_proc: Number of processes to use for dataloading.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.SearchProtocol.search","title":"<code>search(queries, *, task_metadata, hf_split, hf_subset, top_k, encode_kwargs, top_ranked=None, num_proc)</code>","text":"<p>Search the corpus using the given queries.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>QueryDatasetType</code> <p>Queries to find</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Task metadata</p> required <code>hf_split</code> <code>str</code> <p>split of the dataset</p> required <code>hf_subset</code> <code>str</code> <p>subset of the dataset</p> required <code>top_ranked</code> <code>TopRankedDocumentsType | None</code> <p>Top-ranked documents for each query, mapping query IDs to a list of document IDs. Passed only from Reranking tasks.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Number of top documents to return for each query.</p> required <code>encode_kwargs</code> <code>EncodeKwargs</code> <p>Additional arguments to pass to the encoder during indexing.</p> required <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for dataloading.</p> required <p>Returns:</p> Type Description <code>RetrievalOutputType</code> <p>Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def search(\n    self,\n    queries: QueryDatasetType,\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    top_k: int,\n    encode_kwargs: EncodeKwargs,\n    top_ranked: TopRankedDocumentsType | None = None,\n    num_proc: int | None,\n) -&gt; RetrievalOutputType:\n    \"\"\"Search the corpus using the given queries.\n\n    Args:\n        queries: Queries to find\n        task_metadata: Task metadata\n        hf_split: split of the dataset\n        hf_subset: subset of the dataset\n        top_ranked: Top-ranked documents for each query, mapping query IDs to a list of document IDs.\n            Passed only from Reranking tasks.\n        top_k: Number of top documents to return for each query.\n        encode_kwargs: Additional arguments to pass to the encoder during indexing.\n        num_proc: Number of processes to use for dataloading.\n\n    Returns:\n        Dictionary with query IDs as keys with dict as values, where each value is a mapping of document IDs to their relevance scores.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol","title":"<code>mteb.models.CrossEncoderProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>The interface for a CrossEncoder in MTEB.</p> <p>Besides the required functions specified below, the cross-encoder can additionally specify the following signatures seen below. In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>@runtime_checkable\nclass CrossEncoderProtocol(Protocol):\n    \"\"\"The interface for a CrossEncoder in MTEB.\n\n    Besides the required functions specified below, the cross-encoder can additionally specify the following signatures seen below.\n    In general the interface is kept aligned with sentence-transformers interface. In cases where exceptions occurs these are handled within MTEB.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        revision: str | None,\n        device: str | None = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n        Args:\n            model_name: Name of the model\n            revision: revision of the model\n            device: Device used to load the model\n            kwargs: Any additional kwargs\n        \"\"\"\n        ...\n\n    def predict(\n        self,\n        inputs1: DataLoader[BatchedInput],\n        inputs2: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs: Unpack[EncodeKwargs],\n    ) -&gt; Array:\n        \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n        Args:\n            inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n            inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n            task_metadata: Metadata of the current task.\n            hf_split: Split of current task, allows to know some additional information about current split.\n                E.g. Current language\n            hf_subset: Subset of current task. Similar to `hf_split` to get more information\n            prompt_type: The name type of prompt. (query or passage)\n            **kwargs: Additional arguments to pass to the cross-encoder.\n\n        Returns:\n            The predicted relevance scores for each inputs pair.\n        \"\"\"\n        ...\n\n    @property\n    def mteb_model_meta(self) -&gt; ModelMeta:\n        \"\"\"Metadata of the model\"\"\"\n        ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Metadata of the model</p>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.__init__","title":"<code>__init__(model_name, revision, device=None, **kwargs)</code>","text":"<p>The initialization function for the encoder. Used when calling it from the mteb run CLI.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model</p> required <code>revision</code> <code>str | None</code> <p>revision of the model</p> required <code>device</code> <code>str | None</code> <p>Device used to load the model</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Any additional kwargs</p> <code>{}</code> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    revision: str | None,\n    device: str | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"The initialization function for the encoder. Used when calling it from the mteb run CLI.\n\n    Args:\n        model_name: Name of the model\n        revision: revision of the model\n        device: Device used to load the model\n        kwargs: Any additional kwargs\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.CrossEncoderProtocol.predict","title":"<code>predict(inputs1, inputs2, *, task_metadata, hf_split, hf_subset, prompt_type=None, **kwargs)</code>","text":"<p>Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs1</code> <code>DataLoader[BatchedInput]</code> <p>First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks <code>QueryDatasetType</code>).</p> required <code>inputs2</code> <code>DataLoader[BatchedInput]</code> <p>Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks <code>RetrievalOutputType</code>).</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>Metadata of the current task.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task, allows to know some additional information about current split. E.g. Current language</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task. Similar to <code>hf_split</code> to get more information</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>**kwargs</code> <code>Unpack[EncodeKwargs]</code> <p>Additional arguments to pass to the cross-encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The predicted relevance scores for each inputs pair.</p> Source code in <code>mteb/models/models_protocols.py</code> <pre><code>def predict(\n    self,\n    inputs1: DataLoader[BatchedInput],\n    inputs2: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    **kwargs: Unpack[EncodeKwargs],\n) -&gt; Array:\n    \"\"\"Predicts relevance scores for pairs of inputs. Note that, unlike the encoder, the cross-encoder can compare across inputs.\n\n    Args:\n        inputs1: First Dataloader of inputs to encode. For reranking tasks, these are queries (for text only tasks `QueryDatasetType`).\n        inputs2: Second Dataloader of inputs to encode. For reranking, these are documents (for text only tasks `RetrievalOutputType`).\n        task_metadata: Metadata of the current task.\n        hf_split: Split of current task, allows to know some additional information about current split.\n            E.g. Current language\n        hf_subset: Subset of current task. Similar to `hf_split` to get more information\n        prompt_type: The name type of prompt. (query or passage)\n        **kwargs: Additional arguments to pass to the cross-encoder.\n\n    Returns:\n        The predicted relevance scores for each inputs pair.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/model/#mteb.models.MTEBModels","title":"<code>mteb.models.MTEBModels = EncoderProtocol | CrossEncoderProtocol | SearchProtocol</code>  <code>module-attribute</code>","text":"<p>Type alias for all MTEB model types as many models implement multiple protocols and many tasks can be solved by multiple model types.</p>"},{"location":"api/model/#cache-wrappers","title":"Cache Wrappers","text":""},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper","title":"<code>mteb.models.CachedEmbeddingWrapper</code>","text":"<p>Wraps an encoder and caches embeddings for text and images.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; from mteb.models.cache_wrappers import CachedEmbeddingWrapper\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; model = mteb.get_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n&gt;&gt;&gt; cache_path = Path.cwd() / \"cache\"\n&gt;&gt;&gt; cached_model = CachedEmbeddingWrapper(model, cache_path)\n&gt;&gt;&gt; task = mteb.get_task(\"NanoArguAnaRetrieval\")\n&gt;&gt;&gt; mteb.evaluate(cached_model, task)\n</code></pre> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>class CachedEmbeddingWrapper:\n    \"\"\"Wraps an encoder and caches embeddings for text and images.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; from mteb.models.cache_wrappers import CachedEmbeddingWrapper\n        &gt;&gt;&gt; from pathlib import Path\n        &gt;&gt;&gt; model = mteb.get_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n        &gt;&gt;&gt; cache_path = Path.cwd() / \"cache\"\n        &gt;&gt;&gt; cached_model = CachedEmbeddingWrapper(model, cache_path)\n        &gt;&gt;&gt; task = mteb.get_task(\"NanoArguAnaRetrieval\")\n        &gt;&gt;&gt; mteb.evaluate(cached_model, task)\n    \"\"\"\n\n    def __init__(\n        self,\n        model: EncoderProtocol,\n        cache_path: str | Path,\n        cache_backend: type[CacheBackendProtocol] = NumpyCache,\n    ) -&gt; None:\n        \"\"\"Init\n\n        Args:\n            model: Model to be wrapped.\n            cache_path: Path to the directory where cached embeddings are stored.\n            cache_backend: Cache backend class to use for storing embeddings.\n        \"\"\"\n        self._model = model\n        self.cache_path = Path(cache_path)\n        self.cache_path.mkdir(parents=True, exist_ok=True)\n        if not hasattr(model, \"encode\"):\n            raise ValueError(\"Model must have an 'encode' method.\")\n        self.cache_backend = cache_backend\n        self.cache_dict: dict[str, CacheBackendProtocol] = {}\n        logger.info(\"Initialized CachedEmbeddingWrapper\")\n\n    @property\n    def mteb_model_meta(self) -&gt; ModelMeta | None:\n        \"\"\"Return wrapped model meta data.\"\"\"\n        return self._model.mteb_model_meta\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        batch_size: int = 32,\n        **kwargs: Any,\n    ) -&gt; Array:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: Batch of inputs to encode.\n            task_metadata: The metadata of the task.\n            hf_split: Split of current task\n            hf_subset: Subset of current task\n            prompt_type: The name type of prompt. (query or passage)\n            batch_size: Batch size\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n        \"\"\"\n        task_name = task_metadata.name\n        try:\n            cache = self._get_or_create_cache(task_name)\n\n            uncached_items: list[dict[str, Any]] = []\n            uncached_indices: list[int] = []\n            all_items: Dataset = inputs.dataset\n            cached_vectors: dict[int, Array] = {}\n\n            for i, item in enumerate(all_items):\n                vector = cache.get_vector(item)\n                if vector is not None:\n                    cached_vectors[i] = vector\n                else:\n                    uncached_items.append(item)\n                    uncached_indices.append(i)\n\n            newly_encoded: dict[int, Array] = {}\n            if uncached_items:\n                logger.info(f\"Encoding {len(uncached_items)} new items\")\n                # Build a simple DataLoader with only uncached items\n                dataset = Dataset.from_list(uncached_items)\n                dl = create_dataloader(\n                    dataset,\n                    task_metadata=task_metadata,\n                    prompt_type=prompt_type,\n                    **kwargs,\n                )\n                new_vectors = self._model.encode(\n                    dl,\n                    task_metadata=task_metadata,\n                    hf_split=hf_split,\n                    hf_subset=hf_subset,\n                    prompt_type=prompt_type,\n                    batch_size=batch_size,\n                    **kwargs,\n                )\n                if isinstance(new_vectors, torch.Tensor):\n                    new_vectors = new_vectors.cpu().numpy()\n                cache.add(uncached_items, new_vectors)\n                cache.save()\n                for vector, original_idx in zip(new_vectors, uncached_indices):\n                    newly_encoded[original_idx] = vector\n            else:\n                logger.info(\"All items found in cache\")\n\n            final_results = []\n            for i in range(len(all_items)):\n                if i in cached_vectors:\n                    final_results.append(cached_vectors[i])\n                else:\n                    final_results.append(newly_encoded[i])\n\n            return np.array(final_results)\n        except Exception as e:\n            logger.error(f\"Error in cached encoding: {str(e)}\")\n            raise\n\n    def _get_or_create_cache(self, task_name: str) -&gt; CacheBackendProtocol:\n        \"\"\"Get or create cache for a specific task.\n\n        Args:\n            task_name: Name of the task\n\n        Returns:\n            Cache backend instance for the task\n        \"\"\"\n        if task_name not in self.cache_dict:\n            cache = self.cache_backend(self.cache_path / task_name)\n            cache.load()\n            self.cache_dict[task_name] = cache\n        return self.cache_dict[task_name]\n\n    def __del__(self):\n        self.close()\n\n    def close(self) -&gt; None:\n        \"\"\"Unload cache from memory.\"\"\"\n        for task in list(self.cache_dict.keys()):\n            self.cache_dict[task].close()\n\n    def similarity(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Refer to [EncoderProtocol.similarity][mteb.models.EncoderProtocol.similarity] for more details.\"\"\"\n        return self._model.similarity(embeddings1, embeddings2)\n\n    def similarity_pairwise(\n        self,\n        embeddings1: Array,\n        embeddings2: Array,\n    ) -&gt; Array:\n        \"\"\"Refer to [EncoderProtocol.similarity][mteb.models.EncoderProtocol.similarity_pairwise] for more details.\"\"\"\n        return self._model.similarity_pairwise(embeddings1, embeddings2)\n</code></pre>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.mteb_model_meta","title":"<code>mteb_model_meta</code>  <code>property</code>","text":"<p>Return wrapped model meta data.</p>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.__init__","title":"<code>__init__(model, cache_path, cache_backend=NumpyCache)</code>","text":"<p>Init</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>EncoderProtocol</code> <p>Model to be wrapped.</p> required <code>cache_path</code> <code>str | Path</code> <p>Path to the directory where cached embeddings are stored.</p> required <code>cache_backend</code> <code>type[CacheBackendProtocol]</code> <p>Cache backend class to use for storing embeddings.</p> <code>NumpyCache</code> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>def __init__(\n    self,\n    model: EncoderProtocol,\n    cache_path: str | Path,\n    cache_backend: type[CacheBackendProtocol] = NumpyCache,\n) -&gt; None:\n    \"\"\"Init\n\n    Args:\n        model: Model to be wrapped.\n        cache_path: Path to the directory where cached embeddings are stored.\n        cache_backend: Cache backend class to use for storing embeddings.\n    \"\"\"\n    self._model = model\n    self.cache_path = Path(cache_path)\n    self.cache_path.mkdir(parents=True, exist_ok=True)\n    if not hasattr(model, \"encode\"):\n        raise ValueError(\"Model must have an 'encode' method.\")\n    self.cache_backend = cache_backend\n    self.cache_dict: dict[str, CacheBackendProtocol] = {}\n    logger.info(\"Initialized CachedEmbeddingWrapper\")\n</code></pre>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.close","title":"<code>close()</code>","text":"<p>Unload cache from memory.</p> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Unload cache from memory.\"\"\"\n    for task in list(self.cache_dict.keys()):\n        self.cache_dict[task].close()\n</code></pre>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.encode","title":"<code>encode(inputs, *, task_metadata, hf_split, hf_subset, prompt_type=None, batch_size=32, **kwargs)</code>","text":"<p>Encodes the given sentences using the encoder.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>DataLoader[BatchedInput]</code> <p>Batch of inputs to encode.</p> required <code>task_metadata</code> <code>TaskMetadata</code> <p>The metadata of the task.</p> required <code>hf_split</code> <code>str</code> <p>Split of current task</p> required <code>hf_subset</code> <code>str</code> <p>Subset of current task</p> required <code>prompt_type</code> <code>PromptType | None</code> <p>The name type of prompt. (query or passage)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size</p> <code>32</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the encoder.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Array</code> <p>The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).</p> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>def encode(\n    self,\n    inputs: DataLoader[BatchedInput],\n    *,\n    task_metadata: TaskMetadata,\n    hf_split: str,\n    hf_subset: str,\n    prompt_type: PromptType | None = None,\n    batch_size: int = 32,\n    **kwargs: Any,\n) -&gt; Array:\n    \"\"\"Encodes the given sentences using the encoder.\n\n    Args:\n        inputs: Batch of inputs to encode.\n        task_metadata: The metadata of the task.\n        hf_split: Split of current task\n        hf_subset: Subset of current task\n        prompt_type: The name type of prompt. (query or passage)\n        batch_size: Batch size\n        **kwargs: Additional arguments to pass to the encoder.\n\n    Returns:\n        The encoded input in a numpy array or torch tensor of the shape (Number of sentences) x (Embedding dimension).\n    \"\"\"\n    task_name = task_metadata.name\n    try:\n        cache = self._get_or_create_cache(task_name)\n\n        uncached_items: list[dict[str, Any]] = []\n        uncached_indices: list[int] = []\n        all_items: Dataset = inputs.dataset\n        cached_vectors: dict[int, Array] = {}\n\n        for i, item in enumerate(all_items):\n            vector = cache.get_vector(item)\n            if vector is not None:\n                cached_vectors[i] = vector\n            else:\n                uncached_items.append(item)\n                uncached_indices.append(i)\n\n        newly_encoded: dict[int, Array] = {}\n        if uncached_items:\n            logger.info(f\"Encoding {len(uncached_items)} new items\")\n            # Build a simple DataLoader with only uncached items\n            dataset = Dataset.from_list(uncached_items)\n            dl = create_dataloader(\n                dataset,\n                task_metadata=task_metadata,\n                prompt_type=prompt_type,\n                **kwargs,\n            )\n            new_vectors = self._model.encode(\n                dl,\n                task_metadata=task_metadata,\n                hf_split=hf_split,\n                hf_subset=hf_subset,\n                prompt_type=prompt_type,\n                batch_size=batch_size,\n                **kwargs,\n            )\n            if isinstance(new_vectors, torch.Tensor):\n                new_vectors = new_vectors.cpu().numpy()\n            cache.add(uncached_items, new_vectors)\n            cache.save()\n            for vector, original_idx in zip(new_vectors, uncached_indices):\n                newly_encoded[original_idx] = vector\n        else:\n            logger.info(\"All items found in cache\")\n\n        final_results = []\n        for i in range(len(all_items)):\n            if i in cached_vectors:\n                final_results.append(cached_vectors[i])\n            else:\n                final_results.append(newly_encoded[i])\n\n        return np.array(final_results)\n    except Exception as e:\n        logger.error(f\"Error in cached encoding: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.similarity","title":"<code>similarity(embeddings1, embeddings2)</code>","text":"<p>Refer to EncoderProtocol.similarity for more details.</p> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>def similarity(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Refer to [EncoderProtocol.similarity][mteb.models.EncoderProtocol.similarity] for more details.\"\"\"\n    return self._model.similarity(embeddings1, embeddings2)\n</code></pre>"},{"location":"api/model/#mteb.models.CachedEmbeddingWrapper.similarity_pairwise","title":"<code>similarity_pairwise(embeddings1, embeddings2)</code>","text":"<p>Refer to EncoderProtocol.similarity for more details.</p> Source code in <code>mteb/models/cache_wrappers/cache_wrapper.py</code> <pre><code>def similarity_pairwise(\n    self,\n    embeddings1: Array,\n    embeddings2: Array,\n) -&gt; Array:\n    \"\"\"Refer to [EncoderProtocol.similarity][mteb.models.EncoderProtocol.similarity_pairwise] for more details.\"\"\"\n    return self._model.similarity_pairwise(embeddings1, embeddings2)\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol","title":"<code>mteb.models.cache_wrappers.CacheBackendProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for a vector cache map (used to store text/image embeddings).</p> <p>Implementations may back the cache with different storage backends.</p> <p>The cache maps an input item (text or image) to its vector embedding, identified by a deterministic hash.</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>@runtime_checkable\nclass CacheBackendProtocol(Protocol):\n    \"\"\"Protocol for a vector cache map (used to store text/image embeddings).\n\n    Implementations may back the cache with different storage backends.\n\n    The cache maps an input item (text or image) to its vector embedding,\n    identified by a deterministic hash.\n    \"\"\"\n\n    def __init__(self, directory: Path | None = None, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the cache backend.\n\n        Args:\n            directory: Directory path to store cache files.\n            **kwargs: Additional backend-specific arguments.\n        \"\"\"\n\n    def add(self, item: list[dict[str, Any]], vectors: Array) -&gt; None:\n        \"\"\"Add a vector to the cache.\n\n        Args:\n            item: Input item containing 'text' or 'image'.\n            vectors: Embedding vector of shape (dim,) or (1, dim).\n        \"\"\"\n\n    def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n        \"\"\"Retrieve the cached vector for the given item.\n\n        Args:\n            item: Input item.\n\n        Returns:\n            Cached vector as np.ndarray, or None if not found.\n        \"\"\"\n\n    def save(self) -&gt; None:\n        \"\"\"Persist cache data to disk (index + metadata).\"\"\"\n\n    def load(self) -&gt; None:\n        \"\"\"Load cache from disk (index + metadata).\"\"\"\n\n    def close(self) -&gt; None:\n        \"\"\"Release resources or flush data.\"\"\"\n\n    def __contains__(self, item: dict[str, Any]) -&gt; bool:\n        \"\"\"Check whether the cache contains an item.\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.__contains__","title":"<code>__contains__(item)</code>","text":"<p>Check whether the cache contains an item.</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def __contains__(self, item: dict[str, Any]) -&gt; bool:\n    \"\"\"Check whether the cache contains an item.\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.__init__","title":"<code>__init__(directory=None, **kwargs)</code>","text":"<p>Initialize the cache backend.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path | None</code> <p>Directory path to store cache files.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional backend-specific arguments.</p> <code>{}</code> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def __init__(self, directory: Path | None = None, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the cache backend.\n\n    Args:\n        directory: Directory path to store cache files.\n        **kwargs: Additional backend-specific arguments.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.add","title":"<code>add(item, vectors)</code>","text":"<p>Add a vector to the cache.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>list[dict[str, Any]]</code> <p>Input item containing 'text' or 'image'.</p> required <code>vectors</code> <code>Array</code> <p>Embedding vector of shape (dim,) or (1, dim).</p> required Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def add(self, item: list[dict[str, Any]], vectors: Array) -&gt; None:\n    \"\"\"Add a vector to the cache.\n\n    Args:\n        item: Input item containing 'text' or 'image'.\n        vectors: Embedding vector of shape (dim,) or (1, dim).\n    \"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.close","title":"<code>close()</code>","text":"<p>Release resources or flush data.</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release resources or flush data.\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.get_vector","title":"<code>get_vector(item)</code>","text":"<p>Retrieve the cached vector for the given item.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>dict[str, Any]</code> <p>Input item.</p> required <p>Returns:</p> Type Description <code>Array | None</code> <p>Cached vector as np.ndarray, or None if not found.</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n    \"\"\"Retrieve the cached vector for the given item.\n\n    Args:\n        item: Input item.\n\n    Returns:\n        Cached vector as np.ndarray, or None if not found.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.load","title":"<code>load()</code>","text":"<p>Load cache from disk (index + metadata).</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load cache from disk (index + metadata).\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.CacheBackendProtocol.save","title":"<code>save()</code>","text":"<p>Persist cache data to disk (index + metadata).</p> Source code in <code>mteb/models/cache_wrappers/cache_backend_protocol.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Persist cache data to disk (index + metadata).\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache","title":"<code>mteb.models.cache_wrappers.cache_backends.NumpyCache</code>","text":"<p>Generic vector cache for both text and images.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>class NumpyCache:\n    \"\"\"Generic vector cache for both text and images.\"\"\"\n\n    def __init__(self, directory: str | Path, initial_vectors: int = 100_000):\n        self.directory = Path(directory)\n        self.directory.mkdir(parents=True, exist_ok=True)\n        self.vectors_file = self.directory / \"vectors.npy\"\n        self.index_file = self.directory / \"index.json\"\n        self.dimension_file = self.directory / \"dimension\"\n        self.hash_to_index: dict[str, int] = {}\n        self.vectors: np.memmap | None = None\n        self.vector_dim: int | None = None\n        self.initial_vectors = initial_vectors\n        logger.info(f\"Initialized VectorCacheMap in directory: {self.directory}\")\n        self._initialize_vectors_file()\n\n    def add(self, items: list[dict[str, Any]], vectors: Array) -&gt; None:\n        \"\"\"Add a vector to the cache.\"\"\"\n        try:\n            if self.vector_dim is None:\n                self.vector_dim = (\n                    vectors.shape[0] if vectors.ndim == 1 else vectors.shape[1]\n                )\n                self._initialize_vectors_file()\n                self._save_dimension()\n                logger.info(f\"Initialized vector dimension to {self.vector_dim}\")\n\n            if self.vectors is None:\n                raise RuntimeError(\n                    \"Vectors file not initialized. Call _initialize_vectors_file() first.\"\n                )\n\n            for item, vec in zip(items, vectors):\n                item_hash = _hash_item(item)\n                if item_hash in self.hash_to_index:\n                    msg = f\"Hash collision or duplicate item for hash {item_hash}. Overwriting existing vector.\"\n                    logger.warning(msg)\n                    warnings.warn(msg)\n                    index = self.hash_to_index[item_hash]\n                else:\n                    index = len(self.hash_to_index)\n                    if index &gt;= len(self.vectors):\n                        self._double_vectors_file()\n                    self.hash_to_index[item_hash] = index\n\n                self.vectors[index] = vec\n                logger.debug(\n                    f\"Added new item-vector pair. Total pairs: {len(self.hash_to_index)}\"\n                )\n        except Exception as e:\n            logger.error(f\"Error adding item-vector pair: {str(e)}\")\n            raise\n\n    def _initialize_vectors_file(self) -&gt; None:\n        if self.vector_dim is None:\n            logger.info(\"Vector dimension not set. Waiting for first add() call.\")\n            return\n        if not self.vectors_file.exists():\n            logger.info(\n                f\"Creating initial vectors file with {self.initial_vectors} vectors\"\n            )\n            self.vectors = np.memmap(\n                self.vectors_file,\n                dtype=\"float32\",\n                mode=\"w+\",\n                shape=(self.initial_vectors, self.vector_dim),\n            )\n        else:\n            self.vectors = np.memmap(\n                self.vectors_file,\n                dtype=\"float32\",\n                mode=\"r+\",\n                shape=(-1, self.vector_dim),\n            )\n        logger.info(f\"Vectors file initialized with shape: {self.vectors.shape}\")\n\n    def _double_vectors_file(self) -&gt; None:\n        if self.vectors is None or self.vector_dim is None:\n            raise RuntimeError(\n                \"Vectors file not initialized. Call _initialize_vectors_file() first.\"\n            )\n        current_size = len(self.vectors)\n        new_size = current_size * 2\n        logger.info(f\"Doubling vectors file from {current_size} to {new_size} vectors\")\n        self.vectors.flush()\n        new_vectors = np.memmap(\n            str(self.vectors_file),\n            dtype=np.float32,\n            mode=\"r+\",\n            shape=(new_size, self.vector_dim),\n        )\n        new_vectors[:current_size] = self.vectors[:]\n        self.vectors = new_vectors\n\n    def _save_dimension(self) -&gt; None:\n        with self.dimension_file.open(\"w\") as f:\n            f.write(str(self.vector_dim))\n        logger.info(\n            f\"Saved vector dimension {self.vector_dim} to {self.dimension_file}\"\n        )\n\n    def _load_dimension(self) -&gt; None:\n        if self.dimension_file.exists():\n            with self.dimension_file.open() as f:\n                self.vector_dim = int(f.read().strip())\n            logger.info(\n                f\"Loaded vector dimension {self.vector_dim} from {self.dimension_file}\"\n            )\n        else:\n            msg = \"Dimension file not found. Vector dimension remains uninitialized.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n    def save(self) -&gt; None:\n        \"\"\"Persist VectorCacheMap to disk.\"\"\"\n        try:\n            if self.vectors is not None:\n                self.vectors.flush()\n\n            # Convert hash_to_index dict to a format suitable for JSON\n            # JSON doesn't support integer keys, so we keep everything as strings\n            serializable_index = {\n                str(hash_): int(index)  # Ensure indices are serialized as integers\n                for hash_, index in self.hash_to_index.items()\n            }\n\n            with self.index_file.open(\"w\", encoding=\"utf-8\") as f:\n                json.dump(serializable_index, f, indent=2)\n            self._save_dimension()\n            logger.info(f\"Saved VectorCacheMap to {self.directory}\")\n        except Exception as e:\n            logger.error(f\"Error saving VectorCacheMap: {str(e)}\")\n            raise\n\n    def load(self) -&gt; None:\n        \"\"\"Load VectorCacheMap from disk.\"\"\"\n        try:\n            self._load_dimension()\n            if self.index_file.exists() and self.vectors_file.exists():\n                with self.index_file.open(encoding=\"utf-8\") as f:\n                    loaded_index = json.load(f)\n                    self.hash_to_index = {\n                        str(hash_): int(index)  # Ensure we maintain the correct types\n                        for hash_, index in loaded_index.items()\n                    }\n\n                if self.vector_dim is not None:\n                    self.vectors = np.memmap(\n                        self.vectors_file,\n                        dtype=\"float32\",\n                        mode=\"r+\",\n                        shape=(-1, self.vector_dim),\n                    )\n                    logger.info(f\"Loaded vectors file with shape: {self.vectors.shape}\")\n                else:\n                    msg = \"Vector dimension not set. Unable to load vectors file.\"\n                    logger.warning(msg)\n                    warnings.warn(msg)\n                logger.info(f\"Loaded VectorCacheMap from {self.directory}\")\n            else:\n                msg = \"No existing files found. Initialized empty VectorCacheMap.\"\n                logger.warning(msg)\n                warnings.warn(msg)\n        except Exception as e:\n            logger.error(f\"Error loading VectorCacheMap: {str(e)}\")\n            raise\n\n    def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n        \"\"\"Retrieve vector from index by hash.\"\"\"\n        if self.vectors is None:\n            return None\n\n        try:\n            item_hash = _hash_item(item)\n            if item_hash not in self.hash_to_index:\n                logger.debug(f\"Item hash not found in index: {item_hash}\")\n                return None\n            index = self.hash_to_index[item_hash]\n            return self.vectors[index]\n        except Exception as e:\n            logger.error(f\"Error retrieving vector for item: {str(e)}\")\n            raise\n\n    def __contains__(self, item: dict[str, Any]) -&gt; bool:\n        return _hash_item(item) in self.hash_to_index\n\n    def __del__(self):\n        self.close()\n\n    def close(self) -&gt; None:\n        \"\"\"Delete all ve\"\"\"\n        if hasattr(self, \"vectors\") and self.vectors is not None:\n            self.vectors.flush()\n            del self.vectors\n            self.vectors = None\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache.add","title":"<code>add(items, vectors)</code>","text":"<p>Add a vector to the cache.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>def add(self, items: list[dict[str, Any]], vectors: Array) -&gt; None:\n    \"\"\"Add a vector to the cache.\"\"\"\n    try:\n        if self.vector_dim is None:\n            self.vector_dim = (\n                vectors.shape[0] if vectors.ndim == 1 else vectors.shape[1]\n            )\n            self._initialize_vectors_file()\n            self._save_dimension()\n            logger.info(f\"Initialized vector dimension to {self.vector_dim}\")\n\n        if self.vectors is None:\n            raise RuntimeError(\n                \"Vectors file not initialized. Call _initialize_vectors_file() first.\"\n            )\n\n        for item, vec in zip(items, vectors):\n            item_hash = _hash_item(item)\n            if item_hash in self.hash_to_index:\n                msg = f\"Hash collision or duplicate item for hash {item_hash}. Overwriting existing vector.\"\n                logger.warning(msg)\n                warnings.warn(msg)\n                index = self.hash_to_index[item_hash]\n            else:\n                index = len(self.hash_to_index)\n                if index &gt;= len(self.vectors):\n                    self._double_vectors_file()\n                self.hash_to_index[item_hash] = index\n\n            self.vectors[index] = vec\n            logger.debug(\n                f\"Added new item-vector pair. Total pairs: {len(self.hash_to_index)}\"\n            )\n    except Exception as e:\n        logger.error(f\"Error adding item-vector pair: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache.close","title":"<code>close()</code>","text":"<p>Delete all ve</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Delete all ve\"\"\"\n    if hasattr(self, \"vectors\") and self.vectors is not None:\n        self.vectors.flush()\n        del self.vectors\n        self.vectors = None\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache.get_vector","title":"<code>get_vector(item)</code>","text":"<p>Retrieve vector from index by hash.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n    \"\"\"Retrieve vector from index by hash.\"\"\"\n    if self.vectors is None:\n        return None\n\n    try:\n        item_hash = _hash_item(item)\n        if item_hash not in self.hash_to_index:\n            logger.debug(f\"Item hash not found in index: {item_hash}\")\n            return None\n        index = self.hash_to_index[item_hash]\n        return self.vectors[index]\n    except Exception as e:\n        logger.error(f\"Error retrieving vector for item: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache.load","title":"<code>load()</code>","text":"<p>Load VectorCacheMap from disk.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load VectorCacheMap from disk.\"\"\"\n    try:\n        self._load_dimension()\n        if self.index_file.exists() and self.vectors_file.exists():\n            with self.index_file.open(encoding=\"utf-8\") as f:\n                loaded_index = json.load(f)\n                self.hash_to_index = {\n                    str(hash_): int(index)  # Ensure we maintain the correct types\n                    for hash_, index in loaded_index.items()\n                }\n\n            if self.vector_dim is not None:\n                self.vectors = np.memmap(\n                    self.vectors_file,\n                    dtype=\"float32\",\n                    mode=\"r+\",\n                    shape=(-1, self.vector_dim),\n                )\n                logger.info(f\"Loaded vectors file with shape: {self.vectors.shape}\")\n            else:\n                msg = \"Vector dimension not set. Unable to load vectors file.\"\n                logger.warning(msg)\n                warnings.warn(msg)\n            logger.info(f\"Loaded VectorCacheMap from {self.directory}\")\n        else:\n            msg = \"No existing files found. Initialized empty VectorCacheMap.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n    except Exception as e:\n        logger.error(f\"Error loading VectorCacheMap: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.NumpyCache.save","title":"<code>save()</code>","text":"<p>Persist VectorCacheMap to disk.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/numpy_cache.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Persist VectorCacheMap to disk.\"\"\"\n    try:\n        if self.vectors is not None:\n            self.vectors.flush()\n\n        # Convert hash_to_index dict to a format suitable for JSON\n        # JSON doesn't support integer keys, so we keep everything as strings\n        serializable_index = {\n            str(hash_): int(index)  # Ensure indices are serialized as integers\n            for hash_, index in self.hash_to_index.items()\n        }\n\n        with self.index_file.open(\"w\", encoding=\"utf-8\") as f:\n            json.dump(serializable_index, f, indent=2)\n        self._save_dimension()\n        logger.info(f\"Saved VectorCacheMap to {self.directory}\")\n    except Exception as e:\n        logger.error(f\"Error saving VectorCacheMap: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache","title":"<code>mteb.models.cache_wrappers.cache_backends.FaissCache</code>","text":"<p>FAISS-based vector cache that uses embeddings directly as lookup keys.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>class FaissCache:\n    \"\"\"FAISS-based vector cache that uses embeddings directly as lookup keys.\"\"\"\n\n    def __init__(self, directory: str | Path):\n        requires_package(\n            self,\n            \"faiss\",\n            \"FAISS-based vector cache\",\n            install_instruction=\"pip install mteb[faiss-cpu]\",\n        )\n\n        self.directory = Path(directory)\n        self.directory.mkdir(parents=True, exist_ok=True)\n        self.index_file = self.directory / \"vectors.faiss\"\n        self.map_file = self.directory / \"index.json\"\n\n        self.hash_to_index: dict[str, int] = {}\n        self.index: faiss.Index | None = None\n        self.vector_dim: int | None = None\n\n        logger.info(f\"Initialized FAISS VectorCacheMap in {self.directory}\")\n        self.load()\n\n    def add(self, items: list[dict[str, Any]], vectors: Array) -&gt; None:\n        \"\"\"Add vector to FAISS index.\"\"\"\n        import faiss\n\n        if vectors.ndim == 1:\n            vectors = vectors[None, :]\n        if self.vector_dim is None:\n            self.vector_dim = vectors.shape[1]\n            self.index = faiss.IndexFlatL2(self.vector_dim)\n        elif self.index is None:\n            self.index = faiss.IndexFlatL2(self.vector_dim)\n\n        start_id = len(self.hash_to_index)\n        vectors_to_add = []\n        for i, (item, vectors) in enumerate(zip(items, vectors)):\n            item_hash = _hash_item(item)\n            if item_hash in self.hash_to_index:\n                continue\n            self.hash_to_index[item_hash] = start_id + i\n            vectors_to_add.append(vectors)\n        if len(vectors_to_add) &gt; 0:\n            vectors_array = np.vstack(vectors_to_add).astype(np.float32)\n            self.index.add(vectors_array)\n\n    def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n        \"\"\"Retrieve vector from index by hash.\"\"\"\n        if self.index is None:\n            return None\n        item_hash = _hash_item(item)\n        if item_hash not in self.hash_to_index:\n            return None\n        idx = self.hash_to_index[item_hash]\n        try:\n            return self.index.reconstruct(idx)\n        except Exception:\n            msg = f\"Vector id {idx} missing for hash {item_hash}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            return None\n\n    def save(self) -&gt; None:\n        \"\"\"Persist FAISS index and mapping to disk.\"\"\"\n        import faiss\n\n        if self.index is not None:\n            faiss.write_index(self.index, str(self.index_file))\n        with self.map_file.open(\"w\") as f:\n            json.dump(self.hash_to_index, f, indent=2)\n        logger.info(f\"Saved FAISS cache to {self.directory}\")\n\n    def load(self) -&gt; None:\n        \"\"\"Load FAISS index and mapping from disk.\"\"\"\n        import faiss\n\n        if self.map_file.exists():\n            with self.map_file.open() as f:\n                self.hash_to_index = json.load(f)\n        if self.index_file.exists():\n            try:\n                self.index = faiss.read_index(str(self.index_file))\n                logger.info(f\"Loaded FAISS index with {self.index.ntotal} vectors\")\n            except Exception as e:\n                logger.error(f\"Failed to load FAISS index: {e}\")\n                self.index = None\n        else:\n            self.index = None\n\n    def close(self) -&gt; None:\n        \"\"\"Close cache.\"\"\"\n        self.save()\n        self.index = None\n\n    def __contains__(self, item: BatchedInput) -&gt; bool:\n        return _hash_item(item) in self.hash_to_index\n\n    def __del__(self):\n        self.close()\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache.add","title":"<code>add(items, vectors)</code>","text":"<p>Add vector to FAISS index.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>def add(self, items: list[dict[str, Any]], vectors: Array) -&gt; None:\n    \"\"\"Add vector to FAISS index.\"\"\"\n    import faiss\n\n    if vectors.ndim == 1:\n        vectors = vectors[None, :]\n    if self.vector_dim is None:\n        self.vector_dim = vectors.shape[1]\n        self.index = faiss.IndexFlatL2(self.vector_dim)\n    elif self.index is None:\n        self.index = faiss.IndexFlatL2(self.vector_dim)\n\n    start_id = len(self.hash_to_index)\n    vectors_to_add = []\n    for i, (item, vectors) in enumerate(zip(items, vectors)):\n        item_hash = _hash_item(item)\n        if item_hash in self.hash_to_index:\n            continue\n        self.hash_to_index[item_hash] = start_id + i\n        vectors_to_add.append(vectors)\n    if len(vectors_to_add) &gt; 0:\n        vectors_array = np.vstack(vectors_to_add).astype(np.float32)\n        self.index.add(vectors_array)\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache.close","title":"<code>close()</code>","text":"<p>Close cache.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close cache.\"\"\"\n    self.save()\n    self.index = None\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache.get_vector","title":"<code>get_vector(item)</code>","text":"<p>Retrieve vector from index by hash.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>def get_vector(self, item: dict[str, Any]) -&gt; Array | None:\n    \"\"\"Retrieve vector from index by hash.\"\"\"\n    if self.index is None:\n        return None\n    item_hash = _hash_item(item)\n    if item_hash not in self.hash_to_index:\n        return None\n    idx = self.hash_to_index[item_hash]\n    try:\n        return self.index.reconstruct(idx)\n    except Exception:\n        msg = f\"Vector id {idx} missing for hash {item_hash}\"\n        logger.warning(msg)\n        warnings.warn(msg)\n        return None\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache.load","title":"<code>load()</code>","text":"<p>Load FAISS index and mapping from disk.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load FAISS index and mapping from disk.\"\"\"\n    import faiss\n\n    if self.map_file.exists():\n        with self.map_file.open() as f:\n            self.hash_to_index = json.load(f)\n    if self.index_file.exists():\n        try:\n            self.index = faiss.read_index(str(self.index_file))\n            logger.info(f\"Loaded FAISS index with {self.index.ntotal} vectors\")\n        except Exception as e:\n            logger.error(f\"Failed to load FAISS index: {e}\")\n            self.index = None\n    else:\n        self.index = None\n</code></pre>"},{"location":"api/model/#mteb.models.cache_wrappers.cache_backends.FaissCache.save","title":"<code>save()</code>","text":"<p>Persist FAISS index and mapping to disk.</p> Source code in <code>mteb/models/cache_wrappers/cache_backends/faiss_cache.py</code> <pre><code>def save(self) -&gt; None:\n    \"\"\"Persist FAISS index and mapping to disk.\"\"\"\n    import faiss\n\n    if self.index is not None:\n        faiss.write_index(self.index, str(self.index_file))\n    with self.map_file.open(\"w\") as f:\n        json.dump(self.hash_to_index, f, indent=2)\n    logger.info(f\"Saved FAISS cache to {self.directory}\")\n</code></pre>"},{"location":"api/model/#search-index-backends","title":"Search Index Backends","text":""},{"location":"api/model/#mteb.models.search_encoder_index.search_backend_protocol.IndexEncoderSearchProtocol","title":"<code>mteb.models.search_encoder_index.search_backend_protocol.IndexEncoderSearchProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for search backends used in encoder-based retrieval.</p> Source code in <code>mteb/models/search_encoder_index/search_backend_protocol.py</code> <pre><code>class IndexEncoderSearchProtocol(Protocol):\n    \"\"\"Protocol for search backends used in encoder-based retrieval.\"\"\"\n\n    def add_documents(\n        self,\n        embeddings: Array,\n        idxs: list[str],\n    ) -&gt; None:\n        \"\"\"Add documents to the search backend.\n\n        Args:\n            embeddings: Embeddings of the documents to add.\n            idxs: IDs of the documents to add.\n        \"\"\"\n\n    def search(\n        self,\n        embeddings: Array,\n        top_k: int,\n        similarity_fn: Callable[[Array, Array], Array],\n        top_ranked: TopRankedDocumentsType | None = None,\n        query_idx_to_id: dict[int, str] | None = None,\n    ) -&gt; tuple[list[list[float]], list[list[int]]]:\n        \"\"\"Search through added corpus embeddings or rerank top-ranked documents.\n\n        Supports both full-corpus and reranking search modes:\n            - Full-corpus mode: `top_ranked=None`, uses added corpus embeddings.\n            - Reranking mode:  `top_ranked` contains mapping {query_id: [doc_ids]}.\n\n        Args:\n            embeddings: Query embeddings, shape (num_queries, dim).\n            top_k: Number of top results to return.\n            similarity_fn: Function to compute similarity between query and corpus.\n            top_ranked: Mapping of query_id -&gt; list of candidate doc_ids. Used for reranking.\n            query_idx_to_id: Mapping of query index -&gt; query_id. Used for reranking.\n\n        Returns:\n            A tuple (top_k_values, top_k_indices), for each query:\n                - top_k_values: List of top-k similarity scores.\n                - top_k_indices: List of indices of the top-k documents in the added corpus.\n        \"\"\"\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all stored documents and embeddings from the backend.\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_backend_protocol.IndexEncoderSearchProtocol.add_documents","title":"<code>add_documents(embeddings, idxs)</code>","text":"<p>Add documents to the search backend.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Array</code> <p>Embeddings of the documents to add.</p> required <code>idxs</code> <code>list[str]</code> <p>IDs of the documents to add.</p> required Source code in <code>mteb/models/search_encoder_index/search_backend_protocol.py</code> <pre><code>def add_documents(\n    self,\n    embeddings: Array,\n    idxs: list[str],\n) -&gt; None:\n    \"\"\"Add documents to the search backend.\n\n    Args:\n        embeddings: Embeddings of the documents to add.\n        idxs: IDs of the documents to add.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_backend_protocol.IndexEncoderSearchProtocol.clear","title":"<code>clear()</code>","text":"<p>Clear all stored documents and embeddings from the backend.</p> Source code in <code>mteb/models/search_encoder_index/search_backend_protocol.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all stored documents and embeddings from the backend.\"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_backend_protocol.IndexEncoderSearchProtocol.search","title":"<code>search(embeddings, top_k, similarity_fn, top_ranked=None, query_idx_to_id=None)</code>","text":"<p>Search through added corpus embeddings or rerank top-ranked documents.</p> Supports both full-corpus and reranking search modes <ul> <li>Full-corpus mode: <code>top_ranked=None</code>, uses added corpus embeddings.</li> <li>Reranking mode:  <code>top_ranked</code> contains mapping {query_id: [doc_ids]}.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>Array</code> <p>Query embeddings, shape (num_queries, dim).</p> required <code>top_k</code> <code>int</code> <p>Number of top results to return.</p> required <code>similarity_fn</code> <code>Callable[[Array, Array], Array]</code> <p>Function to compute similarity between query and corpus.</p> required <code>top_ranked</code> <code>TopRankedDocumentsType | None</code> <p>Mapping of query_id -&gt; list of candidate doc_ids. Used for reranking.</p> <code>None</code> <code>query_idx_to_id</code> <code>dict[int, str] | None</code> <p>Mapping of query index -&gt; query_id. Used for reranking.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[list[list[float]], list[list[int]]]</code> <p>A tuple (top_k_values, top_k_indices), for each query: - top_k_values: List of top-k similarity scores. - top_k_indices: List of indices of the top-k documents in the added corpus.</p> Source code in <code>mteb/models/search_encoder_index/search_backend_protocol.py</code> <pre><code>def search(\n    self,\n    embeddings: Array,\n    top_k: int,\n    similarity_fn: Callable[[Array, Array], Array],\n    top_ranked: TopRankedDocumentsType | None = None,\n    query_idx_to_id: dict[int, str] | None = None,\n) -&gt; tuple[list[list[float]], list[list[int]]]:\n    \"\"\"Search through added corpus embeddings or rerank top-ranked documents.\n\n    Supports both full-corpus and reranking search modes:\n        - Full-corpus mode: `top_ranked=None`, uses added corpus embeddings.\n        - Reranking mode:  `top_ranked` contains mapping {query_id: [doc_ids]}.\n\n    Args:\n        embeddings: Query embeddings, shape (num_queries, dim).\n        top_k: Number of top results to return.\n        similarity_fn: Function to compute similarity between query and corpus.\n        top_ranked: Mapping of query_id -&gt; list of candidate doc_ids. Used for reranking.\n        query_idx_to_id: Mapping of query index -&gt; query_id. Used for reranking.\n\n    Returns:\n        A tuple (top_k_values, top_k_indices), for each query:\n            - top_k_values: List of top-k similarity scores.\n            - top_k_indices: List of indices of the top-k documents in the added corpus.\n    \"\"\"\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_indexes.faiss_search_index.FaissSearchIndex","title":"<code>mteb.models.search_encoder_index.search_indexes.faiss_search_index.FaissSearchIndex</code>","text":"<p>FAISS-based backend for encoder-based search.</p> <p>Supports both full-corpus retrieval and reranking (via <code>top_ranked</code>).</p> Notes <ul> <li>Stores all embeddings in memory (IndexFlatIP or IndexFlatL2).</li> <li>Expects embeddings to be normalized if cosine similarity is desired.</li> </ul> Source code in <code>mteb/models/search_encoder_index/search_indexes/faiss_search_index.py</code> <pre><code>class FaissSearchIndex:\n    \"\"\"FAISS-based backend for encoder-based search.\n\n    Supports both full-corpus retrieval and reranking (via `top_ranked`).\n\n    Notes:\n        - Stores *all* embeddings in memory (IndexFlatIP or IndexFlatL2).\n        - Expects embeddings to be normalized if cosine similarity is desired.\n    \"\"\"\n\n    _normalize: bool = False\n\n    def __init__(self, model: EncoderProtocol) -&gt; None:\n        requires_package(\n            self,\n            \"faiss\",\n            \"FAISS-based search\",\n            install_instruction=\"pip install mteb[faiss-cpu]\",\n        )\n\n        from faiss import IndexFlatIP, IndexFlatL2\n\n        # https://github.com/facebookresearch/faiss/wiki/Faiss-indexes\n        if model.mteb_model_meta.similarity_fn_name is ScoringFunction.DOT_PRODUCT:\n            self.index_type = IndexFlatIP\n        elif model.mteb_model_meta.similarity_fn_name is ScoringFunction.COSINE:\n            self.index_type = IndexFlatIP\n            self._normalize = True\n        elif model.mteb_model_meta.similarity_fn_name is ScoringFunction.EUCLIDEAN:\n            self.index_type = IndexFlatL2\n        else:\n            raise ValueError(\n                f\"FAISS backend does not support similarity function {model.mteb_model_meta.similarity_fn_name}. \"\n                f\"Available: {ScoringFunction.DOT_PRODUCT}, {ScoringFunction.COSINE}.\"\n            )\n\n        self.idxs: list[str] = []\n        self.index: faiss.Index | None = None\n\n    def add_documents(self, embeddings: Array, idxs: list[str]) -&gt; None:\n        \"\"\"Add all document embeddings and their IDs to FAISS index.\"\"\"\n        import faiss\n\n        if isinstance(embeddings, torch.Tensor):\n            embeddings = embeddings.detach().cpu().numpy()\n\n        embeddings = embeddings.astype(np.float32)\n        self.idxs.extend(idxs)\n\n        if self._normalize:\n            faiss.normalize_L2(embeddings)\n\n        dim = embeddings.shape[1]\n        if self.index is None:\n            self.index = self.index_type(dim)\n\n        self.index.add(embeddings)\n        logger.info(f\"FAISS index built with {len(idxs)} vectors of dim {dim}.\")\n\n    def search(\n        self,\n        embeddings: Array,\n        top_k: int,\n        similarity_fn: Callable[[Array, Array], Array],\n        top_ranked: TopRankedDocumentsType | None = None,\n        query_idx_to_id: dict[int, str] | None = None,\n    ) -&gt; tuple[list[list[float]], list[list[int]]]:\n        \"\"\"Search using FAISS.\"\"\"\n        import faiss\n\n        if self.index is None:\n            raise ValueError(\"No index built. Call add_document() first.\")\n\n        if isinstance(embeddings, torch.Tensor):\n            embeddings = embeddings.detach().cpu().numpy()\n\n        if self._normalize:\n            faiss.normalize_L2(embeddings)\n\n        if top_ranked is not None:\n            if query_idx_to_id is None:\n                raise ValueError(\"query_idx_to_id must be provided when reranking.\")\n\n            similarities, ids = self._reranking(\n                embeddings,\n                top_k,\n                top_ranked=top_ranked,\n                query_idx_to_id=query_idx_to_id,\n            )\n        else:\n            similarities, ids = self.index.search(embeddings.astype(np.float32), top_k)\n            similarities = similarities.tolist()\n            ids = ids.tolist()\n\n        if issubclass(self.index_type, faiss.IndexFlatL2):\n            similarities = (-np.sqrt(np.maximum(similarities, 0))).tolist()\n\n        return similarities, ids\n\n    def _reranking(\n        self,\n        embeddings: Array,\n        top_k: int,\n        top_ranked: TopRankedDocumentsType,\n        query_idx_to_id: dict[int, str],\n    ) -&gt; tuple[list[list[float]], list[list[int]]]:\n        doc_id_to_idx = {doc_id: i for i, doc_id in enumerate(self.idxs)}\n        scores_all: list[list[float]] = []\n        idxs_all: list[list[int]] = []\n\n        for query_idx, query_emb in enumerate(embeddings):\n            query_id = query_idx_to_id[query_idx]\n            ranked_ids = top_ranked.get(query_id)\n            if not ranked_ids:\n                msg = f\"No top-ranked documents for query {query_id}\"\n                logger.warning(msg)\n                warnings.warn(msg)\n                scores_all.append([])\n                idxs_all.append([])\n                continue\n\n            candidate_indices = [doc_id_to_idx[doc_id] for doc_id in ranked_ids]\n            d = self.index.d  # type: ignore[union-attr]\n            candidate_embs = np.vstack(\n                [self.index.reconstruct(idx) for idx in candidate_indices]  # type: ignore[union-attr]\n            )\n            sub_reranking_index = self.index_type(d)\n            sub_reranking_index.add(candidate_embs)\n\n            # Search returns scores and indices in one call\n            scores, local_indices = sub_reranking_index.search(\n                query_emb.reshape(1, -1).astype(np.float32),\n                min(top_k, len(candidate_indices)),\n            )\n            # faiss will output 2d arrays even for single query\n            scores_all.append(scores[0].tolist())\n            idxs_all.append(local_indices[0].tolist())\n\n        return scores_all, idxs_all\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear all stored documents and embeddings from the backend.\"\"\"\n        self.index = None\n        self.idxs = []\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_indexes.faiss_search_index.FaissSearchIndex.add_documents","title":"<code>add_documents(embeddings, idxs)</code>","text":"<p>Add all document embeddings and their IDs to FAISS index.</p> Source code in <code>mteb/models/search_encoder_index/search_indexes/faiss_search_index.py</code> <pre><code>def add_documents(self, embeddings: Array, idxs: list[str]) -&gt; None:\n    \"\"\"Add all document embeddings and their IDs to FAISS index.\"\"\"\n    import faiss\n\n    if isinstance(embeddings, torch.Tensor):\n        embeddings = embeddings.detach().cpu().numpy()\n\n    embeddings = embeddings.astype(np.float32)\n    self.idxs.extend(idxs)\n\n    if self._normalize:\n        faiss.normalize_L2(embeddings)\n\n    dim = embeddings.shape[1]\n    if self.index is None:\n        self.index = self.index_type(dim)\n\n    self.index.add(embeddings)\n    logger.info(f\"FAISS index built with {len(idxs)} vectors of dim {dim}.\")\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_indexes.faiss_search_index.FaissSearchIndex.clear","title":"<code>clear()</code>","text":"<p>Clear all stored documents and embeddings from the backend.</p> Source code in <code>mteb/models/search_encoder_index/search_indexes/faiss_search_index.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear all stored documents and embeddings from the backend.\"\"\"\n    self.index = None\n    self.idxs = []\n</code></pre>"},{"location":"api/model/#mteb.models.search_encoder_index.search_indexes.faiss_search_index.FaissSearchIndex.search","title":"<code>search(embeddings, top_k, similarity_fn, top_ranked=None, query_idx_to_id=None)</code>","text":"<p>Search using FAISS.</p> Source code in <code>mteb/models/search_encoder_index/search_indexes/faiss_search_index.py</code> <pre><code>def search(\n    self,\n    embeddings: Array,\n    top_k: int,\n    similarity_fn: Callable[[Array, Array], Array],\n    top_ranked: TopRankedDocumentsType | None = None,\n    query_idx_to_id: dict[int, str] | None = None,\n) -&gt; tuple[list[list[float]], list[list[int]]]:\n    \"\"\"Search using FAISS.\"\"\"\n    import faiss\n\n    if self.index is None:\n        raise ValueError(\"No index built. Call add_document() first.\")\n\n    if isinstance(embeddings, torch.Tensor):\n        embeddings = embeddings.detach().cpu().numpy()\n\n    if self._normalize:\n        faiss.normalize_L2(embeddings)\n\n    if top_ranked is not None:\n        if query_idx_to_id is None:\n            raise ValueError(\"query_idx_to_id must be provided when reranking.\")\n\n        similarities, ids = self._reranking(\n            embeddings,\n            top_k,\n            top_ranked=top_ranked,\n            query_idx_to_id=query_idx_to_id,\n        )\n    else:\n        similarities, ids = self.index.search(embeddings.astype(np.float32), top_k)\n        similarities = similarities.tolist()\n        ids = ids.tolist()\n\n    if issubclass(self.index_type, faiss.IndexFlatL2):\n        similarities = (-np.sqrt(np.maximum(similarities, 0))).tolist()\n\n    return similarities, ids\n</code></pre>"},{"location":"api/results/","title":"Results","text":"<p>When a models is evaluated in MTEB it produces results. These results consist of:</p> <ul> <li><code>TaskResult</code>: Result for a single task</li> <li><code>ModelResult</code>: Result for a model on a set of tasks</li> <li><code>BenchmarkResults</code>: Result for a set of models on a set of tasks</li> </ul> <p></p> <p>In normal use these come up when running a model: <pre><code># ...\nmodels_results = mteb.evaluate(model, tasks)\ntype(models_results) # mteb.results.ModelResults\n\ntask_result = models_results.task_results\ntype(models_results) # mteb.results.TaskResult\n</code></pre></p>"},{"location":"api/results/#results-cache","title":"Results cache","text":""},{"location":"api/results/#mteb.cache.ResultCache","title":"<code>mteb.cache.ResultCache</code>","text":"<p>Class to handle the local cache of MTEB results.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")  # default\n&gt;&gt;&gt; cache.download_from_remote()  # download the latest results from the remote repository\n&gt;&gt;&gt; result = cache.load_results(\"task_name\", \"model_name\")\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>class ResultCache:\n    \"\"\"Class to handle the local cache of MTEB results.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")  # default\n        &gt;&gt;&gt; cache.download_from_remote()  # download the latest results from the remote repository\n        &gt;&gt;&gt; result = cache.load_results(\"task_name\", \"model_name\")\n    \"\"\"\n\n    cache_path: Path\n\n    def __init__(self, cache_path: Path | str | None = None) -&gt; None:\n        if cache_path is not None:\n            self.cache_path = Path(cache_path)\n        else:\n            self.cache_path = self.default_cache_path\n        self.cache_path.mkdir(parents=True, exist_ok=True)\n\n    @property\n    def has_remote(self) -&gt; bool:\n        \"\"\"Check if the remote results repository exists in the cache directory.\n\n        Returns:\n            True if the remote results repository exists, False otherwise.\n        \"\"\"\n        return (self.cache_path / \"remote\").exists()\n\n    def get_task_result_path(\n        self,\n        task_name: str,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n        remote: bool = False,\n        experiment_name: str | None = None,\n    ) -&gt; Path:\n        \"\"\"Get the path to the results of a specific task for a specific model and revision.\n\n        Args:\n            task_name: The name of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n            remote: If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.\n            experiment_name: The name of the experiment as a valid directory name. If model_name is a ModelMeta object, its experiment_name will be used.\n\n        Returns:\n            The path to the results of the task.\n        \"\"\"\n        results_folder = (\n            self.cache_path / \"results\"\n            if not remote\n            else self.cache_path / \"remote\" / \"results\"\n        )\n\n        if isinstance(model_name, ModelMeta):\n            if model_revision is not None:\n                logger.warning(\n                    \"model_revision and experiment_name is ignored when model_name is a ModelMeta object\"\n                )\n            model_revision = model_name.revision\n            experiment_name = model_name.experiment_name\n            model_name = model_name.model_name_as_path()\n        elif isinstance(model_name, str):\n            model_name = model_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n        model_path = results_folder / model_name\n\n        if model_revision is None:\n            msg = \"`model_revision` is not specified, attempting to load the latest revision. To disable this behavior, specify the 'model_revision` explicitly.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            # get revs from paths\n            revisions = [p for p in model_path.glob(\"*\") if p.is_dir()]\n            if not revisions:\n                model_revision = \"no_revision_available\"\n            else:\n                if len(revisions) &gt; 1:\n                    logger.warning(\n                        f\"Multiple revisions found for model {model_name}: {revisions}. Using the latest one (according to latest edit).\"\n                    )\n                    # sort folder by latest edit time\n                    revisions.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n                model_revision = revisions[0].name\n\n        if experiment_name:\n            return (\n                model_path\n                / model_revision\n                / _EXPERIMENTS_FOLDER_NAME\n                / experiment_name\n                / f\"{task_name}.json\"\n            )\n        return model_path / model_revision / f\"{task_name}.json\"\n\n    def load_task_result(\n        self,\n        task_name: str,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n        raise_if_not_found: bool = False,\n        prioritize_remote: bool = False,\n        experiment_name: str | None = None,\n    ) -&gt; TaskResult | None:\n        \"\"\"Load the results from the local cache directory.\n\n        Args:\n            task_name: The name of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n            raise_if_not_found: If True, raise an error if the results are not found.\n            prioritize_remote: If True, it will first try to load the results from the remote repository, if available.\n            experiment_name: Optional experiment folder name (a valid directory name). If None, the default is used.\n\n        Returns:\n            The results of the task, or None if not found.\n        \"\"\"\n        result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_name,\n            experiment_name=experiment_name,\n        )\n\n        if self.has_remote:\n            remote_result_path = self.get_task_result_path(\n                model_name=model_name,\n                model_revision=model_revision,\n                task_name=task_name,\n                remote=True,\n                experiment_name=experiment_name,\n            )\n            if remote_result_path.exists() and prioritize_remote:\n                result_path = remote_result_path\n            elif not result_path.exists():\n                result_path = remote_result_path\n\n        if not result_path.exists():\n            msg = f\"Results for {model_name} on {task_name} not found in {result_path}\"\n            if raise_if_not_found:\n                raise FileNotFoundError(msg)\n            logger.debug(msg)\n            return None\n\n        return TaskResult.from_disk(result_path)\n\n    def save_to_cache(\n        self,\n        task_result: TaskResult,\n        model_name: str | ModelMeta,\n        model_revision: str | None = None,\n    ) -&gt; None:\n        \"\"\"Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.\n\n        Where model_name is a path-normalized model name.\n        In addition we also save a model_meta.json in the revision folder to preserve the model metadata.\n\n        Args:\n            task_result: The results of the task.\n            model_name: The name of the model as a valid directory name or a ModelMeta object.\n            model_revision: The revision of the model. Must be specified if model_name is a string.\n        \"\"\"\n        result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_result.task_name,\n        )\n        result_path.parent.mkdir(parents=True, exist_ok=True)\n        task_result.to_disk(result_path)\n\n        model_meta_path = result_path.parent / \"model_meta.json\"\n        if isinstance(model_name, ModelMeta):\n            meta = model_name\n            with model_meta_path.open(\"w\") as f:\n                json.dump(meta.to_dict(), f, default=str, indent=4)\n\n    @property\n    def default_cache_path(self) -&gt; Path:\n        \"\"\"Get the local cache directory for MTEB results.\n\n        Returns:\n            The path to the local cache directory.\n        \"\"\"\n        default_cache_directory = Path.home() / \".cache\" / \"mteb\"\n\n        _cache_directory = os.environ.get(\"MTEB_CACHE\", None)\n        cache_directory = (\n            Path(_cache_directory) if _cache_directory else default_cache_directory\n        )\n        return cache_directory\n\n    def download_from_remote(\n        self,\n        remote: str = \"https://github.com/embeddings-benchmark/results\",\n        download_latest: bool = True,\n        revision: str | None = None,\n    ) -&gt; Path:\n        \"\"\"Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.\n\n        Args:\n            remote: The URL of the results repository on GitHub.\n            download_latest: If True it will download the latest version of the repository, otherwise it will only update the existing repository.\n            revision: If specified, it will checkout the given revision after cloning or pulling the repository.\n\n        Returns:\n            The path to the local cache directory.\n        \"\"\"\n        if not self.cache_path.exists() and not self.cache_path.is_dir():\n            logger.info(\n                f\"Cache directory {self.cache_path} does not exist, creating it\"\n            )\n\n        # if \"results\" folder already exists update it\n        results_directory = self.cache_path / \"remote\"\n\n        if results_directory.exists():\n            # check repository in the directory is the same as the remote\n            remote_url = subprocess.run(\n                [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n                cwd=results_directory,\n                capture_output=True,\n                text=True,\n            ).stdout.strip()\n            if remote_url != remote:\n                msg = (\n                    f\"remote repository '{remote}' does not match the one in {results_directory},  which is '{remote_url}'.\"\n                    + \" Please remove the directory and try again.\"\n                )\n                raise ValueError(msg)\n\n            if revision or download_latest:\n                logger.info(\n                    f\"remote repository already exists in {results_directory}, fetching updates\"\n                )\n                subprocess.run(\n                    [\"git\", \"fetch\", \"--all\", \"--tags\"],\n                    cwd=results_directory,\n                    check=True,\n                )\n            else:\n                logger.debug(\n                    f\"Results repository already exists in {results_directory}, skipping update, \"\n                    f\"set download_latest=True to update it\"\n                )\n\n            if revision:\n                logger.info(f\"Checking out revision '{revision}'\")\n                subprocess.run(\n                    [\"git\", \"checkout\", revision],\n                    cwd=results_directory,\n                    check=True,\n                )\n            return results_directory\n\n        logger.info(\n            f\"No results repository found in {results_directory}, cloning it from {remote}\"\n        )\n\n        clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\"]\n\n        if revision:\n            logger.info(f\"Cloning repository at revision '{revision}'\")\n            clone_cmd.append(f\"--revision={revision}\")\n        clone_cmd.extend([remote, \"remote\"])\n\n        subprocess.run(\n            clone_cmd,\n            cwd=self.cache_path,\n            check=True,\n        )\n\n        return results_directory\n\n    def _download_cached_results_from_branch(\n        self,\n        branch: str = \"cached-data\",\n        filename: str = \"__cached_results.json.gz\",\n        output_path: Path | None = None,\n        remote: str = \"https://github.com/embeddings-benchmark/results\",\n        timeout: int = 60,\n        max_size_mb: int = 500,\n    ) -&gt; Path:\n        \"\"\"Download pre-computed cached results from a specific branch.\n\n        This is significantly faster than download_from_remote() since it downloads\n        only a compressed cache file instead of cloning the entire repository.\n\n        The method performs the following steps:\n        1. Downloads a gzipped JSON file from the specified branch\n        2. Validates file size and content type\n        3. Decompresses the gzip content\n        4. Writes the decompressed JSON to disk\n\n        Args:\n            branch: Branch name to download from (default: \"cached-data\")\n            filename: Name of the cached results file (default: \"__cached_results.json.gz\")\n            output_path: Where to save the file. If None, uses {cache_path}/leaderboard/__cached_results.json\n            remote: Base URL of the results repository\n            timeout: Request timeout in seconds (default: 60)\n            max_size_mb: Maximum allowed file size in megabytes (default: 500)\n\n        Returns:\n            Path to the downloaded and decompressed cache file\n\n        Raises:\n            requests.exceptions.RequestException: On HTTP errors\n            ValueError: On validation failures (size, content-type)\n            gzip.BadGzipFile: If content is not valid gzip\n            UnicodeDecodeError: If content cannot be decoded as UTF-8\n            PermissionError: If file cannot be written due to permissions\n            OSError: On other file system errors\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; cache = mteb.ResultCache()\n            &gt;&gt;&gt; # Download optimized cached results\n            &gt;&gt;&gt; cache_file = cache._download_cached_results_from_branch()\n            &gt;&gt;&gt; # Use custom output path\n            &gt;&gt;&gt; cache_file = cache._download_cached_results_from_branch(\n            ...     output_path=Path(\"/tmp/my_cache.json\")\n            ... )\n        \"\"\"\n        if output_path is None:\n            # Default to saving in {cache_path}/leaderboard/__cached_results.json\n            output_path = self.cache_path / \"leaderboard\" / \"__cached_results.json\"\n\n        # Extract repository owner and name from the remote URL\n        # e.g., \"https://github.com/embeddings-benchmark/results\" -&gt; \"embeddings-benchmark/results\"\n        repo_path = remote.replace(\"https://github.com/\", \"\").replace(\n            \"http://github.com/\", \"\"\n        )\n\n        url = f\"https://raw.githubusercontent.com/{repo_path}/{branch}/{filename}\"\n        logger.info(f\"Downloading cached results from {url}\")\n\n        # Step 1: Download with validation\n        max_size_bytes = max_size_mb * 1024 * 1024\n\n        try:\n            response = requests.get(url, timeout=timeout)\n            response.raise_for_status()\n\n            # Check if this is a Git LFS pointer file\n            content_type = response.headers.get(\"content-type\", \"\").lower()\n            if (\n                content_type == \"text/plain; charset=utf-8\"\n                and b\"git-lfs\" in response.content\n            ):\n                # Try Git LFS media URL instead\n                media_url = f\"https://media.githubusercontent.com/media/{repo_path}/{branch}/{filename}\"\n                logger.info(f\"Detected Git LFS file, trying media URL: {media_url}\")\n                response = requests.get(media_url, timeout=timeout)\n                response.raise_for_status()\n                content_type = response.headers.get(\"content-type\", \"\").lower()\n\n            # Validate content-type header\n            expected_content_types = [\n                \"application/gzip\",\n                \"application/octet-stream\",\n                \"application/x-gzip\",\n            ]\n            if content_type and not any(\n                ct in content_type for ct in expected_content_types\n            ):\n                raise Exception(\n                    f\"Unexpected content-type: {content_type}. Expected one of: {expected_content_types}\"\n                )\n\n            # Validate file size\n            content_length = len(response.content)\n            if content_length &gt; max_size_bytes:\n                raise ValueError(\n                    f\"Downloaded file too large: {content_length} bytes (max: {max_size_bytes})\"\n                )\n\n            logger.info(\n                f\"HTTP request successful, content length: {content_length} bytes\"\n            )\n            content = response.content\n\n        except Exception as e:\n            logger.error(f\"Unexpected HTTP error: {type(e).__name__}: {e}\")\n            raise e\n\n        # Step 2: Decompress gzip data\n        logger.info(\"Attempting gzip decompression...\")\n\n        try:\n            with gzip.open(io.BytesIO(content), \"rt\", encoding=\"utf-8\") as gz_file:\n                data = gz_file.read()\n            logger.info(f\"Decompression successful, data length: {len(data)} chars\")\n\n        except Exception as e:\n            logger.error(f\"Unexpected decompression error: {type(e).__name__}: {e}\")\n            raise e\n\n        # Step 3: Write to disk\n        logger.info(f\"Attempting to write to: {output_path}\")\n\n        # Check parent directory exists and is writable\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n\n        try:\n            output_path.write_text(data, encoding=\"utf-8\")\n            logger.info(\n                f\"File write successful, size: {output_path.stat().st_size} bytes\"\n            )\n        except Exception as e:\n            logger.error(f\"Unexpected file write error: {type(e).__name__}: {e}\")\n            raise e\n\n        return output_path\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear the local cache directory.\"\"\"\n        if self.cache_path.exists() and self.cache_path.is_dir():\n            shutil.rmtree(self.cache_path)\n            logger.info(f\"Cache directory {self.cache_path} cleared.\")\n        else:\n            msg = f\"Cache directory `{self.cache_path}` does not exist.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n    def _load_from_cache(\n        self,\n        cache_filename: str = \"__cached_results.json\",\n        rebuild: bool = False,\n    ) -&gt; BenchmarkResults:\n        \"\"\"Load benchmark results using the best available strategy.\n\n        Args:\n            cache_filename: Name of the cache file. The full path will be constructed as\n                {cache_path}/leaderboard/{cache_filename}.\n            rebuild: If True, force a full rebuild from the results repository, bypassing any\n                     pre-computed JSON cache.\n\n        Strategy:\n            1. If rebuild=False and local cache exists at cache_path \u2192 load and return\n            2. If rebuild=False, try downloading pre-computed cache from 'cached-data' branch\n               \u2192 save to cache_path and return\n            3. Fallback (or if rebuild=True): clone the full results repository, build from\n               individual model files, call results.to_disk(cache_path), and return.\n\n        Returns:\n            BenchmarkResults ready for leaderboard display\n        \"\"\"\n        cache_path = self.cache_path / \"leaderboard\" / cache_filename\n\n        # If rebuild=True, skip directly to full repository rebuild\n        if rebuild:\n            logger.info(\n                \"Rebuild requested, forcing full repository clone and rebuild...\"\n            )\n            return self._rebuild_from_full_repository(cache_path)\n\n        # Strategy 1: Try loading from existing local quick cache\n        if cache_path.exists():\n            logger.info(f\"Loading existing quick cache from {cache_path}\")\n            try:\n                return BenchmarkResults.from_disk(cache_path)\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to load quick cache: {e}. Trying other strategies...\"\n                )\n\n        # Strategy 2: Try downloading from cached-data branch\n        try:\n            logger.info(\n                \"Attempting to download pre-computed cache from cached-data branch...\"\n            )\n            downloaded_path = self._download_cached_results_from_branch(\n                output_path=cache_path\n            )\n            logger.info(f\"Downloaded cache to {downloaded_path}\")\n            return BenchmarkResults.from_disk(downloaded_path)\n        except Exception as e:\n            logger.warning(f\"Failed to download from cached-data branch: {e}\")\n\n        # Strategy 3: Fallback to full repository clone\n        logger.info(\"Falling back to full repository clone and rebuild...\")\n        return self._rebuild_from_full_repository(cache_path)\n\n    def _rebuild_from_full_repository(self, quick_cache_path: Path) -&gt; BenchmarkResults:\n        \"\"\"Clone/pull the full results repository and build BenchmarkResults from individual files.\n\n        This method performs a full rebuild by:\n        1. Downloading or updating the full results repository\n        2. Loading results from all individual model files\n        3. Saving the aggregated results to the quick cache path\n        4. Returning the BenchmarkResults object\n\n        Args:\n            quick_cache_path: Path where the rebuilt cache should be saved\n\n        Returns:\n            BenchmarkResults built from the full repository\n        \"\"\"\n        # Download or update the full repository\n        self.download_from_remote()\n\n        all_model_names = [\n            model_meta.name\n            for model_meta in mteb.get_model_metas()\n            if model_meta.name is not None\n        ]\n\n        all_results = self.load_results(\n            models=all_model_names,\n            only_main_score=True,\n            require_model_meta=False,\n            include_remote=True,\n        )\n\n        # Save to disk for future use\n        logger.info(f\"Saving rebuilt cache to {quick_cache_path}\")\n        all_results.to_disk(quick_cache_path)\n\n        return all_results\n\n    def __repr__(self) -&gt; str:\n        return f\"ResultCache(cache_path={self.cache_path})\"\n\n    def get_cache_paths(\n        self,\n        models: Sequence[str] | Iterable[ModelMeta] | None = None,\n        tasks: Sequence[str] | Iterable[AbsTask] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n        load_experiments: LoadExperimentEnum | str = LoadExperimentEnum.NO_EXPERIMENTS,\n    ) -&gt; list[Path]:\n        \"\"\"Get all paths to result JSON files in the cache directory.\n\n        These paths can then be used to fetch task results, like:\n        ```python\n        for path in paths:\n            task_result = TaskResult.from_disk(path)\n        ```\n\n        Args:\n            models: A list of model names or ModelMeta objects to filter the paths.\n            tasks: A list of task names to filter the paths.\n            require_model_meta: If True, only return paths that have a model_meta.json file.\n            include_remote: If True, include remote results in the returned paths.\n            load_experiments: If True, include experiments in the returned paths.\n\n        Returns:\n            A list of paths in the cache directory.\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; cache = mteb.ResultCache()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths\n            &gt;&gt;&gt; paths = cache.get_cache_paths()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific task\n            &gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific model\n            &gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get all cache paths for a specific model and revision\n            &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n            &gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n        \"\"\"\n        if isinstance(load_experiments, str):\n            load_experiments = LoadExperimentEnum.from_str(load_experiments)\n\n        def _cache_paths(base_path: Path) -&gt; list[Path]:\n            return [\n                p\n                for p in base_path.glob(\"*/*/*.json\")  # model/revision/task.json\n                if p.name != \"model_meta.json\"\n            ]\n\n        def _experiments_paths(base_path: Path) -&gt; list[Path]:\n            return [\n                p\n                for p in base_path.glob(f\"*/*/{_EXPERIMENTS_FOLDER_NAME}/*/*.json\")\n                if p.name != \"model_meta.json\"\n            ]\n\n        def _get_paths(base_path: Path, experiments: LoadExperimentEnum) -&gt; list[Path]:\n            paths = _cache_paths(base_path)\n            if not experiments == LoadExperimentEnum.NO_EXPERIMENTS:\n                paths += _experiments_paths(base_path)\n            return paths\n\n        results_path = self.cache_path / \"results\"\n        remote_path = self.cache_path / \"remote\" / \"results\"\n\n        cache_paths = _get_paths(results_path, load_experiments)\n\n        if include_remote:\n            cache_paths += _get_paths(remote_path, load_experiments)\n\n        cache_paths = self._filter_paths_by_model_and_revision(\n            cache_paths,\n            models=models,\n            load_experiments=load_experiments,\n        )\n        cache_paths = self._filter_paths_by_task(cache_paths, tasks=tasks)\n\n        if require_model_meta:\n            cache_paths = [\n                p for p in cache_paths if (p.parent / \"model_meta.json\").exists()\n            ]\n        return cache_paths\n\n    def get_models(\n        self,\n        tasks: Sequence[str] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n    ) -&gt; list[tuple[ModelName, Revision]]:\n        \"\"\"Get all models in the cache directory.\n\n        Args:\n            tasks: A list of task names to filter the models.\n            require_model_meta: If True, only return models that have a model_meta.json file.\n            include_remote: If True, include remote results in the returned models.\n\n        Returns:\n            A list of tuples containing the model name and revision.\n        \"\"\"\n        cache_paths = self.get_cache_paths(\n            tasks=tasks,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n        )\n        models = [(p.parent.parent.name, p.parent.name) for p in cache_paths]\n        return list(set(models))\n\n    def get_task_names(\n        self,\n        models: list[str] | list[ModelMeta] | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n    ) -&gt; list[str]:\n        \"\"\"Get all task names in the cache directory.\n\n        Args:\n            models: A list of model names or ModelMeta objects to filter the task names.\n            require_model_meta: If True, only return task names that have a model_meta.json file\n            include_remote: If True, include remote results in the returned task names.\n\n        Returns:\n            A list of task names in the cache directory.\n        \"\"\"\n        cache_paths = self.get_cache_paths(\n            models=models,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n        )\n        tasks = [p.stem for p in cache_paths]\n        return list(set(tasks))\n\n    @staticmethod\n    def _get_model_name_and_revision_from_path(\n        revision_path: Path,\n    ) -&gt; tuple[ModelName, Revision, str | None]:\n        \"\"\"Get model name, revision and experiment name from the given path.\n\n        Args:\n            revision_path: The path to the revision folder, which should contain a model_meta.json file. If the file is not found, it will attempt to extract the model name and revision from the path.\n\n        Returns:\n            A tuple containing the model name, revision and experiment name (if available).\n\n        \"\"\"\n        model_meta = revision_path / \"model_meta.json\"\n        model_path = revision_path.parent\n\n        if not model_meta.exists():\n            logger.debug(\n                f\"model_meta.json not found in {revision_path}, extracting model_name and revision from the path\"\n            )\n            if _EXPERIMENTS_FOLDER_NAME in revision_path.parts:\n                logger.debug(\n                    f\"Path {revision_path} contains an experiment folder, extracting model_name and revision accordingly\"\n                )\n                experiment_name = revision_path.name\n                revision = revision_path.parent.parent.name\n                model_name = revision_path.parent.parent.parent.name.replace(\"__\", \"/\")\n                return model_name, revision, experiment_name\n            model_name = model_path.name.replace(\"__\", \"/\")\n            revision = revision_path.name\n            return model_name, revision, None\n        with model_meta.open(\"r\") as f:\n            model_meta_json = json.load(f)\n        model_name = model_meta_json[\"name\"]\n        revision = model_meta_json[\"revision\"]\n        experiment_kwargs = model_meta_json.get(\"experiment_kwargs\", None)\n        experiment_name_ = _serialize_experiment_kwargs_to_name(experiment_kwargs)\n        return model_name, revision, experiment_name_\n\n    @staticmethod\n    def _filter_paths_by_model_and_revision(\n        paths: list[Path],\n        models: Sequence[str] | Iterable[ModelMeta] | None = None,\n        load_experiments: LoadExperimentEnum | None = None,\n    ) -&gt; list[Path]:\n        \"\"\"Filter a list of paths by model name and optional revision.\n\n        Returns:\n            A list of paths that match the specified model names and revisions.\n        \"\"\"\n        if not models:\n            return paths\n\n        first_model = next(iter(models))\n        if isinstance(first_model, ModelMeta):\n            models = cast(\"Iterable[ModelMeta]\", models)\n            name_and_revision = {\n                (\n                    m.model_name_as_path(),\n                    m.revision or \"no_revision_available\",\n                    m.experiment_name\n                    if load_experiments is LoadExperimentEnum.MATCH_KWARGS\n                    else None,\n                )\n                for m in models\n            }\n            model_name_and_revision = list()\n            for path in paths:\n                if _EXPERIMENTS_FOLDER_NAME in path.parts:\n                    revision = path.parent.parent.parent.name\n                    model_name = path.parent.parent.parent.parent.name\n                    experiment_name = (\n                        path.parent.name\n                        if load_experiments is LoadExperimentEnum.MATCH_KWARGS\n                        else None\n                    )\n                else:\n                    revision = path.parent.name\n                    model_name = path.parent.parent.name\n                    experiment_name = None\n                model_name_and_revision.append((model_name, revision, experiment_name))\n            return [\n                p\n                for model_revision, p in zip(model_name_and_revision, paths)\n                if model_revision in name_and_revision\n            ]\n\n        str_models = cast(\"Sequence[str]\", models)\n        model_names = {m.replace(\"/\", \"__\").replace(\" \", \"_\") for m in str_models}\n        filtered_paths = []\n        for p in paths:\n            if _EXPERIMENTS_FOLDER_NAME in p.parts:\n                model_name = p.parent.parent.parent.parent.name\n            else:\n                model_name = p.parent.parent.name\n            if model_name in model_names:\n                filtered_paths.append(p)\n        return filtered_paths\n\n    @staticmethod\n    def _filter_paths_by_task(\n        paths: list[Path],\n        tasks: Sequence[str] | Iterable[AbsTask] | None = None,\n    ) -&gt; list[Path]:\n        if tasks is not None:\n            task_names = set()\n\n            for task in tasks:\n                if isinstance(task, AbsTask):\n                    task_names.add(task.metadata.name)\n                else:\n                    task_names.add(task)\n\n            paths = [p for p in paths if p.stem in task_names]\n        return paths\n\n    def load_results(\n        self,\n        models: Sequence[str] | Iterable[ModelMeta] | None = None,\n        tasks: Sequence[str] | Iterable[AbsTask] | Benchmark | str | None = None,\n        require_model_meta: bool = True,\n        include_remote: bool = True,\n        validate_and_filter: bool = False,\n        only_main_score: bool = False,\n        load_experiments: LoadExperimentEnum | str = LoadExperimentEnum.MATCH_KWARGS,\n        experiment_kwargs: Mapping[str, Any] | list[Mapping[str, Any]] | None = None,\n    ) -&gt; BenchmarkResults:\n        \"\"\"Loads the results from the cache directory and returns a BenchmarkResults object.\n\n        Args:\n            models: A list of model names to load the results for. If None it will load the results for all models.\n            tasks: A list of task names to load the results for. If str is passed, then benchmark will be loaded.\n                If Benchmark is passed, then all tasks in the benchmark will be loaded.\n                If None it will load the results for all tasks.\n            require_model_meta: If True it will ignore results that do not have a model_meta.json file. If false it attempt to\n                extract the model name and revision from the path.\n            include_remote: If True, it will include results from the remote repository.\n            validate_and_filter: If True it will validate that the results object for the task contains the correct splits and filter out\n                splits from the results object that are not default in the task metadata.\n            only_main_score: If True, only the main score will be loaded.\n            load_experiments: If True, it will also load results from experiment folders.\n            experiment_kwargs: If specified, it will only load results from experiments with the specified kwargs. Only used if load_experiments is True.\n\n        Returns:\n            A BenchmarkResults object containing the results for the specified models and tasks.\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; cache = mteb.ResultCache()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Load results for specific models and tasks\n            &gt;&gt;&gt; results = cache.load_results(\n            ...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n            ...     tasks=[\"STS12\"],\n            ...     require_model_meta=True,\n            ... )\n        \"\"\"\n        if isinstance(tasks, str):\n            tasks = mteb.get_benchmark(tasks)\n\n        if isinstance(load_experiments, str):\n            load_experiments = LoadExperimentEnum.from_str(load_experiments)\n\n        if (\n            load_experiments is not LoadExperimentEnum.MATCH_KWARGS\n            and experiment_kwargs is not None\n        ):\n            warnings.warn(\n                \"experiment_kwargs is specified but load_experiments is not set to MATCH_KWARGS.\"\n                \"No results will be loaded.\"\n            )\n\n        models_as_model_meta = models is not None and isinstance(\n            next(iter(models)), ModelMeta\n        )\n\n        paths = self.get_cache_paths(\n            models=models,\n            tasks=tasks,\n            require_model_meta=require_model_meta,\n            include_remote=include_remote,\n            load_experiments=load_experiments,\n        )\n        models_results = defaultdict(list)\n\n        task_names: dict[str, AbsTask | None] = {}\n        if tasks is not None:\n            for task in tasks:\n                if isinstance(task, AbsTask):\n                    task_names[task.metadata.name] = task\n                else:\n                    task_names[task] = None\n\n        experiment_names = set()\n        if isinstance(experiment_kwargs, Mapping):\n            experiment_kwargs = [experiment_kwargs]\n        if isinstance(experiment_kwargs, list):\n            experiment_names = {\n                _serialize_experiment_kwargs_to_name(params)\n                for params in experiment_kwargs\n            }\n        for path in paths:\n            task_result = TaskResult.from_disk(path)\n\n            if only_main_score:\n                task_result = task_result.only_main_score()\n            model_name, revision, experiment_name = (\n                self._get_model_name_and_revision_from_path(path.parent)\n            )\n\n            if validate_and_filter:\n                task_instance = task_names[task_result.task_name]\n                try:\n                    task_result = task_result.validate_and_filter_scores(\n                        task=task_instance\n                    )\n                except ValidationError as e:\n                    logger.info(\n                        f\"Validation failed for {task_result.task_name} in {model_name} {revision}: {e}\"\n                    )\n                    continue\n\n            if len(experiment_names) &gt; 0 and experiment_name not in experiment_names:\n                logger.debug(\n                    f\"Skipping experiment {experiment_name} as it is not in the specified experiment names\"\n                )\n                continue\n\n            if (\n                load_experiments is LoadExperimentEnum.MATCH_KWARGS\n                and not models_as_model_meta  # for models meta path are prefiltered\n                and len(experiment_names) == 0\n                and experiment_name is not None\n            ):\n                continue\n\n            models_results[(model_name, revision, experiment_name)].append(task_result)\n\n        # create BenchmarkResults object\n        models_results_object = [\n            ModelResult(\n                model_name=model_name,\n                model_revision=revision,\n                task_results=task_results,\n                experiment_name=experiment_name,\n            )\n            for (\n                model_name,\n                revision,\n                experiment_name,\n            ), task_results in models_results.items()\n        ]\n\n        return BenchmarkResults(\n            model_results=models_results_object,\n            benchmark=tasks if isinstance(tasks, Benchmark) else None,\n        )\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.default_cache_path","title":"<code>default_cache_path</code>  <code>property</code>","text":"<p>Get the local cache directory for MTEB results.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the local cache directory.</p>"},{"location":"api/results/#mteb.cache.ResultCache.has_remote","title":"<code>has_remote</code>  <code>property</code>","text":"<p>Check if the remote results repository exists in the cache directory.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the remote results repository exists, False otherwise.</p>"},{"location":"api/results/#mteb.cache.ResultCache.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the local cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear the local cache directory.\"\"\"\n    if self.cache_path.exists() and self.cache_path.is_dir():\n        shutil.rmtree(self.cache_path)\n        logger.info(f\"Cache directory {self.cache_path} cleared.\")\n    else:\n        msg = f\"Cache directory `{self.cache_path}` does not exist.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.download_from_remote","title":"<code>download_from_remote(remote='https://github.com/embeddings-benchmark/results', download_latest=True, revision=None)</code>","text":"<p>Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.</p> <p>Parameters:</p> Name Type Description Default <code>remote</code> <code>str</code> <p>The URL of the results repository on GitHub.</p> <code>'https://github.com/embeddings-benchmark/results'</code> <code>download_latest</code> <code>bool</code> <p>If True it will download the latest version of the repository, otherwise it will only update the existing repository.</p> <code>True</code> <code>revision</code> <code>str | None</code> <p>If specified, it will checkout the given revision after cloning or pulling the repository.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the local cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def download_from_remote(\n    self,\n    remote: str = \"https://github.com/embeddings-benchmark/results\",\n    download_latest: bool = True,\n    revision: str | None = None,\n) -&gt; Path:\n    \"\"\"Downloads the latest version of the results repository from GitHub to a local cache directory. Required git to be installed.\n\n    Args:\n        remote: The URL of the results repository on GitHub.\n        download_latest: If True it will download the latest version of the repository, otherwise it will only update the existing repository.\n        revision: If specified, it will checkout the given revision after cloning or pulling the repository.\n\n    Returns:\n        The path to the local cache directory.\n    \"\"\"\n    if not self.cache_path.exists() and not self.cache_path.is_dir():\n        logger.info(\n            f\"Cache directory {self.cache_path} does not exist, creating it\"\n        )\n\n    # if \"results\" folder already exists update it\n    results_directory = self.cache_path / \"remote\"\n\n    if results_directory.exists():\n        # check repository in the directory is the same as the remote\n        remote_url = subprocess.run(\n            [\"git\", \"config\", \"--get\", \"remote.origin.url\"],\n            cwd=results_directory,\n            capture_output=True,\n            text=True,\n        ).stdout.strip()\n        if remote_url != remote:\n            msg = (\n                f\"remote repository '{remote}' does not match the one in {results_directory},  which is '{remote_url}'.\"\n                + \" Please remove the directory and try again.\"\n            )\n            raise ValueError(msg)\n\n        if revision or download_latest:\n            logger.info(\n                f\"remote repository already exists in {results_directory}, fetching updates\"\n            )\n            subprocess.run(\n                [\"git\", \"fetch\", \"--all\", \"--tags\"],\n                cwd=results_directory,\n                check=True,\n            )\n        else:\n            logger.debug(\n                f\"Results repository already exists in {results_directory}, skipping update, \"\n                f\"set download_latest=True to update it\"\n            )\n\n        if revision:\n            logger.info(f\"Checking out revision '{revision}'\")\n            subprocess.run(\n                [\"git\", \"checkout\", revision],\n                cwd=results_directory,\n                check=True,\n            )\n        return results_directory\n\n    logger.info(\n        f\"No results repository found in {results_directory}, cloning it from {remote}\"\n    )\n\n    clone_cmd = [\"git\", \"clone\", \"--depth\", \"1\"]\n\n    if revision:\n        logger.info(f\"Cloning repository at revision '{revision}'\")\n        clone_cmd.append(f\"--revision={revision}\")\n    clone_cmd.extend([remote, \"remote\"])\n\n    subprocess.run(\n        clone_cmd,\n        cwd=self.cache_path,\n        check=True,\n    )\n\n    return results_directory\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_cache_paths","title":"<code>get_cache_paths(models=None, tasks=None, require_model_meta=True, include_remote=True, load_experiments=LoadExperimentEnum.NO_EXPERIMENTS)</code>","text":"<p>Get all paths to result JSON files in the cache directory.</p> <p>These paths can then be used to fetch task results, like: <pre><code>for path in paths:\n    task_result = TaskResult.from_disk(path)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Sequence[str] | Iterable[ModelMeta] | None</code> <p>A list of model names or ModelMeta objects to filter the paths.</p> <code>None</code> <code>tasks</code> <code>Sequence[str] | Iterable[AbsTask] | None</code> <p>A list of task names to filter the paths.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return paths that have a model_meta.json file.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned paths.</p> <code>True</code> <code>load_experiments</code> <code>LoadExperimentEnum | str</code> <p>If True, include experiments in the returned paths.</p> <code>NO_EXPERIMENTS</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>A list of paths in the cache directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; cache = mteb.ResultCache()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths\n&gt;&gt;&gt; paths = cache.get_cache_paths()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific task\n&gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific model\n&gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get all cache paths for a specific model and revision\n&gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n&gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>def get_cache_paths(\n    self,\n    models: Sequence[str] | Iterable[ModelMeta] | None = None,\n    tasks: Sequence[str] | Iterable[AbsTask] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n    load_experiments: LoadExperimentEnum | str = LoadExperimentEnum.NO_EXPERIMENTS,\n) -&gt; list[Path]:\n    \"\"\"Get all paths to result JSON files in the cache directory.\n\n    These paths can then be used to fetch task results, like:\n    ```python\n    for path in paths:\n        task_result = TaskResult.from_disk(path)\n    ```\n\n    Args:\n        models: A list of model names or ModelMeta objects to filter the paths.\n        tasks: A list of task names to filter the paths.\n        require_model_meta: If True, only return paths that have a model_meta.json file.\n        include_remote: If True, include remote results in the returned paths.\n        load_experiments: If True, include experiments in the returned paths.\n\n    Returns:\n        A list of paths in the cache directory.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; cache = mteb.ResultCache()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths\n        &gt;&gt;&gt; paths = cache.get_cache_paths()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific task\n        &gt;&gt;&gt; paths = cache.get_cache_paths(tasks=[\"STS12\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific model\n        &gt;&gt;&gt; paths = cache.get_cache_paths(models=[\"sentence-transformers/all-MiniLM-L6-v2\"])\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get all cache paths for a specific model and revision\n        &gt;&gt;&gt; model_meta = mteb.get_model_meta(\"sentence-transformers/all-MiniLM-L6-v2\")\n        &gt;&gt;&gt; paths = cache.get_cache_paths(models=[model_meta])\n    \"\"\"\n    if isinstance(load_experiments, str):\n        load_experiments = LoadExperimentEnum.from_str(load_experiments)\n\n    def _cache_paths(base_path: Path) -&gt; list[Path]:\n        return [\n            p\n            for p in base_path.glob(\"*/*/*.json\")  # model/revision/task.json\n            if p.name != \"model_meta.json\"\n        ]\n\n    def _experiments_paths(base_path: Path) -&gt; list[Path]:\n        return [\n            p\n            for p in base_path.glob(f\"*/*/{_EXPERIMENTS_FOLDER_NAME}/*/*.json\")\n            if p.name != \"model_meta.json\"\n        ]\n\n    def _get_paths(base_path: Path, experiments: LoadExperimentEnum) -&gt; list[Path]:\n        paths = _cache_paths(base_path)\n        if not experiments == LoadExperimentEnum.NO_EXPERIMENTS:\n            paths += _experiments_paths(base_path)\n        return paths\n\n    results_path = self.cache_path / \"results\"\n    remote_path = self.cache_path / \"remote\" / \"results\"\n\n    cache_paths = _get_paths(results_path, load_experiments)\n\n    if include_remote:\n        cache_paths += _get_paths(remote_path, load_experiments)\n\n    cache_paths = self._filter_paths_by_model_and_revision(\n        cache_paths,\n        models=models,\n        load_experiments=load_experiments,\n    )\n    cache_paths = self._filter_paths_by_task(cache_paths, tasks=tasks)\n\n    if require_model_meta:\n        cache_paths = [\n            p for p in cache_paths if (p.parent / \"model_meta.json\").exists()\n        ]\n    return cache_paths\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_models","title":"<code>get_models(tasks=None, require_model_meta=True, include_remote=True)</code>","text":"<p>Get all models in the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[str] | None</code> <p>A list of task names to filter the models.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return models that have a model_meta.json file.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned models.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[ModelName, Revision]]</code> <p>A list of tuples containing the model name and revision.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_models(\n    self,\n    tasks: Sequence[str] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n) -&gt; list[tuple[ModelName, Revision]]:\n    \"\"\"Get all models in the cache directory.\n\n    Args:\n        tasks: A list of task names to filter the models.\n        require_model_meta: If True, only return models that have a model_meta.json file.\n        include_remote: If True, include remote results in the returned models.\n\n    Returns:\n        A list of tuples containing the model name and revision.\n    \"\"\"\n    cache_paths = self.get_cache_paths(\n        tasks=tasks,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n    )\n    models = [(p.parent.parent.name, p.parent.name) for p in cache_paths]\n    return list(set(models))\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_task_names","title":"<code>get_task_names(models=None, require_model_meta=True, include_remote=True)</code>","text":"<p>Get all task names in the cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list[str] | list[ModelMeta] | None</code> <p>A list of model names or ModelMeta objects to filter the task names.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True, only return task names that have a model_meta.json file</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, include remote results in the returned task names.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names in the cache directory.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_task_names(\n    self,\n    models: list[str] | list[ModelMeta] | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n) -&gt; list[str]:\n    \"\"\"Get all task names in the cache directory.\n\n    Args:\n        models: A list of model names or ModelMeta objects to filter the task names.\n        require_model_meta: If True, only return task names that have a model_meta.json file\n        include_remote: If True, include remote results in the returned task names.\n\n    Returns:\n        A list of task names in the cache directory.\n    \"\"\"\n    cache_paths = self.get_cache_paths(\n        models=models,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n    )\n    tasks = [p.stem for p in cache_paths]\n    return list(set(tasks))\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.get_task_result_path","title":"<code>get_task_result_path(task_name, model_name, model_revision=None, remote=False, experiment_name=None)</code>","text":"<p>Get the path to the results of a specific task for a specific model and revision.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> <code>remote</code> <code>bool</code> <p>If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.</p> <code>False</code> <code>experiment_name</code> <code>str | None</code> <p>The name of the experiment as a valid directory name. If model_name is a ModelMeta object, its experiment_name will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the results of the task.</p> Source code in <code>mteb/cache.py</code> <pre><code>def get_task_result_path(\n    self,\n    task_name: str,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n    remote: bool = False,\n    experiment_name: str | None = None,\n) -&gt; Path:\n    \"\"\"Get the path to the results of a specific task for a specific model and revision.\n\n    Args:\n        task_name: The name of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n        remote: If True, it will return the path to the remote results repository, otherwise it will return the path to the local results repository.\n        experiment_name: The name of the experiment as a valid directory name. If model_name is a ModelMeta object, its experiment_name will be used.\n\n    Returns:\n        The path to the results of the task.\n    \"\"\"\n    results_folder = (\n        self.cache_path / \"results\"\n        if not remote\n        else self.cache_path / \"remote\" / \"results\"\n    )\n\n    if isinstance(model_name, ModelMeta):\n        if model_revision is not None:\n            logger.warning(\n                \"model_revision and experiment_name is ignored when model_name is a ModelMeta object\"\n            )\n        model_revision = model_name.revision\n        experiment_name = model_name.experiment_name\n        model_name = model_name.model_name_as_path()\n    elif isinstance(model_name, str):\n        model_name = model_name.replace(\"/\", \"__\").replace(\" \", \"_\")\n\n    model_path = results_folder / model_name\n\n    if model_revision is None:\n        msg = \"`model_revision` is not specified, attempting to load the latest revision. To disable this behavior, specify the 'model_revision` explicitly.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n        # get revs from paths\n        revisions = [p for p in model_path.glob(\"*\") if p.is_dir()]\n        if not revisions:\n            model_revision = \"no_revision_available\"\n        else:\n            if len(revisions) &gt; 1:\n                logger.warning(\n                    f\"Multiple revisions found for model {model_name}: {revisions}. Using the latest one (according to latest edit).\"\n                )\n                # sort folder by latest edit time\n                revisions.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n            model_revision = revisions[0].name\n\n    if experiment_name:\n        return (\n            model_path\n            / model_revision\n            / _EXPERIMENTS_FOLDER_NAME\n            / experiment_name\n            / f\"{task_name}.json\"\n        )\n    return model_path / model_revision / f\"{task_name}.json\"\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.load_results","title":"<code>load_results(models=None, tasks=None, require_model_meta=True, include_remote=True, validate_and_filter=False, only_main_score=False, load_experiments=LoadExperimentEnum.MATCH_KWARGS, experiment_kwargs=None)</code>","text":"<p>Loads the results from the cache directory and returns a BenchmarkResults object.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Sequence[str] | Iterable[ModelMeta] | None</code> <p>A list of model names to load the results for. If None it will load the results for all models.</p> <code>None</code> <code>tasks</code> <code>Sequence[str] | Iterable[AbsTask] | Benchmark | str | None</code> <p>A list of task names to load the results for. If str is passed, then benchmark will be loaded. If Benchmark is passed, then all tasks in the benchmark will be loaded. If None it will load the results for all tasks.</p> <code>None</code> <code>require_model_meta</code> <code>bool</code> <p>If True it will ignore results that do not have a model_meta.json file. If false it attempt to extract the model name and revision from the path.</p> <code>True</code> <code>include_remote</code> <code>bool</code> <p>If True, it will include results from the remote repository.</p> <code>True</code> <code>validate_and_filter</code> <code>bool</code> <p>If True it will validate that the results object for the task contains the correct splits and filter out splits from the results object that are not default in the task metadata.</p> <code>False</code> <code>only_main_score</code> <code>bool</code> <p>If True, only the main score will be loaded.</p> <code>False</code> <code>load_experiments</code> <code>LoadExperimentEnum | str</code> <p>If True, it will also load results from experiment folders.</p> <code>MATCH_KWARGS</code> <code>experiment_kwargs</code> <code>Mapping[str, Any] | list[Mapping[str, Any]] | None</code> <p>If specified, it will only load results from experiments with the specified kwargs. Only used if load_experiments is True.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A BenchmarkResults object containing the results for the specified models and tasks.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; cache = mteb.ResultCache()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Load results for specific models and tasks\n&gt;&gt;&gt; results = cache.load_results(\n...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n...     tasks=[\"STS12\"],\n...     require_model_meta=True,\n... )\n</code></pre> Source code in <code>mteb/cache.py</code> <pre><code>def load_results(\n    self,\n    models: Sequence[str] | Iterable[ModelMeta] | None = None,\n    tasks: Sequence[str] | Iterable[AbsTask] | Benchmark | str | None = None,\n    require_model_meta: bool = True,\n    include_remote: bool = True,\n    validate_and_filter: bool = False,\n    only_main_score: bool = False,\n    load_experiments: LoadExperimentEnum | str = LoadExperimentEnum.MATCH_KWARGS,\n    experiment_kwargs: Mapping[str, Any] | list[Mapping[str, Any]] | None = None,\n) -&gt; BenchmarkResults:\n    \"\"\"Loads the results from the cache directory and returns a BenchmarkResults object.\n\n    Args:\n        models: A list of model names to load the results for. If None it will load the results for all models.\n        tasks: A list of task names to load the results for. If str is passed, then benchmark will be loaded.\n            If Benchmark is passed, then all tasks in the benchmark will be loaded.\n            If None it will load the results for all tasks.\n        require_model_meta: If True it will ignore results that do not have a model_meta.json file. If false it attempt to\n            extract the model name and revision from the path.\n        include_remote: If True, it will include results from the remote repository.\n        validate_and_filter: If True it will validate that the results object for the task contains the correct splits and filter out\n            splits from the results object that are not default in the task metadata.\n        only_main_score: If True, only the main score will be loaded.\n        load_experiments: If True, it will also load results from experiment folders.\n        experiment_kwargs: If specified, it will only load results from experiments with the specified kwargs. Only used if load_experiments is True.\n\n    Returns:\n        A BenchmarkResults object containing the results for the specified models and tasks.\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; cache = mteb.ResultCache()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Load results for specific models and tasks\n        &gt;&gt;&gt; results = cache.load_results(\n        ...     models=[\"sentence-transformers/all-MiniLM-L6-v2\"],\n        ...     tasks=[\"STS12\"],\n        ...     require_model_meta=True,\n        ... )\n    \"\"\"\n    if isinstance(tasks, str):\n        tasks = mteb.get_benchmark(tasks)\n\n    if isinstance(load_experiments, str):\n        load_experiments = LoadExperimentEnum.from_str(load_experiments)\n\n    if (\n        load_experiments is not LoadExperimentEnum.MATCH_KWARGS\n        and experiment_kwargs is not None\n    ):\n        warnings.warn(\n            \"experiment_kwargs is specified but load_experiments is not set to MATCH_KWARGS.\"\n            \"No results will be loaded.\"\n        )\n\n    models_as_model_meta = models is not None and isinstance(\n        next(iter(models)), ModelMeta\n    )\n\n    paths = self.get_cache_paths(\n        models=models,\n        tasks=tasks,\n        require_model_meta=require_model_meta,\n        include_remote=include_remote,\n        load_experiments=load_experiments,\n    )\n    models_results = defaultdict(list)\n\n    task_names: dict[str, AbsTask | None] = {}\n    if tasks is not None:\n        for task in tasks:\n            if isinstance(task, AbsTask):\n                task_names[task.metadata.name] = task\n            else:\n                task_names[task] = None\n\n    experiment_names = set()\n    if isinstance(experiment_kwargs, Mapping):\n        experiment_kwargs = [experiment_kwargs]\n    if isinstance(experiment_kwargs, list):\n        experiment_names = {\n            _serialize_experiment_kwargs_to_name(params)\n            for params in experiment_kwargs\n        }\n    for path in paths:\n        task_result = TaskResult.from_disk(path)\n\n        if only_main_score:\n            task_result = task_result.only_main_score()\n        model_name, revision, experiment_name = (\n            self._get_model_name_and_revision_from_path(path.parent)\n        )\n\n        if validate_and_filter:\n            task_instance = task_names[task_result.task_name]\n            try:\n                task_result = task_result.validate_and_filter_scores(\n                    task=task_instance\n                )\n            except ValidationError as e:\n                logger.info(\n                    f\"Validation failed for {task_result.task_name} in {model_name} {revision}: {e}\"\n                )\n                continue\n\n        if len(experiment_names) &gt; 0 and experiment_name not in experiment_names:\n            logger.debug(\n                f\"Skipping experiment {experiment_name} as it is not in the specified experiment names\"\n            )\n            continue\n\n        if (\n            load_experiments is LoadExperimentEnum.MATCH_KWARGS\n            and not models_as_model_meta  # for models meta path are prefiltered\n            and len(experiment_names) == 0\n            and experiment_name is not None\n        ):\n            continue\n\n        models_results[(model_name, revision, experiment_name)].append(task_result)\n\n    # create BenchmarkResults object\n    models_results_object = [\n        ModelResult(\n            model_name=model_name,\n            model_revision=revision,\n            task_results=task_results,\n            experiment_name=experiment_name,\n        )\n        for (\n            model_name,\n            revision,\n            experiment_name,\n        ), task_results in models_results.items()\n    ]\n\n    return BenchmarkResults(\n        model_results=models_results_object,\n        benchmark=tasks if isinstance(tasks, Benchmark) else None,\n    )\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.load_task_result","title":"<code>load_task_result(task_name, model_name, model_revision=None, raise_if_not_found=False, prioritize_remote=False, experiment_name=None)</code>","text":"<p>Load the results from the local cache directory.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> <code>raise_if_not_found</code> <code>bool</code> <p>If True, raise an error if the results are not found.</p> <code>False</code> <code>prioritize_remote</code> <code>bool</code> <p>If True, it will first try to load the results from the remote repository, if available.</p> <code>False</code> <code>experiment_name</code> <code>str | None</code> <p>Optional experiment folder name (a valid directory name). If None, the default is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TaskResult | None</code> <p>The results of the task, or None if not found.</p> Source code in <code>mteb/cache.py</code> <pre><code>def load_task_result(\n    self,\n    task_name: str,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n    raise_if_not_found: bool = False,\n    prioritize_remote: bool = False,\n    experiment_name: str | None = None,\n) -&gt; TaskResult | None:\n    \"\"\"Load the results from the local cache directory.\n\n    Args:\n        task_name: The name of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n        raise_if_not_found: If True, raise an error if the results are not found.\n        prioritize_remote: If True, it will first try to load the results from the remote repository, if available.\n        experiment_name: Optional experiment folder name (a valid directory name). If None, the default is used.\n\n    Returns:\n        The results of the task, or None if not found.\n    \"\"\"\n    result_path = self.get_task_result_path(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_name=task_name,\n        experiment_name=experiment_name,\n    )\n\n    if self.has_remote:\n        remote_result_path = self.get_task_result_path(\n            model_name=model_name,\n            model_revision=model_revision,\n            task_name=task_name,\n            remote=True,\n            experiment_name=experiment_name,\n        )\n        if remote_result_path.exists() and prioritize_remote:\n            result_path = remote_result_path\n        elif not result_path.exists():\n            result_path = remote_result_path\n\n    if not result_path.exists():\n        msg = f\"Results for {model_name} on {task_name} not found in {result_path}\"\n        if raise_if_not_found:\n            raise FileNotFoundError(msg)\n        logger.debug(msg)\n        return None\n\n    return TaskResult.from_disk(result_path)\n</code></pre>"},{"location":"api/results/#mteb.cache.ResultCache.save_to_cache","title":"<code>save_to_cache(task_result, model_name, model_revision=None)</code>","text":"<p>Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.</p> <p>Where model_name is a path-normalized model name. In addition we also save a model_meta.json in the revision folder to preserve the model metadata.</p> <p>Parameters:</p> Name Type Description Default <code>task_result</code> <code>TaskResult</code> <p>The results of the task.</p> required <code>model_name</code> <code>str | ModelMeta</code> <p>The name of the model as a valid directory name or a ModelMeta object.</p> required <code>model_revision</code> <code>str | None</code> <p>The revision of the model. Must be specified if model_name is a string.</p> <code>None</code> Source code in <code>mteb/cache.py</code> <pre><code>def save_to_cache(\n    self,\n    task_result: TaskResult,\n    model_name: str | ModelMeta,\n    model_revision: str | None = None,\n) -&gt; None:\n    \"\"\"Save the task results to the local cache directory in the location {model_name}/{model_revision}/{task_name}.json.\n\n    Where model_name is a path-normalized model name.\n    In addition we also save a model_meta.json in the revision folder to preserve the model metadata.\n\n    Args:\n        task_result: The results of the task.\n        model_name: The name of the model as a valid directory name or a ModelMeta object.\n        model_revision: The revision of the model. Must be specified if model_name is a string.\n    \"\"\"\n    result_path = self.get_task_result_path(\n        model_name=model_name,\n        model_revision=model_revision,\n        task_name=task_result.task_name,\n    )\n    result_path.parent.mkdir(parents=True, exist_ok=True)\n    task_result.to_disk(result_path)\n\n    model_meta_path = result_path.parent / \"model_meta.json\"\n    if isinstance(model_name, ModelMeta):\n        meta = model_name\n        with model_meta_path.open(\"w\") as f:\n            json.dump(meta.to_dict(), f, default=str, indent=4)\n</code></pre>"},{"location":"api/results/#result-objects","title":"Result Objects","text":""},{"location":"api/results/#mteb.results.TaskResult","title":"<code>mteb.results.TaskResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class to represent the MTEB result.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>The name of the MTEB task.</p> <code>dataset_revision</code> <code>str</code> <p>The revision dataset for the task on HuggingFace dataset hub.</p> <code>mteb_version</code> <code>str | None</code> <p>The version of the MTEB used to evaluate the model.</p> <code>scores</code> <code>dict[SplitName, list[ScoresDict]]</code> <p>The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, list[Scores]]. Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of the dataset.</p> <code>evaluation_time</code> <code>float | None</code> <p>The time taken to evaluate the model.</p> <code>kg_co2_emissions</code> <code>float | None</code> <p>The kg of CO2 emissions produced by the model during evaluation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scores = {\n...     \"evaluation_time\": 100,\n...     \"train\": {\n...         \"en-de\": {\n...             \"main_score\": 0.5,\n...         },\n...         \"en-fr\": {\n...             \"main_score\": 0.6,\n...         },\n...     },\n... }\n&gt;&gt;&gt; sample_task = ... # some MTEB task\n&gt;&gt;&gt; mteb_results = TaskResult.from_task_results(sample_task, scores)\n&gt;&gt;&gt; mteb_results.get_score()  # get the main score for all languages\n0.55\n&gt;&gt;&gt; mteb_results.get_score(languages=[\"fra\"])  # get the main score for French\n0.6\n&gt;&gt;&gt; mteb_results.to_dict()\n{'dataset_revision': '1.0', 'task_name': 'sample_task', 'mteb_version': '1.0.0', 'evaluation_time': 100, 'scores': {'train':\n    [\n        {'main_score': 0.5, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']},\n        {'main_score': 0.6, 'hf_subset': 'en-fr', 'languages': ['eng-Latn', 'fra-Latn']}\n    ]}\n}\n</code></pre> Source code in <code>mteb/results/task_result.py</code> <pre><code>class TaskResult(BaseModel):\n    \"\"\"A class to represent the MTEB result.\n\n    Attributes:\n        task_name: The name of the MTEB task.\n        dataset_revision: The revision dataset for the task on HuggingFace dataset hub.\n        mteb_version: The version of the MTEB used to evaluate the model.\n        scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, list[Scores]].\n            Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n            the dataset.\n        evaluation_time: The time taken to evaluate the model.\n        kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n\n    Examples:\n        &gt;&gt;&gt; scores = {\n        ...     \"evaluation_time\": 100,\n        ...     \"train\": {\n        ...         \"en-de\": {\n        ...             \"main_score\": 0.5,\n        ...         },\n        ...         \"en-fr\": {\n        ...             \"main_score\": 0.6,\n        ...         },\n        ...     },\n        ... }\n        &gt;&gt;&gt; sample_task = ... # some MTEB task\n        &gt;&gt;&gt; mteb_results = TaskResult.from_task_results(sample_task, scores)\n        &gt;&gt;&gt; mteb_results.get_score()  # get the main score for all languages\n        0.55\n        &gt;&gt;&gt; mteb_results.get_score(languages=[\"fra\"])  # get the main score for French\n        0.6\n        &gt;&gt;&gt; mteb_results.to_dict()\n        {'dataset_revision': '1.0', 'task_name': 'sample_task', 'mteb_version': '1.0.0', 'evaluation_time': 100, 'scores': {'train':\n            [\n                {'main_score': 0.5, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']},\n                {'main_score': 0.6, 'hf_subset': 'en-fr', 'languages': ['eng-Latn', 'fra-Latn']}\n            ]}\n        }\n    \"\"\"\n\n    dataset_revision: str\n    task_name: str\n    mteb_version: str | None\n    scores: dict[SplitName, list[ScoresDict]]\n    evaluation_time: float | None\n    kg_co2_emissions: float | None = None\n    date: datetime.datetime | None = None\n\n    @classmethod\n    def from_task_results(\n        cls,\n        task: AbsTask | type[AbsTask],\n        scores: dict[SplitName, Mapping[HFSubset, ScoresDict]],\n        evaluation_time: float,\n        kg_co2_emissions: float | None = None,\n        date: datetime.datetime | None = None,\n    ) -&gt; TaskResult:\n        \"\"\"Create a TaskResult from the task and scores.\n\n        Args:\n            task: The task to create the TaskResult from.\n            scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]].\n                Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n                the dataset.\n            evaluation_time: The time taken to evaluate the model.\n            kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n            date: The date the model was trained on.\n        \"\"\"\n        task_meta = task.metadata\n        subset2langscripts = task_meta.hf_subsets_to_langscripts\n        flat_scores = defaultdict(list)\n        for split, hf_subset_scores in scores.items():\n            for hf_subset, hf_scores in hf_subset_scores.items():\n                eval_langs = subset2langscripts[hf_subset]\n                _scores = {\n                    **hf_scores,\n                    \"hf_subset\": hf_subset,\n                    \"languages\": eval_langs,\n                }\n                flat_scores[split].append(_scores)\n\n        return TaskResult(\n            dataset_revision=task.metadata.revision,\n            task_name=task.metadata.name,\n            mteb_version=version(\"mteb\"),\n            scores=flat_scores,\n            evaluation_time=evaluation_time,\n            kg_co2_emissions=kg_co2_emissions,\n            date=date,\n        )\n\n    @field_validator(\"scores\")\n    @classmethod\n    def _validate_scores(\n        cls, v: dict[SplitName, list[ScoresDict]]\n    ) -&gt; dict[SplitName, list[ScoresDict]]:\n        for split, hf_subset_scores in v.items():\n            for hf_subset_score in hf_subset_scores:\n                if not isinstance(hf_subset_score, dict):\n                    raise ValueError(\"Scores should be a dictionary\")\n                cls._validate_scores_dict(hf_subset_score)\n        return v\n\n    @staticmethod\n    def _validate_scores_dict(scores: ScoresDict) -&gt; None:\n        if \"main_score\" not in scores:\n            raise ValueError(\"'main_score' should be in scores\")\n        if \"hf_subset\" not in scores or not isinstance(scores[\"hf_subset\"], str):\n            raise ValueError(\"hf_subset should be in scores and should be a string\")\n        if \"languages\" not in scores or not isinstance(scores[\"languages\"], list):\n            raise ValueError(\"languages should be in scores and should be a list\")\n\n        # check that it is json serializable\n        try:\n            _ = json.dumps(scores)\n        except Exception as e:\n            raise ValueError(f\"Scores are not json serializable: {e}\")\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get the languages present in the scores.\"\"\"\n        langs = []\n        for split, split_res in self.scores.items():\n            for entry in split_res:\n                langs.extend([lang.split(\"-\")[0] for lang in entry[\"languages\"]])\n        return list(set(langs))\n\n    @cached_property\n    def task(self) -&gt; AbsTask:\n        \"\"\"Get the task associated with the result.\"\"\"\n        from mteb.get_tasks import get_task\n\n        return get_task(self.task_name)\n\n    @property\n    def domains(self) -&gt; list[TaskDomain]:\n        \"\"\"Get the domains of the task.\"\"\"\n        doms = self.task.metadata.domains\n        if doms is None:\n            doms = []\n        return doms\n\n    @property\n    def task_type(self) -&gt; str:\n        \"\"\"Get the type of the task.\"\"\"\n        return self.task.metadata.type\n\n    @property\n    def is_public(self) -&gt; bool:\n        \"\"\"Check if the task is public.\"\"\"\n        return self.task.metadata.is_public\n\n    @property\n    def hf_subsets(self) -&gt; list[str]:\n        \"\"\"Get the hf_subsets present in the scores.\"\"\"\n        hf_subsets = set()\n        for split, split_res in self.scores.items():\n            for entry in split_res:\n                hf_subsets.add(entry[\"hf_subset\"])\n        return list(hf_subsets)\n\n    @property\n    def eval_splits(self) -&gt; list[str]:\n        \"\"\"Get the eval splits present in the scores.\"\"\"\n        return list(self.scores.keys())\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert the TaskResult to a dictionary.\n\n        Returns:\n            The TaskResult as a dictionary.\n        \"\"\"\n        return self.model_dump()\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; Self:\n        \"\"\"Create a TaskResult from a dictionary.\n\n        Args:\n            data: The dictionary to create the TaskResult from.\n\n        Returns:\n            The created TaskResult object.\n        \"\"\"\n        return cls.model_validate(data)\n\n    def _round_scores(self, scores: dict[SplitName, list[ScoresDict]], n: int) -&gt; None:\n        \"\"\"Recursively round scores to n decimal places\"\"\"\n        for key, value in scores.items():\n            if isinstance(value, dict):\n                self._round_scores(value, n)\n            elif isinstance(value, list):\n                for i, v in enumerate(value):\n                    if isinstance(v, dict):\n                        self._round_scores(v, n)\n                    elif isinstance(v, float):\n                        value[i] = round(v, n)  # type: ignore[call-overload]\n\n            elif isinstance(value, float):\n                scores[key] = round(value, n)\n\n    def to_disk(self, path: Path) -&gt; None:\n        \"\"\"Save TaskResult to disk.\n\n        Args:\n            path: The path to the file to save.\n        \"\"\"\n        json_obj = self.model_dump()\n        json_obj[\"date\"] = self.date.timestamp() if self.date else None\n        self._round_scores(json_obj[\"scores\"], 6)\n\n        with path.open(\"w\") as f:\n            json.dump(json_obj, f, indent=2)\n\n    @classmethod\n    def from_disk(cls, path: Path, load_historic_data: bool = True) -&gt; TaskResult:\n        \"\"\"Load TaskResult from disk.\n\n        Args:\n            path: The path to the file to load.\n            load_historic_data: Whether to attempt to load historic data from before v1.11.0.\n\n        Returns:\n            The loaded TaskResult object.\n        \"\"\"\n        with path.open(\"r\", encoding=\"utf-8\") as f:\n            json_str = f.read()\n\n        if not load_historic_data:\n            try:\n                return cls.model_validate_json(json_str)\n            except Exception as e:\n                raise ValueError(\n                    f\"Error loading TaskResult from disk. You can try to load historic data by setting `load_historic_data=True`. Error: {e}\"\n                )\n        data = json.loads(json_str)\n        pre_1_11_load = (\n            (\n                \"mteb_version\" in data\n                and data[\"mteb_version\"] is not None\n                and Version(data[\"mteb_version\"]) &lt; Version(\"1.11.0\")\n            )\n            or \"mteb_version\" not in data\n        )  # assume it is before 1.11.0 if the version is not present\n\n        try:\n            obj: TaskResult = cls.model_validate_json(json_str)\n        except Exception as e:\n            if not pre_1_11_load:\n                raise e\n            logger.debug(\n                f\"Could not load TaskResult from disk, got error: {e}. Attempting to load from disk using format from before v1.11.0\"\n            )\n            obj = cls._convert_from_before_v1_11_0(data)\n\n        pre_v_12_48 = (\n            \"mteb_version\" in data\n            and data[\"mteb_version\"] is not None\n            and Version(data[\"mteb_version\"]) &lt; Version(\"1.12.48\")\n        )\n\n        if pre_v_12_48:\n            cls._fix_pair_classification_scores(obj)\n\n        return obj\n\n    @classmethod\n    def _fix_pair_classification_scores(cls, obj: TaskResult) -&gt; None:\n        from mteb import get_task\n\n        task_name = obj.task_name\n        task: AbsTask | type[AbsTask]\n        if task_name in outdated_tasks:\n            task = outdated_tasks[task_name]\n        else:\n            task = get_task(obj.task_name)\n\n        if task.metadata.type == \"PairClassification\":\n            for split, split_scores in obj.scores.items():\n                for hf_subset_scores in split_scores:\n                    # concatenate score e.g. [\"max\"][\"ap\"] -&gt; [\"max_ap\"]\n                    for key in list(hf_subset_scores.keys()):\n                        if isinstance(hf_subset_scores[key], dict):\n                            for k, v in hf_subset_scores[key].items():\n                                hf_subset_scores[f\"{key}_{k}\"] = v  # type: ignore[index]\n                            hf_subset_scores.pop(key)  # type: ignore[attr-defined]\n\n    @classmethod\n    def _convert_from_before_v1_11_0(cls, data: dict) -&gt; TaskResult:\n        from mteb.get_tasks import _TASKS_REGISTRY\n\n        # in case the task name is not found in the registry, try to find a lower case version\n        lower_case_registry = {k.lower(): v for k, v in _TASKS_REGISTRY.items()}\n\n        scores = {**data}\n\n        dataset_revision = scores.pop(\n            \"dataset_revision\", \"dataset revision not available\"\n        )\n        task_name = scores.pop(\"mteb_dataset_name\")\n        mteb_version = scores.pop(\"mteb_version\", \"mteb version not available\")\n\n        # calculate evaluation time across all splits (move to top level)\n        evaluation_time = 0\n        for split, split_score in scores.items():\n            if \"evaluation_time\" in split_score:\n                evaluation_time += split_score.pop(\"evaluation_time\")\n\n        # normalize the scores to always be {split: {hf_subset: scores}}\n        contains_hf_subset = any(\n            isinstance(hf_subset_scores, dict)\n            for split_scores in scores.values()\n            for k, hf_subset_scores in split_scores.items()\n            if k\n            not in {\"v_measures\", \"cos_sim\", \"euclidean\", \"manhattan\", \"dot\", \"max\"}\n        )\n        if not contains_hf_subset:\n            for split, split_score in scores.items():\n                scores[split] = {\"default\": split_score.copy()}\n\n        if task_name in outdated_tasks:\n            logger.debug(\n                f\"Loading {task_name} as a dummy task as it no longer exists within MTEB. To avoid this set `load_historic_data=False`\"\n            )\n            task = outdated_tasks[task_name]\n        else:\n            if task_name in renamed_tasks:\n                task_name = renamed_tasks[task_name]\n            task = _TASKS_REGISTRY.get(\n                task_name, lower_case_registry[task_name.lower()]\n            )\n\n        # make sure that main score exists\n        main_score = task.metadata.main_score\n        for split, split_score in scores.items():\n            for hf_subset, hf_subset_scores in split_score.items():\n                for name, prev_name in [\n                    (ScoringFunction.COSINE.value, \"cos_sim\"),\n                    (ScoringFunction.MANHATTAN.value, \"manhattan\"),\n                    (ScoringFunction.EUCLIDEAN.value, \"euclidean\"),\n                    (ScoringFunction.DOT_PRODUCT.value, \"dot\"),\n                    (\"max\", \"max\"),\n                    (\"similarity\", \"similarity\"),\n                ]:\n                    prev_name_scores = hf_subset_scores.pop(prev_name, None)\n                    if prev_name_scores is not None:\n                        for k, v in prev_name_scores.items():\n                            hf_subset_scores[f\"{name}_{k}\"] = v\n\n                if \"main_score\" not in hf_subset_scores:\n                    if main_score in hf_subset_scores:\n                        hf_subset_scores[\"main_score\"] = hf_subset_scores[main_score]\n                    else:\n                        msg = f\"Main score {main_score} not found in scores\"\n                        logger.warning(msg)\n                        warnings.warn(msg)\n                        hf_subset_scores[\"main_score\"] = None\n\n        # specific fixes:\n        if task_name == \"MLSUMClusteringP2P\" and mteb_version in [\n            \"1.1.2.dev0\",\n            \"1.1.3.dev0\",\n        ]:  # back then it was only the french subsection which was implemented\n            scores[\"test\"][\"fr\"] = scores[\"test\"].pop(\"default\")\n        if task_name == \"MLSUMClusteringS2S\" and mteb_version in [\n            \"1.1.2.dev0\",\n            \"1.1.3.dev0\",\n        ]:\n            scores[\"test\"][\"fr\"] = scores[\"test\"].pop(\"default\")\n        if task_name == \"XPQARetrieval\":  # subset were renamed from \"fr\" to \"fra-fra\"\n            if \"test\" in scores and \"fr\" in scores[\"test\"]:\n                scores[\"test\"][\"fra-fra\"] = scores[\"test\"].pop(\"fr\")\n\n        result: TaskResult = TaskResult.from_task_results(\n            task,\n            scores,\n            evaluation_time,\n            kg_co2_emissions=None,\n        )\n        result.dataset_revision = dataset_revision\n        result.mteb_version = mteb_version\n        return result\n\n    def get_score(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] = lambda scores: scores[\"main_score\"],\n        aggregation: Callable[[list[Score]], Any] = np.mean,\n    ) -&gt; Any:\n        \"\"\"Get a score for the specified splits, languages, scripts and aggregation function.\n\n        Args:\n            splits: The splits to consider.\n            languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n            scripts: The scripts to consider.\n            getter: A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".\n            aggregation: The aggregation function to use.\n\n        Returns:\n            The result of the aggregation function on the scores.\n        \"\"\"\n        if splits is None:\n            splits = list(self.scores.keys())\n\n        lang_scripts = LanguageScripts.from_languages_and_scripts(languages, scripts)\n\n        values = []\n        for split in splits:\n            if split not in self.scores:\n                raise ValueError(f\"Split {split} not found in scores\")\n\n            for scores in self.scores[split]:\n                eval_langs = scores[\"languages\"]\n                for lang in eval_langs:\n                    if lang_scripts.contains_language(lang):\n                        values.append(getter(scores))\n                        break\n\n        return aggregation(values)\n\n    def _get_score_fast(\n        self,\n        splits: Iterable[str] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        subsets: Iterable[str] | None = None,\n    ) -&gt; float:\n        \"\"\"Sped up version of get_score that will be used if no aggregation, script or getter needs to be specified.\n\n        Args:\n            splits: The splits to consider.\n            languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n            subsets: The hf_subsets to consider.\n\n        Returns:\n            The mean main score for the specified splits, languages and subsets.\n        \"\"\"\n        if splits is None:\n            splits = self.scores.keys()\n        val_sum = 0\n        n_val = 0\n        for split in splits:\n            if split not in self.scores:\n                raise ValueError(f\"Split missing from scores: {split}\")\n\n            for scores in self.scores[split]:\n                langs = scores[\"languages\"]\n                hf_subset = scores[\"hf_subset\"]\n                main_score = scores.get(\"main_score\", None)\n                if main_score is None:\n                    raise ValueError(f\"Missing main score for subset: {hf_subset}\")\n                if subsets and hf_subset not in subsets:\n                    continue\n                elif subsets:\n                    val_sum += main_score\n                    n_val += 1\n                    continue\n\n                if languages is None:\n                    val_sum += main_score\n                    n_val += 1\n                    continue\n                for lang in langs:\n                    if lang.split(\"-\")[0] in languages:\n                        val_sum += main_score\n                        n_val += 1\n                        logger.info(f\"{val_sum=}, {n_val=}\")\n                        break\n        if n_val == 0:\n            raise ValueError(\"No splits had scores for the specified languages.\")\n        return val_sum / n_val\n\n    @classmethod\n    def from_validated(cls, **data) -&gt; TaskResult:\n        \"\"\"Create a TaskResult from validated data.\n\n        Returns:\n            The created TaskResult object.\n        \"\"\"\n        return cls.model_construct(**data)\n\n    def __repr__(self) -&gt; str:\n        return f\"TaskResult(task_name={self.task_name}, scores=...)\"\n\n    def only_main_score(self) -&gt; TaskResult:\n        \"\"\"Return a new TaskResult object with only the main score.\n\n        Returns:\n            A new TaskResult object with only the main score.\n        \"\"\"\n        new_scores: dict[str, list[Score]] = {}\n        for split in self.scores:\n            new_scores[split] = []\n            for subset_scores in self.scores[split]:\n                new_scores[split].append(\n                    {\n                        \"hf_subset\": subset_scores.get(\"hf_subset\", \"default\"),\n                        \"main_score\": subset_scores.get(\"main_score\", np.nan),\n                        \"languages\": subset_scores.get(\"languages\", []),\n                    }\n                )\n        new_res = {**self.to_dict(), \"scores\": new_scores}\n        return TaskResult.from_validated(**new_res)\n\n    def validate_and_filter_scores(\n        self,\n        task: AbsTask | None = None,\n    ) -&gt; TaskResult:\n        \"\"\"Validate and filter the scores against the task metadata.\n\n        This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata.\n        Additionally it also ensure that all of the splits required as well as the languages are present in the scores.\n        Returns new TaskResult object.\n\n        Args:\n            task: The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages,\n                the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.\n\n        Returns:\n            A new TaskResult object with the validated and filtered scores.\n        \"\"\"\n        from mteb.get_tasks import get_task\n\n        if task is None:\n            task = get_task(self.task_name)\n\n        splits = task.eval_splits\n        hf_subsets = set(task.hf_subsets)  # Convert to set once\n\n        new_scores: dict[str, list[Score]] = {}\n        seen_splits = set()\n        for split in self.scores:\n            if split not in splits:\n                continue\n            seen_subsets = set()\n            if task.is_aggregate:\n                # aggregate tasks only have the default subset, but in metadata can be multiple\n                new_scores[split] = [\n                    _scores\n                    for _scores in self.scores[split]\n                    if _scores[\"hf_subset\"] == \"default\"\n                ]\n                seen_subsets = {\"default\"}\n            else:\n                new_scores[split] = [\n                    _scores\n                    for _scores in self.scores[split]\n                    if _scores[\"hf_subset\"] in hf_subsets\n                ]\n            for _scores in new_scores[split]:\n                seen_subsets.add(_scores[\"hf_subset\"])\n\n            if seen_subsets != hf_subsets and not (\n                task.is_aggregate and \"default\" in seen_subsets\n            ):\n                missing_subsets = hf_subsets - seen_subsets\n                if len(missing_subsets) &gt; 2:\n                    subset1, subset2 = list(missing_subsets)[:2]\n                    missing_subsets_str = f\"{{'{subset1}', '{subset2}', ...}}\"\n                else:\n                    missing_subsets_str = str(missing_subsets)\n\n                msg = f\"{task.metadata.name}: Missing subsets {missing_subsets_str} for split {split}\"\n                logger.warning(msg)\n                warnings.warn(msg)\n                for missing_subset in missing_subsets:\n                    new_scores[split].append(\n                        {\n                            \"hf_subset\": missing_subset,\n                            \"main_score\": np.nan,\n                            \"languages\": task.metadata.hf_subsets_to_langscripts.get(\n                                missing_subset, []\n                            ),\n                        }\n                    )\n            seen_splits.add(split)\n        if seen_splits != set(splits):\n            msg = f\"{task.metadata.name}: Missing splits {set(splits) - seen_splits}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            for missing_split in set(splits) - seen_splits:\n                new_scores[missing_split] = []\n                for missing_subset in hf_subsets:\n                    new_scores[missing_split].append(\n                        {\n                            \"hf_subset\": missing_subset,\n                            \"main_score\": np.nan,\n                            \"languages\": task.metadata.hf_subsets_to_langscripts.get(\n                                missing_subset, []\n                            ),\n                        }\n                    )\n        data = self.model_dump()\n        data[\"scores\"] = new_scores\n        return type(self).model_construct(**data)\n\n    def is_mergeable(\n        self,\n        result: TaskResult | AbsTask,\n        criteria: list[str] | list[Criteria] = [\n            \"mteb_version\",\n            \"dataset_revision\",\n        ],\n        raise_error: bool = False,\n    ) -&gt; bool:\n        \"\"\"Checks if the TaskResult object can be merged with another TaskResult or Task.\n\n        Args:\n            result: The TaskResult or Task object to check against.\n            criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n                It will always check that the task name match.\n            raise_error: If True, raises an error if the objects cannot be merged. If False, returns False.\n\n        Returns:\n            True if the TaskResult object can be merged with the other object, False otherwise.\n        \"\"\"\n        criteria = [Criteria.from_str(c) if isinstance(c, str) else c for c in criteria]\n        if isinstance(result, TaskResult):\n            name = result.task_name\n            revision = result.dataset_revision\n            mteb_version = result.mteb_version\n        elif isinstance(result, AbsTask):\n            mteb_version = version(\"mteb\")\n            name = result.metadata.name\n            revision = result.metadata.revision\n        else:\n            msg = \"result must be a TaskResult or AbsTask object\"\n            if raise_error:\n                raise ValueError(msg)\n            logger.debug(msg)\n            return False\n\n        if self.task_name != name:\n            msg = f\"Cannot merge TaskResult objects as they are derived from different tasks ({self.task_name} and {name})\"\n            if raise_error:\n                raise ValueError(msg)\n            logger.debug(msg)\n            return False\n\n        if Criteria.MTEB_VERSION in criteria and self.mteb_version != mteb_version:\n            msg = f\"Cannot merge TaskResult objects as they are derived from different MTEB versions ({self.mteb_version} (loaded) and {mteb_version} (current))\"\n            if raise_error:\n                raise ValueError(msg)\n            logger.debug(msg)\n            return False\n\n        if Criteria.DATASET_REVISION in criteria and self.dataset_revision != revision:\n            msg = f\"Cannot merge TaskResult objects as they are derived from different dataset revisions ({self.dataset_revision} and {revision})\"\n            if raise_error:\n                raise ValueError(msg)\n            logger.debug(msg)\n            return False\n\n        return True\n\n    def merge(\n        self,\n        new_results: TaskResult,\n        criteria: list[str] | list[Criteria] = [\n            \"mteb_version\",\n            \"dataset_revision\",\n        ],\n    ) -&gt; TaskResult:\n        \"\"\"Merges two TaskResult objects.\n\n        Args:\n            new_results: The new TaskResult object to merge with the current one.\n            criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n                It will always check that the task name match.\n\n        Returns:\n            A new TaskResult object with the merged scores.\n        \"\"\"\n        self.is_mergeable(new_results, criteria=criteria, raise_error=True)\n\n        merged_scores = self.scores.copy()\n\n        for split, scores in new_results.scores.items():\n            if split in merged_scores:\n                merged_scores[split] = self._merge_split_scores(\n                    merged_scores[split], scores\n                )\n            else:\n                merged_scores[split] = scores\n\n        existing_kg_co2_emissions = (\n            self.kg_co2_emissions if self.kg_co2_emissions else 0\n        )\n        new_kg_co2_emissions = (\n            new_results.kg_co2_emissions if new_results.kg_co2_emissions else 0\n        )\n        merged_kg_co2_emissions = None\n        if existing_kg_co2_emissions and new_kg_co2_emissions:\n            merged_kg_co2_emissions = existing_kg_co2_emissions + new_kg_co2_emissions\n\n        merged_evaluation_time = None\n        if self.evaluation_time and new_results.evaluation_time:\n            merged_evaluation_time = self.evaluation_time + new_results.evaluation_time\n        date = self.date\n        if new_results.date is not None and (date is None or new_results.date &gt; date):\n            date = new_results.date\n        merged_results = TaskResult(\n            dataset_revision=new_results.dataset_revision,\n            task_name=new_results.task_name,\n            mteb_version=new_results.mteb_version,\n            scores=merged_scores,\n            evaluation_time=merged_evaluation_time,\n            kg_co2_emissions=merged_kg_co2_emissions,\n            date=date,\n        )\n\n        return merged_results\n\n    @staticmethod\n    def _merge_split_scores(\n        existing_scores: list[ScoresDict], new_scores: list[ScoresDict]\n    ) -&gt; list[ScoresDict]:\n        merged = {score[\"hf_subset\"]: score for score in existing_scores}\n        for score in new_scores:\n            merged[score[\"hf_subset\"]] = score\n        return list(merged.values())\n\n    def get_missing_evaluations(self, task: AbsTask) -&gt; dict[str, list[str]]:\n        \"\"\"Checks which splits and subsets are missing from the results.\n\n        Args:\n            task: The task to check against.\n\n        Returns:\n            A dictionary with the splits as keys and a list of missing subsets as values.\n        \"\"\"\n        missing_splits = {}\n        for splits in task.eval_splits:\n            if splits not in self.scores:  # split it fully missing\n                missing_splits[splits] = task.hf_subsets\n            if splits in self.scores:\n                hf_subsets = {score[\"hf_subset\"] for score in self.scores[splits]}\n                missing_subsets = list(set(task.hf_subsets) - hf_subsets)\n                if missing_subsets:\n                    missing_splits[splits] = missing_subsets\n\n        return missing_splits\n\n    @deprecated(\n        \"HF deprecated `EvalResults` in favor of `Benchmarks` and it's results. \"\n        \"To push new results use ModelMeta.push_eval_results().\"\n    )\n    def get_hf_eval_results(self) -&gt; list[EvalResult]:\n        \"\"\"Create HF evaluation results objects from TaskResult objects.\n\n        Returns:\n            List of EvalResult objects for each split and subset.\n        \"\"\"\n        task_metadata = self.task.metadata\n        task_type = task_metadata._hf_task_type()[0]\n        results = []\n        for split, scores in self.scores.items():\n            for subset_results in scores:\n                subset = subset_results.get(\"hf_subset\", \"default\")\n                results.append(\n                    EvalResult(\n                        task_type=task_type,\n                        task_name=task_metadata.type,\n                        dataset_type=task_metadata.dataset[\"path\"],\n                        dataset_name=f\"{task_metadata.name} ({subset})\",\n                        dataset_config=subset,\n                        dataset_split=split,\n                        dataset_revision=task_metadata.dataset[\"revision\"],\n                        metric_type=task_metadata.main_score,\n                        metric_name=task_metadata.main_score,\n                        metric_value=subset_results[\"main_score\"],\n                        source_name=\"MTEB\",\n                        source_url=\"https://github.com/embeddings-benchmark/mteb/\",\n                    )\n                )\n        return results\n\n    def _to_hf_benchmark_result(self, user: str | None = None) -&gt; HFEvalResults:\n        task_metadata = mteb.get_task(self.task_name).metadata\n        dataset_id = task_metadata.dataset[\"path\"]\n        dataset_revision = task_metadata.dataset[\"revision\"]\n        eval_results = []\n        evaluated_splits = set(self.scores.keys())\n        evaluated_subsets = set()\n\n        notes = f\"Obtained using MTEB v{self.mteb_version}\"\n        source = HFEvalResultSource(\n            url=\"https://github.com/embeddings-benchmark/mteb/\",\n            name=notes,\n            user=user,\n        )\n\n        for split, split_results in self.scores.items():\n            for subset_results in split_results:\n                subset_name = subset_results.get(\"hf_subset\", \"default\")\n                task_id = f\"{self.task_name}_{subset_name}_{split}\"\n                eval_results.append(\n                    HFEvalResult(\n                        dataset=HFEvalResultDataset(\n                            id=dataset_id,\n                            task_id=task_id,\n                            revision=dataset_revision,\n                        ),\n                        value=round(subset_results[\"main_score\"] * 100, 5),\n                        source=source,\n                        date=self.date,\n                        notes=notes,\n                    )\n                )\n                evaluated_subsets.add(subset_name)\n\n        if len(evaluated_splits) == len(task_metadata.eval_splits) and len(\n            evaluated_subsets\n        ) == len(task_metadata.hf_subsets):\n            # overall score\n            eval_results.append(\n                HFEvalResult(\n                    dataset=HFEvalResultDataset(\n                        id=dataset_id,\n                        task_id=task_metadata.name,\n                        revision=dataset_revision,\n                    ),\n                    value=round(self.get_score() * 100, 5),\n                    source=source,\n                    date=self.date,\n                    notes=notes,\n                )\n            )\n        return HFEvalResults.model_validate(eval_results)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get the domains of the task.</p>"},{"location":"api/results/#mteb.results.TaskResult.eval_splits","title":"<code>eval_splits</code>  <code>property</code>","text":"<p>Get the eval splits present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.hf_subsets","title":"<code>hf_subsets</code>  <code>property</code>","text":"<p>Get the hf_subsets present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.is_public","title":"<code>is_public</code>  <code>property</code>","text":"<p>Check if the task is public.</p>"},{"location":"api/results/#mteb.results.TaskResult.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get the languages present in the scores.</p>"},{"location":"api/results/#mteb.results.TaskResult.task","title":"<code>task</code>  <code>cached</code> <code>property</code>","text":"<p>Get the task associated with the result.</p>"},{"location":"api/results/#mteb.results.TaskResult.task_type","title":"<code>task_type</code>  <code>property</code>","text":"<p>Get the type of the task.</p>"},{"location":"api/results/#mteb.results.TaskResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The dictionary to create the TaskResult from.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The created TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Self:\n    \"\"\"Create a TaskResult from a dictionary.\n\n    Args:\n        data: The dictionary to create the TaskResult from.\n\n    Returns:\n        The created TaskResult object.\n    \"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_disk","title":"<code>from_disk(path, load_historic_data=True)</code>  <code>classmethod</code>","text":"<p>Load TaskResult from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to load.</p> required <code>load_historic_data</code> <code>bool</code> <p>Whether to attempt to load historic data from before v1.11.0.</p> <code>True</code> <p>Returns:</p> Type Description <code>TaskResult</code> <p>The loaded TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path, load_historic_data: bool = True) -&gt; TaskResult:\n    \"\"\"Load TaskResult from disk.\n\n    Args:\n        path: The path to the file to load.\n        load_historic_data: Whether to attempt to load historic data from before v1.11.0.\n\n    Returns:\n        The loaded TaskResult object.\n    \"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        json_str = f.read()\n\n    if not load_historic_data:\n        try:\n            return cls.model_validate_json(json_str)\n        except Exception as e:\n            raise ValueError(\n                f\"Error loading TaskResult from disk. You can try to load historic data by setting `load_historic_data=True`. Error: {e}\"\n            )\n    data = json.loads(json_str)\n    pre_1_11_load = (\n        (\n            \"mteb_version\" in data\n            and data[\"mteb_version\"] is not None\n            and Version(data[\"mteb_version\"]) &lt; Version(\"1.11.0\")\n        )\n        or \"mteb_version\" not in data\n    )  # assume it is before 1.11.0 if the version is not present\n\n    try:\n        obj: TaskResult = cls.model_validate_json(json_str)\n    except Exception as e:\n        if not pre_1_11_load:\n            raise e\n        logger.debug(\n            f\"Could not load TaskResult from disk, got error: {e}. Attempting to load from disk using format from before v1.11.0\"\n        )\n        obj = cls._convert_from_before_v1_11_0(data)\n\n    pre_v_12_48 = (\n        \"mteb_version\" in data\n        and data[\"mteb_version\"] is not None\n        and Version(data[\"mteb_version\"]) &lt; Version(\"1.12.48\")\n    )\n\n    if pre_v_12_48:\n        cls._fix_pair_classification_scores(obj)\n\n    return obj\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_task_results","title":"<code>from_task_results(task, scores, evaluation_time, kg_co2_emissions=None, date=None)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from the task and scores.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask | type[AbsTask]</code> <p>The task to create the TaskResult from.</p> required <code>scores</code> <code>dict[SplitName, Mapping[HFSubset, ScoresDict]]</code> <p>The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]]. Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of the dataset.</p> required <code>evaluation_time</code> <code>float</code> <p>The time taken to evaluate the model.</p> required <code>kg_co2_emissions</code> <code>float | None</code> <p>The kg of CO2 emissions produced by the model during evaluation.</p> <code>None</code> <code>date</code> <code>datetime | None</code> <p>The date the model was trained on.</p> <code>None</code> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_task_results(\n    cls,\n    task: AbsTask | type[AbsTask],\n    scores: dict[SplitName, Mapping[HFSubset, ScoresDict]],\n    evaluation_time: float,\n    kg_co2_emissions: float | None = None,\n    date: datetime.datetime | None = None,\n) -&gt; TaskResult:\n    \"\"\"Create a TaskResult from the task and scores.\n\n    Args:\n        task: The task to create the TaskResult from.\n        scores: The scores of the model on the dataset. The scores is a dictionary with the following structure; dict[SplitName, dict[HFSubset, Scores]].\n            Where Scores is a dictionary with the following structure; dict[str, Any]. Where the keys and values are scores. Split is the split of\n            the dataset.\n        evaluation_time: The time taken to evaluate the model.\n        kg_co2_emissions: The kg of CO2 emissions produced by the model during evaluation.\n        date: The date the model was trained on.\n    \"\"\"\n    task_meta = task.metadata\n    subset2langscripts = task_meta.hf_subsets_to_langscripts\n    flat_scores = defaultdict(list)\n    for split, hf_subset_scores in scores.items():\n        for hf_subset, hf_scores in hf_subset_scores.items():\n            eval_langs = subset2langscripts[hf_subset]\n            _scores = {\n                **hf_scores,\n                \"hf_subset\": hf_subset,\n                \"languages\": eval_langs,\n            }\n            flat_scores[split].append(_scores)\n\n    return TaskResult(\n        dataset_revision=task.metadata.revision,\n        task_name=task.metadata.name,\n        mteb_version=version(\"mteb\"),\n        scores=flat_scores,\n        evaluation_time=evaluation_time,\n        kg_co2_emissions=kg_co2_emissions,\n        date=date,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create a TaskResult from validated data.</p> <p>Returns:</p> Type Description <code>TaskResult</code> <p>The created TaskResult object.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data) -&gt; TaskResult:\n    \"\"\"Create a TaskResult from validated data.\n\n    Returns:\n        The created TaskResult object.\n    \"\"\"\n    return cls.model_construct(**data)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_hf_eval_results","title":"<code>get_hf_eval_results()</code>","text":"<p>Create HF evaluation results objects from TaskResult objects.</p> <p>Returns:</p> Type Description <code>list[EvalResult]</code> <p>List of EvalResult objects for each split and subset.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>@deprecated(\n    \"HF deprecated `EvalResults` in favor of `Benchmarks` and it's results. \"\n    \"To push new results use ModelMeta.push_eval_results().\"\n)\ndef get_hf_eval_results(self) -&gt; list[EvalResult]:\n    \"\"\"Create HF evaluation results objects from TaskResult objects.\n\n    Returns:\n        List of EvalResult objects for each split and subset.\n    \"\"\"\n    task_metadata = self.task.metadata\n    task_type = task_metadata._hf_task_type()[0]\n    results = []\n    for split, scores in self.scores.items():\n        for subset_results in scores:\n            subset = subset_results.get(\"hf_subset\", \"default\")\n            results.append(\n                EvalResult(\n                    task_type=task_type,\n                    task_name=task_metadata.type,\n                    dataset_type=task_metadata.dataset[\"path\"],\n                    dataset_name=f\"{task_metadata.name} ({subset})\",\n                    dataset_config=subset,\n                    dataset_split=split,\n                    dataset_revision=task_metadata.dataset[\"revision\"],\n                    metric_type=task_metadata.main_score,\n                    metric_name=task_metadata.main_score,\n                    metric_value=subset_results[\"main_score\"],\n                    source_name=\"MTEB\",\n                    source_url=\"https://github.com/embeddings-benchmark/mteb/\",\n                )\n            )\n    return results\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_missing_evaluations","title":"<code>get_missing_evaluations(task)</code>","text":"<p>Checks which splits and subsets are missing from the results.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask</code> <p>The task to check against.</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dictionary with the splits as keys and a list of missing subsets as values.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def get_missing_evaluations(self, task: AbsTask) -&gt; dict[str, list[str]]:\n    \"\"\"Checks which splits and subsets are missing from the results.\n\n    Args:\n        task: The task to check against.\n\n    Returns:\n        A dictionary with the splits as keys and a list of missing subsets as values.\n    \"\"\"\n    missing_splits = {}\n    for splits in task.eval_splits:\n        if splits not in self.scores:  # split it fully missing\n            missing_splits[splits] = task.hf_subsets\n        if splits in self.scores:\n            hf_subsets = {score[\"hf_subset\"] for score in self.scores[splits]}\n            missing_subsets = list(set(task.hf_subsets) - hf_subsets)\n            if missing_subsets:\n                missing_splits[splits] = missing_subsets\n\n    return missing_splits\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.get_score","title":"<code>get_score(splits=None, languages=None, scripts=None, getter=lambda scores: scores['main_score'], aggregation=np.mean)</code>","text":"<p>Get a score for the specified splits, languages, scripts and aggregation function.</p> <p>Parameters:</p> Name Type Description Default <code>splits</code> <code>list[SplitName] | None</code> <p>The splits to consider.</p> <code>None</code> <code>languages</code> <code>list[ISOLanguage | ISOLanguageScript] | None</code> <p>The languages to consider. Can be ISO language codes or ISO language script codes.</p> <code>None</code> <code>scripts</code> <code>list[ISOLanguageScript] | None</code> <p>The scripts to consider.</p> <code>None</code> <code>getter</code> <code>Callable[[ScoresDict], Score]</code> <p>A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".</p> <code>lambda scores: scores['main_score']</code> <code>aggregation</code> <code>Callable[[list[Score]], Any]</code> <p>The aggregation function to use.</p> <code>mean</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the aggregation function on the scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def get_score(\n    self,\n    splits: list[SplitName] | None = None,\n    languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n    scripts: list[ISOLanguageScript] | None = None,\n    getter: Callable[[ScoresDict], Score] = lambda scores: scores[\"main_score\"],\n    aggregation: Callable[[list[Score]], Any] = np.mean,\n) -&gt; Any:\n    \"\"\"Get a score for the specified splits, languages, scripts and aggregation function.\n\n    Args:\n        splits: The splits to consider.\n        languages: The languages to consider. Can be ISO language codes or ISO language script codes.\n        scripts: The scripts to consider.\n        getter: A function that takes a scores dictionary and returns a score e.g. \"main_score\" or \"evaluation_time\".\n        aggregation: The aggregation function to use.\n\n    Returns:\n        The result of the aggregation function on the scores.\n    \"\"\"\n    if splits is None:\n        splits = list(self.scores.keys())\n\n    lang_scripts = LanguageScripts.from_languages_and_scripts(languages, scripts)\n\n    values = []\n    for split in splits:\n        if split not in self.scores:\n            raise ValueError(f\"Split {split} not found in scores\")\n\n        for scores in self.scores[split]:\n            eval_langs = scores[\"languages\"]\n            for lang in eval_langs:\n                if lang_scripts.contains_language(lang):\n                    values.append(getter(scores))\n                    break\n\n    return aggregation(values)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.is_mergeable","title":"<code>is_mergeable(result, criteria=['mteb_version', 'dataset_revision'], raise_error=False)</code>","text":"<p>Checks if the TaskResult object can be merged with another TaskResult or Task.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>TaskResult | AbsTask</code> <p>The TaskResult or Task object to check against.</p> required <code>criteria</code> <code>list[str] | list[Criteria]</code> <p>Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\". It will always check that the task name match.</p> <code>['mteb_version', 'dataset_revision']</code> <code>raise_error</code> <code>bool</code> <p>If True, raises an error if the objects cannot be merged. If False, returns False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the TaskResult object can be merged with the other object, False otherwise.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def is_mergeable(\n    self,\n    result: TaskResult | AbsTask,\n    criteria: list[str] | list[Criteria] = [\n        \"mteb_version\",\n        \"dataset_revision\",\n    ],\n    raise_error: bool = False,\n) -&gt; bool:\n    \"\"\"Checks if the TaskResult object can be merged with another TaskResult or Task.\n\n    Args:\n        result: The TaskResult or Task object to check against.\n        criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n            It will always check that the task name match.\n        raise_error: If True, raises an error if the objects cannot be merged. If False, returns False.\n\n    Returns:\n        True if the TaskResult object can be merged with the other object, False otherwise.\n    \"\"\"\n    criteria = [Criteria.from_str(c) if isinstance(c, str) else c for c in criteria]\n    if isinstance(result, TaskResult):\n        name = result.task_name\n        revision = result.dataset_revision\n        mteb_version = result.mteb_version\n    elif isinstance(result, AbsTask):\n        mteb_version = version(\"mteb\")\n        name = result.metadata.name\n        revision = result.metadata.revision\n    else:\n        msg = \"result must be a TaskResult or AbsTask object\"\n        if raise_error:\n            raise ValueError(msg)\n        logger.debug(msg)\n        return False\n\n    if self.task_name != name:\n        msg = f\"Cannot merge TaskResult objects as they are derived from different tasks ({self.task_name} and {name})\"\n        if raise_error:\n            raise ValueError(msg)\n        logger.debug(msg)\n        return False\n\n    if Criteria.MTEB_VERSION in criteria and self.mteb_version != mteb_version:\n        msg = f\"Cannot merge TaskResult objects as they are derived from different MTEB versions ({self.mteb_version} (loaded) and {mteb_version} (current))\"\n        if raise_error:\n            raise ValueError(msg)\n        logger.debug(msg)\n        return False\n\n    if Criteria.DATASET_REVISION in criteria and self.dataset_revision != revision:\n        msg = f\"Cannot merge TaskResult objects as they are derived from different dataset revisions ({self.dataset_revision} and {revision})\"\n        if raise_error:\n            raise ValueError(msg)\n        logger.debug(msg)\n        return False\n\n    return True\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.merge","title":"<code>merge(new_results, criteria=['mteb_version', 'dataset_revision'])</code>","text":"<p>Merges two TaskResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>new_results</code> <code>TaskResult</code> <p>The new TaskResult object to merge with the current one.</p> required <code>criteria</code> <code>list[str] | list[Criteria]</code> <p>Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\". It will always check that the task name match.</p> <code>['mteb_version', 'dataset_revision']</code> <p>Returns:</p> Type Description <code>TaskResult</code> <p>A new TaskResult object with the merged scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def merge(\n    self,\n    new_results: TaskResult,\n    criteria: list[str] | list[Criteria] = [\n        \"mteb_version\",\n        \"dataset_revision\",\n    ],\n) -&gt; TaskResult:\n    \"\"\"Merges two TaskResult objects.\n\n    Args:\n        new_results: The new TaskResult object to merge with the current one.\n        criteria: Additional criteria to check for merging. Can be \"mteb_version\" or \"dataset_revision\".\n            It will always check that the task name match.\n\n    Returns:\n        A new TaskResult object with the merged scores.\n    \"\"\"\n    self.is_mergeable(new_results, criteria=criteria, raise_error=True)\n\n    merged_scores = self.scores.copy()\n\n    for split, scores in new_results.scores.items():\n        if split in merged_scores:\n            merged_scores[split] = self._merge_split_scores(\n                merged_scores[split], scores\n            )\n        else:\n            merged_scores[split] = scores\n\n    existing_kg_co2_emissions = (\n        self.kg_co2_emissions if self.kg_co2_emissions else 0\n    )\n    new_kg_co2_emissions = (\n        new_results.kg_co2_emissions if new_results.kg_co2_emissions else 0\n    )\n    merged_kg_co2_emissions = None\n    if existing_kg_co2_emissions and new_kg_co2_emissions:\n        merged_kg_co2_emissions = existing_kg_co2_emissions + new_kg_co2_emissions\n\n    merged_evaluation_time = None\n    if self.evaluation_time and new_results.evaluation_time:\n        merged_evaluation_time = self.evaluation_time + new_results.evaluation_time\n    date = self.date\n    if new_results.date is not None and (date is None or new_results.date &gt; date):\n        date = new_results.date\n    merged_results = TaskResult(\n        dataset_revision=new_results.dataset_revision,\n        task_name=new_results.task_name,\n        mteb_version=new_results.mteb_version,\n        scores=merged_scores,\n        evaluation_time=merged_evaluation_time,\n        kg_co2_emissions=merged_kg_co2_emissions,\n        date=date,\n    )\n\n    return merged_results\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.only_main_score","title":"<code>only_main_score()</code>","text":"<p>Return a new TaskResult object with only the main score.</p> <p>Returns:</p> Type Description <code>TaskResult</code> <p>A new TaskResult object with only the main score.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def only_main_score(self) -&gt; TaskResult:\n    \"\"\"Return a new TaskResult object with only the main score.\n\n    Returns:\n        A new TaskResult object with only the main score.\n    \"\"\"\n    new_scores: dict[str, list[Score]] = {}\n    for split in self.scores:\n        new_scores[split] = []\n        for subset_scores in self.scores[split]:\n            new_scores[split].append(\n                {\n                    \"hf_subset\": subset_scores.get(\"hf_subset\", \"default\"),\n                    \"main_score\": subset_scores.get(\"main_score\", np.nan),\n                    \"languages\": subset_scores.get(\"languages\", []),\n                }\n            )\n    new_res = {**self.to_dict(), \"scores\": new_scores}\n    return TaskResult.from_validated(**new_res)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the TaskResult to a dictionary.</p> <p>Returns:</p> Type Description <code>dict</code> <p>The TaskResult as a dictionary.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert the TaskResult to a dictionary.\n\n    Returns:\n        The TaskResult as a dictionary.\n    \"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save TaskResult to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to save.</p> required Source code in <code>mteb/results/task_result.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n    \"\"\"Save TaskResult to disk.\n\n    Args:\n        path: The path to the file to save.\n    \"\"\"\n    json_obj = self.model_dump()\n    json_obj[\"date\"] = self.date.timestamp() if self.date else None\n    self._round_scores(json_obj[\"scores\"], 6)\n\n    with path.open(\"w\") as f:\n        json.dump(json_obj, f, indent=2)\n</code></pre>"},{"location":"api/results/#mteb.results.TaskResult.validate_and_filter_scores","title":"<code>validate_and_filter_scores(task=None)</code>","text":"<p>Validate and filter the scores against the task metadata.</p> <p>This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata. Additionally it also ensure that all of the splits required as well as the languages are present in the scores. Returns new TaskResult object.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>AbsTask | None</code> <p>The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages, the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.</p> <code>None</code> <p>Returns:</p> Type Description <code>TaskResult</code> <p>A new TaskResult object with the validated and filtered scores.</p> Source code in <code>mteb/results/task_result.py</code> <pre><code>def validate_and_filter_scores(\n    self,\n    task: AbsTask | None = None,\n) -&gt; TaskResult:\n    \"\"\"Validate and filter the scores against the task metadata.\n\n    This ensures that the scores are correct for the given task, by removing any splits besides those specified in the task metadata.\n    Additionally it also ensure that all of the splits required as well as the languages are present in the scores.\n    Returns new TaskResult object.\n\n    Args:\n        task: The task to validate the scores against. E.g. if the task supplied is limited to certain splits and languages,\n            the scores will be filtered to only include those splits and languages. If None it will attempt to get the task from the task_name.\n\n    Returns:\n        A new TaskResult object with the validated and filtered scores.\n    \"\"\"\n    from mteb.get_tasks import get_task\n\n    if task is None:\n        task = get_task(self.task_name)\n\n    splits = task.eval_splits\n    hf_subsets = set(task.hf_subsets)  # Convert to set once\n\n    new_scores: dict[str, list[Score]] = {}\n    seen_splits = set()\n    for split in self.scores:\n        if split not in splits:\n            continue\n        seen_subsets = set()\n        if task.is_aggregate:\n            # aggregate tasks only have the default subset, but in metadata can be multiple\n            new_scores[split] = [\n                _scores\n                for _scores in self.scores[split]\n                if _scores[\"hf_subset\"] == \"default\"\n            ]\n            seen_subsets = {\"default\"}\n        else:\n            new_scores[split] = [\n                _scores\n                for _scores in self.scores[split]\n                if _scores[\"hf_subset\"] in hf_subsets\n            ]\n        for _scores in new_scores[split]:\n            seen_subsets.add(_scores[\"hf_subset\"])\n\n        if seen_subsets != hf_subsets and not (\n            task.is_aggregate and \"default\" in seen_subsets\n        ):\n            missing_subsets = hf_subsets - seen_subsets\n            if len(missing_subsets) &gt; 2:\n                subset1, subset2 = list(missing_subsets)[:2]\n                missing_subsets_str = f\"{{'{subset1}', '{subset2}', ...}}\"\n            else:\n                missing_subsets_str = str(missing_subsets)\n\n            msg = f\"{task.metadata.name}: Missing subsets {missing_subsets_str} for split {split}\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            for missing_subset in missing_subsets:\n                new_scores[split].append(\n                    {\n                        \"hf_subset\": missing_subset,\n                        \"main_score\": np.nan,\n                        \"languages\": task.metadata.hf_subsets_to_langscripts.get(\n                            missing_subset, []\n                        ),\n                    }\n                )\n        seen_splits.add(split)\n    if seen_splits != set(splits):\n        msg = f\"{task.metadata.name}: Missing splits {set(splits) - seen_splits}\"\n        logger.warning(msg)\n        warnings.warn(msg)\n        for missing_split in set(splits) - seen_splits:\n            new_scores[missing_split] = []\n            for missing_subset in hf_subsets:\n                new_scores[missing_split].append(\n                    {\n                        \"hf_subset\": missing_subset,\n                        \"main_score\": np.nan,\n                        \"languages\": task.metadata.hf_subsets_to_langscripts.get(\n                            missing_subset, []\n                        ),\n                    }\n                )\n    data = self.model_dump()\n    data[\"scores\"] = new_scores\n    return type(self).model_construct(**data)\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult","title":"<code>mteb.results.ModelResult</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data class to hold the results of a model on a set of tasks.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the model.</p> <code>model_revision</code> <code>str | None</code> <p>Revision of the model.</p> <code>task_results</code> <code>list[TaskResult]</code> <p>List of TaskResult objects.</p> Source code in <code>mteb/results/model_result.py</code> <pre><code>class ModelResult(BaseModel):\n    \"\"\"Data class to hold the results of a model on a set of tasks.\n\n    Attributes:\n        model_name: Name of the model.\n        model_revision: Revision of the model.\n        task_results: List of TaskResult objects.\n    \"\"\"\n\n    model_name: str\n    model_revision: str | None\n    task_results: list[TaskResult]\n    default_modalities: list[Modalities] = Field(\n        default_factory=lambda: [cast(\"Modalities\", \"text\")], alias=\"modalities\"\n    )\n    model_config = (\n        ConfigDict(  # to free up the name model_* which is otherwise protected\n            protected_namespaces=(),\n        )\n    )\n    exceptions: list[TaskError] | None = None\n    experiment_name: str | None = None\n\n    def __repr__(self) -&gt; str:\n        n_entries = len(self.task_results)\n        return (\n            f\"ModelResult(model_name={self.model_name}, model_revision={self.model_revision}, \"\n            f\"{'experiment_name=' + self.experiment_name + ', ' if self.experiment_name else ''}\"\n            f\"task_results=[...](#{n_entries}))\"\n        )\n\n    @classmethod\n    def from_validated(cls, **data: dict[str, Any]) -&gt; ModelResult:\n        \"\"\"Create a ModelResult from validated data.\n\n        Args:\n            data: The validated data.\n        \"\"\"\n        data[\"task_results\"] = [  # type: ignore[assignment]\n            TaskResult.from_validated(**res)  # type: ignore[arg-type]\n            for res in data[\"task_results\"]\n        ]\n        return cls.model_construct(**data)  # type: ignore[arg-type]\n\n    def _filter_tasks(\n        self,\n        task_names: list[str] | None = None,\n        languages: list[str] | None = None,\n        domains: list[TaskDomain] | None = None,\n        task_types: list[TaskType] | None = None,\n        modalities: list[Modalities] | None = None,\n        is_public: bool | None = None,\n    ) -&gt; ModelResult:\n        new_task_results = []\n        for task_result in self.task_results:\n            if (task_names is not None) and (task_result.task_name not in task_names):\n                continue\n            if languages is not None:\n                task_languages = task_result.languages\n                if not any(lang in task_languages for lang in languages):\n                    continue\n            if domains is not None:\n                task_domains = task_result.domains\n                if not any(domain in task_domains for domain in domains):\n                    continue\n            if (task_types is not None) and (task_result.task_type not in task_types):\n                continue\n            if modalities is not None:\n                task_modalities = getattr(task_result, \"modalities\", [])\n                if not any(modality in task_modalities for modality in modalities):\n                    continue\n            if (is_public is not None) and (task_result.is_public is not is_public):\n                continue\n            new_task_results.append(task_result)\n        return type(self).model_construct(\n            model_name=self.model_name,\n            model_revision=self.model_revision,\n            task_results=new_task_results,\n            experiment_name=self.experiment_name,\n        )\n\n    def select_tasks(self, tasks: Iterable[AbsTask]) -&gt; ModelResult:\n        \"\"\"Select tasks from the ModelResult based on a list of AbsTask objects.\n\n        Args:\n            tasks: A sequence of AbsTask objects to select from the ModelResult.\n        \"\"\"\n        task_name_to_task = {task.metadata.name: task for task in tasks}\n        new_task_results = [\n            task_res.validate_and_filter_scores(task_name_to_task[task_res.task_name])\n            for task_res in self.task_results\n            if task_res.task_name in task_name_to_task\n        ]\n        return type(self).model_construct(\n            model_name=self.model_name,\n            model_revision=self.model_revision,\n            task_results=new_task_results,\n            experiment_name=self.experiment_name,\n        )\n\n    @overload\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"wide\"] = \"wide\",\n    ) -&gt; dict: ...\n\n    @overload\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"long\"] = \"long\",\n    ) -&gt; list: ...\n\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; dict | list:\n        if (getter is not None) or (aggregation is not None) or (scripts is not None):\n            use_fast = False\n            getter = (\n                getter if getter is not None else lambda scores: scores[\"main_score\"]\n            )\n            aggregation = aggregation if aggregation is not None else np.mean\n        else:\n            use_fast = True\n        aggregation = cast(\"Callable[[list[Score]], Any]\", aggregation)\n        getter = cast(\"Callable[[ScoresDict], Score]\", getter)\n\n        if format == \"wide\":\n            scores = {}\n            for res in self.task_results:\n                try:\n                    if use_fast:\n                        scores[res.task_name] = res._get_score_fast(\n                            splits=splits,\n                            languages=languages,\n                        )\n                    else:\n                        scores[res.task_name] = res.get_score(\n                            splits=splits,\n                            languages=languages,\n                            aggregation=aggregation,\n                            getter=getter,\n                            scripts=scripts,\n                        )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {res.task_name} due to {e}.\"\n                    )\n            return scores\n        if format == \"long\":\n            entries = []\n            for task_res in self.task_results:\n                try:\n                    if use_fast:\n                        score = task_res._get_score_fast(\n                            splits=splits,\n                            languages=languages,\n                        )\n                    else:\n                        score = task_res.get_score(\n                            splits=splits,\n                            languages=languages,\n                            aggregation=aggregation,\n                            getter=getter,\n                            scripts=scripts,\n                        )\n                    entry = dict(\n                        model_name=self.model_name,\n                        model_revision=self.model_revision,\n                        task_name=task_res.task_name,\n                        score=score,\n                        mteb_version=task_res.mteb_version,\n                        dataset_revision=task_res.dataset_revision,\n                        evaluation_time=task_res.evaluation_time,\n                        kg_co2_emissions=task_res.kg_co2_emissions,\n                    )\n                    entries.append(entry)\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {task_res.task_name} due to {e}.\"\n                    )\n            return entries\n\n    def _get_score_for_table(self) -&gt; list[dict[str, str | float | list[str]]]:\n        scores_data = []\n        model_name = self.model_name\n        for task_result in self.task_results:\n            task_name = task_result.task_name\n            for split, scores_list in task_result.scores.items():\n                for score_item in scores_list:\n                    row = {\n                        \"model_name\": model_name,\n                        \"model_revision\": self.model_revision,\n                        \"task_name\": task_name,\n                        \"split\": split,\n                        \"language\": score_item.get(\"languages\", [\"Unknown\"]),\n                        \"subset\": score_item.get(\"hf_subset\", \"default\"),\n                        \"score\": score_item.get(\"main_score\", None),\n                    }\n                    scores_data.append(row)\n\n        return scores_data\n\n    def to_dataframe(\n        self,\n        aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n        aggregation_fn: Callable[[list[Score]], Any] | None = None,\n        include_model_revision: bool = False,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n        The DataFrame will have the following columns in addition to the metadata columns:\n\n        - model_name: The name of the model.\n        - task_name: The name of the task.\n        - score: The main score of the model on the task.\n\n        In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n        - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n        - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n        Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n        Args:\n            aggregation_level: The aggregation to use. Can be one of:\n                - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n                - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n                - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n            aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n            include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n            format: The format of the DataFrame. Can be one of:\n                - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n                - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n        Returns:\n            A DataFrame with the scores for all models and tasks.\n        \"\"\"\n        scores_data = self._get_score_for_table()\n\n        if not scores_data:\n            msg = \"No scores data available. Returning empty DataFrame.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            return pd.DataFrame()\n\n        # Create DataFrame\n        df = pd.DataFrame(scores_data)\n\n        _columns = [\"model_name\"]\n        if include_model_revision is False:\n            df = df.drop(columns=[\"model_revision\"])\n        else:\n            _columns.append(\"model_revision\")\n\n        return _aggregate_and_pivot(\n            df,\n            columns=_columns,\n            aggregation_level=aggregation_level,\n            format=format,\n            aggregation_fn=aggregation_fn,\n        )\n\n    def __hash__(self) -&gt; int:\n        return id(self)\n\n    def __iter__(self) -&gt; Iterable[TaskResult]:  # type: ignore[override]\n        return iter(self.task_results)\n\n    def __getitem__(self, index) -&gt; TaskResult:\n        return self.task_results[index]\n\n    def __len__(self) -&gt; int:\n        return len(self.task_results)\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get all languages in the model results.\n\n        Returns:\n            A list of languages in the model results.\n        \"\"\"\n        langs = []\n        for task_res in self.task_results:\n            langs.extend(task_res.languages)\n        return list(set(langs))\n\n    @property\n    def domains(self) -&gt; list[str]:\n        \"\"\"Get all domains in the model results.\n\n        Returns:\n            A list of domains in the model results.\n\n        \"\"\"\n        ds = []\n        for task_res in self.task_results:\n            ds.extend(task_res.domains)\n        return list(set(ds))\n\n    @property\n    def task_types(self) -&gt; list[str]:\n        \"\"\"Get all task types in the model results.\n\n        Returns:\n            A list of task types in the model results.\n        \"\"\"\n        return list({task_res.task_type for task_res in self.task_results})\n\n    @property\n    def task_names(self) -&gt; list[str]:\n        \"\"\"Get all task names in the model results.\n\n        Returns:\n            A list of task names in the model results.\n        \"\"\"\n        return [task_res.task_name for task_res in self.task_results]\n\n    @property\n    def modalities(self) -&gt; list[Modalities]:\n        \"\"\"Get all modalities in the task results.\n\n        Returns:\n            A list of modalities in the task results.\n        \"\"\"\n        mods: list[Modalities] = []\n        for task_res in self.task_results:\n            task_modalities = getattr(task_res, \"modalities\", [])\n            mods.extend(task_modalities)\n        if not mods:\n            mods = self.default_modalities\n        return list(set(mods))\n\n    def to_disk(self, path: Path) -&gt; None:\n        \"\"\"Save ModelResult to disk as JSON.\n\n        Args:\n            path: The path to the file to save.\n        \"\"\"\n        with path.open(\"w\") as f:\n            f.write(self.model_dump_json(indent=2))\n\n    @classmethod\n    def from_disk(cls, path: Path) -&gt; ModelResult:\n        \"\"\"Load ModelResult from disk.\n\n        Args:\n            path: The path to the JSON file to load.\n\n        Returns:\n            The loaded ModelResult object.\n        \"\"\"\n        with path.open(\"r\", encoding=\"utf-8\") as f:\n            return cls.model_validate_json(f.read())\n\n    def push_model_results(\n        self, user: str | None = None, *, create_pr: bool = False\n    ) -&gt; None:\n        \"\"\"Push the model results to the Hugging Face Hub.\n\n        Args:\n            user: The user or organization of results source.\n            create_pr: Whether to create a pull request\n        \"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            path = Path(tmpdir)\n            for task_result in self.task_results:\n                task_results = task_result._to_hf_benchmark_result(user)\n                with (path / f\"{task_result.task_name}.yaml\").open(\n                    \"w\", encoding=\"utf-8\"\n                ) as f:\n                    f.write(task_results.to_yaml())\n\n            huggingface_hub.upload_folder(\n                repo_id=self.model_name,\n                repo_type=\"model\",\n                path_in_repo=\".eval_results\",\n                folder_path=path,\n                commit_message=f\"Add evaluation results for model {self.model_name} revision {self.model_revision}\",\n                create_pr=create_pr,\n            )\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get all domains in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of domains in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get all languages in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of languages in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Get all modalities in the task results.</p> <p>Returns:</p> Type Description <code>list[Modalities]</code> <p>A list of modalities in the task results.</p>"},{"location":"api/results/#mteb.results.ModelResult.task_names","title":"<code>task_names</code>  <code>property</code>","text":"<p>Get all task names in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.task_types","title":"<code>task_types</code>  <code>property</code>","text":"<p>Get all task types in the model results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task types in the model results.</p>"},{"location":"api/results/#mteb.results.ModelResult.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load ModelResult from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the JSON file to load.</p> required <p>Returns:</p> Type Description <code>ModelResult</code> <p>The loaded ModelResult object.</p> Source code in <code>mteb/results/model_result.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path) -&gt; ModelResult:\n    \"\"\"Load ModelResult from disk.\n\n    Args:\n        path: The path to the JSON file to load.\n\n    Returns:\n        The loaded ModelResult object.\n    \"\"\"\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        return cls.model_validate_json(f.read())\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create a ModelResult from validated data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The validated data.</p> <code>{}</code> Source code in <code>mteb/results/model_result.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data: dict[str, Any]) -&gt; ModelResult:\n    \"\"\"Create a ModelResult from validated data.\n\n    Args:\n        data: The validated data.\n    \"\"\"\n    data[\"task_results\"] = [  # type: ignore[assignment]\n        TaskResult.from_validated(**res)  # type: ignore[arg-type]\n        for res in data[\"task_results\"]\n    ]\n    return cls.model_construct(**data)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.push_model_results","title":"<code>push_model_results(user=None, *, create_pr=False)</code>","text":"<p>Push the model results to the Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>str | None</code> <p>The user or organization of results source.</p> <code>None</code> <code>create_pr</code> <code>bool</code> <p>Whether to create a pull request</p> <code>False</code> Source code in <code>mteb/results/model_result.py</code> <pre><code>def push_model_results(\n    self, user: str | None = None, *, create_pr: bool = False\n) -&gt; None:\n    \"\"\"Push the model results to the Hugging Face Hub.\n\n    Args:\n        user: The user or organization of results source.\n        create_pr: Whether to create a pull request\n    \"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        for task_result in self.task_results:\n            task_results = task_result._to_hf_benchmark_result(user)\n            with (path / f\"{task_result.task_name}.yaml\").open(\n                \"w\", encoding=\"utf-8\"\n            ) as f:\n                f.write(task_results.to_yaml())\n\n        huggingface_hub.upload_folder(\n            repo_id=self.model_name,\n            repo_type=\"model\",\n            path_in_repo=\".eval_results\",\n            folder_path=path,\n            commit_message=f\"Add evaluation results for model {self.model_name} revision {self.model_revision}\",\n            create_pr=create_pr,\n        )\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.select_tasks","title":"<code>select_tasks(tasks)</code>","text":"<p>Select tasks from the ModelResult based on a list of AbsTask objects.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[AbsTask]</code> <p>A sequence of AbsTask objects to select from the ModelResult.</p> required Source code in <code>mteb/results/model_result.py</code> <pre><code>def select_tasks(self, tasks: Iterable[AbsTask]) -&gt; ModelResult:\n    \"\"\"Select tasks from the ModelResult based on a list of AbsTask objects.\n\n    Args:\n        tasks: A sequence of AbsTask objects to select from the ModelResult.\n    \"\"\"\n    task_name_to_task = {task.metadata.name: task for task in tasks}\n    new_task_results = [\n        task_res.validate_and_filter_scores(task_name_to_task[task_res.task_name])\n        for task_res in self.task_results\n        if task_res.task_name in task_name_to_task\n    ]\n    return type(self).model_construct(\n        model_name=self.model_name,\n        model_revision=self.model_revision,\n        task_results=new_task_results,\n        experiment_name=self.experiment_name,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.to_dataframe","title":"<code>to_dataframe(aggregation_level='task', aggregation_fn=None, include_model_revision=False, format='wide')</code>","text":"<p>Get a DataFrame with the scores for all models and tasks.</p> <p>The DataFrame will have the following columns in addition to the metadata columns:</p> <ul> <li>model_name: The name of the model.</li> <li>task_name: The name of the task.</li> <li>score: The main score of the model on the task.</li> </ul> <p>In addition, the DataFrame can have the following columns depending on the aggregation level:</p> <ul> <li>split: The split of the task. E.g. \"test\", \"train\", \"validation\".</li> <li>subset: The subset of the task. E.g. \"en\", \"fr-en\".</li> </ul> <p>Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_level</code> <code>Literal['subset', 'split', 'task']</code> <p>The aggregation to use. Can be one of: - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset. - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split. - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.</p> <code>'task'</code> <code>aggregation_fn</code> <code>Callable[[list[Score]], Any] | None</code> <p>The function to use for aggregation. If None, the mean will be used.</p> <code>None</code> <code>include_model_revision</code> <code>bool</code> <p>If True, the model revision will be included in the DataFrame. If False, it will be excluded.</p> <code>False</code> <code>format</code> <code>Literal['wide', 'long']</code> <p>The format of the DataFrame. Can be one of: - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells. - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.</p> <code>'wide'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the scores for all models and tasks.</p> Source code in <code>mteb/results/model_result.py</code> <pre><code>def to_dataframe(\n    self,\n    aggregation_level: Literal[\"subset\", \"split\", \"task\"] = \"task\",\n    aggregation_fn: Callable[[list[Score]], Any] | None = None,\n    include_model_revision: bool = False,\n    format: Literal[\"wide\", \"long\"] = \"wide\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n    The DataFrame will have the following columns in addition to the metadata columns:\n\n    - model_name: The name of the model.\n    - task_name: The name of the task.\n    - score: The main score of the model on the task.\n\n    In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n    - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n    - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n    Afterwards, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n    Args:\n        aggregation_level: The aggregation to use. Can be one of:\n            - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n            - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n            - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n        aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n        include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n        format: The format of the DataFrame. Can be one of:\n            - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n            - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n    Returns:\n        A DataFrame with the scores for all models and tasks.\n    \"\"\"\n    scores_data = self._get_score_for_table()\n\n    if not scores_data:\n        msg = \"No scores data available. Returning empty DataFrame.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n        return pd.DataFrame()\n\n    # Create DataFrame\n    df = pd.DataFrame(scores_data)\n\n    _columns = [\"model_name\"]\n    if include_model_revision is False:\n        df = df.drop(columns=[\"model_revision\"])\n    else:\n        _columns.append(\"model_revision\")\n\n    return _aggregate_and_pivot(\n        df,\n        columns=_columns,\n        aggregation_level=aggregation_level,\n        format=format,\n        aggregation_fn=aggregation_fn,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.ModelResult.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save ModelResult to disk as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to save.</p> required Source code in <code>mteb/results/model_result.py</code> <pre><code>def to_disk(self, path: Path) -&gt; None:\n    \"\"\"Save ModelResult to disk as JSON.\n\n    Args:\n        path: The path to the file to save.\n    \"\"\"\n    with path.open(\"w\") as f:\n        f.write(self.model_dump_json(indent=2))\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults","title":"<code>mteb.results.BenchmarkResults</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data class to hold the benchmark results of a model.</p> <p>Attributes:</p> Name Type Description <code>model_results</code> <code>list[ModelResult]</code> <p>List of ModelResult objects.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>class BenchmarkResults(BaseModel):\n    \"\"\"Data class to hold the benchmark results of a model.\n\n    Attributes:\n        model_results: List of ModelResult objects.\n    \"\"\"\n\n    model_results: list[ModelResult]\n    benchmark: Benchmark | None = None\n    model_config = ConfigDict(\n        protected_namespaces=(),  # to free up the name model_results which is otherwise protected\n        arbitrary_types_allowed=True,  # Benchmark is dataclasses.dataclass\n    )\n\n    def __repr__(self) -&gt; str:\n        n_models = len(self.model_results)\n        return f\"BenchmarkResults(model_results=[...](#{n_models}))\"\n\n    def __hash__(self) -&gt; int:\n        return id(self)\n\n    def _filter_tasks(\n        self,\n        task_names: list[str] | None = None,\n        languages: list[str] | None = None,\n        domains: list[TaskDomain] | None = None,\n        task_types: list[TaskType] | None = None,\n        modalities: list[Modalities] | None = None,\n        is_public: bool | None = None,\n    ) -&gt; BenchmarkResults:\n        # TODO: Same as filter_models\n        model_results = [\n            res._filter_tasks(\n                task_names=task_names,\n                languages=languages,\n                domains=domains,\n                task_types=task_types,\n                modalities=modalities,\n                is_public=is_public,\n            )\n            for res in self.model_results\n        ]\n        return type(self).model_construct(\n            model_results=[res for res in model_results if res.task_results]\n        )\n\n    def select_tasks(self, tasks: Iterable[AbsTask]) -&gt; BenchmarkResults:\n        \"\"\"Select tasks from the benchmark results.\n\n        Args:\n            tasks: List of tasks to select. Can be a list of AbsTask objects or task names.\n\n        Returns:\n            A new BenchmarkResults object with the selected tasks.\n        \"\"\"\n        new_model_results = [\n            model_res.select_tasks(tasks) for model_res in self.model_results\n        ]\n        return type(self).model_construct(model_results=new_model_results)\n\n    def select_models(\n        self,\n        names: list[str] | list[ModelMeta],\n        revisions: list[str | None] | None = None,\n    ) -&gt; BenchmarkResults:\n        \"\"\"Get models by name and revision.\n\n        Args:\n            names: List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.\n            revisions: List of model revisions to filter by. If None, all revisions are returned.\n\n        Returns:\n            A new BenchmarkResults object with the filtered models.\n        \"\"\"\n        models_res = []\n        _revisions = revisions if revisions is not None else [None] * len(names)\n\n        name_rev: dict[str, str | None] = {}\n\n        if len(names) != len(_revisions):\n            raise ValueError(\n                \"The length of names and revisions must be the same or revisions must be None.\"\n            )\n\n        for name, revision in zip(names, _revisions):\n            if isinstance(name, ModelMeta):\n                if name.name is None:\n                    raise ValueError(\"name in ModelMeta is None. It must be a string.\")\n                name_rev[name.name] = name.revision\n            else:\n                name_ = cast(\"str\", name)\n                name_rev[name_] = revision\n\n        for model_res in self.model_results:\n            model_name = model_res.model_name\n            revision = model_res.model_revision\n            if model_name in name_rev:\n                if name_rev[model_name] is None or revision == name_rev[model_name]:\n                    models_res.append(model_res)\n\n        return type(self).model_construct(model_results=models_res)\n\n    def _filter_models(\n        self,\n        model_names: Iterable[str] | None = None,\n        languages: Iterable[str] | None = None,\n        open_weights: bool | None = None,\n        frameworks: Iterable[str] | None = None,\n        n_parameters_range: tuple[int | None, int | None] = (None, None),\n        use_instructions: bool | None = None,\n        zero_shot_on: list[AbsTask] | None = None,\n    ) -&gt; BenchmarkResults:\n        # mostly a utility function for the leaderboard app.\n        # I would probably move the filtering of the models outside of this call. No need to call get_model_metas inside the filter.\n        # interface would then be the same as the get_models function\n\n        model_metas = get_model_metas(\n            model_names=model_names,\n            languages=languages,\n            open_weights=open_weights,\n            frameworks=frameworks,\n            n_parameters_range=n_parameters_range,\n            use_instructions=use_instructions,\n            zero_shot_on=zero_shot_on,\n        )\n        models = {meta.name for meta in model_metas}\n        # model_revision_pairs = {(meta.name, meta.revision) for meta in model_metas}\n        new_model_results = []\n        for model_res in self:\n            if model_res.model_name in models:\n                new_model_results.append(model_res)\n\n        return type(self).model_construct(model_results=new_model_results)\n\n    def join_revisions(self) -&gt; BenchmarkResults:\n        \"\"\"Join revisions of the same model.\n\n        In case of conflicts, the following rules are applied:\n        1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object.\n        2) If there is multiple revisions and some of them are None or na, they are filtered out.\n        3) If there is no main revision, we prefer the one run using the latest mteb version.\n\n        Returns:\n            A new BenchmarkResults object with the revisions joined.\n        \"\"\"\n        records = []\n        for model_result in self:\n            for task_result in model_result.task_results:\n                records.append(\n                    dict(\n                        model=model_result.model_name,\n                        revision=model_result.model_revision,\n                        task_name=task_result.task_name,\n                        mteb_version=task_result.mteb_version,\n                        task_result=task_result,\n                        has_scores=bool(task_result.scores),\n                    )\n                )\n        if not records:\n            return BenchmarkResults.model_construct(model_results=[])\n        task_df = pd.DataFrame.from_records(records)\n\n        # Use cached model metas\n        model_to_main_revision = _get_cached_model_metas()\n        task_df[\"main_revision\"] = task_df[\"model\"].map(model_to_main_revision)\n\n        # Use cached version parsing\n        task_df[\"mteb_version\"] = task_df[\"mteb_version\"].map(_parse_version_cached)\n\n        # Filter out rows without scores first\n        task_df = task_df[task_df[\"has_scores\"]]\n\n        # Optimize groupby with vectorized operations\n        # Sort by priority: main_revision match, then mteb_version (descending), then revision\n        task_df[\"is_main_revision\"] = task_df[\"revision\"] == task_df[\"main_revision\"]\n\n        # Handle None/NA/external revisions\n        task_df[\"revision_clean\"] = task_df[\"revision\"].copy()\n        task_df.loc[task_df[\"revision\"].isna(), \"revision_clean\"] = (\n            \"no_revision_available\"\n        )\n        task_df.loc[task_df[\"revision\"] == \"external\", \"revision_clean\"] = (\n            \"no_revision_available\"\n        )\n\n        # Create a priority column for sorting\n        # Higher priority = better to keep\n        # Priority: main_revision (1000), has valid mteb_version (100), has valid revision (10)\n        task_df[\"priority\"] = 0\n        task_df.loc[task_df[\"is_main_revision\"], \"priority\"] += 1000\n        task_df.loc[task_df[\"mteb_version\"].notna(), \"priority\"] += 100\n        task_df.loc[\n            task_df[\"revision_clean\"] != \"no_revision_available\", \"priority\"\n        ] += 10\n\n        # Sort by priority (desc), mteb_version (desc), and take first per group\n        task_df = task_df.sort_values(\n            [\"model\", \"task_name\", \"priority\", \"mteb_version\"],\n            ascending=[True, True, False, False],\n            na_position=\"last\",\n        )\n\n        task_df = task_df.groupby([\"model\", \"task_name\"], as_index=False).first()\n\n        # Reconstruct model results\n        model_results = []\n        # Group by original revision to maintain deterministic behavior\n        # After the first() selection above, each (model, task_name) is unique,\n        # so grouping by original revision ensures consistent ModelResult creation\n        for (model, model_revision), group in task_df.groupby([\"model\", \"revision\"]):\n            model_result = ModelResult.model_construct(\n                model_name=model,  # type: ignore[arg-type]\n                model_revision=model_revision,  # type: ignore[arg-type]\n                task_results=list(group[\"task_result\"]),\n            )\n            model_results.append(model_result)\n        return BenchmarkResults.model_construct(model_results=model_results)\n\n    def _get_scores(\n        self,\n        splits: list[SplitName] | None = None,\n        languages: list[ISOLanguage | ISOLanguageScript] | None = None,\n        scripts: list[ISOLanguageScript] | None = None,\n        getter: Callable[[ScoresDict], Score] | None = None,\n        aggregation: Callable[[list[Score]], Any] | None = None,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; list[dict]:\n        entries = []\n        if format == \"wide\":\n            for model_res in self:\n                try:\n                    model_scores = model_res._get_scores(\n                        splits=splits,\n                        languages=languages,\n                        scripts=scripts,\n                        getter=getter,\n                        aggregation=aggregation,\n                        format=\"wide\",\n                    )\n                    entries.append(\n                        {\n                            \"model\": model_res.model_name,\n                            \"revision\": model_res.model_revision,\n                            **model_scores,\n                        }\n                    )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {model_res.model_name}({model_res.model_revision}), due to: {e}\"\n                    )\n        if format == \"long\":\n            for model_res in self:\n                try:\n                    entries.extend(\n                        model_res._get_scores(\n                            splits=splits,\n                            languages=languages,\n                            scripts=scripts,\n                            getter=getter,\n                            aggregation=aggregation,\n                            format=\"long\",\n                        )\n                    )\n                except Exception as e:\n                    warnings.warn(\n                        f\"Couldn't get scores for {model_res.model_name}({model_res.model_revision}), due to: {e}\"\n                    )\n        return entries\n\n    def to_dataframe(\n        self,\n        aggregation_level: Literal[\"subset\", \"split\", \"task\", \"language\"] = \"task\",\n        aggregation_fn: Callable[[list[Score]], Any] | None = None,\n        include_model_revision: bool = False,\n        format: Literal[\"wide\", \"long\"] = \"wide\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n        The DataFrame will have the following columns in addition to the metadata columns:\n\n        - model_name: The name of the model.\n        - task_name: The name of the task.\n        - score: The main score of the model on the task.\n\n        In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n        - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n        - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n        Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n        Args:\n            aggregation_level: The aggregation to use. Can be one of:\n                - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n                - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n                - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n                - \"language\": Aggregates the scores by language. The DataFrame will have one row per model and language.\n            aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n            include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n                If there are multiple revisions for the same model, they will be joined using the `join_revisions` method.\n            format: The format of the DataFrame. Can be one of:\n                - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n                - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n        Returns:\n            A DataFrame with the scores for all models and tasks.\n        \"\"\"\n        bench_results = self\n        if include_model_revision is False:\n            bench_results = bench_results.join_revisions()\n\n        scores_data = []\n        for model_result in bench_results:\n            scores_data.extend(model_result._get_score_for_table())\n\n        if not scores_data:\n            msg = \"No scores data available. Returning empty DataFrame.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n            return pd.DataFrame()\n\n        # Create DataFrame\n        df = pd.DataFrame(scores_data)\n\n        _columns = [\"model_name\"]\n        if include_model_revision is False:\n            df = df.drop(columns=[\"model_revision\"])\n        else:\n            _columns.append(\"model_revision\")\n\n        # Aggregation\n        return _aggregate_and_pivot(\n            df,\n            columns=_columns,\n            aggregation_level=aggregation_level,\n            aggregation_fn=aggregation_fn,\n            format=format,\n        )\n\n    def get_benchmark_result(self) -&gt; pd.DataFrame:\n        \"\"\"Get aggregated scores for each model in the benchmark.\n\n        Uses the benchmark's summary table creation method to compute scores.\n\n        Returns:\n            A DataFrame with the aggregated benchmark scores for each model.\n        \"\"\"\n        if self.benchmark is None:\n            raise ValueError(\n                \"No benchmark associated with these results (self.benchmark is None). \"\n                \"To get benchmark results, load results with a Benchmark object. \"\n                \"`results = cache.load_results(tasks='MTEB(eng, v2)')`\"\n            )\n\n        return self.benchmark._create_summary_table(self)\n\n    def __iter__(self) -&gt; Iterator[ModelResult]:  # type: ignore[override]\n        return iter(self.model_results)\n\n    def __getitem__(self, index: int) -&gt; ModelResult:\n        return self.model_results[index]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert BenchmarkResults to a dictionary.\"\"\"\n        return self.model_dump()\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; Self:\n        \"\"\"Create BenchmarkResults from a dictionary.\"\"\"\n        return cls.model_validate(data)\n\n    def to_disk(self, path: Path | str) -&gt; None:\n        \"\"\"Save the BenchmarkResults to a JSON file.\"\"\"\n        path = Path(path)\n        with path.open(\"w\") as out_file:\n            out_file.write(self.model_dump_json(indent=2))\n\n    @classmethod\n    def from_validated(cls, **data: Any) -&gt; BenchmarkResults:\n        \"\"\"Create BenchmarkResults from validated data.\n\n        Args:\n            **data: Arbitrary keyword arguments containing the data.\n\n        Returns:\n            An instance of BenchmarkResults.\n        \"\"\"\n        model_results = []\n        for model_res in data[\"model_results\"]:\n            model_results.append(ModelResult.from_validated(**model_res))\n        return cls.model_construct(model_results=model_results)\n\n    @classmethod\n    def from_disk(cls, path: Path | str) -&gt; Self:\n        \"\"\"Load the BenchmarkResults from a JSON file.\n\n        Args:\n            path: Path to the JSON file.\n\n        Returns:\n            An instance of BenchmarkResults.\n        \"\"\"\n        path = Path(path)\n        with path.open() as in_file:\n            data = json.loads(in_file.read())\n        return cls.from_dict(data)\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Get all languages in the benchmark results.\n\n        Returns:\n            A list of languages in ISO 639-1 format.\n        \"\"\"\n        langs = []\n        for model_res in self.model_results:\n            langs.extend(model_res.languages)\n        return list(set(langs))\n\n    @property\n    def domains(self) -&gt; list[str]:\n        \"\"\"Get all domains in the benchmark results.\n\n        Returns:\n            A list of domains in ISO 639-1 format.\n        \"\"\"\n        ds = []\n        for model_res in self.model_results:\n            ds.extend(model_res.domains)\n        return list(set(ds))\n\n    @property\n    def task_types(self) -&gt; list[str]:\n        \"\"\"Get all task types in the benchmark results.\n\n        Returns:\n            A list of task types.\n        \"\"\"\n        ts = []\n        for model_res in self.model_results:\n            ts.extend(model_res.task_types)\n        return list(set(ts))\n\n    @property\n    def task_names(self) -&gt; list[str]:\n        \"\"\"Get all task names in the benchmark results.\n\n        Returns:\n            A list of task names.\n        \"\"\"\n        names = []\n        for model_res in self.model_results:\n            names.extend(model_res.task_names)\n        return list(set(names))\n\n    @property\n    def modalities(self) -&gt; list[str]:\n        \"\"\"Get all modalities in the benchmark results.\n\n        Returns:\n            A list of modalities.\n        \"\"\"\n        mod = []\n        for model_res in self.model_results:\n            mod.extend(model_res.modalities)\n        return list(set(mod))\n\n    @property\n    def model_names(self) -&gt; list[str]:\n        \"\"\"Get all model names in the benchmark results.\n\n        Returns:\n            A list of model names.\n        \"\"\"\n        return [model_res.model_name for model_res in self.model_results]\n\n    @property\n    def model_revisions(self) -&gt; list[dict[str, str | None]]:\n        \"\"\"Get all model revisions in the benchmark results.\n\n        Returns:\n            A list of dictionaries with model names and revisions.\n        \"\"\"\n        return [\n            {\"model_name\": model_res.model_name, \"revision\": model_res.model_revision}\n            for model_res in self.model_results\n        ]\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.domains","title":"<code>domains</code>  <code>property</code>","text":"<p>Get all domains in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of domains in ISO 639-1 format.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Get all languages in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of languages in ISO 639-1 format.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Get all modalities in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of modalities.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.model_names","title":"<code>model_names</code>  <code>property</code>","text":"<p>Get all model names in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of model names.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.model_revisions","title":"<code>model_revisions</code>  <code>property</code>","text":"<p>Get all model revisions in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[dict[str, str | None]]</code> <p>A list of dictionaries with model names and revisions.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.task_names","title":"<code>task_names</code>  <code>property</code>","text":"<p>Get all task names in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task names.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.task_types","title":"<code>task_types</code>  <code>property</code>","text":"<p>Get all task types in the benchmark results.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of task types.</p>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create BenchmarkResults from a dictionary.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Self:\n    \"\"\"Create BenchmarkResults from a dictionary.\"\"\"\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Load the BenchmarkResults from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the JSON file.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>An instance of BenchmarkResults.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: Path | str) -&gt; Self:\n    \"\"\"Load the BenchmarkResults from a JSON file.\n\n    Args:\n        path: Path to the JSON file.\n\n    Returns:\n        An instance of BenchmarkResults.\n    \"\"\"\n    path = Path(path)\n    with path.open() as in_file:\n        data = json.loads(in_file.read())\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.from_validated","title":"<code>from_validated(**data)</code>  <code>classmethod</code>","text":"<p>Create BenchmarkResults from validated data.</p> <p>Parameters:</p> Name Type Description Default <code>**data</code> <code>Any</code> <p>Arbitrary keyword arguments containing the data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>An instance of BenchmarkResults.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>@classmethod\ndef from_validated(cls, **data: Any) -&gt; BenchmarkResults:\n    \"\"\"Create BenchmarkResults from validated data.\n\n    Args:\n        **data: Arbitrary keyword arguments containing the data.\n\n    Returns:\n        An instance of BenchmarkResults.\n    \"\"\"\n    model_results = []\n    for model_res in data[\"model_results\"]:\n        model_results.append(ModelResult.from_validated(**model_res))\n    return cls.model_construct(model_results=model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.get_benchmark_result","title":"<code>get_benchmark_result()</code>","text":"<p>Get aggregated scores for each model in the benchmark.</p> <p>Uses the benchmark's summary table creation method to compute scores.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the aggregated benchmark scores for each model.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def get_benchmark_result(self) -&gt; pd.DataFrame:\n    \"\"\"Get aggregated scores for each model in the benchmark.\n\n    Uses the benchmark's summary table creation method to compute scores.\n\n    Returns:\n        A DataFrame with the aggregated benchmark scores for each model.\n    \"\"\"\n    if self.benchmark is None:\n        raise ValueError(\n            \"No benchmark associated with these results (self.benchmark is None). \"\n            \"To get benchmark results, load results with a Benchmark object. \"\n            \"`results = cache.load_results(tasks='MTEB(eng, v2)')`\"\n        )\n\n    return self.benchmark._create_summary_table(self)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.join_revisions","title":"<code>join_revisions()</code>","text":"<p>Join revisions of the same model.</p> <p>In case of conflicts, the following rules are applied: 1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object. 2) If there is multiple revisions and some of them are None or na, they are filtered out. 3) If there is no main revision, we prefer the one run using the latest mteb version.</p> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A new BenchmarkResults object with the revisions joined.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def join_revisions(self) -&gt; BenchmarkResults:\n    \"\"\"Join revisions of the same model.\n\n    In case of conflicts, the following rules are applied:\n    1) If the main revision is present, it is kept. The main revision is the defined in the models ModelMeta object.\n    2) If there is multiple revisions and some of them are None or na, they are filtered out.\n    3) If there is no main revision, we prefer the one run using the latest mteb version.\n\n    Returns:\n        A new BenchmarkResults object with the revisions joined.\n    \"\"\"\n    records = []\n    for model_result in self:\n        for task_result in model_result.task_results:\n            records.append(\n                dict(\n                    model=model_result.model_name,\n                    revision=model_result.model_revision,\n                    task_name=task_result.task_name,\n                    mteb_version=task_result.mteb_version,\n                    task_result=task_result,\n                    has_scores=bool(task_result.scores),\n                )\n            )\n    if not records:\n        return BenchmarkResults.model_construct(model_results=[])\n    task_df = pd.DataFrame.from_records(records)\n\n    # Use cached model metas\n    model_to_main_revision = _get_cached_model_metas()\n    task_df[\"main_revision\"] = task_df[\"model\"].map(model_to_main_revision)\n\n    # Use cached version parsing\n    task_df[\"mteb_version\"] = task_df[\"mteb_version\"].map(_parse_version_cached)\n\n    # Filter out rows without scores first\n    task_df = task_df[task_df[\"has_scores\"]]\n\n    # Optimize groupby with vectorized operations\n    # Sort by priority: main_revision match, then mteb_version (descending), then revision\n    task_df[\"is_main_revision\"] = task_df[\"revision\"] == task_df[\"main_revision\"]\n\n    # Handle None/NA/external revisions\n    task_df[\"revision_clean\"] = task_df[\"revision\"].copy()\n    task_df.loc[task_df[\"revision\"].isna(), \"revision_clean\"] = (\n        \"no_revision_available\"\n    )\n    task_df.loc[task_df[\"revision\"] == \"external\", \"revision_clean\"] = (\n        \"no_revision_available\"\n    )\n\n    # Create a priority column for sorting\n    # Higher priority = better to keep\n    # Priority: main_revision (1000), has valid mteb_version (100), has valid revision (10)\n    task_df[\"priority\"] = 0\n    task_df.loc[task_df[\"is_main_revision\"], \"priority\"] += 1000\n    task_df.loc[task_df[\"mteb_version\"].notna(), \"priority\"] += 100\n    task_df.loc[\n        task_df[\"revision_clean\"] != \"no_revision_available\", \"priority\"\n    ] += 10\n\n    # Sort by priority (desc), mteb_version (desc), and take first per group\n    task_df = task_df.sort_values(\n        [\"model\", \"task_name\", \"priority\", \"mteb_version\"],\n        ascending=[True, True, False, False],\n        na_position=\"last\",\n    )\n\n    task_df = task_df.groupby([\"model\", \"task_name\"], as_index=False).first()\n\n    # Reconstruct model results\n    model_results = []\n    # Group by original revision to maintain deterministic behavior\n    # After the first() selection above, each (model, task_name) is unique,\n    # so grouping by original revision ensures consistent ModelResult creation\n    for (model, model_revision), group in task_df.groupby([\"model\", \"revision\"]):\n        model_result = ModelResult.model_construct(\n            model_name=model,  # type: ignore[arg-type]\n            model_revision=model_revision,  # type: ignore[arg-type]\n            task_results=list(group[\"task_result\"]),\n        )\n        model_results.append(model_result)\n    return BenchmarkResults.model_construct(model_results=model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.select_models","title":"<code>select_models(names, revisions=None)</code>","text":"<p>Get models by name and revision.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>list[str] | list[ModelMeta]</code> <p>List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.</p> required <code>revisions</code> <code>list[str | None] | None</code> <p>List of model revisions to filter by. If None, all revisions are returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A new BenchmarkResults object with the filtered models.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def select_models(\n    self,\n    names: list[str] | list[ModelMeta],\n    revisions: list[str | None] | None = None,\n) -&gt; BenchmarkResults:\n    \"\"\"Get models by name and revision.\n\n    Args:\n        names: List of model names to filter by. Can also be a list of ModelMeta objects. In which case, the revision is ignored.\n        revisions: List of model revisions to filter by. If None, all revisions are returned.\n\n    Returns:\n        A new BenchmarkResults object with the filtered models.\n    \"\"\"\n    models_res = []\n    _revisions = revisions if revisions is not None else [None] * len(names)\n\n    name_rev: dict[str, str | None] = {}\n\n    if len(names) != len(_revisions):\n        raise ValueError(\n            \"The length of names and revisions must be the same or revisions must be None.\"\n        )\n\n    for name, revision in zip(names, _revisions):\n        if isinstance(name, ModelMeta):\n            if name.name is None:\n                raise ValueError(\"name in ModelMeta is None. It must be a string.\")\n            name_rev[name.name] = name.revision\n        else:\n            name_ = cast(\"str\", name)\n            name_rev[name_] = revision\n\n    for model_res in self.model_results:\n        model_name = model_res.model_name\n        revision = model_res.model_revision\n        if model_name in name_rev:\n            if name_rev[model_name] is None or revision == name_rev[model_name]:\n                models_res.append(model_res)\n\n    return type(self).model_construct(model_results=models_res)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.select_tasks","title":"<code>select_tasks(tasks)</code>","text":"<p>Select tasks from the benchmark results.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[AbsTask]</code> <p>List of tasks to select. Can be a list of AbsTask objects or task names.</p> required <p>Returns:</p> Type Description <code>BenchmarkResults</code> <p>A new BenchmarkResults object with the selected tasks.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def select_tasks(self, tasks: Iterable[AbsTask]) -&gt; BenchmarkResults:\n    \"\"\"Select tasks from the benchmark results.\n\n    Args:\n        tasks: List of tasks to select. Can be a list of AbsTask objects or task names.\n\n    Returns:\n        A new BenchmarkResults object with the selected tasks.\n    \"\"\"\n    new_model_results = [\n        model_res.select_tasks(tasks) for model_res in self.model_results\n    ]\n    return type(self).model_construct(model_results=new_model_results)\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_dataframe","title":"<code>to_dataframe(aggregation_level='task', aggregation_fn=None, include_model_revision=False, format='wide')</code>","text":"<p>Get a DataFrame with the scores for all models and tasks.</p> <p>The DataFrame will have the following columns in addition to the metadata columns:</p> <ul> <li>model_name: The name of the model.</li> <li>task_name: The name of the task.</li> <li>score: The main score of the model on the task.</li> </ul> <p>In addition, the DataFrame can have the following columns depending on the aggregation level:</p> <ul> <li>split: The split of the task. E.g. \"test\", \"train\", \"validation\".</li> <li>subset: The subset of the task. E.g. \"en\", \"fr-en\".</li> </ul> <p>Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.</p> <p>Parameters:</p> Name Type Description Default <code>aggregation_level</code> <code>Literal['subset', 'split', 'task', 'language']</code> <p>The aggregation to use. Can be one of: - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset. - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split. - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task. - \"language\": Aggregates the scores by language. The DataFrame will have one row per model and language.</p> <code>'task'</code> <code>aggregation_fn</code> <code>Callable[[list[Score]], Any] | None</code> <p>The function to use for aggregation. If None, the mean will be used.</p> <code>None</code> <code>include_model_revision</code> <code>bool</code> <p>If True, the model revision will be included in the DataFrame. If False, it will be excluded. If there are multiple revisions for the same model, they will be joined using the <code>join_revisions</code> method.</p> <code>False</code> <code>format</code> <code>Literal['wide', 'long']</code> <p>The format of the DataFrame. Can be one of: - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells. - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.</p> <code>'wide'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame with the scores for all models and tasks.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_dataframe(\n    self,\n    aggregation_level: Literal[\"subset\", \"split\", \"task\", \"language\"] = \"task\",\n    aggregation_fn: Callable[[list[Score]], Any] | None = None,\n    include_model_revision: bool = False,\n    format: Literal[\"wide\", \"long\"] = \"wide\",\n) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame with the scores for all models and tasks.\n\n    The DataFrame will have the following columns in addition to the metadata columns:\n\n    - model_name: The name of the model.\n    - task_name: The name of the task.\n    - score: The main score of the model on the task.\n\n    In addition, the DataFrame can have the following columns depending on the aggregation level:\n\n    - split: The split of the task. E.g. \"test\", \"train\", \"validation\".\n    - subset: The subset of the task. E.g. \"en\", \"fr-en\".\n\n    Afterward, the DataFrame will be aggregated according to the aggregation method and pivoted to either a wide format.\n\n    Args:\n        aggregation_level: The aggregation to use. Can be one of:\n            - \"subset\"/None: No aggregation will be done. The DataFrame will have one row per model, task, split and subset.\n            - \"split\": Aggregates the scores by split. The DataFrame will have one row per model, task and split.\n            - \"task\": Aggregates the scores by task. The DataFrame will have one row per model and task.\n            - \"language\": Aggregates the scores by language. The DataFrame will have one row per model and language.\n        aggregation_fn: The function to use for aggregation. If None, the mean will be used.\n        include_model_revision: If True, the model revision will be included in the DataFrame. If False, it will be excluded.\n            If there are multiple revisions for the same model, they will be joined using the `join_revisions` method.\n        format: The format of the DataFrame. Can be one of:\n            - \"wide\": The DataFrame will be of shape (number of tasks, number of models). Scores will be in the cells.\n            - \"long\": The DataFrame will of length (number of tasks * number of model). Scores will be in columns.\n\n    Returns:\n        A DataFrame with the scores for all models and tasks.\n    \"\"\"\n    bench_results = self\n    if include_model_revision is False:\n        bench_results = bench_results.join_revisions()\n\n    scores_data = []\n    for model_result in bench_results:\n        scores_data.extend(model_result._get_score_for_table())\n\n    if not scores_data:\n        msg = \"No scores data available. Returning empty DataFrame.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n        return pd.DataFrame()\n\n    # Create DataFrame\n    df = pd.DataFrame(scores_data)\n\n    _columns = [\"model_name\"]\n    if include_model_revision is False:\n        df = df.drop(columns=[\"model_revision\"])\n    else:\n        _columns.append(\"model_revision\")\n\n    # Aggregation\n    return _aggregate_and_pivot(\n        df,\n        columns=_columns,\n        aggregation_level=aggregation_level,\n        aggregation_fn=aggregation_fn,\n        format=format,\n    )\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert BenchmarkResults to a dictionary.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert BenchmarkResults to a dictionary.\"\"\"\n    return self.model_dump()\n</code></pre>"},{"location":"api/results/#mteb.results.BenchmarkResults.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Save the BenchmarkResults to a JSON file.</p> Source code in <code>mteb/results/benchmark_results.py</code> <pre><code>def to_disk(self, path: Path | str) -&gt; None:\n    \"\"\"Save the BenchmarkResults to a JSON file.\"\"\"\n    path = Path(path)\n    with path.open(\"w\") as out_file:\n        out_file.write(self.model_dump_json(indent=2))\n</code></pre>"},{"location":"api/task/","title":"Tasks","text":"<p>A task is an implementation of a dataset for evaluation. It could, for instance, be the MIRACL dataset consisting of queries, a corpus of documents ,and the correct documents to retrieve for a given query. In addition to the dataset, a task includes the specifications for how a model should be run on the dataset and how its output should be evaluated. Each task also comes with extensive metadata including the license, who annotated the data, etc.</p> An overview of the tasks within <code>mteb</code>"},{"location":"api/task/#utilities","title":"Utilities","text":""},{"location":"api/task/#mteb.get_tasks","title":"<code>mteb.get_tasks</code>","text":"<p>This script contains functions that are used to get an overview of the MTEB benchmark.</p>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks","title":"<code>MTEBTasks</code>","text":"<p>               Bases: <code>tuple[AbsTask]</code></p> <p>A tuple of tasks with additional methods to get an overview of the tasks.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>class MTEBTasks(tuple[AbsTask]):\n    \"\"\"A tuple of tasks with additional methods to get an overview of the tasks.\"\"\"\n\n    def __repr__(self) -&gt; str:\n        return \"MTEBTasks\" + super().__repr__()\n\n    @staticmethod\n    def _extract_property_from_task(task: AbsTask, property: str):\n        if hasattr(task.metadata, property):\n            return getattr(task.metadata, property)\n        elif hasattr(task, property):\n            return getattr(task, property)\n        else:\n            raise KeyError(\"Property neither in Task attribute or in task metadata.\")\n\n    @property\n    def languages(self) -&gt; set:\n        \"\"\"Return all languages from tasks\"\"\"\n        langs = set()\n        for task in self:\n            for lg in task.languages:\n                langs.add(lg)\n        return langs\n\n    def count_languages(self) -&gt; Counter:\n        \"\"\"Summarize count of all languages from tasks\n\n        Returns:\n            Counter with language as key and count as value.\n        \"\"\"\n        langs = []\n        for task in self:\n            langs.extend(task.languages)\n        return Counter(langs)\n\n    def to_markdown(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n        limit_n_entries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Generate markdown table with tasks summary\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n            limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n                there are more entries.\n\n        Returns:\n            string with a markdown table.\n        \"\"\"\n\n        def _limit_entries_in_cell_inner(cell: Any):\n            if isinstance(cell, list | set):\n                return self._limit_entries_in_cell(cell, limit_n_entries)\n            return cell\n\n        markdown_table = \"| Task\" + \"\".join([f\"| {p}  \" for p in properties]) + \"|\\n\"\n        _head_sep = \"| ---\" * (len(properties) + 1) + \" |\\n\"\n        markdown_table += _head_sep\n        for task in self:\n            markdown_table += f\"| {task.metadata.name} \"\n            markdown_table += \"\".join(\n                [\n                    f\"| {_limit_entries_in_cell_inner(self._extract_property_from_task(task, p))} \"\n                    for p in properties\n                ]\n            )\n            markdown_table += \" |\\n\"\n        return markdown_table\n\n    def to_dataframe(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate pandas DataFrame with tasks summary\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n\n        Returns:\n            pandas DataFrame.\n        \"\"\"\n        data = []\n        for task in self:\n            data.append(\n                {p: self._extract_property_from_task(task, p) for p in properties}\n            )\n        return pd.DataFrame(data)\n\n    @staticmethod\n    def _limit_entries_in_cell(\n        cell: list | set, limit_n_entries: int | None = 3\n    ) -&gt; str:\n        if limit_n_entries and len(cell) &gt; limit_n_entries:\n            ending = \"]\" if isinstance(cell, list) else \"}\"\n            cell = sorted(cell)\n            return str(cell[:limit_n_entries])[:-1] + \", ...\" + ending\n        else:\n            return str(cell)\n\n    def to_latex(\n        self,\n        properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n        group_indices: Sequence[str] | None = (\"type\", \"name\"),\n        include_citation_in_name: bool = True,\n        limit_n_entries: int | None = 3,\n    ) -&gt; str:\n        \"\"\"Generate a LaTeX table of the tasks.\n\n        Args:\n            properties: list of metadata to summarize from a Task class.\n            group_indices: list of properties to group the table by.\n            include_citation_in_name: Whether to include the citation in the name.\n            limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n                there are more entries.\n\n        Returns:\n            string with a LaTeX table.\n        \"\"\"\n        if include_citation_in_name and \"name\" in properties:\n            df = self.to_dataframe(tuple(properties) + (\"intext_citation\",))\n            df[\"name\"] = df[\"name\"] + \" \" + df[\"intext_citation\"]  # type: ignore[operator]\n            df = df.drop(columns=[\"intext_citation\"])\n        else:\n            df = self.to_dataframe(properties)\n\n        if limit_n_entries and df.shape[0]:  # ensure that there are entries\n            for col in df.columns:\n                # check if content is a list or set\n                if isinstance(df[col].iloc[0], list | set):\n                    _col = []\n                    for val in df[col]:\n                        str_col = self._limit_entries_in_cell(val, limit_n_entries)\n\n                        # escape } and { characters\n                        str_col = str_col.replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n                        _col.append(str_col)\n                    df[col] = _col\n\n        if group_indices:\n            df = df.set_index(group_indices)\n\n        return df.to_latex()\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Return all languages from tasks</p>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.count_languages","title":"<code>count_languages()</code>","text":"<p>Summarize count of all languages from tasks</p> <p>Returns:</p> Type Description <code>Counter</code> <p>Counter with language as key and count as value.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def count_languages(self) -&gt; Counter:\n    \"\"\"Summarize count of all languages from tasks\n\n    Returns:\n        Counter with language as key and count as value.\n    \"\"\"\n    langs = []\n    for task in self:\n        langs.extend(task.languages)\n    return Counter(langs)\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_dataframe","title":"<code>to_dataframe(properties=_DEFAULT_PROPRIETIES)</code>","text":"<p>Generate pandas DataFrame with tasks summary</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas DataFrame.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_dataframe(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate pandas DataFrame with tasks summary\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n\n    Returns:\n        pandas DataFrame.\n    \"\"\"\n    data = []\n    for task in self:\n        data.append(\n            {p: self._extract_property_from_task(task, p) for p in properties}\n        )\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_latex","title":"<code>to_latex(properties=_DEFAULT_PROPRIETIES, group_indices=('type', 'name'), include_citation_in_name=True, limit_n_entries=3)</code>","text":"<p>Generate a LaTeX table of the tasks.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <code>group_indices</code> <code>Sequence[str] | None</code> <p>list of properties to group the table by.</p> <code>('type', 'name')</code> <code>include_citation_in_name</code> <code>bool</code> <p>Whether to include the citation in the name.</p> <code>True</code> <code>limit_n_entries</code> <code>int | None</code> <p>Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that there are more entries.</p> <code>3</code> <p>Returns:</p> Type Description <code>str</code> <p>string with a LaTeX table.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_latex(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    group_indices: Sequence[str] | None = (\"type\", \"name\"),\n    include_citation_in_name: bool = True,\n    limit_n_entries: int | None = 3,\n) -&gt; str:\n    \"\"\"Generate a LaTeX table of the tasks.\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n        group_indices: list of properties to group the table by.\n        include_citation_in_name: Whether to include the citation in the name.\n        limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n            there are more entries.\n\n    Returns:\n        string with a LaTeX table.\n    \"\"\"\n    if include_citation_in_name and \"name\" in properties:\n        df = self.to_dataframe(tuple(properties) + (\"intext_citation\",))\n        df[\"name\"] = df[\"name\"] + \" \" + df[\"intext_citation\"]  # type: ignore[operator]\n        df = df.drop(columns=[\"intext_citation\"])\n    else:\n        df = self.to_dataframe(properties)\n\n    if limit_n_entries and df.shape[0]:  # ensure that there are entries\n        for col in df.columns:\n            # check if content is a list or set\n            if isinstance(df[col].iloc[0], list | set):\n                _col = []\n                for val in df[col]:\n                    str_col = self._limit_entries_in_cell(val, limit_n_entries)\n\n                    # escape } and { characters\n                    str_col = str_col.replace(\"{\", \"\\\\{\").replace(\"}\", \"\\\\}\")\n                    _col.append(str_col)\n                df[col] = _col\n\n    if group_indices:\n        df = df.set_index(group_indices)\n\n    return df.to_latex()\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.MTEBTasks.to_markdown","title":"<code>to_markdown(properties=_DEFAULT_PROPRIETIES, limit_n_entries=3)</code>","text":"<p>Generate markdown table with tasks summary</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Sequence[str]</code> <p>list of metadata to summarize from a Task class.</p> <code>_DEFAULT_PROPRIETIES</code> <code>limit_n_entries</code> <code>int | None</code> <p>Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that there are more entries.</p> <code>3</code> <p>Returns:</p> Type Description <code>str</code> <p>string with a markdown table.</p> Source code in <code>mteb/get_tasks.py</code> <pre><code>def to_markdown(\n    self,\n    properties: Sequence[str] = _DEFAULT_PROPRIETIES,\n    limit_n_entries: int | None = 3,\n) -&gt; str:\n    \"\"\"Generate markdown table with tasks summary\n\n    Args:\n        properties: list of metadata to summarize from a Task class.\n        limit_n_entries: Limit the number of entries for cell values, e.g. number of languages and domains. Will use \"...\" to indicate that\n            there are more entries.\n\n    Returns:\n        string with a markdown table.\n    \"\"\"\n\n    def _limit_entries_in_cell_inner(cell: Any):\n        if isinstance(cell, list | set):\n            return self._limit_entries_in_cell(cell, limit_n_entries)\n        return cell\n\n    markdown_table = \"| Task\" + \"\".join([f\"| {p}  \" for p in properties]) + \"|\\n\"\n    _head_sep = \"| ---\" * (len(properties) + 1) + \" |\\n\"\n    markdown_table += _head_sep\n    for task in self:\n        markdown_table += f\"| {task.metadata.name} \"\n        markdown_table += \"\".join(\n            [\n                f\"| {_limit_entries_in_cell_inner(self._extract_property_from_task(task, p))} \"\n                for p in properties\n            ]\n        )\n        markdown_table += \" |\\n\"\n    return markdown_table\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.get_task","title":"<code>get_task(task_name, languages=None, script=None, eval_splits=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Get a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task to fetch.</p> required <code>languages</code> <code>Sequence[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>Sequence[str] | None</code> <p>A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts</p> <code>None</code> <code>eval_splits</code> <code>Sequence[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>hf_subsets</code> <code>Sequence[str] | None</code> <p>A list of Huggingface subsets to evaluate on.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>AbsTask</code> <p>An initialized task object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_task(\n    task_name: str,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    eval_splits: Sequence[str] | None = None,\n    hf_subsets: Sequence[str] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; AbsTask:\n    \"\"\"Get a task by name.\n\n    Args:\n        task_name: The name of the task to fetch.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        hf_subsets: A list of Huggingface subsets to evaluate on.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        An initialized task object.\n\n    Examples:\n        &gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n    \"\"\"\n    if task_name in _TASK_RENAMES:\n        _task_name = _TASK_RENAMES[task_name]\n        msg = f\"The task with the given name '{task_name}' has been renamed to '{_task_name}'. To prevent this warning use the new name.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n\n    if task_name not in _TASKS_REGISTRY:\n        close_matches = difflib.get_close_matches(task_name, _TASKS_REGISTRY.keys())\n        if close_matches:\n            suggestion = f\"KeyError: '{task_name}' not found. Did you mean: '{close_matches[0]}'?\"\n        else:\n            suggestion = (\n                f\"KeyError: '{task_name}' not found and no similar keys were found.\"\n            )\n        raise KeyError(suggestion)\n    task = _TASKS_REGISTRY[task_name]()\n    if eval_splits:\n        task.filter_eval_splits(eval_splits=eval_splits)\n    return task.filter_languages(\n        languages,\n        script,\n        hf_subsets=hf_subsets,\n        exclusive_language_filter=exclusive_language_filter,\n    )\n</code></pre>"},{"location":"api/task/#mteb.get_tasks.get_tasks","title":"<code>get_tasks(tasks=None, *, languages=None, script=None, domains=None, task_types=None, categories=None, exclude_superseded=True, eval_splits=None, exclusive_language_filter=False, modalities=None, exclusive_modality_filter=False, exclude_aggregate=False, exclude_private=True)</code>","text":"<p>Get a list of tasks based on the specified filters.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Sequence[str] | None</code> <p>A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.</p> <code>None</code> <code>languages</code> <code>Sequence[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>Sequence[str] | None</code> <p>A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts that are not in the specified list.</p> <code>None</code> <code>domains</code> <code>Sequence[TaskDomain] | None</code> <p>A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".</p> <code>None</code> <code>task_types</code> <code>Sequence[TaskType] | None</code> <p>A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.</p> <code>None</code> <code>categories</code> <code>Sequence[TaskCategory] | None</code> <p>A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.</p> <code>None</code> <code>exclude_superseded</code> <code>bool</code> <p>A boolean flag to exclude datasets which are superseded by another.</p> <code>True</code> <code>eval_splits</code> <code>Sequence[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <code>modalities</code> <code>Sequence[Modalities] | None</code> <p>A list of modalities to include. If None, all modalities are included.</p> <code>None</code> <code>exclusive_modality_filter</code> <code>bool</code> <p>If True, only keep tasks where all filter modalities are included in the task's modalities and ALL task modalities are in filter modalities (exact match). If False, keep tasks if any of the task's modalities match the filter modalities.</p> <code>False</code> <code>exclude_aggregate</code> <code>bool</code> <p>If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.</p> <code>False</code> <code>exclude_private</code> <code>bool</code> <p>If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.</p> <code>True</code> <p>Returns:</p> Type Description <code>MTEBTasks</code> <p>A list of all initialized tasks objects which pass all of the filters (AND operation).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Classification\"])\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Clustering\"], exclude_superseded=False)\n&gt;&gt;&gt; get_tasks(languages=[\"eng\"], tasks=[\"WikipediaRetrievalMultilingual\"], eval_splits=[\"test\"])\n&gt;&gt;&gt; get_tasks(tasks=[\"STS22\"], languages=[\"eng\"], exclusive_language_filter=True) # don't include multilingual subsets containing English\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_tasks(\n    tasks: Sequence[str] | None = None,\n    *,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    domains: Sequence[TaskDomain] | None = None,\n    task_types: Sequence[TaskType] | None = None,\n    categories: Sequence[TaskCategory] | None = None,\n    exclude_superseded: bool = True,\n    eval_splits: Sequence[str] | None = None,\n    exclusive_language_filter: bool = False,\n    modalities: Sequence[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = True,\n) -&gt; MTEBTasks:\n    \"\"\"Get a list of tasks based on the specified filters.\n\n    Args:\n        tasks: A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts\n            that are not in the specified list.\n        domains: A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".\n        task_types: A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.\n        categories: A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.\n        exclude_superseded: A boolean flag to exclude datasets which are superseded by another.\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n        modalities: A list of modalities to include. If None, all modalities are included.\n        exclusive_modality_filter: If True, only keep tasks where _all_ filter modalities are included in the\n            task's modalities and ALL task modalities are in filter modalities (exact match).\n            If False, keep tasks if _any_ of the task's modalities match the filter modalities.\n        exclude_aggregate: If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.\n        exclude_private: If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.\n\n    Returns:\n        A list of all initialized tasks objects which pass all of the filters (AND operation).\n\n    Examples:\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Classification\"])\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], script=[\"Latn\"], task_types=[\"Clustering\"], exclude_superseded=False)\n        &gt;&gt;&gt; get_tasks(languages=[\"eng\"], tasks=[\"WikipediaRetrievalMultilingual\"], eval_splits=[\"test\"])\n        &gt;&gt;&gt; get_tasks(tasks=[\"STS22\"], languages=[\"eng\"], exclusive_language_filter=True) # don't include multilingual subsets containing English\n    \"\"\"\n    if tasks:\n        if domains or task_types or categories:\n            logger.warning(\n                \"When `tasks` is provided, other filters like domains, task_types, and categories are ignored. \"\n                + \"If you want to filter a list of tasks, please use `mteb.filter_tasks` instead.\"\n            )\n        _tasks = [\n            get_task(\n                task,\n                languages,\n                script,\n                eval_splits=eval_splits,\n                exclusive_language_filter=exclusive_language_filter,\n            )\n            for task in tasks\n        ]\n        return MTEBTasks(_tasks)\n\n    tasks_: Sequence[type[AbsTask]] = filter_tasks(\n        TASK_LIST,\n        languages=languages,\n        script=script,\n        domains=domains,\n        task_types=task_types,\n        categories=categories,\n        modalities=modalities,\n        exclusive_modality_filter=exclusive_modality_filter,\n        exclude_superseded=exclude_superseded,\n        exclude_aggregate=exclude_aggregate,\n        exclude_private=exclude_private,\n    )\n    return MTEBTasks(\n        [\n            cls().filter_languages(languages, script).filter_eval_splits(eval_splits)\n            for cls in tasks_\n        ]\n    )\n</code></pre>"},{"location":"api/task/#mteb.get_task","title":"<code>mteb.get_task(task_name, languages=None, script=None, eval_splits=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Get a task by name.</p> <p>Parameters:</p> Name Type Description Default <code>task_name</code> <code>str</code> <p>The name of the task to fetch.</p> required <code>languages</code> <code>Sequence[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>Sequence[str] | None</code> <p>A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts</p> <code>None</code> <code>eval_splits</code> <code>Sequence[str] | None</code> <p>A list of evaluation splits to include. If None, all splits are included.</p> <code>None</code> <code>hf_subsets</code> <code>Sequence[str] | None</code> <p>A list of Huggingface subsets to evaluate on.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>AbsTask</code> <p>An initialized task object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n</code></pre> Source code in <code>mteb/get_tasks.py</code> <pre><code>def get_task(\n    task_name: str,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    eval_splits: Sequence[str] | None = None,\n    hf_subsets: Sequence[str] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; AbsTask:\n    \"\"\"Get a task by name.\n\n    Args:\n        task_name: The name of the task to fetch.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes). If None, all scripts are included. For multilingual tasks this will also remove scripts\n        eval_splits: A list of evaluation splits to include. If None, all splits are included.\n        hf_subsets: A list of Huggingface subsets to evaluate on.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        An initialized task object.\n\n    Examples:\n        &gt;&gt;&gt; get_task(\"BornholmBitextMining\")\n    \"\"\"\n    if task_name in _TASK_RENAMES:\n        _task_name = _TASK_RENAMES[task_name]\n        msg = f\"The task with the given name '{task_name}' has been renamed to '{_task_name}'. To prevent this warning use the new name.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n\n    if task_name not in _TASKS_REGISTRY:\n        close_matches = difflib.get_close_matches(task_name, _TASKS_REGISTRY.keys())\n        if close_matches:\n            suggestion = f\"KeyError: '{task_name}' not found. Did you mean: '{close_matches[0]}'?\"\n        else:\n            suggestion = (\n                f\"KeyError: '{task_name}' not found and no similar keys were found.\"\n            )\n        raise KeyError(suggestion)\n    task = _TASKS_REGISTRY[task_name]()\n    if eval_splits:\n        task.filter_eval_splits(eval_splits=eval_splits)\n    return task.filter_languages(\n        languages,\n        script,\n        hf_subsets=hf_subsets,\n        exclusive_language_filter=exclusive_language_filter,\n    )\n</code></pre>"},{"location":"api/task/#mteb.filter_tasks","title":"<code>mteb.filter_tasks</code>","text":"<p>This script contains functions that are used to get an overview of the MTEB benchmark.</p>"},{"location":"api/task/#mteb.filter_tasks.filter_tasks","title":"<code>filter_tasks(tasks, *, languages=None, script=None, domains=None, task_types=None, categories=None, modalities=None, exclusive_modality_filter=False, exclude_superseded=False, exclude_aggregate=False, exclude_private=False)</code>","text":"<pre><code>filter_tasks(\n    tasks: Iterable[AbsTask],\n    *,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    domains: Iterable[TaskDomain] | None = None,\n    task_types: Iterable[TaskType] | None = None,\n    categories: Iterable[TaskCategory] | None = None,\n    modalities: Iterable[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_superseded: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = False,\n) -&gt; list[AbsTask]\n</code></pre><pre><code>filter_tasks(\n    tasks: Iterable[type[AbsTask]],\n    *,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    domains: Iterable[TaskDomain] | None = None,\n    task_types: Iterable[TaskType] | None = None,\n    categories: Iterable[TaskCategory] | None = None,\n    modalities: Iterable[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_superseded: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = False,\n) -&gt; list[type[AbsTask]]\n</code></pre> <p>Filter tasks based on the specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[AbsTask] | Iterable[type[AbsTask]]</code> <p>A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.</p> required <code>languages</code> <code>Sequence[str] | None</code> <p>A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g. \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.</p> <code>None</code> <code>script</code> <code>Sequence[str] | None</code> <p>A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts that are not in the specified list.</p> <code>None</code> <code>domains</code> <code>Iterable[TaskDomain] | None</code> <p>A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".</p> <code>None</code> <code>task_types</code> <code>Iterable[TaskType] | None</code> <p>A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.</p> <code>None</code> <code>categories</code> <code>Iterable[TaskCategory] | None</code> <p>A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.</p> <code>None</code> <code>exclude_superseded</code> <code>bool</code> <p>A boolean flag to exclude datasets which are superseded by another.</p> <code>False</code> <code>modalities</code> <code>Iterable[Modalities] | None</code> <p>A list of modalities to include. If None, all modalities are included.</p> <code>None</code> <code>exclusive_modality_filter</code> <code>bool</code> <p>If True, only keep tasks where all filter modalities are included in the task's modalities and ALL task modalities are in filter modalities (exact match). If False, keep tasks if any of the task's modalities match the filter modalities.</p> <code>False</code> <code>exclude_aggregate</code> <code>bool</code> <p>If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.</p> <code>False</code> <code>exclude_private</code> <code>bool</code> <p>If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[AbsTask] | list[type[AbsTask]]</code> <p>A list of tasks objects which pass all of the filters.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; text_classification_tasks = filter_tasks(my_tasks, task_types=[\"Classification\"], modalities=[\"text\"])\n&gt;&gt;&gt; medical_tasks = filter_tasks(my_tasks, domains=[\"Medical\"])\n&gt;&gt;&gt; english_tasks = filter_tasks(my_tasks, languages=[\"eng\"])\n&gt;&gt;&gt; latin_script_tasks = filter_tasks(my_tasks, script=[\"Latn\"])\n&gt;&gt;&gt; text_image_tasks = filter_tasks(my_tasks, modalities=[\"text\", \"image\"], exclusive_modality_filter=True)\n</code></pre> Source code in <code>mteb/filter_tasks.py</code> <pre><code>def filter_tasks(\n    tasks: Iterable[AbsTask] | Iterable[type[AbsTask]],\n    *,\n    languages: Sequence[str] | None = None,\n    script: Sequence[str] | None = None,\n    domains: Iterable[TaskDomain] | None = None,\n    task_types: Iterable[TaskType] | None = None,\n    categories: Iterable[TaskCategory] | None = None,\n    modalities: Iterable[Modalities] | None = None,\n    exclusive_modality_filter: bool = False,\n    exclude_superseded: bool = False,\n    exclude_aggregate: bool = False,\n    exclude_private: bool = False,\n) -&gt; list[AbsTask] | list[type[AbsTask]]:\n    \"\"\"Filter tasks based on the specified criteria.\n\n    Args:\n        tasks: A list of task names to include. If None, all tasks which pass the filters are included. If passed, other filters are ignored.\n        languages: A list of languages either specified as 3 letter languages codes (ISO 639-3, e.g. \"eng\") or as script languages codes e.g.\n            \"eng-Latn\". For multilingual tasks this will also remove languages that are not in the specified list.\n        script: A list of script codes (ISO 15924 codes, e.g. \"Latn\"). If None, all scripts are included. For multilingual tasks this will also remove scripts\n            that are not in the specified list.\n        domains: A list of task domains, e.g. \"Legal\", \"Medical\", \"Fiction\".\n        task_types: A string specifying the type of task e.g. \"Classification\" or \"Retrieval\". If None, all tasks are included.\n        categories: A list of task categories these include \"t2t\" (text to text), \"t2i\" (text to image). See TaskMetadata for the full list.\n        exclude_superseded: A boolean flag to exclude datasets which are superseded by another.\n        modalities: A list of modalities to include. If None, all modalities are included.\n        exclusive_modality_filter: If True, only keep tasks where _all_ filter modalities are included in the\n            task's modalities and ALL task modalities are in filter modalities (exact match).\n            If False, keep tasks if _any_ of the task's modalities match the filter modalities.\n        exclude_aggregate: If True, exclude aggregate tasks. If False, both aggregate and non-aggregate tasks are returned.\n        exclude_private: If True (default), exclude private/closed datasets (is_public=False). If False, include both public and private datasets.\n\n    Returns:\n        A list of tasks objects which pass all of the filters.\n\n    Examples:\n        &gt;&gt;&gt; text_classification_tasks = filter_tasks(my_tasks, task_types=[\"Classification\"], modalities=[\"text\"])\n        &gt;&gt;&gt; medical_tasks = filter_tasks(my_tasks, domains=[\"Medical\"])\n        &gt;&gt;&gt; english_tasks = filter_tasks(my_tasks, languages=[\"eng\"])\n        &gt;&gt;&gt; latin_script_tasks = filter_tasks(my_tasks, script=[\"Latn\"])\n        &gt;&gt;&gt; text_image_tasks = filter_tasks(my_tasks, modalities=[\"text\", \"image\"], exclusive_modality_filter=True)\n\n    \"\"\"\n    langs_to_keep = None\n    if languages:\n        [_check_is_valid_language(lang) for lang in languages]  # type: ignore[func-returns-value]\n        langs_to_keep = set(languages)\n\n    script_to_keep = None\n    if script:\n        [_check_is_valid_script(s) for s in script]  # type: ignore[func-returns-value]\n        script_to_keep = set(script)\n\n    domains_to_keep = None\n    if domains:\n        domains_to_keep = set(domains)\n\n    def _convert_to_set(domain: list[TaskDomain] | None) -&gt; set:\n        return set(domain) if domain is not None else set()\n\n    task_types_to_keep = None\n    if task_types:\n        task_types_to_keep = set(task_types)\n\n    categories_to_keep = None\n    if categories:\n        categories_to_keep = set(categories)\n\n    modalities_to_keep = None\n    if modalities:\n        modalities_to_keep = set(modalities)\n\n    _tasks = []\n    for t in tasks:\n        # For metadata and superseded_by, we can access them directly\n        metadata = t.metadata\n\n        if langs_to_keep and not langs_to_keep.intersection(metadata.languages):\n            continue\n        if script_to_keep and not script_to_keep.intersection(metadata.scripts):\n            continue\n        if domains_to_keep and not domains_to_keep.intersection(\n            _convert_to_set(metadata.domains)\n        ):\n            continue\n        if task_types_to_keep and metadata.type not in task_types_to_keep:\n            continue\n        if categories_to_keep and metadata.category not in categories_to_keep:\n            continue\n        if modalities_to_keep:\n            if exclusive_modality_filter:\n                if set(metadata.modalities) != modalities_to_keep:\n                    continue\n            else:\n                if not modalities_to_keep.intersection(metadata.modalities):\n                    continue\n        if exclude_superseded and metadata.superseded_by is not None:\n            continue\n        is_aggregate = (\n            issubclass(t, AbsTaskAggregate)\n            if isinstance(t, type)\n            else isinstance(t, AbsTaskAggregate)\n        )\n        if exclude_aggregate and is_aggregate:\n            continue\n        if exclude_private and not metadata.is_public:\n            continue\n\n        _tasks.append(t)\n\n    return _tasks  # type: ignore[return-value]  # type checker cannot infer the overload return type\n</code></pre>"},{"location":"api/task/#metadata","title":"Metadata","text":"<p>Each task also contains extensive metadata. We annotate this using the following object, which allows us to use pydantic to validate the metadata.</p>"},{"location":"api/task/#mteb.TaskMetadata","title":"<code>mteb.TaskMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata for a task.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>MetadataDatasetDict</code> <p>All arguments to pass to datasets.load_dataset to load the dataset for the task.</p> <code>name</code> <code>str</code> <p>The name of the task.</p> <code>description</code> <code>str</code> <p>A description of the task.</p> <code>type</code> <code>TaskType</code> <p>The type of the task. This includes \"Classification\", \"Summarization\", \"STS\", \"Retrieval\", \"Reranking\", \"Clustering\", \"PairClassification\", \"BitextMining\". The type should match the abstask type.</p> <code>category</code> <code>TaskCategory | None</code> <p>The category of the task. E.g. includes \"t2t\" (text to text), \"t2i\" (text to image).</p> <code>reference</code> <code>StrURL | None</code> <p>A URL to the documentation of the task. E.g. a published paper.</p> <code>eval_splits</code> <code>list[str]</code> <p>The splits of the dataset used for evaluation.</p> <code>eval_langs</code> <code>Languages</code> <p>The languages of the dataset used for evaluation. Languages follows a ETF BCP 47 standard consisting of \"{language}-{script}\" tag (e.g. \"eng-Latn\"). Where language is specified as a list of ISO 639-3 language codes (e.g. \"eng\") followed by ISO 15924 script codes (e.g. \"Latn\"). Can be either a list of languages or a dictionary mapping huggingface subsets to lists of languages (e.g. if a the huggingface dataset contain different languages).</p> <code>main_score</code> <code>str</code> <p>The main score used for evaluation.</p> <code>date</code> <code>tuple[StrDate, StrDate] | None</code> <p>The date when the data was collected. Specified as a tuple of two dates.</p> <code>domains</code> <code>list[TaskDomain] | None</code> <p>The domains of the data. This includes \"Non-fiction\", \"Social\", \"Fiction\", \"News\", \"Academic\", \"Blog\", \"Encyclopaedic\", \"Government\", \"Legal\", \"Medical\", \"Poetry\", \"Religious\", \"Reviews\", \"Web\", \"Spoken\", \"Written\". A dataset can belong to multiple domains.</p> <code>task_subtypes</code> <code>list[TaskSubtype] | None</code> <p>The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". Feel free to update the list as needed.</p> <code>license</code> <code>Licenses | StrURL | None</code> <p>The license of the data specified as lowercase, e.g. \"cc-by-nc-4.0\". If the license is not specified, use \"not specified\". For custom licenses a URL is used.</p> <code>annotations_creators</code> <code>AnnotatorType | None</code> <p>The type of the annotators. Includes \"expert-annotated\" (annotated by experts), \"human-annotated\" (annotated e.g. by mturkers), \"derived\" (derived from structure in the data).</p> <code>dialect</code> <code>list[str] | None</code> <p>The dialect of the data, if applicable. Ideally specified as a BCP-47 language tag. Empty list if no dialects are present.</p> <code>sample_creation</code> <code>SampleCreationMethod | None</code> <p>The method of text creation. Includes \"found\", \"created\", \"machine-translated\", \"machine-translated and verified\", and \"machine-translated and localized\".</p> <code>prompt</code> <code>str | PromptDict | None</code> <p>The prompt used for the task. Can be a string or a dictionary containing the query and passage prompts.</p> <code>bibtex_citation</code> <code>str | None</code> <p>The BibTeX citation for the dataset. Should be an empty string if no citation is available.</p> <code>adapted_from</code> <code>Sequence[str] | None</code> <p>Datasets adapted (translated, sampled from, etc.) from other datasets.</p> <code>is_public</code> <code>bool</code> <p>Whether the dataset is publicly available. If False (closed/private), a HuggingFace token is required to run the datasets.</p> <code>superseded_by</code> <code>str | None</code> <p>Denotes the task that this task is superseded by. Used to issue warning to users of outdated datasets, while maintaining reproducibility of existing benchmarks.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>class TaskMetadata(BaseModel):\n    \"\"\"Metadata for a task.\n\n    Attributes:\n        dataset: All arguments to pass to [datasets.load_dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/loading_methods#datasets.load_dataset) to load the dataset for the task.\n        name: The name of the task.\n        description: A description of the task.\n        type: The type of the task. This includes \"Classification\", \"Summarization\", \"STS\", \"Retrieval\", \"Reranking\", \"Clustering\",\n            \"PairClassification\", \"BitextMining\". The type should match the abstask type.\n        category: The category of the task. E.g. includes \"t2t\" (text to text), \"t2i\" (text to image).\n        reference: A URL to the documentation of the task. E.g. a published paper.\n        eval_splits: The splits of the dataset used for evaluation.\n        eval_langs: The languages of the dataset used for evaluation. Languages follows a ETF BCP 47 standard consisting of \"{language}-{script}\"\n            tag (e.g. \"eng-Latn\"). Where language is specified as a list of ISO 639-3 language codes (e.g. \"eng\") followed by ISO 15924 script codes\n            (e.g. \"Latn\"). Can be either a list of languages or a dictionary mapping huggingface subsets to lists of languages (e.g. if a the\n            huggingface dataset contain different languages).\n        main_score: The main score used for evaluation.\n        date: The date when the data was collected. Specified as a tuple of two dates.\n        domains: The domains of the data. This includes \"Non-fiction\", \"Social\", \"Fiction\", \"News\", \"Academic\", \"Blog\", \"Encyclopaedic\",\n            \"Government\", \"Legal\", \"Medical\", \"Poetry\", \"Religious\", \"Reviews\", \"Web\", \"Spoken\", \"Written\". A dataset can belong to multiple domains.\n        task_subtypes: The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". Feel free to update the list as needed.\n        license: The license of the data specified as lowercase, e.g. \"cc-by-nc-4.0\". If the license is not specified, use \"not specified\". For custom licenses a URL is used.\n        annotations_creators: The type of the annotators. Includes \"expert-annotated\" (annotated by experts), \"human-annotated\" (annotated e.g. by\n            mturkers), \"derived\" (derived from structure in the data).\n        dialect: The dialect of the data, if applicable. Ideally specified as a BCP-47 language tag. Empty list if no dialects are present.\n        sample_creation: The method of text creation. Includes \"found\", \"created\", \"machine-translated\", \"machine-translated and verified\", and\n            \"machine-translated and localized\".\n        prompt: The prompt used for the task. Can be a string or a dictionary containing the query and passage prompts.\n        bibtex_citation: The BibTeX citation for the dataset. Should be an empty string if no citation is available.\n        adapted_from: Datasets adapted (translated, sampled from, etc.) from other datasets.\n        is_public: Whether the dataset is publicly available. If False (closed/private), a HuggingFace token is required to run the datasets.\n        superseded_by: Denotes the task that this task is superseded by. Used to issue warning to users of outdated datasets, while maintaining\n            reproducibility of existing benchmarks.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    dataset: MetadataDatasetDict\n\n    name: str\n    description: str\n    prompt: str | PromptDict | None = None\n    type: TaskType\n    modalities: list[Modalities] = [\"text\"]\n    category: TaskCategory | None = None\n    reference: StrURL | None = None\n\n    eval_splits: list[str] = [\"test\"]\n    eval_langs: Languages\n    main_score: str\n\n    date: tuple[StrDate, StrDate] | None = None\n    domains: list[TaskDomain] | None = None\n    task_subtypes: list[TaskSubtype] | None = None\n    license: Licenses | StrURL | None = None\n\n    annotations_creators: AnnotatorType | None = None\n    dialect: list[str] | None = None\n\n    sample_creation: SampleCreationMethod | None = None\n    bibtex_citation: str | None = None\n    adapted_from: Sequence[str] | None = None\n    is_public: bool = True\n    superseded_by: str | None = None\n\n    def _validate_metadata(self) -&gt; None:\n        self._eval_langs_are_valid(self.eval_langs)\n\n    @field_validator(\"prompt\")\n    @classmethod\n    def _check_prompt_is_valid(\n        cls, prompt: str | PromptDict | None\n    ) -&gt; str | PromptDict | None:\n        if isinstance(prompt, dict):\n            for key in prompt:\n                if key not in [e.value for e in PromptType]:\n                    raise ValueError(\n                        \"The prompt dictionary should only contain the keys 'query' and 'passage'.\"\n                    )\n        return prompt\n\n    def _eval_langs_are_valid(self, eval_langs: Languages) -&gt; None:\n        \"\"\"This method checks that the eval_langs are specified as a list of languages.\"\"\"\n        if isinstance(eval_langs, dict):\n            for langs in eval_langs.values():\n                for code in langs:\n                    check_language_code(code)\n        else:\n            for code in eval_langs:\n                check_language_code(code)\n\n    @property\n    def bcp47_codes(self) -&gt; list[ISOLanguageScript]:\n        \"\"\"Return the languages and script codes of the dataset formatting in accordance with the BCP-47 standard.\"\"\"\n        if isinstance(self.eval_langs, dict):\n            return sorted(\n                {lang for langs in self.eval_langs.values() for lang in langs}\n            )\n        return sorted(set(self.eval_langs))\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Return the languages of the dataset as iso639-3 codes.\"\"\"\n\n        def get_lang(lang: str) -&gt; str:\n            return lang.split(\"-\")[0]\n\n        if isinstance(self.eval_langs, dict):\n            return sorted(\n                {get_lang(lang) for langs in self.eval_langs.values() for lang in langs}\n            )\n        return sorted({get_lang(lang) for lang in self.eval_langs})\n\n    @property\n    def scripts(self) -&gt; set[str]:\n        \"\"\"Return the scripts of the dataset as iso15924 codes.\"\"\"\n\n        def get_script(lang: str) -&gt; str:\n            return lang.split(\"-\")[1]\n\n        if isinstance(self.eval_langs, dict):\n            return {\n                get_script(lang) for langs in self.eval_langs.values() for lang in langs\n            }\n        return {get_script(lang) for lang in self.eval_langs}\n\n    def is_filled(self) -&gt; bool:\n        \"\"\"Check if all the metadata fields are filled.\n\n        Returns:\n            True if all the metadata fields are filled, False otherwise.\n        \"\"\"\n        return all(\n            getattr(self, field_name) is not None\n            for field_name in self.model_fields\n            if field_name not in [\"prompt\", \"adapted_from\", \"superseded_by\"]\n        )\n\n    @property\n    def hf_subsets_to_langscripts(self) -&gt; dict[HFSubset, list[ISOLanguageScript]]:\n        \"\"\"Return a dictionary mapping huggingface subsets to languages.\"\"\"\n        if isinstance(self.eval_langs, dict):\n            return self.eval_langs\n        return {\"default\": cast(\"list[str]\", self.eval_langs)}\n\n    @property\n    def intext_citation(self, include_cite: bool = True) -&gt; str:\n        \"\"\"Create an in-text citation for the dataset.\"\"\"\n        cite = \"\"\n        if self.bibtex_citation:\n            cite = f\"{self.bibtex_citation.split(',')[0].split('{')[1]}\"\n        if include_cite and cite:\n            # check for whitespace in the citation\n            if \" \" in cite:\n                msg = \"Citation contains whitespace. Please ensure that the citation is correctly formatted.\"\n                logger.warning(msg)\n            return f\"\\\\cite{{{cite}}}\"\n        return cite\n\n    @property\n    def descriptive_stats(self) -&gt; dict[str, DescriptiveStatistics] | None:\n        \"\"\"Return the descriptive statistics for the dataset.\"\"\"\n        if self.descriptive_stat_path.exists():\n            with self.descriptive_stat_path.open(\"r\") as f:\n                return json.load(f)\n        return None\n\n    @property\n    def descriptive_stat_path(self) -&gt; Path:\n        \"\"\"Return the path to the descriptive statistics file.\"\"\"\n        descriptive_stat_base_dir = Path(__file__).parent.parent / \"descriptive_stats\"\n        if self.type in MIEB_TASK_TYPE:\n            descriptive_stat_base_dir = descriptive_stat_base_dir / \"Image\"\n        task_type_dir = descriptive_stat_base_dir / self.type\n        if not descriptive_stat_base_dir.exists():\n            descriptive_stat_base_dir.mkdir()\n        if not task_type_dir.exists():\n            task_type_dir.mkdir()\n        return task_type_dir / f\"{self.name}.json\"\n\n    @property\n    def n_samples(self) -&gt; dict[str, int] | None:\n        \"\"\"Returns the number of samples in the dataset\"\"\"\n        stats = self.descriptive_stats\n        if not stats:\n            return None\n\n        n_samples = {}\n        for subset, subset_value in stats.items():\n            if subset == \"hf_subset_descriptive_stats\":\n                continue\n            n_samples[subset] = subset_value[\"num_samples\"]\n        return n_samples\n\n    @property\n    def hf_subsets(self) -&gt; list[str]:\n        \"\"\"Return the huggingface subsets.\"\"\"\n        return list(self.hf_subsets_to_langscripts.keys())\n\n    @property\n    def is_multilingual(self) -&gt; bool:\n        \"\"\"Check if the task is multilingual.\"\"\"\n        return isinstance(self.eval_langs, dict)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.model_dump_json())\n\n    @property\n    def revision(self) -&gt; str:\n        \"\"\"Return the dataset revision.\"\"\"\n        return self.dataset[\"revision\"]\n\n    def get_modalities(self, prompt_type: PromptType | None = None) -&gt; list[Modalities]:\n        \"\"\"Get the modalities for the task based category if prompt_type provided.\n\n        Args:\n            prompt_type: The prompt type to get the modalities for.\n\n        Returns:\n            A list of modalities for the task.\n\n        Raises:\n            ValueError: If the prompt type is not recognized.\n        \"\"\"\n        if prompt_type is None or self.category is None:\n            return self.modalities\n        query_modalities, doc_modalities = self.category.split(\"2\")\n        category_to_modality: dict[str, Modalities] = {\n            \"t\": \"text\",\n            \"i\": \"image\",\n            \"a\": \"audio\",\n        }\n        if prompt_type == PromptType.query:\n            return [\n                category_to_modality[query_modality]\n                for query_modality in query_modalities\n            ]\n        if prompt_type == PromptType.document:\n            return [\n                category_to_modality[doc_modality] for doc_modality in doc_modalities\n            ]\n        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n\n    def _create_dataset_card_data(\n        self,\n        existing_dataset_card_data: CardData | None = None,\n    ) -&gt; tuple[DatasetCardData, dict[str, Any]]:\n        \"\"\"Create a DatasetCardData object from the task metadata.\n\n        Args:\n            existing_dataset_card_data: The existing DatasetCardData object to update. If None, a new object will be created.\n\n        Returns:\n            A DatasetCardData object with the metadata for the task with kwargs to card\n        \"\"\"\n        if existing_dataset_card_data is None:\n            existing_dataset_card_data = DatasetCardData()\n\n        dataset_type = [\n            *self._hf_task_type(),\n            *self._hf_task_category(),\n        ]\n        languages = self._hf_languages()\n\n        multilinguality = \"monolingual\" if len(languages) == 1 else \"multilingual\"\n        if self.sample_creation and \"translated\" in self.sample_creation:\n            multilinguality = \"translated\"\n\n        if self.adapted_from is not None:\n            source_datasets = [\n                task.metadata.dataset[\"path\"]\n                for task in mteb.get_tasks(self.adapted_from)\n            ]\n            source_datasets.append(self.dataset[\"path\"])\n        else:\n            source_datasets = None if not self.dataset else [self.dataset[\"path\"]]\n\n        tags = [\"mteb\"] + self.modalities\n\n        descriptive_stats = \"\"\n        if self.descriptive_stats is not None:\n            descriptive_stats_ = self.descriptive_stats\n            for split, split_stat in descriptive_stats_.items():\n                if len(split_stat.get(\"hf_subset_descriptive_stats\", {})) &gt; 10:\n                    split_stat.pop(\"hf_subset_descriptive_stats\", {})\n            descriptive_stats = json.dumps(descriptive_stats_, indent=4)\n\n        dataset_card_data_params = existing_dataset_card_data.to_dict()\n        # override the existing values\n        dataset_card_data_params.update(\n            dict(\n                language=languages,\n                license=self._hf_license(),\n                annotations_creators=[self.annotations_creators]\n                if self.annotations_creators\n                else None,\n                multilinguality=multilinguality,\n                source_datasets=source_datasets,\n                task_categories=dataset_type,\n                task_ids=self._hf_subtypes(),\n                tags=tags,\n            )\n        )\n\n        return (\n            DatasetCardData(**dataset_card_data_params),\n            # parameters for readme generation\n            dict(\n                citation=self.bibtex_citation,\n                dataset_description=self.description,\n                dataset_reference=self.reference,\n                descriptive_stats=descriptive_stats,\n                dataset_task_name=self.name,\n                category=self.category,\n                domains=\", \".join(self.domains) if self.domains else None,\n            ),\n        )\n\n    def generate_dataset_card(\n        self,\n        existing_dataset_card: DatasetCard | None = None,\n    ) -&gt; DatasetCard:\n        \"\"\"Generates a dataset card for the task.\n\n        Args:\n            existing_dataset_card: The existing dataset card to update. If None, a new dataset card will be created.\n\n        Returns:\n            DatasetCard: The dataset card for the task.\n        \"\"\"\n        path = Path(__file__).parent / \"dataset_card_template.md\"\n        existing_dataset_card_data = (\n            existing_dataset_card.data if existing_dataset_card else None\n        )\n        dataset_card_data, template_kwargs = self._create_dataset_card_data(\n            existing_dataset_card_data\n        )\n        dataset_card = DatasetCard.from_template(\n            card_data=dataset_card_data,\n            template_path=str(path),\n            **template_kwargs,\n        )\n        return dataset_card\n\n    def push_dataset_card_to_hub(self, repo_name: str) -&gt; None:\n        \"\"\"Pushes the dataset card to the huggingface hub.\n\n        Args:\n            repo_name: The name of the repository to push the dataset card to.\n        \"\"\"\n        dataset_card = None\n        if repo_exists(\n            repo_name, repo_type=constants.REPO_TYPE_DATASET\n        ) and file_exists(\n            repo_name, constants.REPOCARD_NAME, repo_type=constants.REPO_TYPE_DATASET\n        ):\n            dataset_card = DatasetCard.load(repo_name)\n        dataset_card = self.generate_dataset_card(dataset_card)\n        dataset_card.push_to_hub(repo_name, commit_message=\"Add dataset card\")\n\n    def _hf_subtypes(self) -&gt; list[str]:\n        # to get full list of available task_ids execute\n        # https://huggingface.co/api/datasets-tags-by-type?type=task_ids\n        # ref https://huggingface-openapi.hf.space/#tag/datasets/GET/api/datasets-tags-by-type\n        mteb_to_hf_subtype = {\n            \"Article retrieval\": [\"document-retrieval\"],\n            \"Conversational retrieval\": [\"conversational\", \"utterance-retrieval\"],\n            \"Dialect pairing\": [],\n            \"Dialog Systems\": [\"dialogue-modeling\", \"dialogue-generation\"],\n            \"Discourse coherence\": [],\n            \"Duplicate Image Retrieval\": [],\n            \"Language identification\": [\"language-identification\"],\n            \"Linguistic acceptability\": [\"acceptability-classification\"],\n            \"Political classification\": [],\n            \"Question answering\": [\n                \"multiple-choice-qa\",\n                \"question-answering\",\n            ],\n            \"Sentiment/Hate speech\": [\n                \"sentiment-analysis\",\n                \"sentiment-scoring\",\n                \"sentiment-classification\",\n                \"hate-speech-detection\",\n            ],\n            \"Thematic clustering\": [],\n            \"Scientific Reranking\": [\"text-scoring\"],\n            \"Claim verification\": [\"fact-checking\", \"fact-checking-retrieval\"],\n            \"Topic classification\": [\"topic-classification\"],\n            \"Code retrieval\": [],\n            \"False Friends\": [],\n            \"Cross-Lingual Semantic Discrimination\": [],\n            \"Textual Entailment\": [\"natural-language-inference\"],\n            \"Counterfactual Detection\": [],\n            \"Emotion classification\": [\"sentiment-classification\"],\n            \"Reasoning as Retrieval\": [],\n            \"Rendered Texts Understanding\": [],\n            \"Image Text Retrieval\": [],\n            \"Object recognition\": [],\n            \"Scene recognition\": [],\n            \"Caption Pairing\": [\"image-captioning\"],\n            \"Emotion recognition\": [\"sentiment-scoring\"],\n            \"Textures recognition\": [],\n            \"Activity recognition\": [],\n            \"Tumor detection\": [],\n            \"Duplicate Detection\": [],\n            \"Rendered semantic textual similarity\": [\n                \"semantic-similarity-scoring\",\n                \"semantic-similarity-classification\",\n            ],\n            \"Intent classification\": [\n                \"intent-classification\",\n            ],\n            \"Accent identification\": [],\n            \"Environment Sound Classification\": [],\n            \"Gunshot Audio Classification\": [],\n            \"Keyword Spotting\": [],\n            \"Instrument Source Classification\": [],\n            \"Music Genre Classification\": [],\n            \"Music Instrument Recognition\": [],\n            \"Spoken Language Identification\": [],\n            \"Stroke Classification of Musical Instrument\": [],\n            \"Tonic Classification of Musical Instrument\": [],\n            \"Speaker Count Identification\": [],\n            \"Species Classification\": [],\n            \"Spoken Digit Classification\": [],\n            \"Gender Clustering\": [],\n            \"Vocal Sound Classification\": [],\n            \"Music Clustering\": [],\n            \"Accent Clustering\": [],\n            \"Sentiment Clustering\": [],\n            \"Emotion Clustering\": [\"audio-emotion-recognition\"],\n            \"Sentiment Analysis\": [],\n            \"Vehicle Clustering\": [],\n            \"Environment Sound Clustering\": [],\n            \"Environment Sound Reranking\": [],\n            \"Emotion Reranking\": [\"audio-emotion-recognition\"],\n            \"Music Genre Reranking\": [],\n            \"Gender Classification\": [],\n            \"Age Classification\": [],\n            \"Song Lyrics Retrieval\": [],\n            \"Natural Sound Retrieval\": [],\n            \"Music Caption Retrieval\": [],\n            \"Speech Transcription Retrieval\": [],\n            \"Emotional Speech Retrieval\": [\"audio-emotion-recognition\"],\n            \"Environment Sound Retrieval\": [],\n            \"Speech Retrieval\": [],\n            \"Question Answering Retrieval\": [],\n            \"Reading Comprehension\": [],\n        }\n        subtypes = []\n        if self.task_subtypes:\n            for subtype in self.task_subtypes:\n                subtypes.extend(mteb_to_hf_subtype.get(subtype, []))\n        return subtypes\n\n    def _hf_task_type(self) -&gt; list[str]:\n        # to get full list of task_types execute:\n        # https://huggingface.co/api/datasets-tags-by-type?type=task_categories\n        # ref https://huggingface-openapi.hf.space/#tag/datasets/GET/api/datasets-tags-by-type\n        mteb_task_type_to_datasets = {\n            # Text\n            \"BitextMining\": [\"translation\"],\n            \"Classification\": [\"text-classification\"],\n            \"MultilabelClassification\": [\"text-classification\"],\n            \"Clustering\": [\"text-classification\"],\n            \"PairClassification\": [\"text-classification\"],\n            \"Reranking\": [\"text-ranking\"],\n            \"Retrieval\": [\"text-retrieval\"],\n            \"STS\": [\"sentence-similarity\"],\n            \"Summarization\": [\"summarization\"],\n            \"InstructionRetrieval\": [\"text-retrieval\"],\n            \"InstructionReranking\": [\"text-ranking\"],\n            # Image\n            \"Any2AnyMultiChoice\": [\"visual-question-answering\"],\n            \"Any2AnyMultilingualRetrieval\": [\"visual-document-retrieval\"],\n            \"VisionCentricQA\": [\"visual-question-answering\"],\n            \"ImageClustering\": [\"image-feature-extraction\"],\n            \"ImageClassification\": [\"image-classification\"],\n            \"ImageMultilabelClassification\": [\"image-classification\"],\n            \"DocumentUnderstanding\": [\"visual-document-retrieval\"],\n            \"VisualSTS(eng)\": [\"other\"],\n            \"VisualSTS(multi)\": [\"other\"],\n            \"ZeroShotClassification\": [\"zero-shot-classification\"],\n            \"Compositionality\": [\"other\"],\n            # audio\n            \"AudioClustering\": [\"audio-classification\"],\n            \"AudioMultilabelClassification\": [\"audio-classification\"],\n            \"AudioReranking\": [\"other\"],\n            \"AudioZeroshotClassification\": [\"other\"],\n            \"AudioClassification\": [\"audio-classification\"],\n            \"AudioCrossFoldClassification\": [\"audio-classification\"],\n            \"AudioPairClassification\": [\"audio-classification\"],\n        }\n        if self.type == \"ZeroShotClassification\":\n            if self.modalities == [\"image\"]:\n                return [\"zero-shot-image-classification\"]\n            return [\"zero-shot-classification\"]\n\n        if self.type == \"Any2AnyRetrieval\":\n            if self.modalities == [\"image\"]:\n                return [\"visual-document-retrieval\"]\n            return [\"other\"]\n\n        return mteb_task_type_to_datasets[self.type]\n\n    def _hf_task_category(self) -&gt; list[str]:\n        dataset_type = []\n        if self.category in [\"i2i\", \"it2i\", \"i2it\", \"it2it\"]:\n            dataset_type.append(\"image-to-image\")\n        if self.category in [\"i2t\", \"t2i\", \"it2t\", \"it2i\", \"t2it\", \"i2it\", \"it2it\"]:\n            dataset_type.extend([\"image-to-text\", \"text-to-image\"])\n        if self.category in [\"it2t\", \"it2i\", \"t2it\", \"i2it\", \"it2it\"]:\n            dataset_type.extend([\"image-text-to-text\"])\n        if self.category in [\"a2a\", \"at2a\", \"a2at\", \"at2at\"]:\n            dataset_type.append(\"audio-to-audio\")\n        if self.category in [\"a2t\", \"t2a\", \"at2t\", \"t2at\", \"at2at\", \"a2at\"]:\n            dataset_type.extend([\"text-to-audio\"])\n        return dataset_type\n\n    def _hf_languages(self) -&gt; list[str]:\n        languages: list[str] = []\n        if self.is_multilingual and isinstance(self.eval_langs, dict):\n            for val in self.eval_langs.values():\n                languages.extend(val)\n        else:\n            languages = cast(\"list[str]\", self.eval_langs)\n        # value \"python\" is not valid. It must be an ISO 639-1, 639-2 or 639-3 code (two/three letters),\n        # or a special value like \"code\", \"multilingual\".\n        readme_langs = []\n        for lang in languages:\n            lang_name, family = lang.split(\"-\")\n            if family == \"Code\":\n                readme_langs.append(\"code\")\n            else:\n                readme_langs.append(lang_name)\n        return sorted(set(readme_langs))\n\n    def _hf_license(self) -&gt; str | None:\n        dataset_license = self.license\n        if dataset_license:\n            license_mapping = {\n                \"not specified\": \"unknown\",\n                \"msr-la-nc\": \"other\",\n                \"cc-by-nd-2.1-jp\": \"other\",\n            }\n            dataset_license = license_mapping.get(\n                dataset_license,\n                \"other\" if dataset_license.startswith(\"http\") else dataset_license,\n            )\n        return dataset_license\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.bcp47_codes","title":"<code>bcp47_codes</code>  <code>property</code>","text":"<p>Return the languages and script codes of the dataset formatting in accordance with the BCP-47 standard.</p>"},{"location":"api/task/#mteb.TaskMetadata.descriptive_stat_path","title":"<code>descriptive_stat_path</code>  <code>property</code>","text":"<p>Return the path to the descriptive statistics file.</p>"},{"location":"api/task/#mteb.TaskMetadata.descriptive_stats","title":"<code>descriptive_stats</code>  <code>property</code>","text":"<p>Return the descriptive statistics for the dataset.</p>"},{"location":"api/task/#mteb.TaskMetadata.hf_subsets","title":"<code>hf_subsets</code>  <code>property</code>","text":"<p>Return the huggingface subsets.</p>"},{"location":"api/task/#mteb.TaskMetadata.hf_subsets_to_langscripts","title":"<code>hf_subsets_to_langscripts</code>  <code>property</code>","text":"<p>Return a dictionary mapping huggingface subsets to languages.</p>"},{"location":"api/task/#mteb.TaskMetadata.intext_citation","title":"<code>intext_citation</code>  <code>property</code>","text":"<p>Create an in-text citation for the dataset.</p>"},{"location":"api/task/#mteb.TaskMetadata.is_multilingual","title":"<code>is_multilingual</code>  <code>property</code>","text":"<p>Check if the task is multilingual.</p>"},{"location":"api/task/#mteb.TaskMetadata.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Return the languages of the dataset as iso639-3 codes.</p>"},{"location":"api/task/#mteb.TaskMetadata.n_samples","title":"<code>n_samples</code>  <code>property</code>","text":"<p>Returns the number of samples in the dataset</p>"},{"location":"api/task/#mteb.TaskMetadata.revision","title":"<code>revision</code>  <code>property</code>","text":"<p>Return the dataset revision.</p>"},{"location":"api/task/#mteb.TaskMetadata.scripts","title":"<code>scripts</code>  <code>property</code>","text":"<p>Return the scripts of the dataset as iso15924 codes.</p>"},{"location":"api/task/#mteb.TaskMetadata.generate_dataset_card","title":"<code>generate_dataset_card(existing_dataset_card=None)</code>","text":"<p>Generates a dataset card for the task.</p> <p>Parameters:</p> Name Type Description Default <code>existing_dataset_card</code> <code>DatasetCard | None</code> <p>The existing dataset card to update. If None, a new dataset card will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DatasetCard</code> <code>DatasetCard</code> <p>The dataset card for the task.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def generate_dataset_card(\n    self,\n    existing_dataset_card: DatasetCard | None = None,\n) -&gt; DatasetCard:\n    \"\"\"Generates a dataset card for the task.\n\n    Args:\n        existing_dataset_card: The existing dataset card to update. If None, a new dataset card will be created.\n\n    Returns:\n        DatasetCard: The dataset card for the task.\n    \"\"\"\n    path = Path(__file__).parent / \"dataset_card_template.md\"\n    existing_dataset_card_data = (\n        existing_dataset_card.data if existing_dataset_card else None\n    )\n    dataset_card_data, template_kwargs = self._create_dataset_card_data(\n        existing_dataset_card_data\n    )\n    dataset_card = DatasetCard.from_template(\n        card_data=dataset_card_data,\n        template_path=str(path),\n        **template_kwargs,\n    )\n    return dataset_card\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.get_modalities","title":"<code>get_modalities(prompt_type=None)</code>","text":"<p>Get the modalities for the task based category if prompt_type provided.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_type</code> <code>PromptType | None</code> <p>The prompt type to get the modalities for.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Modalities]</code> <p>A list of modalities for the task.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the prompt type is not recognized.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def get_modalities(self, prompt_type: PromptType | None = None) -&gt; list[Modalities]:\n    \"\"\"Get the modalities for the task based category if prompt_type provided.\n\n    Args:\n        prompt_type: The prompt type to get the modalities for.\n\n    Returns:\n        A list of modalities for the task.\n\n    Raises:\n        ValueError: If the prompt type is not recognized.\n    \"\"\"\n    if prompt_type is None or self.category is None:\n        return self.modalities\n    query_modalities, doc_modalities = self.category.split(\"2\")\n    category_to_modality: dict[str, Modalities] = {\n        \"t\": \"text\",\n        \"i\": \"image\",\n        \"a\": \"audio\",\n    }\n    if prompt_type == PromptType.query:\n        return [\n            category_to_modality[query_modality]\n            for query_modality in query_modalities\n        ]\n    if prompt_type == PromptType.document:\n        return [\n            category_to_modality[doc_modality] for doc_modality in doc_modalities\n        ]\n    raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.is_filled","title":"<code>is_filled()</code>","text":"<p>Check if all the metadata fields are filled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if all the metadata fields are filled, False otherwise.</p> Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def is_filled(self) -&gt; bool:\n    \"\"\"Check if all the metadata fields are filled.\n\n    Returns:\n        True if all the metadata fields are filled, False otherwise.\n    \"\"\"\n    return all(\n        getattr(self, field_name) is not None\n        for field_name in self.model_fields\n        if field_name not in [\"prompt\", \"adapted_from\", \"superseded_by\"]\n    )\n</code></pre>"},{"location":"api/task/#mteb.TaskMetadata.push_dataset_card_to_hub","title":"<code>push_dataset_card_to_hub(repo_name)</code>","text":"<p>Pushes the dataset card to the huggingface hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_name</code> <code>str</code> <p>The name of the repository to push the dataset card to.</p> required Source code in <code>mteb/abstasks/task_metadata.py</code> <pre><code>def push_dataset_card_to_hub(self, repo_name: str) -&gt; None:\n    \"\"\"Pushes the dataset card to the huggingface hub.\n\n    Args:\n        repo_name: The name of the repository to push the dataset card to.\n    \"\"\"\n    dataset_card = None\n    if repo_exists(\n        repo_name, repo_type=constants.REPO_TYPE_DATASET\n    ) and file_exists(\n        repo_name, constants.REPOCARD_NAME, repo_type=constants.REPO_TYPE_DATASET\n    ):\n        dataset_card = DatasetCard.load(repo_name)\n    dataset_card = self.generate_dataset_card(dataset_card)\n    dataset_card.push_to_hub(repo_name, commit_message=\"Add dataset card\")\n</code></pre>"},{"location":"api/task/#metadata-types","title":"Metadata Types","text":""},{"location":"api/task/#mteb.abstasks.task_metadata.AnnotatorType","title":"<code>mteb.abstasks.task_metadata.AnnotatorType = Literal['expert-annotated', 'human-annotated', 'derived', 'LM-generated', 'LM-generated and reviewed', 'automatic', 'automatic-and-reviewed', 'algorithmic']</code>  <code>module-attribute</code>","text":"<p>The type of the annotators. Is often important for understanding the quality of a dataset.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.SampleCreationMethod","title":"<code>mteb.abstasks.task_metadata.SampleCreationMethod = Literal['found', 'created', 'created and machine-translated', 'human-translated and localized', 'human-translated', 'machine-translated', 'machine-translated and verified', 'machine-translated and localized', 'LM-generated and verified', 'machine-translated and LM verified', 'rendered', 'multiple']</code>  <code>module-attribute</code>","text":"<p>How the text was created. It can be an important factor for understanding the quality of a dataset. E.g. used to filter out machine-translated datasets.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskCategory","title":"<code>mteb.abstasks.task_metadata.TaskCategory = Literal['t2t', 't2c', 'i2i', 'i2c', 'i2t', 't2i', 'it2t', 'it2i', 'i2it', 't2it', 'it2it', 'a2a', 'a2c', 'a2t', 't2a', 'at2t', 'at2a', 'a2at', 't2at', 'at2at']</code>  <code>module-attribute</code>","text":"<p>The category of the task.</p> <ol> <li>t2t: text to text</li> <li>t2c: text to category</li> <li>i2i: image to image</li> <li>i2c: image to category</li> <li>i2t: image to text</li> <li>t2i: text to image</li> <li>it2t: image+text to text</li> <li>it2i: image+text to image</li> <li>i2it: image to image+text</li> <li>t2it: text to image+text</li> <li>it2it: image+text to image+text</li> <li>a2a: audio to audio</li> <li>a2c: audio to category</li> <li>a2t: audio to text</li> <li>t2a: text to audio</li> <li>at2t: audio+text to text</li> <li>at2a: audio+text to audio</li> <li>a2at: audio to audio+text</li> <li>t2at: text to audio+text</li> <li>at2at: audio+text to audio+text</li> </ol>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskDomain","title":"<code>mteb.abstasks.task_metadata.TaskDomain = Literal['Academic', 'Blog', 'Constructed', 'Encyclopaedic', 'Engineering', 'Fiction', 'Government', 'Legal', 'Medical', 'News', 'Non-fiction', 'Poetry', 'Religious', 'Reviews', 'Scene', 'Social', 'Spoken', 'Subtitles', 'Web', 'Written', 'Programming', 'Chemistry', 'Financial', 'Entertainment', 'E-commerce', 'AudioScene', 'Speech', 'Spoken', 'Music', 'Bioacoustics']</code>  <code>module-attribute</code>","text":"<p>The domains follow the categories used in the Universal Dependencies project, though  we updated them where deemed appropriate. These do not have to be mutually exclusive.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskType","title":"<code>mteb.abstasks.task_metadata.TaskType = Literal[_TASK_TYPE]</code>  <code>module-attribute</code>","text":"<p>The type of the task. E.g. includes \"Classification\", \"Retrieval\" and \"Clustering\".</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.TaskSubtype","title":"<code>mteb.abstasks.task_metadata.TaskSubtype = Literal['Article retrieval', 'Patent retrieval', 'Conversational retrieval', 'Dialect pairing', 'Dialog Systems', 'Discourse coherence', 'Duplicate Image Retrieval', 'Language identification', 'Linguistic acceptability', 'Political classification', 'Question answering', 'Sentiment/Hate speech', 'Thematic clustering', 'Scientific Reranking', 'Claim verification', 'Topic classification', 'Code retrieval', 'False Friends', 'Cross-Lingual Semantic Discrimination', 'Textual Entailment', 'Counterfactual Detection', 'Emotion classification', 'Reasoning as Retrieval', 'Rendered Texts Understanding', 'Image Text Retrieval', 'Object recognition', 'Scene recognition', 'Caption Pairing', 'Emotion recognition', 'Textures recognition', 'Activity recognition', 'Tumor detection', 'Duplicate Detection', 'Rendered semantic textual similarity', 'Intent classification', 'Product Reranking', 'Query-Product Relevance', 'Accent identification', 'Environment Sound Classification', 'Gunshot Audio Classification', 'Keyword Spotting', 'Instrument Source Classification', 'Music Genre Classification', 'Music Instrument Recognition', 'Spoken Language Identification', 'Stroke Classification of Musical Instrument', 'Tonic Classification of Musical Instrument', 'Speaker Count Identification', 'Species Classification', 'Spoken Digit Classification', 'Gender Clustering', 'Vocal Sound Classification', 'Music Clustering', 'Accent Clustering', 'Sentiment Clustering', 'Emotion Clustering', 'Sentiment Analysis', 'Vehicle Clustering', 'Environment Sound Clustering', 'Environment Sound Reranking', 'Emotion Reranking', 'Music Genre Reranking', 'Gender Classification', 'Age Classification', 'Song Lyrics Retrieval', 'Natural Sound Retrieval', 'Music Caption Retrieval', 'Speech Transcription Retrieval', 'Emotional Speech Retrieval', 'Environment Sound Retrieval', 'Speech Retrieval', 'Question Answering Retrieval', 'Reading Comprehension', 'Intent Classification']</code>  <code>module-attribute</code>","text":"<p>The subtypes of the task. E.g. includes \"Sentiment/Hate speech\", \"Thematic Clustering\". This list can be updated as needed.</p>"},{"location":"api/task/#mteb.abstasks.task_metadata.PromptDict","title":"<code>mteb.abstasks.task_metadata.PromptDict = TypedDict('PromptDict', {(prompt_type.value): str for prompt_type in PromptType}, total=False)</code>  <code>module-attribute</code>","text":"<p>A dictionary containing the prompt used for the task.</p> <p>Attributes:</p> Name Type Description <code>query</code> <p>The prompt used for the queries in the task.</p> <code>document</code> <p>The prompt used for the passages in the task.</p>"},{"location":"api/task/#the-task-object","title":"The Task Object","text":"<p>All tasks in <code>mteb</code> inherits from the following abstract class.</p>"},{"location":"api/task/#mteb.AbsTask","title":"<code>mteb.AbsTask</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class for the tasks. All tasks in <code>mteb</code> inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>TaskMetadata</code> <p>The metadata describing the task</p> <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>The dataset represented as a dictionary on the form {\"hf subset\": {\"split\": Dataset}} where \"split\" is the dataset split (e.g. \"test\") and Dataset is a datasets.Dataset object. \"hf subset\" is the data subset on Huggingface typically used to denote the language e.g. datasets.load_dataset(\"data\", \"en\"). If the dataset does not have a subset this is simply \"default\".</p> <code>seed</code> <p>The random seed used for reproducibility.</p> <code>hf_subsets</code> <code>list[HFSubset]</code> <p>The list of Huggingface subsets to use.</p> <code>data_loaded</code> <code>bool</code> <p>Denotes if the dataset is loaded or not. This is used to avoid loading the dataset multiple times.</p> <code>abstask_prompt</code> <code>str</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>fast_loading</code> <code>bool</code> <p>Deprecated. Denotes if the task should be loaded using the fast loading method. This is only possible if the dataset have a \"default\" config. We don't recommend to use this method, and suggest to use different subsets for loading datasets. This was used only for historical reasons and will be removed in the future.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>class AbsTask(ABC):\n    \"\"\"The abstract class for the tasks. All tasks in `mteb` inherit from this class.\n\n    Attributes:\n        metadata: The metadata describing the task\n        dataset: The dataset represented as a dictionary on the form {\"hf subset\": {\"split\": Dataset}} where \"split\" is the dataset split (e.g. \"test\")\n            and Dataset is a datasets.Dataset object. \"hf subset\" is the data subset on Huggingface typically used to denote the language e.g.\n            datasets.load_dataset(\"data\", \"en\"). If the dataset does not have a subset this is simply \"default\".\n        seed: The random seed used for reproducibility.\n        hf_subsets: The list of Huggingface subsets to use.\n        data_loaded: Denotes if the dataset is loaded or not. This is used to avoid loading the dataset multiple times.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        fast_loading: **Deprecated**. Denotes if the task should be loaded using the fast loading method.\n            This is only possible if the dataset have a \"default\" config. We don't recommend to use this method, and suggest to use different subsets for loading datasets.\n            This was used only for historical reasons and will be removed in the future.\n    \"\"\"\n\n    metadata: TaskMetadata\n    abstask_prompt: str\n    _eval_splits: Sequence[str] | None = None\n    dataset: dict[HFSubset, DatasetDict] | None = None\n    data_loaded: bool = False\n    hf_subsets: list[HFSubset]\n    fast_loading: bool = False\n\n    _support_cross_encoder: bool = False\n    _support_search: bool = False\n\n    def __init__(self, seed: int = 42, **kwargs: Any) -&gt; None:\n        \"\"\"The init function. This is called primarily to set the seed.\n\n        Args:\n            seed: An integer seed.\n            kwargs: arguments passed to subclasses.\n        \"\"\"\n        self.seed = seed\n        self.rng_state, self.np_rng = _set_seed(seed)\n        self.hf_subsets = self.metadata.hf_subsets\n\n    def check_if_dataset_is_superseded(self) -&gt; None:\n        \"\"\"Check if the dataset is superseded by a newer version.\"\"\"\n        if self.superseded_by:\n            msg = f\"Dataset '{self.metadata.name}' is superseded by '{self.superseded_by}'. We recommend using the newer version of the dataset unless you are running a specific benchmark. See `get_task('{self.superseded_by}').metadata.description` to get a description of the task and changes.\"\n            logger.warning(msg)\n            warnings.warn(msg)\n\n    def dataset_transform(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n        \"\"\"A transform operations applied to the dataset after loading.\n\n        This method is useful when the dataset from Huggingface is not in an `mteb` compatible format.\n        Override this method if your dataset requires additional transformation.\n\n        Args:\n            num_proc: Number of processes to use for the transformation.\n            kwargs: Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.\n        \"\"\"\n        pass\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Mapping[HFSubset, ScoresDict]:\n        \"\"\"Evaluates an MTEB compatible model on the task.\n\n        Args:\n            model: MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings\n            split: Which split (e.g. *\"test\"*) to be used.\n            subsets_to_run: List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.\n            encode_kwargs: Additional keyword arguments that are passed to the model's `encode` method.\n            prediction_folder: Folder to save model predictions\n            num_proc: Number of processes to use for loading the dataset or processing.\n            kwargs: Additional keyword arguments that are passed to the _evaluate_subset method.\n\n        Returns:\n            A dictionary with the scores for each subset.\n\n        Raises:\n            TypeError: If the model is a CrossEncoder and the task does not support CrossEncoders.\n            TypeError: If the model is a SearchProtocol and the task does not support Search.\n        \"\"\"\n        if isinstance(model, CrossEncoderProtocol) and not self._support_cross_encoder:\n            raise TypeError(\n                f\"Model {model} is a CrossEncoder, but this task {self.metadata.name} does not support CrossEncoders. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        # encoders might implement search protocols\n        if (\n            isinstance(model, SearchProtocol)\n            and not isinstance(model, EncoderProtocol)\n            and not self._support_search\n        ):\n            raise TypeError(\n                f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        if not self.data_loaded:\n            self.load_data()\n\n        self.dataset = cast(\"dict[HFSubset, DatasetDict]\", self.dataset)\n\n        scores = {}\n        if self.hf_subsets is None:\n            hf_subsets = list(self.dataset.keys())\n        else:\n            hf_subsets = copy(self.hf_subsets)\n\n        if subsets_to_run is not None:  # allow overwrites of pre-filtering\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Running task {self.metadata.name} ({split=}, {hf_subset=})...\"\n            )\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                data_split = self.dataset[split]\n            else:\n                data_split = self.dataset[hf_subset][split]\n            scores[hf_subset] = self._evaluate_subset(\n                model,\n                data_split,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                num_proc=num_proc,\n                **kwargs,\n            )\n            self._add_main_score(scores[hf_subset])\n        return scores\n\n    @abstractmethod\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        raise NotImplementedError(\n            \"If you are using the default evaluate method, you must implement _evaluate_subset method.\"\n        )\n\n    def _save_task_predictions(\n        self,\n        predictions: Mapping[str, Any] | list[Any],\n        model: MTEBModels,\n        prediction_folder: Path,\n        hf_split: str,\n        hf_subset: str,\n    ) -&gt; None:\n        \"\"\"Saves the predictions of the model on the task to a json file.\n\n        Args:\n            predictions: Dictionary containing the predictions.\n            model: The model used to generate the predictions.\n            prediction_folder: The folder to save the predictions to.\n            hf_split: The split of the dataset (e.g. \"test\").\n            hf_subset: The subset of the dataset (e.g. \"en\").\n        \"\"\"\n        predictions_path = self._predictions_path(prediction_folder)\n        existing_results: dict[str, Any] = {\n            \"mteb_model_meta\": {\n                \"model_name\": model.mteb_model_meta.name,\n                \"revision\": model.mteb_model_meta.revision,\n            }\n        }\n        if predictions_path.exists():\n            with predictions_path.open(\"r\") as predictions_file:\n                existing_results = json.load(predictions_file)\n\n        if hf_subset not in existing_results:\n            existing_results[hf_subset] = {}\n\n        existing_results[hf_subset][hf_split] = predictions\n        with predictions_path.open(\"w\") as predictions_file:\n            json.dump(existing_results, predictions_file)\n\n    def _predictions_path(\n        self,\n        output_folder: Path | str,\n    ) -&gt; Path:\n        if isinstance(output_folder, str):\n            output_folder = Path(output_folder)\n\n        if not output_folder.exists():\n            output_folder.mkdir(parents=True, exist_ok=True)\n        return output_folder / self.prediction_file_name\n\n    @property\n    def prediction_file_name(self) -&gt; str:\n        \"\"\"The name of the prediction file in format {task_name}_predictions.json\"\"\"\n        return f\"{self.metadata.name}_predictions.json\"\n\n    @staticmethod\n    def stratified_subsampling(\n        dataset_dict: DatasetDict,\n        seed: int,\n        splits: list[str] = [\"test\"],\n        label: str = \"label\",\n        n_samples: int = 2048,\n    ) -&gt; DatasetDict:\n        \"\"\"Subsamples the dataset with stratification by the supplied label.\n\n        Args:\n            dataset_dict: the DatasetDict object.\n            seed: the random seed.\n            splits: the splits of the dataset.\n            label: the label with which the stratified sampling is based on.\n            n_samples: Optional, number of samples to subsample. Default is max_n_samples.\n\n        Returns:\n            A subsampled DatasetDict object.\n        \"\"\"\n        # Can only do this if the label column is of ClassLabel.\n        if not isinstance(dataset_dict[splits[0]].features[label], ClassLabel):\n            try:\n                dataset_dict = dataset_dict.class_encode_column(label)\n            except ValueError as e:\n                if isinstance(dataset_dict[splits[0]][label][0], Sequence):\n                    return _multilabel_subsampling(\n                        dataset_dict, seed, splits, label, n_samples\n                    )\n                else:\n                    raise e\n\n        for split in splits:\n            if n_samples &gt;= len(dataset_dict[split]):\n                logger.debug(\n                    f\"Subsampling not needed for split {split}, as n_samples is equal or greater than the number of samples.\"\n                )\n                continue\n            dataset_dict.update(\n                {\n                    split: dataset_dict[split].train_test_split(\n                        test_size=n_samples, seed=seed, stratify_by_column=label\n                    )[\"test\"]\n                }\n            )  # only take the specified test split.\n        return dataset_dict\n\n    def load_data(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n        \"\"\"Loads dataset from HuggingFace hub\n\n        This is the main loading function for Task. Do not overwrite this, instead we recommend using `dataset_transform`, which is called after the\n        dataset is loaded using `datasets.load_dataset`.\n\n        Args:\n            num_proc: Number of processes to use for loading the dataset.\n            kwargs: Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.\n        \"\"\"\n        if self.data_loaded:\n            return\n        if self.metadata.is_multilingual:\n            if self.fast_loading:\n                self.fast_load()\n            else:\n                self.dataset = {}\n                for hf_subset in self.hf_subsets:\n                    self.dataset[hf_subset] = load_dataset(\n                        name=hf_subset,\n                        **self.metadata.dataset,\n                        num_proc=num_proc,\n                    )\n        else:\n            # some of monolingual datasets explicitly adding the split name to the dataset name\n            self.dataset = load_dataset(**self.metadata.dataset, num_proc=num_proc)\n        self.dataset_transform(num_proc=num_proc)\n        self.data_loaded = True\n\n    def fast_load(self) -&gt; None:\n        \"\"\"**Deprecated**. Load all subsets at once, then group by language. Using fast loading has two requirements:\n\n        - Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair\n        - The datasets must have a 'default' config that loads all the subsets of the dataset (see more [here](https://huggingface.co/docs/datasets/en/repository_structure#configurations))\n        \"\"\"\n        self.dataset = {}\n        merged_dataset = load_dataset(**self.metadata.dataset)  # load \"default\" subset\n        for split in merged_dataset.keys():\n            df_split = merged_dataset[split].to_polars()\n            df_grouped = dict(df_split.group_by([\"lang\"]))\n            for lang in set(df_split[\"lang\"].unique()) &amp; set(self.hf_subsets):\n                self.dataset.setdefault(lang, {})\n                self.dataset[lang][split] = Dataset.from_polars(\n                    df_grouped[(lang,)].drop(\"lang\")\n                )  # Remove lang column and convert back to HF datasets, not strictly necessary but better for compatibility\n        for lang, subset in self.dataset.items():\n            self.dataset[lang] = DatasetDict(subset)\n\n    def calculate_descriptive_statistics(\n        self,\n        overwrite_results: bool = False,\n        num_proc: int | None = None,\n    ) -&gt; dict[str, DescriptiveStatistics]:\n        \"\"\"Calculates descriptive statistics from the dataset.\n\n        Args:\n            overwrite_results: Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.\n            num_proc: Number of processes to use for loading the dataset.\n\n        Returns:\n            A dictionary containing descriptive statistics for each split.\n        \"\"\"\n        from mteb.abstasks import AbsTaskClassification\n\n        existing_stats = self.metadata.descriptive_stats\n\n        if existing_stats is not None and not overwrite_results:\n            logger.info(\"Loading metadata descriptive statistics from cache.\")\n            return existing_stats\n\n        if not self.data_loaded:\n            self.load_data(num_proc=num_proc)\n\n        descriptive_stats: dict[str, DescriptiveStatistics] = {}\n        hf_subset_stat: Literal[\"hf_subset_descriptive_stats\"] = (\n            \"hf_subset_descriptive_stats\"\n        )\n        eval_splits = self.metadata.eval_splits\n        if isinstance(self, AbsTaskClassification):\n            eval_splits.append(self.train_split)\n\n        pbar_split = tqdm(eval_splits, desc=\"Processing Splits...\")\n        for split in pbar_split:\n            pbar_split.set_postfix_str(f\"Split: {split}\")\n            logger.info(f\"Processing metadata for split {split}\")\n            if self.metadata.is_multilingual:\n                descriptive_stats[split] = (\n                    self._calculate_descriptive_statistics_from_split(  # type: ignore[assignment]\n                        split, compute_overall=True\n                    )\n                )\n                descriptive_stats[split][hf_subset_stat] = {}\n\n                pbar_subsets = tqdm(\n                    self.metadata.hf_subsets,\n                    desc=\"Processing Languages...\",\n                )\n                for hf_subset in pbar_subsets:\n                    pbar_subsets.set_postfix_str(f\"Huggingface subset: {hf_subset}\")\n                    logger.info(f\"Processing metadata for subset {hf_subset}\")\n                    split_details = self._calculate_descriptive_statistics_from_split(\n                        split, hf_subset\n                    )\n                    descriptive_stats[split][hf_subset_stat][hf_subset] = split_details\n            else:\n                split_details = self._calculate_descriptive_statistics_from_split(split)\n                descriptive_stats[split] = split_details  # type: ignore[assignment]\n\n        with self.metadata.descriptive_stat_path.open(\"w\") as f:\n            json.dump(descriptive_stats, f, indent=4)\n\n        return descriptive_stats\n\n    def calculate_metadata_metrics(\n        self, overwrite_results: bool = False\n    ) -&gt; dict[str, DescriptiveStatistics]:\n        \"\"\"Old name of `calculate_descriptive_statistics`, kept for backward compatibility.\"\"\"\n        return self.calculate_descriptive_statistics(\n            overwrite_results=overwrite_results\n        )\n\n    @abstractmethod\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; SplitDescriptiveStatistics:\n        raise NotImplementedError\n\n    @property\n    def languages(self) -&gt; list[str]:\n        \"\"\"Returns the languages of the task.\"\"\"\n        if self.hf_subsets:\n            eval_langs = self.metadata.hf_subsets_to_langscripts\n            languages = []\n\n            for lang in self.hf_subsets:\n                for langscript in eval_langs[lang]:\n                    iso_lang, script = langscript.split(\"-\")\n                    languages.append(iso_lang)\n\n            return sorted(set(languages))\n\n        return self.metadata.languages\n\n    def filter_eval_splits(self, eval_splits: Sequence[str] | None) -&gt; Self:\n        \"\"\"Filter the evaluation splits of the task.\n\n        Args:\n            eval_splits: A list of evaluation splits to keep. If None, all splits are kept.\n\n        Returns:\n            The filtered task\n        \"\"\"\n        self._eval_splits = eval_splits\n        return self\n\n    def filter_languages(\n        self,\n        languages: Sequence[str] | None,\n        script: Sequence[str] | None = None,\n        hf_subsets: Sequence[HFSubset] | None = None,\n        exclusive_language_filter: bool = False,\n    ) -&gt; Self:\n        \"\"\"Filter the languages of the task.\n\n        Args:\n            languages: list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script\n                (e.g. \"eng-Latn\")\n            script: A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included.\n                If the language code does not specify the script the intersection of the language and script will be used.\n            hf_subsets: A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language,\n                but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.\n            exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n                exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n                specified will be kept.\n\n        Returns:\n            The filtered task\n        \"\"\"\n        lang_scripts = LanguageScripts.from_languages_and_scripts(languages, script)\n\n        subsets_to_keep = []\n\n        for hf_subset, langs in self.metadata.hf_subsets_to_langscripts.items():\n            if (hf_subsets is not None) and (hf_subset not in hf_subsets):\n                continue\n            if exclusive_language_filter is False:\n                for langscript in langs:\n                    if lang_scripts.contains_language(\n                        langscript\n                    ) or lang_scripts.contains_script(langscript):\n                        subsets_to_keep.append(hf_subset)\n                        break\n\n            if exclusive_language_filter is True and languages:\n                if lang_scripts.contains_languages(langs):\n                    subsets_to_keep.append(hf_subset)\n\n        if len(subsets_to_keep) == 0:\n            raise ValueError(\n                f\"No subsets were found for {self.metadata.name} with filters: language code {languages}, script {script}, hf subsets {hf_subsets}.\"\n            )\n\n        self.hf_subsets = subsets_to_keep\n        return self\n\n    def _add_main_score(self, scores: ScoresDict) -&gt; None:\n        scores[\"main_score\"] = scores[self.metadata.main_score]\n\n    def _upload_dataset_to_hub(\n        self,\n        repo_name: str,\n        fields: list[str] | dict[str, str],\n        num_proc: int | None = None,\n    ) -&gt; None:\n        if self.dataset is None:\n            raise ValueError(\"Dataset not loaded\")\n        if self.metadata.is_multilingual:\n            for config in self.metadata.eval_langs:\n                logger.info(f\"Converting {config} of {self.metadata.name}\")\n                sentences = {}\n                for split in self.dataset[config]:\n                    if isinstance(fields, dict):\n                        sentences[split] = Dataset.from_dict(\n                            {\n                                mapped_name: self.dataset[config][split][original_name]\n                                for original_name, mapped_name in fields.items()\n                            }\n                        )\n                    else:\n                        sentences[split] = Dataset.from_dict(\n                            {\n                                field: self.dataset[config][split][field]\n                                for field in fields\n                            }\n                        )\n                sentences = DatasetDict(sentences)\n                sentences.push_to_hub(\n                    repo_name,\n                    config,\n                    commit_message=f\"Add {config} dataset\",\n                    num_proc=num_proc,\n                )\n        else:\n            sentences = {}\n            for split in self.dataset:\n                if isinstance(fields, dict):\n                    sentences[split] = Dataset.from_dict(\n                        {\n                            mapped_name: self.dataset[split][original_name]\n                            for original_name, mapped_name in fields.items()\n                        }\n                    )\n                else:\n                    sentences[split] = Dataset.from_dict(\n                        {field: self.dataset[split][field] for field in fields}\n                    )\n            sentences = DatasetDict(sentences)\n            sentences.push_to_hub(\n                repo_name, commit_message=\"Add dataset\", num_proc=num_proc\n            )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        raise NotImplementedError\n\n    def push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n        *,\n        push_eval: bool = False,\n    ) -&gt; None:\n        \"\"\"Push the dataset to the HuggingFace Hub.\n\n        Args:\n            repo_name: The name of the repository to push the dataset to.\n            num_proc: Number of processes to use for loading the dataset.\n            push_eval: Whether to also push the eval.yaml file to the Hub\n\n        Examples:\n            &gt;&gt;&gt; import mteb\n            &gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n            &gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n            &gt;&gt;&gt; # Push the dataset to the Hub\n            &gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n        \"\"\"\n        if not self.data_loaded:\n            self.load_data()\n\n        self._push_dataset_to_hub(repo_name, num_proc)\n        # dataset repo not creating when pushing card\n        self.metadata.push_dataset_card_to_hub(repo_name)\n        if push_eval:\n            self.push_eval_to_hub(repo_name)\n\n    def push_eval_to_hub(\n        self,\n        repo_name: str,\n        *,\n        create_pr: bool = False,\n    ) -&gt; None:\n        \"\"\"Push `eval.yaml` to the HuggingFace Hub\n\n        Args:\n            repo_name: repository name\n            create_pr: Whether to create the PR\n        \"\"\"\n        eval_file_name = \"eval.yaml\"\n\n        existing_eval_path = _get_file_on_hub(\n            repo_id=repo_name,\n            file_name=eval_file_name,\n            repo_type=\"dataset\",\n        )\n\n        # handle multiple tasks in one repo (e.g. MIRACLRetrievalHardNegatives, MIRACLRetrievalHardNegativesV2)\n        existing_eval = None\n        if existing_eval_path is not None:\n            with Path(existing_eval_path).open() as f:\n                existing_eval_dict = yaml.safe_load(f)\n            if existing_eval_dict is not None:\n                existing_eval = HFEvalMeta.model_validate(existing_eval_dict)\n\n        task_config = self._create_task_hf_config(existing_eval)\n\n        with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_file:\n            tmp_file.write(task_config.to_yaml())\n            tmp_file.flush()\n\n            huggingface_hub.upload_file(\n                path_or_fileobj=tmp_file.name,\n                path_in_repo=eval_file_name,\n                repo_id=repo_name,\n                repo_type=\"dataset\",\n                commit_message=\"Add eval config\",\n                create_pr=create_pr,\n            )\n\n    def _create_task_hf_config(\n        self, existing_eval: HFEvalMeta | None = None\n    ) -&gt; HFEvalMeta:\n        eval_task_config = [\n            # scores across all subsets and splits\n            HFEvalTaskConfig(\n                id=self.metadata.name,\n                split=None,\n                config=None,\n            )\n        ]\n\n        for subset in self.metadata.hf_subsets:\n            for split in self.metadata.eval_splits:\n                eval_task_config.append(\n                    HFEvalTaskConfig(\n                        id=f\"{self.metadata.name}_{subset}_{split}\",\n                        config=subset,\n                        split=split,\n                    )\n                )\n\n        task_config = HFEvalMeta(\n            name=self.metadata.name,\n            description=self.metadata.description,\n            tasks=eval_task_config,\n        )\n        if existing_eval is not None:\n            task_config = task_config.merge(existing_eval)\n        return task_config\n\n    @property\n    def is_aggregate(self) -&gt; bool:\n        \"\"\"Whether the task is an aggregate of multiple tasks.\"\"\"\n        return False\n\n    @property\n    def eval_splits(self) -&gt; Sequence[str]:\n        \"\"\"Returns the evaluation splits of the task.\"\"\"\n        if self._eval_splits:\n            return self._eval_splits\n        return self.metadata.eval_splits\n\n    @property\n    def modalities(self) -&gt; list[Modalities]:\n        \"\"\"Returns the modalities of the task.\"\"\"\n        return self.metadata.modalities\n\n    def __repr__(self) -&gt; str:\n        # Format the representation of the task such that it appears as:\n        # TaskObjectName(name='{name}', languages={lang1, lang2, ...})\n\n        langs = self.languages\n        if len(langs) &gt; 3:\n            langs = langs[:3]\n            langs.append(\"...\")\n        return (\n            f\"{self.__class__.__name__}(name='{self.metadata.name}', languages={langs})\"\n        )\n\n    def __hash__(self) -&gt; int:\n        return hash(self.metadata)\n\n    def unload_data(self) -&gt; None:\n        \"\"\"Unloads the dataset from memory\"\"\"\n        if self.data_loaded:\n            self.dataset = None\n            self.data_loaded = False\n            logger.info(f\"Unloaded dataset {self.metadata.name} from memory.\")\n        else:\n            msg = f\"Dataset `{self.metadata.name}` is not loaded, cannot unload it.\"\n            logger.warning(msg)\n\n    @property\n    def superseded_by(self) -&gt; str | None:\n        \"\"\"If the dataset is superseded by another dataset, return the name of the new dataset.\"\"\"\n        return self.metadata.superseded_by\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.eval_splits","title":"<code>eval_splits</code>  <code>property</code>","text":"<p>Returns the evaluation splits of the task.</p>"},{"location":"api/task/#mteb.AbsTask.is_aggregate","title":"<code>is_aggregate</code>  <code>property</code>","text":"<p>Whether the task is an aggregate of multiple tasks.</p>"},{"location":"api/task/#mteb.AbsTask.languages","title":"<code>languages</code>  <code>property</code>","text":"<p>Returns the languages of the task.</p>"},{"location":"api/task/#mteb.AbsTask.modalities","title":"<code>modalities</code>  <code>property</code>","text":"<p>Returns the modalities of the task.</p>"},{"location":"api/task/#mteb.AbsTask.prediction_file_name","title":"<code>prediction_file_name</code>  <code>property</code>","text":"<p>The name of the prediction file in format {task_name}_predictions.json</p>"},{"location":"api/task/#mteb.AbsTask.superseded_by","title":"<code>superseded_by</code>  <code>property</code>","text":"<p>If the dataset is superseded by another dataset, return the name of the new dataset.</p>"},{"location":"api/task/#mteb.AbsTask.__init__","title":"<code>__init__(seed=42, **kwargs)</code>","text":"<p>The init function. This is called primarily to set the seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>An integer seed.</p> <code>42</code> <code>kwargs</code> <code>Any</code> <p>arguments passed to subclasses.</p> <code>{}</code> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def __init__(self, seed: int = 42, **kwargs: Any) -&gt; None:\n    \"\"\"The init function. This is called primarily to set the seed.\n\n    Args:\n        seed: An integer seed.\n        kwargs: arguments passed to subclasses.\n    \"\"\"\n    self.seed = seed\n    self.rng_state, self.np_rng = _set_seed(seed)\n    self.hf_subsets = self.metadata.hf_subsets\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.calculate_descriptive_statistics","title":"<code>calculate_descriptive_statistics(overwrite_results=False, num_proc=None)</code>","text":"<p>Calculates descriptive statistics from the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite_results</code> <code>bool</code> <p>Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.</p> <code>False</code> <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for loading the dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, DescriptiveStatistics]</code> <p>A dictionary containing descriptive statistics for each split.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def calculate_descriptive_statistics(\n    self,\n    overwrite_results: bool = False,\n    num_proc: int | None = None,\n) -&gt; dict[str, DescriptiveStatistics]:\n    \"\"\"Calculates descriptive statistics from the dataset.\n\n    Args:\n        overwrite_results: Whether to overwrite existing results. If False and results already exist, the existing results will be loaded from cache.\n        num_proc: Number of processes to use for loading the dataset.\n\n    Returns:\n        A dictionary containing descriptive statistics for each split.\n    \"\"\"\n    from mteb.abstasks import AbsTaskClassification\n\n    existing_stats = self.metadata.descriptive_stats\n\n    if existing_stats is not None and not overwrite_results:\n        logger.info(\"Loading metadata descriptive statistics from cache.\")\n        return existing_stats\n\n    if not self.data_loaded:\n        self.load_data(num_proc=num_proc)\n\n    descriptive_stats: dict[str, DescriptiveStatistics] = {}\n    hf_subset_stat: Literal[\"hf_subset_descriptive_stats\"] = (\n        \"hf_subset_descriptive_stats\"\n    )\n    eval_splits = self.metadata.eval_splits\n    if isinstance(self, AbsTaskClassification):\n        eval_splits.append(self.train_split)\n\n    pbar_split = tqdm(eval_splits, desc=\"Processing Splits...\")\n    for split in pbar_split:\n        pbar_split.set_postfix_str(f\"Split: {split}\")\n        logger.info(f\"Processing metadata for split {split}\")\n        if self.metadata.is_multilingual:\n            descriptive_stats[split] = (\n                self._calculate_descriptive_statistics_from_split(  # type: ignore[assignment]\n                    split, compute_overall=True\n                )\n            )\n            descriptive_stats[split][hf_subset_stat] = {}\n\n            pbar_subsets = tqdm(\n                self.metadata.hf_subsets,\n                desc=\"Processing Languages...\",\n            )\n            for hf_subset in pbar_subsets:\n                pbar_subsets.set_postfix_str(f\"Huggingface subset: {hf_subset}\")\n                logger.info(f\"Processing metadata for subset {hf_subset}\")\n                split_details = self._calculate_descriptive_statistics_from_split(\n                    split, hf_subset\n                )\n                descriptive_stats[split][hf_subset_stat][hf_subset] = split_details\n        else:\n            split_details = self._calculate_descriptive_statistics_from_split(split)\n            descriptive_stats[split] = split_details  # type: ignore[assignment]\n\n    with self.metadata.descriptive_stat_path.open(\"w\") as f:\n        json.dump(descriptive_stats, f, indent=4)\n\n    return descriptive_stats\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.calculate_metadata_metrics","title":"<code>calculate_metadata_metrics(overwrite_results=False)</code>","text":"<p>Old name of <code>calculate_descriptive_statistics</code>, kept for backward compatibility.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def calculate_metadata_metrics(\n    self, overwrite_results: bool = False\n) -&gt; dict[str, DescriptiveStatistics]:\n    \"\"\"Old name of `calculate_descriptive_statistics`, kept for backward compatibility.\"\"\"\n    return self.calculate_descriptive_statistics(\n        overwrite_results=overwrite_results\n    )\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.check_if_dataset_is_superseded","title":"<code>check_if_dataset_is_superseded()</code>","text":"<p>Check if the dataset is superseded by a newer version.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def check_if_dataset_is_superseded(self) -&gt; None:\n    \"\"\"Check if the dataset is superseded by a newer version.\"\"\"\n    if self.superseded_by:\n        msg = f\"Dataset '{self.metadata.name}' is superseded by '{self.superseded_by}'. We recommend using the newer version of the dataset unless you are running a specific benchmark. See `get_task('{self.superseded_by}').metadata.description` to get a description of the task and changes.\"\n        logger.warning(msg)\n        warnings.warn(msg)\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.dataset_transform","title":"<code>dataset_transform(num_proc=None, **kwargs)</code>","text":"<p>A transform operations applied to the dataset after loading.</p> <p>This method is useful when the dataset from Huggingface is not in an <code>mteb</code> compatible format. Override this method if your dataset requires additional transformation.</p> <p>Parameters:</p> Name Type Description Default <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for the transformation.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.</p> <code>{}</code> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def dataset_transform(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n    \"\"\"A transform operations applied to the dataset after loading.\n\n    This method is useful when the dataset from Huggingface is not in an `mteb` compatible format.\n    Override this method if your dataset requires additional transformation.\n\n    Args:\n        num_proc: Number of processes to use for the transformation.\n        kwargs: Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, num_proc=None, **kwargs)</code>","text":"<p>Evaluates an MTEB compatible model on the task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MTEBModels</code> <p>MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings</p> required <code>split</code> <code>str</code> <p>Which split (e.g. \"test\") to be used.</p> <code>'test'</code> <code>subsets_to_run</code> <code>list[HFSubset] | None</code> <p>List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.</p> <code>None</code> <code>encode_kwargs</code> <code>EncodeKwargs</code> <p>Additional keyword arguments that are passed to the model's <code>encode</code> method.</p> required <code>prediction_folder</code> <code>Path | None</code> <p>Folder to save model predictions</p> <code>None</code> <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for loading the dataset or processing.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments that are passed to the _evaluate_subset method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[HFSubset, ScoresDict]</code> <p>A dictionary with the scores for each subset.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the model is a CrossEncoder and the task does not support CrossEncoders.</p> <code>TypeError</code> <p>If the model is a SearchProtocol and the task does not support Search.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: EncodeKwargs,\n    prediction_folder: Path | None = None,\n    num_proc: int | None = None,\n    **kwargs: Any,\n) -&gt; Mapping[HFSubset, ScoresDict]:\n    \"\"\"Evaluates an MTEB compatible model on the task.\n\n    Args:\n        model: MTEB compatible model. Implements a encode(sentences) method, that encodes sentences and returns an array of embeddings\n        split: Which split (e.g. *\"test\"*) to be used.\n        subsets_to_run: List of huggingface subsets (HFSubsets) to evaluate. If None, all subsets are evaluated.\n        encode_kwargs: Additional keyword arguments that are passed to the model's `encode` method.\n        prediction_folder: Folder to save model predictions\n        num_proc: Number of processes to use for loading the dataset or processing.\n        kwargs: Additional keyword arguments that are passed to the _evaluate_subset method.\n\n    Returns:\n        A dictionary with the scores for each subset.\n\n    Raises:\n        TypeError: If the model is a CrossEncoder and the task does not support CrossEncoders.\n        TypeError: If the model is a SearchProtocol and the task does not support Search.\n    \"\"\"\n    if isinstance(model, CrossEncoderProtocol) and not self._support_cross_encoder:\n        raise TypeError(\n            f\"Model {model} is a CrossEncoder, but this task {self.metadata.name} does not support CrossEncoders. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    # encoders might implement search protocols\n    if (\n        isinstance(model, SearchProtocol)\n        and not isinstance(model, EncoderProtocol)\n        and not self._support_search\n    ):\n        raise TypeError(\n            f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    if not self.data_loaded:\n        self.load_data()\n\n    self.dataset = cast(\"dict[HFSubset, DatasetDict]\", self.dataset)\n\n    scores = {}\n    if self.hf_subsets is None:\n        hf_subsets = list(self.dataset.keys())\n    else:\n        hf_subsets = copy(self.hf_subsets)\n\n    if subsets_to_run is not None:  # allow overwrites of pre-filtering\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    for hf_subset in hf_subsets:\n        logger.info(\n            f\"Running task {self.metadata.name} ({split=}, {hf_subset=})...\"\n        )\n        if hf_subset not in self.dataset and hf_subset == \"default\":\n            data_split = self.dataset[split]\n        else:\n            data_split = self.dataset[hf_subset][split]\n        scores[hf_subset] = self._evaluate_subset(\n            model,\n            data_split,\n            hf_split=split,\n            hf_subset=hf_subset,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            num_proc=num_proc,\n            **kwargs,\n        )\n        self._add_main_score(scores[hf_subset])\n    return scores\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.fast_load","title":"<code>fast_load()</code>","text":"<p>Deprecated. Load all subsets at once, then group by language. Using fast loading has two requirements:</p> <ul> <li>Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair</li> <li>The datasets must have a 'default' config that loads all the subsets of the dataset (see more here)</li> </ul> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def fast_load(self) -&gt; None:\n    \"\"\"**Deprecated**. Load all subsets at once, then group by language. Using fast loading has two requirements:\n\n    - Each row in the dataset should have a 'lang' feature giving the corresponding language/language pair\n    - The datasets must have a 'default' config that loads all the subsets of the dataset (see more [here](https://huggingface.co/docs/datasets/en/repository_structure#configurations))\n    \"\"\"\n    self.dataset = {}\n    merged_dataset = load_dataset(**self.metadata.dataset)  # load \"default\" subset\n    for split in merged_dataset.keys():\n        df_split = merged_dataset[split].to_polars()\n        df_grouped = dict(df_split.group_by([\"lang\"]))\n        for lang in set(df_split[\"lang\"].unique()) &amp; set(self.hf_subsets):\n            self.dataset.setdefault(lang, {})\n            self.dataset[lang][split] = Dataset.from_polars(\n                df_grouped[(lang,)].drop(\"lang\")\n            )  # Remove lang column and convert back to HF datasets, not strictly necessary but better for compatibility\n    for lang, subset in self.dataset.items():\n        self.dataset[lang] = DatasetDict(subset)\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.filter_eval_splits","title":"<code>filter_eval_splits(eval_splits)</code>","text":"<p>Filter the evaluation splits of the task.</p> <p>Parameters:</p> Name Type Description Default <code>eval_splits</code> <code>Sequence[str] | None</code> <p>A list of evaluation splits to keep. If None, all splits are kept.</p> required <p>Returns:</p> Type Description <code>Self</code> <p>The filtered task</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def filter_eval_splits(self, eval_splits: Sequence[str] | None) -&gt; Self:\n    \"\"\"Filter the evaluation splits of the task.\n\n    Args:\n        eval_splits: A list of evaluation splits to keep. If None, all splits are kept.\n\n    Returns:\n        The filtered task\n    \"\"\"\n    self._eval_splits = eval_splits\n    return self\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.filter_languages","title":"<code>filter_languages(languages, script=None, hf_subsets=None, exclusive_language_filter=False)</code>","text":"<p>Filter the languages of the task.</p> <p>Parameters:</p> Name Type Description Default <code>languages</code> <code>Sequence[str] | None</code> <p>list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script (e.g. \"eng-Latn\")</p> required <code>script</code> <code>Sequence[str] | None</code> <p>A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included. If the language code does not specify the script the intersection of the language and script will be used.</p> <code>None</code> <code>hf_subsets</code> <code>Sequence[HFSubset] | None</code> <p>A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language, but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.</p> <code>None</code> <code>exclusive_language_filter</code> <code>bool</code> <p>Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages specified will be kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>Self</code> <p>The filtered task</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def filter_languages(\n    self,\n    languages: Sequence[str] | None,\n    script: Sequence[str] | None = None,\n    hf_subsets: Sequence[HFSubset] | None = None,\n    exclusive_language_filter: bool = False,\n) -&gt; Self:\n    \"\"\"Filter the languages of the task.\n\n    Args:\n        languages: list of languages to filter the task by can be either a 3-letter language code (e.g. \"eng\") or also include the script\n            (e.g. \"eng-Latn\")\n        script: A list of scripts to filter the task by. Will be ignored if language code specified the script. If None, all scripts are included.\n            If the language code does not specify the script the intersection of the language and script will be used.\n        hf_subsets: A list of huggingface subsets to filter on. This is useful if a dataset have multiple subsets containing the desired language,\n            but you only want to test on one. An example is STS22 which e.g. have both \"en\" and \"de-en\" which both contains English.\n        exclusive_language_filter: Some datasets contains more than one language e.g. for STS22 the subset \"de-en\" contain eng and deu. If\n            exclusive_language_filter is set to False both of these will be kept, but if set to True only those that contains all the languages\n            specified will be kept.\n\n    Returns:\n        The filtered task\n    \"\"\"\n    lang_scripts = LanguageScripts.from_languages_and_scripts(languages, script)\n\n    subsets_to_keep = []\n\n    for hf_subset, langs in self.metadata.hf_subsets_to_langscripts.items():\n        if (hf_subsets is not None) and (hf_subset not in hf_subsets):\n            continue\n        if exclusive_language_filter is False:\n            for langscript in langs:\n                if lang_scripts.contains_language(\n                    langscript\n                ) or lang_scripts.contains_script(langscript):\n                    subsets_to_keep.append(hf_subset)\n                    break\n\n        if exclusive_language_filter is True and languages:\n            if lang_scripts.contains_languages(langs):\n                subsets_to_keep.append(hf_subset)\n\n    if len(subsets_to_keep) == 0:\n        raise ValueError(\n            f\"No subsets were found for {self.metadata.name} with filters: language code {languages}, script {script}, hf subsets {hf_subsets}.\"\n        )\n\n    self.hf_subsets = subsets_to_keep\n    return self\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.load_data","title":"<code>load_data(num_proc=None, **kwargs)</code>","text":"<p>Loads dataset from HuggingFace hub</p> <p>This is the main loading function for Task. Do not overwrite this, instead we recommend using <code>dataset_transform</code>, which is called after the dataset is loaded using <code>datasets.load_dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for loading the dataset.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.</p> <code>{}</code> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def load_data(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n    \"\"\"Loads dataset from HuggingFace hub\n\n    This is the main loading function for Task. Do not overwrite this, instead we recommend using `dataset_transform`, which is called after the\n    dataset is loaded using `datasets.load_dataset`.\n\n    Args:\n        num_proc: Number of processes to use for loading the dataset.\n        kwargs: Additional keyword arguments passed to the load_dataset function. Keep for forward compatibility.\n    \"\"\"\n    if self.data_loaded:\n        return\n    if self.metadata.is_multilingual:\n        if self.fast_loading:\n            self.fast_load()\n        else:\n            self.dataset = {}\n            for hf_subset in self.hf_subsets:\n                self.dataset[hf_subset] = load_dataset(\n                    name=hf_subset,\n                    **self.metadata.dataset,\n                    num_proc=num_proc,\n                )\n    else:\n        # some of monolingual datasets explicitly adding the split name to the dataset name\n        self.dataset = load_dataset(**self.metadata.dataset, num_proc=num_proc)\n    self.dataset_transform(num_proc=num_proc)\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.push_dataset_to_hub","title":"<code>push_dataset_to_hub(repo_name, num_proc=None, *, push_eval=False)</code>","text":"<p>Push the dataset to the HuggingFace Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_name</code> <code>str</code> <p>The name of the repository to push the dataset to.</p> required <code>num_proc</code> <code>int | None</code> <p>Number of processes to use for loading the dataset.</p> <code>None</code> <code>push_eval</code> <code>bool</code> <p>Whether to also push the eval.yaml file to the Hub</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import mteb\n&gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n&gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n&gt;&gt;&gt; # Push the dataset to the Hub\n&gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n</code></pre> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def push_dataset_to_hub(\n    self,\n    repo_name: str,\n    num_proc: int | None = None,\n    *,\n    push_eval: bool = False,\n) -&gt; None:\n    \"\"\"Push the dataset to the HuggingFace Hub.\n\n    Args:\n        repo_name: The name of the repository to push the dataset to.\n        num_proc: Number of processes to use for loading the dataset.\n        push_eval: Whether to also push the eval.yaml file to the Hub\n\n    Examples:\n        &gt;&gt;&gt; import mteb\n        &gt;&gt;&gt; task = mteb.get_task(\"Caltech101\")\n        &gt;&gt;&gt; repo_name = f\"myorg/{task.metadata.name}\"\n        &gt;&gt;&gt; # Push the dataset to the Hub\n        &gt;&gt;&gt; task.push_dataset_to_hub(repo_name)\n    \"\"\"\n    if not self.data_loaded:\n        self.load_data()\n\n    self._push_dataset_to_hub(repo_name, num_proc)\n    # dataset repo not creating when pushing card\n    self.metadata.push_dataset_card_to_hub(repo_name)\n    if push_eval:\n        self.push_eval_to_hub(repo_name)\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.push_eval_to_hub","title":"<code>push_eval_to_hub(repo_name, *, create_pr=False)</code>","text":"<p>Push <code>eval.yaml</code> to the HuggingFace Hub</p> <p>Parameters:</p> Name Type Description Default <code>repo_name</code> <code>str</code> <p>repository name</p> required <code>create_pr</code> <code>bool</code> <p>Whether to create the PR</p> <code>False</code> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def push_eval_to_hub(\n    self,\n    repo_name: str,\n    *,\n    create_pr: bool = False,\n) -&gt; None:\n    \"\"\"Push `eval.yaml` to the HuggingFace Hub\n\n    Args:\n        repo_name: repository name\n        create_pr: Whether to create the PR\n    \"\"\"\n    eval_file_name = \"eval.yaml\"\n\n    existing_eval_path = _get_file_on_hub(\n        repo_id=repo_name,\n        file_name=eval_file_name,\n        repo_type=\"dataset\",\n    )\n\n    # handle multiple tasks in one repo (e.g. MIRACLRetrievalHardNegatives, MIRACLRetrievalHardNegativesV2)\n    existing_eval = None\n    if existing_eval_path is not None:\n        with Path(existing_eval_path).open() as f:\n            existing_eval_dict = yaml.safe_load(f)\n        if existing_eval_dict is not None:\n            existing_eval = HFEvalMeta.model_validate(existing_eval_dict)\n\n    task_config = self._create_task_hf_config(existing_eval)\n\n    with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_file:\n        tmp_file.write(task_config.to_yaml())\n        tmp_file.flush()\n\n        huggingface_hub.upload_file(\n            path_or_fileobj=tmp_file.name,\n            path_in_repo=eval_file_name,\n            repo_id=repo_name,\n            repo_type=\"dataset\",\n            commit_message=\"Add eval config\",\n            create_pr=create_pr,\n        )\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.stratified_subsampling","title":"<code>stratified_subsampling(dataset_dict, seed, splits=['test'], label='label', n_samples=2048)</code>  <code>staticmethod</code>","text":"<p>Subsamples the dataset with stratification by the supplied label.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DatasetDict</code> <p>the DatasetDict object.</p> required <code>seed</code> <code>int</code> <p>the random seed.</p> required <code>splits</code> <code>list[str]</code> <p>the splits of the dataset.</p> <code>['test']</code> <code>label</code> <code>str</code> <p>the label with which the stratified sampling is based on.</p> <code>'label'</code> <code>n_samples</code> <code>int</code> <p>Optional, number of samples to subsample. Default is max_n_samples.</p> <code>2048</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A subsampled DatasetDict object.</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>@staticmethod\ndef stratified_subsampling(\n    dataset_dict: DatasetDict,\n    seed: int,\n    splits: list[str] = [\"test\"],\n    label: str = \"label\",\n    n_samples: int = 2048,\n) -&gt; DatasetDict:\n    \"\"\"Subsamples the dataset with stratification by the supplied label.\n\n    Args:\n        dataset_dict: the DatasetDict object.\n        seed: the random seed.\n        splits: the splits of the dataset.\n        label: the label with which the stratified sampling is based on.\n        n_samples: Optional, number of samples to subsample. Default is max_n_samples.\n\n    Returns:\n        A subsampled DatasetDict object.\n    \"\"\"\n    # Can only do this if the label column is of ClassLabel.\n    if not isinstance(dataset_dict[splits[0]].features[label], ClassLabel):\n        try:\n            dataset_dict = dataset_dict.class_encode_column(label)\n        except ValueError as e:\n            if isinstance(dataset_dict[splits[0]][label][0], Sequence):\n                return _multilabel_subsampling(\n                    dataset_dict, seed, splits, label, n_samples\n                )\n            else:\n                raise e\n\n    for split in splits:\n        if n_samples &gt;= len(dataset_dict[split]):\n            logger.debug(\n                f\"Subsampling not needed for split {split}, as n_samples is equal or greater than the number of samples.\"\n            )\n            continue\n        dataset_dict.update(\n            {\n                split: dataset_dict[split].train_test_split(\n                    test_size=n_samples, seed=seed, stratify_by_column=label\n                )[\"test\"]\n            }\n        )  # only take the specified test split.\n    return dataset_dict\n</code></pre>"},{"location":"api/task/#mteb.AbsTask.unload_data","title":"<code>unload_data()</code>","text":"<p>Unloads the dataset from memory</p> Source code in <code>mteb/abstasks/abstask.py</code> <pre><code>def unload_data(self) -&gt; None:\n    \"\"\"Unloads the dataset from memory\"\"\"\n    if self.data_loaded:\n        self.dataset = None\n        self.data_loaded = False\n        logger.info(f\"Unloaded dataset {self.metadata.name} from memory.\")\n    else:\n        msg = f\"Dataset `{self.metadata.name}` is not loaded, cannot unload it.\"\n        logger.warning(msg)\n</code></pre>"},{"location":"api/task/#multimodal-tasks","title":"Multimodal Tasks","text":"<p>Tasks that support any modality (text, image, etc.) inherit from the following abstract class. Retrieval tasks support multimodal input (e.g. image + text queries and image corpus or vice versa).</p>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval","title":"<code>mteb.abstasks.retrieval.AbsTaskRetrieval</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>The class which retrieval tasks inherit from.</p> <p>A retrieval task consists of a corpus of documents, a set of queries, and a mapping of which documents are relevant for each query. The task is to retrieve the relevant documents for each query. The evaluation is done by indexing the corpus and then searching for each query. The retrieved documents are then compared to the relevant documents to calculate the evaluation scores.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[str, dict[str, RetrievalSplitData]]</code> <p>A nested dictionary where the first key is the subset (language or \"default\"),      the second key is the split (e.g., \"train\", \"test\"), and the value is a RetrievalSplitData object.</p> <code>ignore_identical_ids</code> <code>bool</code> <p>If True, identical IDs in queries and corpus are ignored during evaluation.</p> <code>k_values</code> <code>Sequence[int]</code> <p>A sequence of integers representing the k values for evaluation metrics.</p> <code>skip_first_result</code> <code>bool</code> <p>If True, the first result is skipped during evaluation</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>class AbsTaskRetrieval(AbsTask):\n    \"\"\"The class which retrieval tasks inherit from.\n\n    A retrieval task consists of a corpus of documents, a set of queries, and a mapping of which documents are relevant for each query.\n    The task is to retrieve the relevant documents for each query. The evaluation is done by indexing the corpus and then searching for each query.\n    The retrieved documents are then compared to the relevant documents to calculate the evaluation scores.\n\n\n    Attributes:\n        dataset: A nested dictionary where the first key is the subset (language or \"default\"),\n                 the second key is the split (e.g., \"train\", \"test\"), and the value is a RetrievalSplitData object.\n        ignore_identical_ids: If True, identical IDs in queries and corpus are ignored during evaluation.\n        k_values: A sequence of integers representing the k values for evaluation metrics.\n        skip_first_result: If True, the first result is skipped during evaluation\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    ignore_identical_ids: bool = False\n    abstask_prompt = \"Retrieve text based on user query.\"\n    k_values: Sequence[int] = (1, 3, 5, 10, 20, 100, 1000)\n    _top_k: int = max(k_values)\n    dataset: dict[str, dict[str, RetrievalSplitData]]\n    _support_cross_encoder: bool = True\n    _support_search: bool = True\n    _previous_results_model_meta: dict[str, Any] | None = None\n    skip_first_result: bool = False\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        empty_dataset = Dataset.from_dict({})\n        self.dataset = defaultdict(\n            lambda: defaultdict(\n                lambda: RetrievalSplitData(\n                    corpus=empty_dataset,\n                    queries=empty_dataset,\n                    relevant_docs={},\n                    top_ranked=None,\n                )\n            )\n        )\n\n    def convert_v1_dataset_format_to_v2(\n        self,\n        num_proc: int | None,\n    ) -&gt; None:\n        \"\"\"Convert dataset from v1 (from `self.queries`, `self.document`) format to v2 format (`self.dotaset`).\"\"\"\n        # check if dataset is `v1` version\n        if not hasattr(self, \"queries\"):\n            return\n        empty_dataset = Dataset.from_dict({})\n\n        self.dataset = defaultdict(\n            lambda: defaultdict(\n                lambda: RetrievalSplitData(\n                    corpus=empty_dataset,\n                    queries=empty_dataset,\n                    relevant_docs={},\n                    top_ranked=None,\n                )\n            )\n        )\n\n        def _process_split(\n            ds_queries: dict | Dataset, ds_corpus: dict | Dataset\n        ) -&gt; tuple[Dataset, Dataset]:\n            if isinstance(ds_queries, dict):\n                queries = Dataset.from_list(\n                    [{\"id\": k, \"text\": v} for k, v in ds_queries.items()]\n                )\n            elif isinstance(ds_queries, Dataset):\n                queries = ds_queries\n            else:\n                raise ValueError(f\"Can't convert queries of type {type(ds_queries)}\")\n\n            if isinstance(ds_corpus, dict):\n                corpus = Dataset.from_list(\n                    [\n                        {\n                            \"id\": k,\n                            \"text\": v if isinstance(v, str) else v[\"text\"],\n                            \"title\": v.get(\"title\", \"\") if isinstance(v, dict) else \"\",\n                        }\n                        for k, v in ds_corpus.items()\n                    ]\n                )\n            elif isinstance(ds_corpus, Dataset):\n                corpus = ds_corpus\n            else:\n                raise ValueError(f\"Can't convert corpus of type {type(ds_corpus)}\")\n            return queries, corpus\n\n        if self.metadata.is_multilingual:\n            for subset in self.queries:  # type: ignore[attr-defined]\n                for split in self.queries[subset]:  # type: ignore[attr-defined]\n                    queries = self.queries[subset][split]  # type: ignore[attr-defined]\n                    corpus = self.corpus[subset][split]  # type: ignore[attr-defined]\n\n                    (\n                        self.dataset[subset][split][\"queries\"],\n                        self.dataset[subset][split][\"corpus\"],\n                    ) = _process_split(queries, corpus)\n\n                    self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[  # type: ignore[attr-defined]\n                        subset\n                    ][split]\n                    if hasattr(self, \"instructions\"):\n                        instructions = self.instructions[subset][split]\n                        self.dataset[subset][split][\"queries\"] = (\n                            _combine_queries_with_instructions_datasets(\n                                self.dataset[subset][split][\"queries\"],\n                                instructions,\n                                num_proc,\n                            )\n                        )\n                    if hasattr(self, \"top_ranked\"):\n                        self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                            subset\n                        ][split]\n        else:\n            subset = \"default\"\n            for split in self.queries:  # type: ignore[attr-defined]\n                queries = self.queries[split]  # type: ignore[attr-defined]\n                corpus = self.corpus[split]  # type: ignore[attr-defined]\n                (\n                    self.dataset[subset][split][\"queries\"],\n                    self.dataset[subset][split][\"corpus\"],\n                ) = _process_split(queries, corpus)\n\n                self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[  # type: ignore[attr-defined]\n                    split\n                ].copy()\n                if hasattr(self, \"instructions\"):\n                    instructions = self.instructions[split]\n                    self.dataset[subset][split][\"queries\"] = (\n                        _combine_queries_with_instructions_datasets(\n                            self.dataset[subset][split][\"queries\"],\n                            instructions,\n                            num_proc,\n                        )\n                    )\n                if hasattr(self, \"top_ranked\") and self.top_ranked:\n                    self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                        split\n                    ].copy()\n\n        del self.queries  # type: ignore[attr-defined]\n        del self.corpus  # type: ignore[attr-defined]\n        del self.relevant_docs  # type: ignore[attr-defined]\n        if hasattr(self, \"instructions\"):\n            del self.instructions\n        if hasattr(self, \"top_ranked\"):\n            del self.top_ranked\n\n    def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n        \"\"\"Load the dataset for the retrieval task.\"\"\"\n        if self.data_loaded:\n            return\n\n        dataset_path = self.metadata.dataset[\"path\"]\n        eval_splits = self.eval_splits\n        trust_remote_code = self.metadata.dataset.get(\"trust_remote_code\", False)\n        revision = self.metadata.dataset[\"revision\"]\n\n        def _process_data(split: str, hf_subset: str = \"default\"):\n            \"\"\"Helper function to load and process data for a given split and language\"\"\"\n            logger.debug(\n                f\"Loading {split} split for {hf_subset} subset of {self.metadata.name}\"\n            )\n\n            self.dataset[hf_subset][split] = RetrievalDatasetLoader(\n                hf_repo=dataset_path,\n                revision=revision,\n                trust_remote_code=trust_remote_code,\n                split=split,\n                config=hf_subset,\n            ).load(\n                num_proc=num_proc,\n            )\n\n        if self.metadata.is_multilingual:\n            for lang in self.hf_subsets:\n                for split in eval_splits:\n                    _process_data(split, lang)\n        else:\n            for split in eval_splits:\n                _process_data(split)\n        self.dataset_transform(num_proc=num_proc)\n        self.data_loaded = True\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Mapping[HFSubset, ScoresDict]:\n        \"\"\"Evaluate the model on the retrieval task.\n\n        Args:\n            model: Model to evaluate. Model should implement the [SearchProtocol][mteb.models.models_protocols.SearchProtocol]\n                or be an [Encoder][mteb.models.models_protocols.EncoderProtocol] or [CrossEncoderProtocol][mteb.models.models_protocols.CrossEncoderProtocol].\n            split: Split to evaluate on\n            subsets_to_run: Optional list of subsets to evaluate on\n            encode_kwargs: Keyword arguments passed to the encoder\n            prediction_folder: Folder to save model predictions\n            num_proc: Number of processes to use\n            **kwargs: Additional keyword arguments passed to the evaluator\n\n        Returns:\n            Dictionary mapping subsets to their evaluation scores\n        \"\"\"\n        if not self.data_loaded:\n            self.load_data(num_proc=num_proc)\n        # TODO: convert all tasks directly https://github.com/embeddings-benchmark/mteb/issues/2030\n        self.convert_v1_dataset_format_to_v2(num_proc=num_proc)\n\n        return super().evaluate(\n            model,\n            split,\n            subsets_to_run,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            num_proc=num_proc,\n            **kwargs,\n        )\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: RetrievalSplitData,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs,\n    ) -&gt; ScoresDict:\n        \"\"\"Evaluate a model on a specific subset of the data.\n\n        Args:\n            model: Model to evaluate\n            data_split: Data split to evaluate on\n            encode_kwargs: Keyword arguments passed to the encoder\n            hf_split: Split to evaluate on\n            hf_subset: Subset to evaluate on\n            prediction_folder: Folder with results prediction\n            num_proc: Number of processes to use\n            **kwargs: Additional keyword arguments passed to the evaluator\n\n        Returns:\n            Dictionary of evaluation scores\n        \"\"\"\n        # ensure queries format (see #3030)\n        data_split[\"relevant_docs\"], data_split[\"queries\"] = (\n            _filter_queries_without_positives(\n                data_split[\"relevant_docs\"], data_split[\"queries\"]\n            )\n        )\n        retriever = RetrievalEvaluator(\n            corpus=data_split[\"corpus\"],\n            queries=data_split[\"queries\"],\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            top_ranked=data_split[\"top_ranked\"],\n            top_k=self._top_k,\n            **kwargs,\n        )\n\n        search_model: SearchProtocol\n\n        if isinstance(model, EncoderProtocol) and not isinstance(model, SearchProtocol):\n            search_model = SearchEncoderWrapper(model)\n        elif isinstance(model, CrossEncoderProtocol):\n            search_model = SearchCrossEncoderWrapper(model)\n        elif isinstance(model, SearchProtocol):\n            search_model = model\n        else:\n            raise TypeError(\n                f\"RetrievalEvaluator expects a SearchInterface, Encoder, or CrossEncoder, got {type(model)}\"\n            )\n\n        start_time = time()\n        results = retriever(\n            search_model,\n            encode_kwargs=encode_kwargs,\n            num_proc=num_proc,\n        )\n        end_time = time()\n        logger.debug(\n            f\"Running retrieval task - Time taken to retrieve: {end_time - start_time:.2f} seconds\"\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                results,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        logger.info(\"Running retrieval task - Evaluating retrieval scores...\")\n        (\n            all_scores,\n            ndcg,\n            _map,\n            recall,\n            precision,\n            naucs,\n            mrr,\n            naucs_mrr,\n            hit_rate,\n        ) = retriever.evaluate(\n            data_split[\"relevant_docs\"],\n            results,\n            self.k_values,\n            ignore_identical_ids=self.ignore_identical_ids,\n            skip_first_result=self.skip_first_result,\n        )\n        task_specific_scores = self.task_specific_scores(\n            all_scores,\n            data_split[\"relevant_docs\"],\n            results,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n        )\n        logger.info(\"Running retrieval task - Finished.\")\n        return make_score_dict(\n            ndcg,\n            _map,\n            recall,\n            precision,\n            mrr,\n            naucs,\n            naucs_mrr,\n            hit_rate,\n            task_specific_scores,\n            self._previous_results_model_meta,\n        )\n\n    def task_specific_scores(\n        self,\n        scores: dict[str, dict[str, float]],\n        qrels: RelevantDocumentsType,\n        results: dict[str, dict[str, float]],\n        hf_split: str,\n        hf_subset: str,\n    ) -&gt; dict[str, float]:\n        \"\"\"Calculate task specific scores. Override in subclass if needed.\n\n        Args:\n            scores: Dictionary of scores\n            qrels: Relevant documents\n            results: Retrieval results\n            hf_split: Split to evaluate on\n            hf_subset: Subset to evaluate on\n        \"\"\"\n        return {}\n\n    def _calculate_descriptive_statistics_from_split(\n        self,\n        split: str,\n        hf_subset: str | None = None,\n        compute_overall: bool = False,\n        num_proc: int | None = None,\n    ) -&gt; RetrievalDescriptiveStatistics:\n        self.convert_v1_dataset_format_to_v2(num_proc)\n        if hf_subset and hf_subset in self.dataset:\n            split_data = self.dataset[hf_subset][split]\n            queries = split_data[\"queries\"]\n            corpus = split_data[\"corpus\"]\n            relevant_docs = split_data[\"relevant_docs\"]\n            top_ranked = split_data[\"top_ranked\"]\n        elif compute_overall:\n            queries = None\n            corpus = None\n            relevant_docs = {}\n            top_ranked = {}\n            for hf_subset in self.metadata.eval_langs:\n                split_data = self.dataset[hf_subset][split]\n                if queries is None:\n                    queries = split_data[\"queries\"]\n                else:\n                    queries = concatenate_datasets([queries, split_data[\"queries\"]])\n                if corpus is None:\n                    corpus = split_data[\"corpus\"]\n                else:\n                    corpus = concatenate_datasets([corpus, split_data[\"corpus\"]])\n\n                relevant_docs.update(\n                    _process_relevant_docs(\n                        split_data[\"relevant_docs\"], hf_subset, split\n                    )\n                )\n\n                if \"top_ranked\" in split_data and split_data[\"top_ranked\"] is not None:\n                    top_ranked.update(\n                        {\n                            f\"{split}_{hf_subset}_{k}\": v\n                            for k, v in split_data[\"top_ranked\"].items()\n                        }\n                    )\n        else:\n            if \"default\" in self.dataset and split != \"default\":\n                return self._calculate_descriptive_statistics_from_split(\n                    split=split, hf_subset=\"default\"\n                )\n            split_data = self.dataset[\"default\"][split]\n            queries = split_data[\"queries\"]\n            corpus = split_data[\"corpus\"]\n            relevant_docs = split_data[\"relevant_docs\"]\n            top_ranked = split_data[\"top_ranked\"]\n\n        num_documents = len(corpus)\n        num_queries = len(queries)\n\n        if self.metadata.category is None:\n            queries_modalities = \"t\"\n            corpus_modalities = \"t\"\n        else:\n            queries_modalities, corpus_modalities = self.metadata.category.split(\"2\")\n\n        number_of_characters = 0\n\n        documents_text_statistics = None\n        documents_image_statistics = None\n        documents_audio_statistics = None\n        queries_text_statistics = None\n        queries_image_statistics = None\n        queries_audio_statistics = None\n\n        if \"t\" in corpus_modalities:\n            corpus_texts = corpus.map(_corpus_to_dict)[\"text\"]\n            documents_text_statistics = calculate_text_statistics(corpus_texts)\n            number_of_characters += documents_text_statistics[\"total_text_length\"]\n\n        if \"i\" in corpus_modalities:\n            documents_image_statistics = calculate_image_statistics(corpus[\"image\"])\n\n        if \"a\" in corpus_modalities:\n            documents_audio_statistics = calculate_audio_statistics(corpus[\"audio\"])\n\n        if \"t\" in queries_modalities:\n            queries_ = queries\n            if \"instruction\" in queries_[0]:\n                queries_ = queries_.map(_combine_queries_with_instruction_text)\n\n            if isinstance(queries_[\"text\"][0], dict | list):\n                queries_ = queries_.map(_convert_conv_history_to_query)\n            queries_text_statistics = calculate_text_statistics(queries_[\"text\"])\n\n            number_of_characters += queries_text_statistics[\"total_text_length\"]\n\n        if \"i\" in queries_modalities:\n            queries_image_statistics = calculate_image_statistics(queries[\"image\"])\n\n        if \"a\" in queries_modalities:\n            queries_audio_statistics = calculate_audio_statistics(queries[\"audio\"])\n\n        relevant_docs_statistics = calculate_relevant_docs_statistics(relevant_docs)\n\n        if top_ranked is not None and num_queries and len(top_ranked) &gt; 0:\n            top_ranked_statistics = calculate_top_ranked_statistics(\n                top_ranked, num_queries\n            )\n        else:\n            top_ranked_statistics = None\n\n        return RetrievalDescriptiveStatistics(\n            num_samples=num_documents + num_queries,\n            number_of_characters=number_of_characters,\n            documents_text_statistics=documents_text_statistics,\n            documents_image_statistics=documents_image_statistics,\n            documents_audio_statistics=documents_audio_statistics,\n            queries_text_statistics=queries_text_statistics,\n            queries_image_statistics=queries_image_statistics,\n            queries_audio_statistics=queries_audio_statistics,\n            relevant_docs_statistics=relevant_docs_statistics,\n            top_ranked_statistics=top_ranked_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self.convert_v1_dataset_format_to_v2(num_proc)\n\n        def _push_section(\n            data: dict[str, RetrievalSplitData],\n            subset_item: Literal[\"corpus\", \"queries\", \"relevant_docs\", \"top_ranked\"],\n            hf_subset_name: str,\n            converter: Callable[[Any, Any], dict[str, Any]] | None = None,\n        ) -&gt; None:\n            \"\"\"Helper function to push dataset\n\n            Args:\n                data: Dataset with all items\n                subset_item: Select which part to take. E. g. corpus, queries etc\n                hf_subset_name: Name of the current item on HF\n                converter: Function to convert dict to datasets format\n            \"\"\"\n            sections = {}\n            for split in data.keys():\n                # skip empty instructions and top ranked\n                if subset_item not in data[split] or data[split][subset_item] is None:\n                    continue\n                if isinstance(data[split][subset_item], Dataset):\n                    sections[split] = data[split][subset_item]\n                elif converter is not None:\n                    subset_data = data[split][subset_item]\n                    if subset_data is None:\n                        continue\n\n                    sections[split] = Dataset.from_list(\n                        [converter(idx, item) for idx, item in subset_data.items()]\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unexpected subset item type {subset_item} without converter\"\n                    )\n            if len(sections) &gt; 0:\n                DatasetDict(sections).push_to_hub(\n                    repo_name,\n                    hf_subset_name,\n                    commit_message=f\"Add {hf_subset_name}-{subset_item}\",\n                    num_proc=num_proc,\n                )\n\n        for subset in self.dataset:\n            logger.info(f\"Converting {subset} of {self.metadata.name}\")\n            _push_section(\n                self.dataset[subset],\n                \"queries\",\n                f\"{subset}-queries\" if subset != \"default\" else \"queries\",\n            )\n            _push_section(\n                self.dataset[subset],\n                \"corpus\",\n                f\"{subset}-corpus\" if subset != \"default\" else \"corpus\",\n            )\n            # Handle relevant_docs separately since one entry expands to multiple records.\n            relevant_sections = {}\n            for split, values in self.dataset[subset].items():\n                relevant_docs = values[\"relevant_docs\"]\n                entries = []\n                for query_id, docs in relevant_docs.items():\n                    for doc_id, score in docs.items():\n                        entries.append(\n                            {\n                                \"query-id\": query_id,\n                                \"corpus-id\": doc_id,\n                                \"score\": score,\n                            }\n                        )\n                relevant_sections[split] = Dataset.from_list(entries)\n            DatasetDict(relevant_sections).push_to_hub(\n                repo_name,\n                f\"{subset}-qrels\" if subset != \"default\" else \"qrels\",\n                commit_message=f\"Add {subset}-qrels\",\n                num_proc=num_proc,\n            )\n\n            _push_section(\n                self.dataset[subset],\n                \"top_ranked\",\n                f\"{subset}-top_ranked\" if subset != \"default\" else \"top_ranked\",\n                lambda idx, docs: {\"query-id\": idx, \"corpus-ids\": docs},\n            )\n\n    def convert_to_reranking(\n        self,\n        top_ranked_path: str | Path,\n        top_k: int = 10,\n    ) -&gt; Self:\n        \"\"\"Converts a reranking task to re-ranking by loading predictions from previous model run where the `prediction_folder` was specified.\n\n        Args:\n            top_ranked_path: Path to file or folder with the top ranked predictions.\n            top_k: Number of results to load.\n\n        Returns:\n            The current task reformulated as a reranking task\n\n        Raises:\n            FileNotFoundError: If the specified path does not exist.\n            ValueError: If the loaded top ranked results are not in the expected format.\n        \"\"\"\n        self._top_k = top_k\n\n        top_ranked_path = Path(top_ranked_path)\n        if top_ranked_path.is_dir():\n            top_ranked_path = self._predictions_path(top_ranked_path)\n\n        if not top_ranked_path.exists():\n            raise FileNotFoundError(\n                f\"Can't find previous results for this task. File {top_ranked_path} does not exist.\"\n            )\n\n        with top_ranked_path.open(\"r\") as previous_results_file:\n            previous_results = json.load(previous_results_file)\n\n        if not self.data_loaded:\n            self.load_data()\n\n        self._previous_results_model_meta = previous_results[\"mteb_model_meta\"]\n\n        for subset in self.dataset:\n            for split in self.dataset[subset]:\n                top_ranked: RetrievalOutputType = previous_results[subset][split]\n                if not isinstance(top_ranked, dict):\n                    raise ValueError(\"Previous top ranked results is not a dictionary.\")\n\n                top_k_sorted = defaultdict(list)\n                for query_id, values in top_ranked.items():\n                    sorted_keys = sorted(values, key=lambda k: values[k], reverse=True)\n                    top_k_sorted[query_id] = sorted_keys[: self._top_k]\n\n                self.dataset[subset][split][\"top_ranked\"] = top_k_sorted\n        return self\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.convert_to_reranking","title":"<code>convert_to_reranking(top_ranked_path, top_k=10)</code>","text":"<p>Converts a reranking task to re-ranking by loading predictions from previous model run where the <code>prediction_folder</code> was specified.</p> <p>Parameters:</p> Name Type Description Default <code>top_ranked_path</code> <code>str | Path</code> <p>Path to file or folder with the top ranked predictions.</p> required <code>top_k</code> <code>int</code> <p>Number of results to load.</p> <code>10</code> <p>Returns:</p> Type Description <code>Self</code> <p>The current task reformulated as a reranking task</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified path does not exist.</p> <code>ValueError</code> <p>If the loaded top ranked results are not in the expected format.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def convert_to_reranking(\n    self,\n    top_ranked_path: str | Path,\n    top_k: int = 10,\n) -&gt; Self:\n    \"\"\"Converts a reranking task to re-ranking by loading predictions from previous model run where the `prediction_folder` was specified.\n\n    Args:\n        top_ranked_path: Path to file or folder with the top ranked predictions.\n        top_k: Number of results to load.\n\n    Returns:\n        The current task reformulated as a reranking task\n\n    Raises:\n        FileNotFoundError: If the specified path does not exist.\n        ValueError: If the loaded top ranked results are not in the expected format.\n    \"\"\"\n    self._top_k = top_k\n\n    top_ranked_path = Path(top_ranked_path)\n    if top_ranked_path.is_dir():\n        top_ranked_path = self._predictions_path(top_ranked_path)\n\n    if not top_ranked_path.exists():\n        raise FileNotFoundError(\n            f\"Can't find previous results for this task. File {top_ranked_path} does not exist.\"\n        )\n\n    with top_ranked_path.open(\"r\") as previous_results_file:\n        previous_results = json.load(previous_results_file)\n\n    if not self.data_loaded:\n        self.load_data()\n\n    self._previous_results_model_meta = previous_results[\"mteb_model_meta\"]\n\n    for subset in self.dataset:\n        for split in self.dataset[subset]:\n            top_ranked: RetrievalOutputType = previous_results[subset][split]\n            if not isinstance(top_ranked, dict):\n                raise ValueError(\"Previous top ranked results is not a dictionary.\")\n\n            top_k_sorted = defaultdict(list)\n            for query_id, values in top_ranked.items():\n                sorted_keys = sorted(values, key=lambda k: values[k], reverse=True)\n                top_k_sorted[query_id] = sorted_keys[: self._top_k]\n\n            self.dataset[subset][split][\"top_ranked\"] = top_k_sorted\n    return self\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.convert_v1_dataset_format_to_v2","title":"<code>convert_v1_dataset_format_to_v2(num_proc)</code>","text":"<p>Convert dataset from v1 (from <code>self.queries</code>, <code>self.document</code>) format to v2 format (<code>self.dotaset</code>).</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def convert_v1_dataset_format_to_v2(\n    self,\n    num_proc: int | None,\n) -&gt; None:\n    \"\"\"Convert dataset from v1 (from `self.queries`, `self.document`) format to v2 format (`self.dotaset`).\"\"\"\n    # check if dataset is `v1` version\n    if not hasattr(self, \"queries\"):\n        return\n    empty_dataset = Dataset.from_dict({})\n\n    self.dataset = defaultdict(\n        lambda: defaultdict(\n            lambda: RetrievalSplitData(\n                corpus=empty_dataset,\n                queries=empty_dataset,\n                relevant_docs={},\n                top_ranked=None,\n            )\n        )\n    )\n\n    def _process_split(\n        ds_queries: dict | Dataset, ds_corpus: dict | Dataset\n    ) -&gt; tuple[Dataset, Dataset]:\n        if isinstance(ds_queries, dict):\n            queries = Dataset.from_list(\n                [{\"id\": k, \"text\": v} for k, v in ds_queries.items()]\n            )\n        elif isinstance(ds_queries, Dataset):\n            queries = ds_queries\n        else:\n            raise ValueError(f\"Can't convert queries of type {type(ds_queries)}\")\n\n        if isinstance(ds_corpus, dict):\n            corpus = Dataset.from_list(\n                [\n                    {\n                        \"id\": k,\n                        \"text\": v if isinstance(v, str) else v[\"text\"],\n                        \"title\": v.get(\"title\", \"\") if isinstance(v, dict) else \"\",\n                    }\n                    for k, v in ds_corpus.items()\n                ]\n            )\n        elif isinstance(ds_corpus, Dataset):\n            corpus = ds_corpus\n        else:\n            raise ValueError(f\"Can't convert corpus of type {type(ds_corpus)}\")\n        return queries, corpus\n\n    if self.metadata.is_multilingual:\n        for subset in self.queries:  # type: ignore[attr-defined]\n            for split in self.queries[subset]:  # type: ignore[attr-defined]\n                queries = self.queries[subset][split]  # type: ignore[attr-defined]\n                corpus = self.corpus[subset][split]  # type: ignore[attr-defined]\n\n                (\n                    self.dataset[subset][split][\"queries\"],\n                    self.dataset[subset][split][\"corpus\"],\n                ) = _process_split(queries, corpus)\n\n                self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[  # type: ignore[attr-defined]\n                    subset\n                ][split]\n                if hasattr(self, \"instructions\"):\n                    instructions = self.instructions[subset][split]\n                    self.dataset[subset][split][\"queries\"] = (\n                        _combine_queries_with_instructions_datasets(\n                            self.dataset[subset][split][\"queries\"],\n                            instructions,\n                            num_proc,\n                        )\n                    )\n                if hasattr(self, \"top_ranked\"):\n                    self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                        subset\n                    ][split]\n    else:\n        subset = \"default\"\n        for split in self.queries:  # type: ignore[attr-defined]\n            queries = self.queries[split]  # type: ignore[attr-defined]\n            corpus = self.corpus[split]  # type: ignore[attr-defined]\n            (\n                self.dataset[subset][split][\"queries\"],\n                self.dataset[subset][split][\"corpus\"],\n            ) = _process_split(queries, corpus)\n\n            self.dataset[subset][split][\"relevant_docs\"] = self.relevant_docs[  # type: ignore[attr-defined]\n                split\n            ].copy()\n            if hasattr(self, \"instructions\"):\n                instructions = self.instructions[split]\n                self.dataset[subset][split][\"queries\"] = (\n                    _combine_queries_with_instructions_datasets(\n                        self.dataset[subset][split][\"queries\"],\n                        instructions,\n                        num_proc,\n                    )\n                )\n            if hasattr(self, \"top_ranked\") and self.top_ranked:\n                self.dataset[subset][split][\"top_ranked\"] = self.top_ranked[\n                    split\n                ].copy()\n\n    del self.queries  # type: ignore[attr-defined]\n    del self.corpus  # type: ignore[attr-defined]\n    del self.relevant_docs  # type: ignore[attr-defined]\n    if hasattr(self, \"instructions\"):\n        del self.instructions\n    if hasattr(self, \"top_ranked\"):\n        del self.top_ranked\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, num_proc=None, **kwargs)</code>","text":"<p>Evaluate the model on the retrieval task.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>MTEBModels</code> <p>Model to evaluate. Model should implement the SearchProtocol or be an Encoder or CrossEncoderProtocol.</p> required <code>split</code> <code>str</code> <p>Split to evaluate on</p> <code>'test'</code> <code>subsets_to_run</code> <code>list[HFSubset] | None</code> <p>Optional list of subsets to evaluate on</p> <code>None</code> <code>encode_kwargs</code> <code>EncodeKwargs</code> <p>Keyword arguments passed to the encoder</p> required <code>prediction_folder</code> <code>Path | None</code> <p>Folder to save model predictions</p> <code>None</code> <code>num_proc</code> <code>int | None</code> <p>Number of processes to use</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the evaluator</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping[HFSubset, ScoresDict]</code> <p>Dictionary mapping subsets to their evaluation scores</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: EncodeKwargs,\n    prediction_folder: Path | None = None,\n    num_proc: int | None = None,\n    **kwargs: Any,\n) -&gt; Mapping[HFSubset, ScoresDict]:\n    \"\"\"Evaluate the model on the retrieval task.\n\n    Args:\n        model: Model to evaluate. Model should implement the [SearchProtocol][mteb.models.models_protocols.SearchProtocol]\n            or be an [Encoder][mteb.models.models_protocols.EncoderProtocol] or [CrossEncoderProtocol][mteb.models.models_protocols.CrossEncoderProtocol].\n        split: Split to evaluate on\n        subsets_to_run: Optional list of subsets to evaluate on\n        encode_kwargs: Keyword arguments passed to the encoder\n        prediction_folder: Folder to save model predictions\n        num_proc: Number of processes to use\n        **kwargs: Additional keyword arguments passed to the evaluator\n\n    Returns:\n        Dictionary mapping subsets to their evaluation scores\n    \"\"\"\n    if not self.data_loaded:\n        self.load_data(num_proc=num_proc)\n    # TODO: convert all tasks directly https://github.com/embeddings-benchmark/mteb/issues/2030\n    self.convert_v1_dataset_format_to_v2(num_proc=num_proc)\n\n    return super().evaluate(\n        model,\n        split,\n        subsets_to_run,\n        encode_kwargs=encode_kwargs,\n        prediction_folder=prediction_folder,\n        num_proc=num_proc,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.load_data","title":"<code>load_data(num_proc=None, **kwargs)</code>","text":"<p>Load the dataset for the retrieval task.</p> Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n    \"\"\"Load the dataset for the retrieval task.\"\"\"\n    if self.data_loaded:\n        return\n\n    dataset_path = self.metadata.dataset[\"path\"]\n    eval_splits = self.eval_splits\n    trust_remote_code = self.metadata.dataset.get(\"trust_remote_code\", False)\n    revision = self.metadata.dataset[\"revision\"]\n\n    def _process_data(split: str, hf_subset: str = \"default\"):\n        \"\"\"Helper function to load and process data for a given split and language\"\"\"\n        logger.debug(\n            f\"Loading {split} split for {hf_subset} subset of {self.metadata.name}\"\n        )\n\n        self.dataset[hf_subset][split] = RetrievalDatasetLoader(\n            hf_repo=dataset_path,\n            revision=revision,\n            trust_remote_code=trust_remote_code,\n            split=split,\n            config=hf_subset,\n        ).load(\n            num_proc=num_proc,\n        )\n\n    if self.metadata.is_multilingual:\n        for lang in self.hf_subsets:\n            for split in eval_splits:\n                _process_data(split, lang)\n    else:\n        for split in eval_splits:\n            _process_data(split)\n    self.dataset_transform(num_proc=num_proc)\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval.AbsTaskRetrieval.task_specific_scores","title":"<code>task_specific_scores(scores, qrels, results, hf_split, hf_subset)</code>","text":"<p>Calculate task specific scores. Override in subclass if needed.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>dict[str, dict[str, float]]</code> <p>Dictionary of scores</p> required <code>qrels</code> <code>RelevantDocumentsType</code> <p>Relevant documents</p> required <code>results</code> <code>dict[str, dict[str, float]]</code> <p>Retrieval results</p> required <code>hf_split</code> <code>str</code> <p>Split to evaluate on</p> required <code>hf_subset</code> <code>str</code> <p>Subset to evaluate on</p> required Source code in <code>mteb/abstasks/retrieval.py</code> <pre><code>def task_specific_scores(\n    self,\n    scores: dict[str, dict[str, float]],\n    qrels: RelevantDocumentsType,\n    results: dict[str, dict[str, float]],\n    hf_split: str,\n    hf_subset: str,\n) -&gt; dict[str, float]:\n    \"\"\"Calculate task specific scores. Override in subclass if needed.\n\n    Args:\n        scores: Dictionary of scores\n        qrels: Relevant documents\n        results: Retrieval results\n        hf_split: Split to evaluate on\n        hf_subset: Subset to evaluate on\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/task/#mteb.abstasks.retrieval_dataset_loaders.RetrievalSplitData","title":"<code>mteb.abstasks.retrieval_dataset_loaders.RetrievalSplitData</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A dictionary containing the corpus, queries, relevant documents, instructions, and top-ranked documents for a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>corpus</code> <code>CorpusDatasetType</code> <p>The corpus dataset containing documents. Should have columns <code>id</code>, <code>title</code>, <code>text</code> or <code>image</code>.</p> <code>queries</code> <code>QueryDatasetType</code> <p>The queries dataset containing queries. Should have columns <code>id</code>, <code>text</code>, <code>instruction</code> (for instruction retrieval/reranking) or <code>image</code>.</p> <code>relevant_docs</code> <code>RelevantDocumentsType</code> <p>A mapping of query IDs to relevant document IDs and their relevance scores. Should have columns <code>query-id</code>, <code>corpus-id</code>, <code>score</code>.</p> <code>top_ranked</code> <code>TopRankedDocumentsType | None</code> <p>A mapping of query IDs to a list of top-ranked document IDs. Should have columns <code>query-id</code>, <code>corpus-ids</code> (list[str]). This is optional and used for reranking tasks.</p> Source code in <code>mteb/abstasks/retrieval_dataset_loaders.py</code> <pre><code>class RetrievalSplitData(TypedDict):\n    \"\"\"A dictionary containing the corpus, queries, relevant documents, instructions, and top-ranked documents for a retrieval task.\n\n    Attributes:\n        corpus: The corpus dataset containing documents. Should have columns `id`, `title`, `text` or `image`.\n        queries: The queries dataset containing queries. Should have columns `id`, `text`, `instruction` (for instruction retrieval/reranking) or `image`.\n        relevant_docs: A mapping of query IDs to relevant document IDs and their relevance scores. Should have columns `query-id`, `corpus-id`, `score`.\n        top_ranked: A mapping of query IDs to a list of top-ranked document IDs. Should have columns `query-id`, `corpus-ids` (list[str]). This is optional and used for reranking tasks.\n    \"\"\"\n\n    corpus: CorpusDatasetType\n    queries: QueryDatasetType\n    relevant_docs: RelevantDocumentsType\n    top_ranked: TopRankedDocumentsType | None\n</code></pre>"},{"location":"api/task/#mteb.abstasks.classification.AbsTaskClassification","title":"<code>mteb.abstasks.classification.AbsTaskClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>The class which classification tasks inherit from.</p> <p>A classification task consists of a dataset with input data and corresponding labels. The task is to predict the label for each input. The task works by training a sklearn compatible model on samples drawn from the training split of the dataset, where the input data is encoded using the provided model. The trained model is then evaluated on the evaluation split of the dataset. This process is repeated for <code>n_experiments</code> times, and both average and individual scores for each experiment are reported.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Hugging Face dataset containing the data for the task. Should have train split (split name can be changed by train_split. Must contain the following columns: text: str (for text) or PIL.Image (for image). Column name can be changed via <code>input_column_name</code> attribute. label: int. Column name can be changed via <code>label_column_name</code> attribute.</p> <code>evaluator_model</code> <code>SklearnModelProtocol</code> <p>The model to use for evaluation. Can be any sklearn compatible model. Default is <code>LogisticRegression</code>.</p> <code>samples_per_label</code> <code>int</code> <p>Number of samples per label to use for training the evaluator model. Default is 8.</p> <code>n_experiments</code> <code>int</code> <p>Number of experiments to run. Default is 10.</p> <code>train_split</code> <code>str</code> <p>Name of the split to use for training the evaluator model. Default is \"train\".</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels. Default is \"label\".</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input data. Default is \"text\".</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>is_cross_validation</code> <code>bool</code> <p>Is task cross validation</p> <code>n_splits</code> <p>Number of splits for cross-validation</p> Source code in <code>mteb/abstasks/classification.py</code> <pre><code>class AbsTaskClassification(AbsTask):\n    \"\"\"The class which classification tasks inherit from.\n\n    A classification task consists of a dataset with input data and corresponding labels. The task is to predict the label for each input.\n    The task works by training a sklearn compatible model on samples drawn from the training split of the dataset,\n    where the input data is encoded using the provided model.\n    The trained model is then evaluated on the evaluation split of the dataset. This process is repeated for `n_experiments` times, and both average and\n    individual scores for each experiment are reported.\n\n    Attributes:\n        dataset: Hugging Face dataset containing the data for the task. Should have train split (split name can be changed by train_split. Must contain the following columns:\n            text: str (for text) or PIL.Image (for image). Column name can be changed via `input_column_name` attribute.\n            label: int. Column name can be changed via `label_column_name` attribute.\n        evaluator_model: The model to use for evaluation. Can be any sklearn compatible model. Default is `LogisticRegression`.\n        samples_per_label: Number of samples per label to use for training the evaluator model. Default is 8.\n        n_experiments: Number of experiments to run. Default is 10.\n        train_split: Name of the split to use for training the evaluator model. Default is \"train\".\n        label_column_name: Name of the column containing the labels. Default is \"label\".\n        input_column_name: Name of the column containing the input data. Default is \"text\".\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        is_cross_validation: Is task cross validation\n        n_splits: Number of splits for cross-validation\n    \"\"\"\n\n    evaluator: type[SklearnEvaluator] = SklearnEvaluator\n    evaluator_model: SklearnModelProtocol = LogisticRegression(\n        n_jobs=-1,\n        max_iter=100,\n    )\n\n    samples_per_label: int = 8\n    n_experiments: int = 10\n    train_split: str = \"train\"\n    label_column_name: str = \"label\"\n    input_column_name: str = \"text\"\n    abstask_prompt = \"Classify user passages.\"\n    is_cross_validation: bool = False\n    n_splits = 5\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Evaluate a model on the classification task.\n\n        Differs from other tasks as it requires train split.\n        \"\"\"\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\n                f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n                \"Please use a Encoder model instead.\"\n            )\n\n        if not self.data_loaded:\n            self.load_data(num_proc=num_proc)\n\n        if self.dataset is None:\n            raise RuntimeError(\"Dataset not loaded.\")\n\n        if \"random_state\" in self.evaluator_model.get_params():\n            self.evaluator_model = self.evaluator_model.set_params(\n                random_state=self.seed\n            )\n        scores = {}\n        hf_subsets = self.hf_subsets\n        if subsets_to_run is not None:\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n            )\n\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                ds = self.dataset\n            else:\n                ds = self.dataset[hf_subset]\n\n            if isinstance(ds, Dataset | DatasetDict):\n                ds = ds.select_columns([self.label_column_name, self.input_column_name])\n            eval_function = (\n                self._evaluate_subset\n                if not self.is_cross_validation\n                else self._evaluate_subset_cross_validation\n            )\n            scores[hf_subset] = eval_function(\n                model,\n                ds,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                num_proc=num_proc,\n                **kwargs,\n            )\n            self._add_main_score(scores[hf_subset])\n\n        return scores  # type: ignore[return-value]\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: DatasetDict,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; FullClassificationMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        train_split = data_split[self.train_split]\n        eval_split = data_split[hf_split]\n\n        scores = []\n        # we store idxs to make the shuffling reproducible\n        test_cache, idxs = None, None\n\n        all_predictions = []\n        for i in range(self.n_experiments):\n            logger.info(f\"Running experiment ({i}/{self.n_experiments})\")\n            scores_exp, predictions, idxs, test_cache = self._run_experiment(\n                model,\n                train_split,\n                eval_split,\n                experiment_num=i,\n                idxs=idxs,\n                test_cache=test_cache,\n                encode_kwargs=encode_kwargs,\n                hf_split=hf_split,\n                hf_subset=hf_subset,\n                num_proc=num_proc,\n            )\n\n            if prediction_folder:\n                all_predictions.append(predictions)\n            scores.append(scores_exp)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_predictions,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._calculate_avg_scores(scores)\n\n    def _evaluate_subset_cross_validation(\n        self,\n        model: EncoderProtocol,\n        data_split: DatasetDict,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; FullClassificationMetrics:\n        if self.train_split != hf_split:\n            raise ValueError(\n                f\"Performing {self.n_splits}-fold cross validation, but the dataset has a train (`{self.train_split}`) and test split (`{hf_split}`)! Set `is_cross_validation` to False, and retry.\"\n            )\n        logger.info(\n            f\"Performing {self.n_splits}-fold cross-validation on the entire dataset!\"\n        )\n\n        ds = data_split[self.train_split]\n        num_samples = len(ds)\n\n        scores = []\n        idxs = None\n        cross_validation_splitter = KFold(\n            n_splits=self.n_splits, shuffle=True, random_state=self.seed\n        )\n        all_predictions = []\n        dataloader_train = create_dataloader(\n            ds,\n            self.metadata,\n            input_column=self.input_column_name,\n            num_proc=num_proc,\n            **encode_kwargs,\n        )\n        logger.info(\"Running cross-validation - Encoding samples...\")\n        # precompute all embeddings for cross-validation to not recomupute them in different k-folds\n        dataset_embeddings = model.encode(\n            dataloader_train,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **encode_kwargs,\n        )\n        for i, (train_idx, val_idx) in enumerate(\n            cross_validation_splitter.split(range(num_samples))\n        ):\n            train_split = ds.select(train_idx)\n            eval_split = ds.select(val_idx)\n            train_cache = dataset_embeddings[train_idx]\n            test_cache = dataset_embeddings[val_idx]\n            logger.info(f\"Running experiment ({i}/{self.n_experiments})\")\n            scores_exp, predictions, idxs, _ = self._run_experiment(\n                model,\n                train_split,\n                eval_split,\n                experiment_num=i,\n                idxs=idxs,\n                encode_kwargs=encode_kwargs,\n                hf_split=hf_split,\n                hf_subset=hf_subset,\n                test_cache=test_cache,\n                train_cache=train_cache,\n                num_proc=num_proc,\n            )\n\n            if prediction_folder:\n                all_predictions.append(predictions)\n            scores.append(scores_exp)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_predictions,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n        return self._calculate_avg_scores(scores)\n\n    def _run_experiment(\n        self,\n        model: EncoderProtocol,\n        train_split: Dataset,\n        eval_split: Dataset,\n        experiment_num: int,\n        idxs: list[int] | None,\n        test_cache: Array | None,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        train_cache: Array | None = None,\n        num_proc: int | None = None,\n    ) -&gt; tuple[ClassificationMetrics, list[float], list[int], Array]:\n        train_dataset, idxs, selected_idx = self._undersample_data(\n            train_split,\n            experiment_num,\n            idxs,\n        )\n        sub_train_cache = None\n        if train_cache is not None:\n            sub_train_cache = train_cache[selected_idx]\n\n        evaluator = self.evaluator(\n            train_dataset,\n            eval_split,\n            self.input_column_name,\n            self.label_column_name,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            evaluator_model=self.evaluator_model,\n        )\n        y_pred, test_cache = evaluator(\n            model,\n            encode_kwargs=encode_kwargs,\n            test_cache=test_cache,\n            train_cache=sub_train_cache,\n            num_proc=num_proc,\n        )\n        y_test = eval_split[self.label_column_name]\n        return self._calculate_scores(y_test, y_pred), y_pred.tolist(), idxs, test_cache\n\n    def _calculate_avg_scores(\n        self, scores: list[ClassificationMetrics]\n    ) -&gt; FullClassificationMetrics:\n        avg_scores: dict[str, Any] = {\n            # ap will be none for non binary classification tasks\n            k: (\n                float(np.mean(values))\n                if (values := [s[k] for s in scores if s[k] is not None])  # type: ignore[literal-required]\n                else np.nan\n            )\n            for k in scores[0].keys()\n        }\n        logger.info(f\"Running {self.metadata.name} - Finished.\")\n        return FullClassificationMetrics(\n            scores_per_experiment=scores,\n            **avg_scores,  # type: ignore[typeddict-item]\n        )\n\n    def _calculate_scores(\n        self,\n        y_test: NDArray[np.integer] | list[int],\n        y_pred: NDArray[np.integer | np.floating] | list[int],\n    ) -&gt; ClassificationMetrics:\n        scores = ClassificationMetrics(\n            accuracy=accuracy_score(y_test, y_pred),\n            f1=f1_score(y_test, y_pred, average=\"macro\"),\n            f1_weighted=f1_score(y_test, y_pred, average=\"weighted\"),\n            precision=precision_score(y_test, y_pred, average=\"macro\"),\n            precision_weighted=precision_score(y_test, y_pred, average=\"weighted\"),\n            recall=recall_score(y_test, y_pred, average=\"macro\"),\n            recall_weighted=recall_score(y_test, y_pred, average=\"weighted\"),\n            ap=None,\n            ap_weighted=None,\n        )\n\n        # if binary classification\n        if len(np.unique(y_test)) == 2:\n            scores[\"ap\"] = average_precision_score(y_test, y_pred, average=\"macro\")\n            scores[\"ap_weighted\"] = average_precision_score(\n                y_test, y_pred, average=\"weighted\"\n            )\n        return scores\n\n    def _undersample_data(\n        self, dataset: Dataset, experiment_num: int, idxs: list[int] | None = None\n    ) -&gt; tuple[Dataset, list[int], list[int]]:\n        \"\"\"Undersample data to have `samples_per_label` samples of each label.\n\n        Args:\n            dataset: Hugging Face `datasets.Dataset` containing \"text\" and \"label\".\n            experiment_num: Experiment number, used to set the random seed.\n            idxs: Optional indices to shuffle and sample from.\n\n        Returns:\n            Tuple of:\n            - A new Dataset containing undersampled examples.\n            - The shuffled indices used for sampling.\n            - Selected indexes\n        \"\"\"\n        if idxs is None:\n            idxs = list(range(len(dataset)))\n\n        # using RandomState for backward compatibility with `v1`\n        rng_state = np.random.RandomState(self.seed)\n        rng_state.shuffle(idxs)\n\n        label_counter: dict[str, int] = defaultdict(int)\n        sampled_idxs = []\n\n        for i in idxs:\n            label = dataset[i][self.label_column_name]\n            if label_counter[label] &lt; self.samples_per_label:\n                sampled_idxs.append(i)\n                label_counter[label] += 1\n\n        return dataset.select(sampled_idxs), idxs, sampled_idxs\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClassificationDescriptiveStatistics:\n        train_text = []\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            label = self.dataset[hf_subset][split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[hf_subset][self.train_split][\n                    self.input_column_name\n                ]\n        elif compute_overall:\n            inputs = []\n            label = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                label.extend(self.dataset[hf_subset][split][self.label_column_name])\n                if split != self.train_split:\n                    train_text.extend(\n                        self.dataset[hf_subset][self.train_split][\n                            self.input_column_name\n                        ]\n                    )\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            label = self.dataset[split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[self.train_split][self.input_column_name]\n\n        image_statistics = None\n        text_statistics = None\n        audio_statistics = None\n        num_texts_in_train = None\n\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n            num_texts_in_train = (\n                len(set(inputs) &amp; set(train_text))\n                if split != self.train_split\n                else None\n            )\n        if \"audio\" in self.metadata.modalities:\n            audio_statistics = calculate_audio_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(label)\n\n        return ClassificationDescriptiveStatistics(\n            num_samples=len(inputs),\n            number_texts_intersect_with_train=num_texts_in_train,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            audio_statistics=audio_statistics,\n            label_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n            num_proc=num_proc,\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.classification.AbsTaskClassification.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, num_proc=None, **kwargs)</code>","text":"<p>Evaluate a model on the classification task.</p> <p>Differs from other tasks as it requires train split.</p> Source code in <code>mteb/abstasks/classification.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: EncodeKwargs,\n    prediction_folder: Path | None = None,\n    num_proc: int | None = None,\n    **kwargs: Any,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Evaluate a model on the classification task.\n\n    Differs from other tasks as it requires train split.\n    \"\"\"\n    if not isinstance(model, EncoderProtocol):\n        raise TypeError(\n            f\"Model {model} is a SearchProtocol, but this task {self.metadata.name} does not support Search. \"\n            \"Please use a Encoder model instead.\"\n        )\n\n    if not self.data_loaded:\n        self.load_data(num_proc=num_proc)\n\n    if self.dataset is None:\n        raise RuntimeError(\"Dataset not loaded.\")\n\n    if \"random_state\" in self.evaluator_model.get_params():\n        self.evaluator_model = self.evaluator_model.set_params(\n            random_state=self.seed\n        )\n    scores = {}\n    hf_subsets = self.hf_subsets\n    if subsets_to_run is not None:\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    for hf_subset in hf_subsets:\n        logger.info(\n            f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n        )\n\n        if hf_subset not in self.dataset and hf_subset == \"default\":\n            ds = self.dataset\n        else:\n            ds = self.dataset[hf_subset]\n\n        if isinstance(ds, Dataset | DatasetDict):\n            ds = ds.select_columns([self.label_column_name, self.input_column_name])\n        eval_function = (\n            self._evaluate_subset\n            if not self.is_cross_validation\n            else self._evaluate_subset_cross_validation\n        )\n        scores[hf_subset] = eval_function(\n            model,\n            ds,\n            hf_split=split,\n            hf_subset=hf_subset,\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            num_proc=num_proc,\n            **kwargs,\n        )\n        self._add_main_score(scores[hf_subset])\n\n    return scores  # type: ignore[return-value]\n</code></pre>"},{"location":"api/task/#mteb.abstasks.multilabel_classification.AbsTaskMultilabelClassification","title":"<code>mteb.abstasks.multilabel_classification.AbsTaskMultilabelClassification</code>","text":"<p>               Bases: <code>AbsTaskClassification</code></p> <p>Abstract class for multioutput classification tasks</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Huggingface dataset containing the data for the task. Dataset must contain columns specified by input_column_name and label_column_name. Input column must contain the text or image to be classified, and label column must contain a list of labels for each example.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input text.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels.</p> <code>samples_per_label</code> <code>int</code> <p>Number of samples to use pr. label. These samples are embedded and a classifier is fit using the labels and samples.</p> <code>evaluator_model</code> <code>SklearnModelProtocol</code> <p>Classifier to use for evaluation. Must implement the SklearnModelProtocol.</p> Source code in <code>mteb/abstasks/multilabel_classification.py</code> <pre><code>class AbsTaskMultilabelClassification(AbsTaskClassification):\n    \"\"\"Abstract class for multioutput classification tasks\n\n    Attributes:\n        dataset: Huggingface dataset containing the data for the task. Dataset must contain columns specified by input_column_name and label_column_name.\n            Input column must contain the text or image to be classified, and label column must contain a list of labels for each example.\n        input_column_name: Name of the column containing the input text.\n        label_column_name: Name of the column containing the labels.\n        samples_per_label: Number of samples to use pr. label. These samples are embedded and a classifier is fit using the labels and samples.\n        evaluator_model: Classifier to use for evaluation. Must implement the SklearnModelProtocol.\n    \"\"\"\n\n    evaluator_model: SklearnModelProtocol = KNeighborsClassifier(n_neighbors=5)\n    input_column_name: str = \"text\"\n    label_column_name: str = \"label\"\n\n    @override\n    def _evaluate_subset(  # type: ignore[override]\n        self,\n        model: MTEBModels,\n        data_split: DatasetDict,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; FullMultilabelClassificationMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        if isinstance(data_split, DatasetDict):\n            data_split = data_split.select_columns(\n                [self.input_column_name, self.label_column_name]\n            )\n        train_split = data_split[self.train_split]\n        eval_split = data_split[hf_split]\n\n        logger.info(\n            \"Running multilabel classification task - Sampling training data...\"\n        )\n        scores = []\n        # Bootstrap sample indices from training set for each experiment\n        train_samples = []\n        for _ in range(self.n_experiments):\n            sample_indices, _ = self._undersample_data_indices(\n                train_split[self.label_column_name], self.samples_per_label, None\n            )\n            train_samples.append(sample_indices)\n        # Encode all unique sentences at the indices\n        unique_train_indices = list(set(itertools.chain.from_iterable(train_samples)))\n        unique_train_dataset = train_split.select(unique_train_indices).select_columns(\n            self.input_column_name\n        )\n        dataloader_train = create_dataloader(\n            unique_train_dataset,\n            self.metadata,\n            input_column=self.input_column_name,\n            num_proc=num_proc,\n            **encode_kwargs,\n        )\n\n        logger.info(\"Running multilabel classification - Encoding training set...\")\n        _unique_train_embeddings = model.encode(\n            dataloader_train,\n            task_metadata=self.metadata,\n            hf_split=self.train_split,\n            hf_subset=hf_subset,\n            **encode_kwargs,\n        )\n        unique_train_embeddings = dict(\n            zip(unique_train_indices, _unique_train_embeddings)\n        )\n        # Stratified subsampling of test set to 2000 examples.\n        test_dataset = eval_split\n        try:\n            if len(test_dataset) &gt; 2000:\n                split_dataset = eval_split.train_test_split(\n                    test_size=2000, seed=42, stratify_by_column=\"label\"\n                )\n                test_dataset = split_dataset[\"test\"]\n        except ValueError:\n            logger.warning(\"Couldn't subsample, continuing with the entire test set.\")\n\n        dataloader_test = create_dataloader(\n            test_dataset.select_columns(self.input_column_name),\n            self.metadata,\n            input_column=self.input_column_name,\n            **encode_kwargs,\n        )\n\n        logger.info(\"Running multilabel classification - Encoding test set...\")\n        X_test = model.encode(\n            dataloader_test,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **encode_kwargs,\n        )\n        binarizer = MultiLabelBinarizer()\n        y_test = binarizer.fit_transform(test_dataset[self.label_column_name])\n\n        logger.info(\"Running multilabel classification - Evaluating classifiers...\")\n        all_predictions = []\n        for _, sample_indices in enumerate(train_samples):\n            X_train = np.stack([unique_train_embeddings[idx] for idx in sample_indices])\n            y_train = train_split.select(sample_indices)[self.label_column_name]\n            y_train = binarizer.transform(y_train)\n            y_pred, current_classifier = _evaluate_classifier(\n                X_train, y_train, X_test, self.evaluator_model\n            )\n            if prediction_folder:\n                all_predictions.append(y_pred.tolist())\n\n            scores_exp = self._calculate_scores(\n                y_test, y_pred, X_test, current_classifier\n            )\n            scores.append(scores_exp)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_predictions,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        avg_scores: dict[str, Any] = {\n            k: np.mean([s[k] for s in scores])  # type: ignore[literal-required]\n            for k in scores[0].keys()\n        }\n        logger.info(\"Running multilabel classification - Finished.\")\n        return FullMultilabelClassificationMetrics(\n            scores_per_experiment=scores,\n            **avg_scores,  # type: ignore[typeddict-item]\n        )\n\n    def _calculate_scores(  # type: ignore[override]\n        self,\n        y_test: NDArray[np.integer],\n        y_pred: NDArray[np.integer | np.floating],\n        x_test_embedding: Array,\n        current_classifier: SklearnModelProtocol,\n    ) -&gt; MultilabelClassificationMetrics:\n        accuracy = current_classifier.score(x_test_embedding, y_test)\n        if isinstance(current_classifier, MultiOutputClassifier):\n            predictions = current_classifier.predict_proba(x_test_embedding)\n            all_probs = [emb[:, 1] for emb in predictions]\n\n            y_score = np.stack(all_probs, axis=1)  # shape: (n_samples, n_labels)\n            lrap = label_ranking_average_precision_score(y_test, y_score)\n        else:\n            lrap = label_ranking_average_precision_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average=\"macro\")\n        hamming = hamming_score(y_test, y_pred)\n        return MultilabelClassificationMetrics(\n            accuracy=accuracy,\n            lrap=lrap,\n            f1=f1,\n            hamming=hamming,\n        )\n\n    def _undersample_data_indices(\n        self, y: list[list[int]], samples_per_label: int, idxs: list[int] | None = None\n    ) -&gt; tuple[list[int], list[int]]:\n        \"\"\"Undersample data to have samples_per_label samples of each label.\n\n        Currently ensures that each label has at least samples_per_label samples.\n\n        Returns:\n            A tuple containing:\n                - List of sampled indices.\n                - List of all indices after shuffling.\n        \"\"\"\n        sample_indices = []\n        if idxs is None:\n            idxs = list(np.arange(len(y)))\n        self.np_rng.shuffle(idxs)\n        label_counter: dict[int, int] = defaultdict(int)\n        for i in idxs:\n            if any((label_counter[label] &lt; samples_per_label) for label in y[i]):\n                sample_indices.append(i)\n                for label in y[i]:\n                    label_counter[label] += 1\n        return sample_indices, idxs\n</code></pre>"},{"location":"api/task/#mteb.abstasks.clustering.AbsTaskClustering","title":"<code>mteb.abstasks.clustering.AbsTaskClustering</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>The abstract class for clustering tasks.</p> <p>A clustering task consists of a dataset with input data and corresponding cluster labels. The task is to cluster the input data points and compare the cluster assignments with the true cluster labels using V-measure. This class embeds the corpus sentences then samples N samples from the corpus and clusters them. The similarity then is calculated using multiple measures, including a V-measure. This approach is then repeated K times.</p> <p>If the clustering is hierarchical and more than one label is specified in order for each observation, we compute the metrics as outlined for each level separately.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the clustering task. Must contain the following columns <code>sentences</code> that contains inputs (texts or images) and labels columns.</p> <code>max_fraction_of_documents_to_embed</code> <code>float | None</code> <p>Fraction of documents to embed for clustering. Cannot be set at the same time as <code>max_document_to_embed</code>. If both are set to None, the entire dataset will be embedded for clustering.</p> <code>max_document_to_embed</code> <code>int | None</code> <p>Maximum number of documents to embed for clustering. Cannot be set at the same time as <code>max_fraction_of_documents_to_embed</code>. If both are set to None, the entire dataset will be embedded for clustering.</p> <code>max_documents_per_cluster</code> <code>int</code> <p>Number of documents to sample for each clustering experiment.</p> <code>n_clusters</code> <code>int</code> <p>Number of clustering experiments to run.</p> <code>k_mean_batch_size</code> <code>int</code> <p>Batch size to use for k-means clustering.</p> <code>max_depth</code> <p>Maximum depth to evaluate clustering. If None, evaluates all levels.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the input sentences or data points.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the true cluster labels.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/clustering.py</code> <pre><code>class AbsTaskClustering(AbsTask):\n    \"\"\"The abstract class for clustering tasks.\n\n    A clustering task consists of a dataset with input data and corresponding cluster labels. The task is to cluster the input data points and compare\n    the cluster assignments with the true cluster labels using V-measure.\n    This class embeds the corpus sentences then samples N samples from the corpus and clusters them. The similarity then is calculated using multiple\n    measures, including a V-measure. This approach is then repeated K times.\n\n    If the clustering is hierarchical and more than one label is specified in order for each observation, we compute the metrics as outlined\n    for each level separately.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the clustering task. Must contain the following columns `sentences` that contains\n            inputs (texts or images) and labels columns.\n        max_fraction_of_documents_to_embed: Fraction of documents to embed for clustering. Cannot be set at the same time as `max_document_to_embed`.\n            If both are set to None, the entire dataset will be embedded for clustering.\n        max_document_to_embed: Maximum number of documents to embed for clustering. Cannot be set at the same time as `max_fraction_of_documents_to_embed`.\n            If both are set to None, the entire dataset will be embedded for clustering.\n        max_documents_per_cluster: Number of documents to sample for each clustering experiment.\n        n_clusters: Number of clustering experiments to run.\n        k_mean_batch_size: Batch size to use for k-means clustering.\n        max_depth: Maximum depth to evaluate clustering. If None, evaluates all levels.\n        input_column_name: Name of the column containing the input sentences or data points.\n        label_column_name: Name of the column containing the true cluster labels.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    max_fraction_of_documents_to_embed: float | None = 0.04\n    max_document_to_embed: int | None = None\n    max_documents_per_cluster: int = 16_384\n    n_clusters: int = 10\n    k_mean_batch_size: int = 512\n    max_depth = None\n    abstask_prompt = \"Identify categories in user passages.\"\n    input_column_name: str = \"sentences\"\n    label_column_name: str = \"labels\"\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\n                \"Expected encoder model to be an instance of EncoderProtocol.\"\n            )\n        if (\n            self.max_document_to_embed is not None\n            and self.max_fraction_of_documents_to_embed is not None\n        ):\n            raise Exception(\n                \"Both max_document_to_embed and max_fraction_of_documents_to_embed are set. Please only set one.\"\n            )\n\n        logger.info(\"Running clustering - Preparing data...\")\n        if (\n            self.max_document_to_embed is None\n            and self.max_fraction_of_documents_to_embed is None\n        ):\n            downsampled_dataset = data_split\n        else:\n            if self.max_fraction_of_documents_to_embed is not None:\n                max_documents_to_embed = int(\n                    self.max_fraction_of_documents_to_embed * len(data_split)\n                )\n            else:\n                max_documents_to_embed = cast(\"int\", self.max_document_to_embed)\n\n            max_documents_to_embed = min(len(data_split), max_documents_to_embed)\n            example_indices = self.rng_state.sample(\n                range(len(data_split)), k=max_documents_to_embed\n            )\n            downsampled_dataset = data_split.select(example_indices)\n\n        downsampled_dataset = downsampled_dataset.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n\n        logger.info(\"Running clustering - Encoding samples...\")\n        embeddings = model.encode(\n            create_dataloader(\n                downsampled_dataset,\n                self.metadata,\n                input_column=self.input_column_name,\n                num_proc=num_proc,\n                **encode_kwargs,\n            ),\n            task_metadata=self.metadata,\n            hf_subset=hf_subset,\n            hf_split=hf_split,\n            **encode_kwargs,\n        )\n\n        logger.info(\"Running clustering - Evaluating clustering...\")\n        labels = []\n        for label in downsampled_dataset[self.label_column_name]:\n            if not isinstance(label, list):\n                label = [label]\n            labels.append(label)\n\n        all_v_scores, all_assignments = _evaluate_clustering_bootstrapped(\n            embeddings,\n            labels,\n            n_clusters=self.n_clusters,\n            cluster_size=self.max_documents_per_cluster,\n            kmean_batch_size=self.k_mean_batch_size,\n            max_depth=self.max_depth,\n            rng_state=self.rng_state,\n            seed=self.seed,\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                all_assignments,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        v_measures = list(itertools.chain.from_iterable(all_v_scores.values()))\n\n        logger.info(\"Running clustering - Finished.\")\n        mean_v_measure = np.mean(v_measures)\n        v_std = np.std(v_measures)\n        return {\n            \"v_measures\": all_v_scores,\n            \"v_measure\": float(mean_v_measure),\n            \"v_measure_std\": v_std,\n        }\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClusteringFastDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs = []\n            labels = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        if isinstance(inputs[0], list):\n            inputs = [item for sublist in inputs for item in sublist]\n        if isinstance(labels[0], list):\n            labels = [item for sublist in labels for item in sublist]\n\n        text_statistics, image_statistics, audio_statistics = None, None, None\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n        if \"audio\" in self.metadata.modalities:\n            audio_statistics = calculate_audio_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n\n        return ClusteringFastDescriptiveStatistics(\n            num_samples=len(inputs),\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            audio_statistics=audio_statistics,\n            labels_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [self.input_column_name, self.label_column_name],\n            num_proc=num_proc,\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.sts.AbsTaskSTS","title":"<code>mteb.abstasks.sts.AbsTaskSTS</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>The class which semantic textual similarity (STS) tasks inherit from.</p> <p>A semantic textual similarity (STS) task consists of a dataset with pairs of sentences and corresponding similarity scores. The task is to predict the similarity score for each pair of sentences.</p> <p>The task works by encoding the sentences using the provided model and then calculating similarity scores using both the model-defined similarity function (if available) and generic similarity functions, including cosine similarity, Manhattan distance, and Euclidean distance. The predicted similarity scores are then compared to the true similarity scores using Pearson and Spearman correlation coefficients.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Dataset or dict of Datasets for different subsets (e.g., languages). Dataset must contain columns specified in column_names and a 'score' column. Columns in column_names should contain the text or image data to be compared.</p> <code>column_names</code> <code>tuple[str, str]</code> <p>Tuple containing the names of the two columns to compare.</p> <code>min_score</code> <code>int</code> <p>Minimum possible score in the dataset.</p> <code>max_score</code> <code>int</code> <p>Maximum possible score in the dataset.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>input1_prompt_type</code> <code>PromptType | None</code> <p>Type of prompt of first input. Used for asymmetric tasks.</p> <code>input2_prompt_type</code> <code>PromptType | None</code> <p>Type of prompt of second input. Used for asymmetric tasks.</p> Source code in <code>mteb/abstasks/sts.py</code> <pre><code>class AbsTaskSTS(AbsTask):\n    \"\"\"The class which semantic textual similarity (STS) tasks inherit from.\n\n    A semantic textual similarity (STS) task consists of a dataset with pairs of sentences and corresponding similarity scores.\n    The task is to predict the similarity score for each pair of sentences.\n\n    The task works by encoding the sentences using the provided model and then calculating similarity scores using both the model-defined similarity\n    function (if available) and generic similarity functions, including cosine similarity, Manhattan distance, and Euclidean distance.\n    The predicted similarity scores are then compared to the true similarity scores using Pearson and Spearman correlation coefficients.\n\n\n    Attributes:\n        dataset: Dataset or dict of Datasets for different subsets (e.g., languages). Dataset must contain columns specified in column_names and a 'score' column.\n            Columns in column_names should contain the text or image data to be compared.\n        column_names: Tuple containing the names of the two columns to compare.\n        min_score: Minimum possible score in the dataset.\n        max_score: Maximum possible score in the dataset.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        input1_prompt_type: Type of prompt of first input. Used for asymmetric tasks.\n        input2_prompt_type: Type of prompt of second input. Used for asymmetric tasks.\n    \"\"\"\n\n    abstask_prompt = \"Retrieve semantically similar text.\"\n    column_names: tuple[str, str] = (\"sentence1\", \"sentence2\")\n    min_score: int = 0\n    max_score: int = 5\n    input1_prompt_type: PromptType | None = None\n    input2_prompt_type: PromptType | None = None\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; STSMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        normalized_scores = list(map(self._normalize, data_split[\"score\"]))\n        data_split = data_split.select_columns(list(self.column_names))\n\n        evaluator = AnySTSEvaluator(\n            data_split,\n            self.column_names,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            input1_prompt_type=self.input1_prompt_type,\n            input2_prompt_type=self.input2_prompt_type,\n            **kwargs,\n        )\n        scores = evaluator(\n            model,\n            encode_kwargs=encode_kwargs,\n            num_proc=num_proc,\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._calculate_scores(scores, normalized_scores)\n\n    def _calculate_scores(\n        self, scores: STSEvaluatorScores, normalized_scores: list[float]\n    ) -&gt; STSMetrics:\n        def compute_corr(x: list[float], y: list[float]) -&gt; tuple[float, float]:\n            \"\"\"Return (pearson, spearman) correlations between x and y.\"\"\"\n            return float(pearsonr(x, y)[0]), float(spearmanr(x, y)[0])\n\n        cosine_pearson, cosine_spearman = compute_corr(\n            normalized_scores, scores[\"cosine_scores\"]\n        )\n        manhattan_pearson, manhattan_spearman = compute_corr(\n            normalized_scores, scores[\"manhattan_distances\"]\n        )\n        euclidean_pearson, euclidean_spearman = compute_corr(\n            normalized_scores, scores[\"euclidean_distances\"]\n        )\n\n        if scores[\"similarity_scores\"] is not None:\n            pearson, spearman = compute_corr(\n                normalized_scores, scores[\"similarity_scores\"]\n            )\n        else:\n            # if model does not have a similarity function, assume cosine similarity\n            pearson, spearman = cosine_pearson, cosine_spearman\n\n        return STSMetrics(\n            # using the models own similarity score\n            pearson=pearson,\n            spearman=spearman,\n            # generic similarity scores\n            cosine_pearson=cosine_pearson,\n            cosine_spearman=cosine_spearman,\n            manhattan_pearson=manhattan_pearson,\n            manhattan_spearman=manhattan_spearman,\n            euclidean_pearson=euclidean_pearson,\n            euclidean_spearman=euclidean_spearman,\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; AnySTSDescriptiveStatistics:\n        first_column, second_column = self.column_names\n        self.dataset = cast(\"dict[str, dict[str, Dataset]]\", self.dataset)\n\n        if hf_subset:\n            sentence1 = self.dataset[hf_subset][split][first_column]\n            sentence2 = self.dataset[hf_subset][split][second_column]\n            score = self.dataset[hf_subset][split][\"score\"]\n        elif compute_overall:\n            sentence1 = []\n            sentence2 = []\n            score = []\n            for hf_subset in self.metadata.eval_langs:\n                sentence1.extend(self.dataset[hf_subset][split][first_column])\n                sentence2.extend(self.dataset[hf_subset][split][second_column])\n                score.extend(self.dataset[hf_subset][split][\"score\"])\n        else:\n            sentence1 = self.dataset[split][first_column]\n            sentence2 = self.dataset[split][second_column]\n            score = self.dataset[split][\"score\"]\n\n        if \"text\" in self.metadata.modalities:\n            text1_statistics = calculate_text_statistics(sentence1)\n            text2_statistics = calculate_text_statistics(sentence2)\n\n            unique_pairs = len(set(zip(sentence1, sentence2)))\n        else:\n            text1_statistics = None\n            text2_statistics = None\n            unique_pairs = None\n\n        if \"image\" in self.metadata.modalities:\n            image1_statistics = calculate_image_statistics(sentence1)\n            image2_statistics = calculate_image_statistics(sentence2)\n        else:\n            image1_statistics = None\n            image2_statistics = None\n\n        if \"audio\" in self.metadata.modalities:\n            audio1_statistics = calculate_audio_statistics(sentence1)\n            audio2_statistics = calculate_audio_statistics(sentence2)\n        else:\n            audio1_statistics = None\n            audio2_statistics = None\n\n        labels_statistics = calculate_score_statistics(score)\n\n        return AnySTSDescriptiveStatistics(\n            num_samples=len(sentence1),\n            number_of_characters=(\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n                if text1_statistics\n                else None\n            ),\n            unique_pairs=unique_pairs,\n            text1_statistics=text1_statistics,\n            text2_statistics=text2_statistics,\n            image1_statistics=image1_statistics,\n            image2_statistics=image2_statistics,\n            audio1_statistics=audio1_statistics,\n            audio2_statistics=audio2_statistics,\n            label_statistics=labels_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [self.column_names[0], self.column_names[1], \"score\"],\n            num_proc=num_proc,\n        )\n\n    def _normalize(self, x: float) -&gt; float:\n        return (x - self.min_score) / (self.max_score - self.min_score)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification","title":"<code>mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for ZeroShot Classification tasks for any modality.</p> <p>The similarity between an input (can be image or text) and candidate text prompts, such as this is a dog/this is a cat.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>Huggingface dataset containing the data for the task. Dataset must contain columns specified by self.input_column_name and self.label_column_name.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the inputs (image or text).</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the labels (str).</p> Source code in <code>mteb/abstasks/zeroshot_classification.py</code> <pre><code>class AbsTaskZeroShotClassification(AbsTask):\n    \"\"\"Abstract class for ZeroShot Classification tasks for any modality.\n\n    The similarity between an input (can be image or text) and candidate text prompts, such as this is a dog/this is a cat.\n\n    Attributes:\n        dataset: Huggingface dataset containing the data for the task. Dataset must contain columns specified by self.input_column_name and self.label_column_name.\n        input_column_name: Name of the column containing the inputs (image or text).\n        label_column_name: Name of the column containing the labels (str).\n    \"\"\"\n\n    input_column_name: str = \"image\"\n    label_column_name: str = \"label\"\n\n    def dataset_transform(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n        \"\"\"Keep only eval splits. Zero-shot tasks don't need train splits.\"\"\"\n        if self.dataset is None:\n            return\n        splits_to_keep = set(self.metadata.eval_splits)\n        for split in list(self.dataset.keys()):\n            if split not in splits_to_keep:\n                del self.dataset[split]\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ZeroShotClassificationDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs, labels = [], []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        num_samples = len(inputs)\n\n        image_statistics = None\n        text_statistics = None\n        audio_statistics = None\n\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n        if self.metadata.modalities == [\"text\"]:\n            text_statistics = calculate_text_statistics(inputs)\n\n        if \"audio\" in self.metadata.modalities:\n            audio_statistics = calculate_audio_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n        candidate_lens = calculate_text_statistics(self.get_candidate_labels())\n\n        return ZeroShotClassificationDescriptiveStatistics(\n            num_samples=num_samples,\n            number_of_characters=None,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            audio_statistics=audio_statistics,\n            label_statistics=label_statistics,\n            candidates_labels_text_statistics=candidate_lens,\n        )\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs,\n    ) -&gt; ZeroShotClassificationMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        candidate_labels = self.get_candidate_labels()\n        data_split = data_split.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n        evaluator = ZeroShotClassificationEvaluator(\n            data_split,\n            self.input_column_name,\n            candidate_labels,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        probs = evaluator(\n            model,\n            encode_kwargs=encode_kwargs,\n            num_proc=num_proc,\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                probs.tolist(),\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._calculate_scores(\n            data_split[self.label_column_name],\n            torch.tensor(probs).argmax(dim=1).tolist(),\n        )\n\n    def _calculate_scores(\n        self,\n        labels: list[int],\n        predictions: list[float],\n    ) -&gt; ZeroShotClassificationMetrics:\n        return ZeroShotClassificationMetrics(\n            accuracy=metrics.accuracy_score(labels, predictions),\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n            num_proc=num_proc,\n        )\n        labels_dataset = Dataset.from_dict({\"labels\": self.get_candidate_labels()})\n        labels_dataset.push_to_hub(repo_name, config_name=\"labels\")\n\n    def get_candidate_labels(self) -&gt; list[str]:\n        \"\"\"Return the text candidates for zeroshot classification\"\"\"\n        raise NotImplementedError(\"This method should be overridden by subclasses\")\n</code></pre>"},{"location":"api/task/#mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification.dataset_transform","title":"<code>dataset_transform(num_proc=None, **kwargs)</code>","text":"<p>Keep only eval splits. Zero-shot tasks don't need train splits.</p> Source code in <code>mteb/abstasks/zeroshot_classification.py</code> <pre><code>def dataset_transform(self, num_proc: int | None = None, **kwargs: Any) -&gt; None:\n    \"\"\"Keep only eval splits. Zero-shot tasks don't need train splits.\"\"\"\n    if self.dataset is None:\n        return\n    splits_to_keep = set(self.metadata.eval_splits)\n    for split in list(self.dataset.keys()):\n        if split not in splits_to_keep:\n            del self.dataset[split]\n</code></pre>"},{"location":"api/task/#mteb.abstasks.zeroshot_classification.AbsTaskZeroShotClassification.get_candidate_labels","title":"<code>get_candidate_labels()</code>","text":"<p>Return the text candidates for zeroshot classification</p> Source code in <code>mteb/abstasks/zeroshot_classification.py</code> <pre><code>def get_candidate_labels(self) -&gt; list[str]:\n    \"\"\"Return the text candidates for zeroshot classification\"\"\"\n    raise NotImplementedError(\"This method should be overridden by subclasses\")\n</code></pre>"},{"location":"api/task/#mteb.abstasks.regression.AbsTaskRegression","title":"<code>mteb.abstasks.regression.AbsTaskRegression</code>","text":"<p>               Bases: <code>AbsTaskClassification</code></p> <p>Abstract class for regression tasks</p> <p>self.load_data() must generate a huggingface dataset with a split matching self.metadata.eval_splits, and assign it to self.dataset. It must contain the following columns:     text: str     value: float</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the regression task. It must contain the following columns: input_column_name and label_column_name. Input can be any text or images, and label must be a continuous value.</p> <code>input_column_name</code> <code>str</code> <p>Name of the column containing the text inputs.</p> <code>label_column_name</code> <code>str</code> <p>Name of the column containing the continuous values.</p> <code>train_split</code> <code>str</code> <p>Name of the training split in the dataset.</p> <code>n_experiments</code> <code>int</code> <p>Number of experiments to run with different random seeds.</p> <code>n_samples</code> <code>int</code> <p>Number of samples to use for training the regression model. If the dataset has fewer samples than n_samples, all samples are used.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>evaluator_model</code> <code>SklearnModelProtocol</code> <p>The model to use for evaluation. Can be any sklearn compatible model. Default is <code>LinearRegression</code>.</p> Source code in <code>mteb/abstasks/regression.py</code> <pre><code>class AbsTaskRegression(AbsTaskClassification):\n    \"\"\"Abstract class for regression tasks\n\n    self.load_data() must generate a huggingface dataset with a split matching self.metadata.eval_splits, and assign it to self.dataset. It\n    must contain the following columns:\n        text: str\n        value: float\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the regression task. It must contain the following columns: input_column_name and label_column_name.\n            Input can be any text or images, and label must be a continuous value.\n        input_column_name: Name of the column containing the text inputs.\n        label_column_name: Name of the column containing the continuous values.\n        train_split: Name of the training split in the dataset.\n        n_experiments: Number of experiments to run with different random seeds.\n        n_samples: Number of samples to use for training the regression model. If the dataset has fewer samples than n_samples, all samples are used.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        evaluator_model: The model to use for evaluation. Can be any sklearn compatible model. Default is `LinearRegression`.\n    \"\"\"\n\n    evaluator: type[SklearnEvaluator] = SklearnEvaluator\n    evaluator_model: SklearnModelProtocol = LinearRegression(n_jobs=-1)\n\n    train_split: str = \"train\"\n    label_column_name: str = \"value\"\n    input_column_name: str = \"text\"\n    abstask_prompt = \"Predict the value of the user passage.\"\n\n    n_experiments: int = 10\n    n_samples: int = 2048\n\n    def _undersample_data(\n        self, dataset: Dataset, experiment_num: int, idxs: list[int] | None = None\n    ) -&gt; tuple[Dataset, list[int], list[int]]:\n        if self.n_samples &gt;= len(dataset):\n            train_split_sampled = dataset\n        else:\n            train_split_sampled = self.stratified_subsampling(\n                datasets.DatasetDict({\"train\": dataset}),\n                seed=self.seed + experiment_num,\n                splits=[\"train\"],\n                label=self.label_column_name,\n                n_samples=self.n_samples,\n            )[\"train\"]\n        return train_split_sampled, [], []\n\n    def _calculate_scores(  # type: ignore[override]\n        self,\n        y_test: NDArray[np.floating] | list[float],\n        y_pred: NDArray[np.floating] | list[float],\n    ) -&gt; RegressionMetrics:\n        mse = mean_squared_error(y_test, y_pred)\n        return RegressionMetrics(\n            mse=mse,\n            mae=mean_absolute_error(y_test, y_pred),\n            r2=r2_score(y_test, y_pred),\n            kendalltau=kendalltau(y_test, y_pred).statistic,\n            rmse=np.sqrt(mse),\n        )\n\n    @staticmethod\n    def stratified_subsampling(\n        dataset_dict: datasets.DatasetDict,\n        seed: int,\n        splits: list[str] = [\"test\"],\n        label: str = \"value\",\n        n_samples: int = 2048,\n        n_bins: int = 10,\n    ) -&gt; datasets.DatasetDict:\n        \"\"\"Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.\n\n        The continuous values are bucketized into `n_bins` bins based on quantiles.\n\n        Args:\n            dataset_dict: the DatasetDict object.\n            seed: the random seed.\n            splits: the splits of the dataset.\n            label: the label with which the stratified sampling is based on.\n            n_samples: Optional, number of samples to subsample.\n            n_bins: Optional, number of bins to bucketize the continuous label.\n\n        Returns:\n            A subsampled DatasetDict object.\n        \"\"\"\n        stratify_col_name = f\"{label}_binned_for_stratification\"\n\n        for split in splits:\n            if n_samples &gt;= len(dataset_dict[split]):\n                logger.debug(\n                    \"Subsampling not needed for split %s, as n_samples is equal or greater than the number of samples.\",\n                    split,\n                )\n                continue\n\n            dataset = dataset_dict[split]\n            labels = dataset[label]\n\n            binned_labels = pd.qcut(labels, q=n_bins, labels=False, duplicates=\"drop\")\n            dataset_with_bins: datasets.Dataset = dataset.add_column(\n                name=stratify_col_name,\n                column=binned_labels.tolist(),\n            )\n            dataset_with_bins = dataset_with_bins.cast_column(\n                stratify_col_name,\n                datasets.ClassLabel(names=np.unique(binned_labels).tolist()),\n            )\n\n            subsampled_dataset = dataset_with_bins.train_test_split(\n                test_size=n_samples, seed=seed, stratify_by_column=stratify_col_name\n            )[\"test\"]\n\n            subsampled_dataset = subsampled_dataset.remove_columns([stratify_col_name])\n            dataset_dict[split] = subsampled_dataset\n\n        return dataset_dict\n\n    def _calculate_descriptive_statistics_from_split(  # type: ignore[override]\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; RegressionDescriptiveStatistics:\n        train_text = []\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            values = self.dataset[hf_subset][split][self.label_column_name]\n            if split != self.train_split:\n                train_text = self.dataset[hf_subset][self.train_split][\n                    self.input_column_name\n                ]\n        elif compute_overall:\n            inputs = []\n            values = []\n            for lang_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[lang_subset][split][self.input_column_name])\n                values.extend(self.dataset[lang_subset][split][self.label_column_name])\n                if split != \"train\":\n                    train_text.extend(\n                        self.dataset[lang_subset][self.train_split][\n                            self.input_column_name\n                        ]\n                    )\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            values = self.dataset[split][self.label_column_name]\n            if split != \"train\":\n                train_text = self.dataset[self.train_split][self.input_column_name]\n\n        text_statistics = None\n        image_statistics = None\n        audio_statistics = None\n        num_texts_in_train = None\n        if self.metadata.modalities == [\"text\"]:\n            text_statistics = calculate_text_statistics(inputs)\n            num_texts_in_train = (\n                len(set(inputs) &amp; set(train_text))\n                if split != self.train_split\n                else None\n            )\n        elif self.metadata.modalities == [\"image\"]:\n            image_statistics = calculate_image_statistics(inputs)\n        elif self.metadata.modalities == [\"audio\"]:\n            audio_statistics = calculate_audio_statistics(inputs)\n\n        return RegressionDescriptiveStatistics(\n            num_samples=len(inputs),\n            num_texts_in_train=num_texts_in_train,\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            audio_statistics=audio_statistics,\n            values_statistics=calculate_score_statistics(values),\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.regression.AbsTaskRegression.stratified_subsampling","title":"<code>stratified_subsampling(dataset_dict, seed, splits=['test'], label='value', n_samples=2048, n_bins=10)</code>  <code>staticmethod</code>","text":"<p>Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.</p> <p>The continuous values are bucketized into <code>n_bins</code> bins based on quantiles.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>DatasetDict</code> <p>the DatasetDict object.</p> required <code>seed</code> <code>int</code> <p>the random seed.</p> required <code>splits</code> <code>list[str]</code> <p>the splits of the dataset.</p> <code>['test']</code> <code>label</code> <code>str</code> <p>the label with which the stratified sampling is based on.</p> <code>'value'</code> <code>n_samples</code> <code>int</code> <p>Optional, number of samples to subsample.</p> <code>2048</code> <code>n_bins</code> <code>int</code> <p>Optional, number of bins to bucketize the continuous label.</p> <code>10</code> <p>Returns:</p> Type Description <code>DatasetDict</code> <p>A subsampled DatasetDict object.</p> Source code in <code>mteb/abstasks/regression.py</code> <pre><code>@staticmethod\ndef stratified_subsampling(\n    dataset_dict: datasets.DatasetDict,\n    seed: int,\n    splits: list[str] = [\"test\"],\n    label: str = \"value\",\n    n_samples: int = 2048,\n    n_bins: int = 10,\n) -&gt; datasets.DatasetDict:\n    \"\"\"Subsamples the dataset with stratification by the supplied label, which is assumed to be a continuous value.\n\n    The continuous values are bucketized into `n_bins` bins based on quantiles.\n\n    Args:\n        dataset_dict: the DatasetDict object.\n        seed: the random seed.\n        splits: the splits of the dataset.\n        label: the label with which the stratified sampling is based on.\n        n_samples: Optional, number of samples to subsample.\n        n_bins: Optional, number of bins to bucketize the continuous label.\n\n    Returns:\n        A subsampled DatasetDict object.\n    \"\"\"\n    stratify_col_name = f\"{label}_binned_for_stratification\"\n\n    for split in splits:\n        if n_samples &gt;= len(dataset_dict[split]):\n            logger.debug(\n                \"Subsampling not needed for split %s, as n_samples is equal or greater than the number of samples.\",\n                split,\n            )\n            continue\n\n        dataset = dataset_dict[split]\n        labels = dataset[label]\n\n        binned_labels = pd.qcut(labels, q=n_bins, labels=False, duplicates=\"drop\")\n        dataset_with_bins: datasets.Dataset = dataset.add_column(\n            name=stratify_col_name,\n            column=binned_labels.tolist(),\n        )\n        dataset_with_bins = dataset_with_bins.cast_column(\n            stratify_col_name,\n            datasets.ClassLabel(names=np.unique(binned_labels).tolist()),\n        )\n\n        subsampled_dataset = dataset_with_bins.train_test_split(\n            test_size=n_samples, seed=seed, stratify_by_column=stratify_col_name\n        )[\"test\"]\n\n        subsampled_dataset = subsampled_dataset.remove_columns([stratify_col_name])\n        dataset_dict[split] = subsampled_dataset\n\n    return dataset_dict\n</code></pre>"},{"location":"api/task/#mteb.abstasks.clustering_legacy.AbsTaskClusteringLegacy","title":"<code>mteb.abstasks.clustering_legacy.AbsTaskClusteringLegacy</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Legacy abstract task for clustering. For new tasks, we recommend using AbsTaskClustering because it is faster, more sample-efficient, and produces more robust statistical estimates.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the clustering task. It must contain the following columns: sentences: List of inputs to be clustered. Can be text, images, etc. Name can be changed via <code>input_column_name</code>. labels: List of integer labels representing the true cluster assignments. Name can be changed via <code>label_column_name</code>.</p> <code>input_column_name</code> <code>str</code> <p>The name of the column in the dataset that contains the input sentences or data points.</p> <code>label_column_name</code> <code>str</code> <p>The name of the column in the dataset that contains the true cluster labels.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/clustering_legacy.py</code> <pre><code>class AbsTaskClusteringLegacy(AbsTask):\n    \"\"\"Legacy abstract task for clustering. For new tasks, we recommend using AbsTaskClustering because it is faster, more sample-efficient, and produces more robust statistical estimates.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the clustering task. It must contain the following columns:\n            sentences: List of inputs to be clustered. Can be text, images, etc. Name can be changed via `input_column_name`.\n            labels: List of integer labels representing the true cluster assignments. Name can be changed via `label_column_name`.\n        input_column_name: The name of the column in the dataset that contains the input sentences or data points.\n        label_column_name: The name of the column in the dataset that contains the true cluster labels.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    abstask_prompt = \"Identify categories in user passages.\"\n    evaluator: type[ClusteringEvaluator] = ClusteringEvaluator\n    input_column_name: str = \"sentences\"\n    label_column_name: str = \"labels\"\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; ScoresDict:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        data_split = data_split.select_columns(\n            [self.input_column_name, self.label_column_name]\n        )\n        # MTEB text clustering requires renaming and eval per subset.\n        if self.metadata.modalities == [\"text\"]:\n            all_metrics = []\n            clusters = []\n            for i, cluster_set in enumerate(data_split):\n                logger.info(\n                    f\"Running clustering on cluster ({i + 1}/{len(data_split)})\"\n                )\n                clustering_dataset = Dataset.from_dict(cluster_set).select_columns(\n                    [self.input_column_name, self.label_column_name]\n                )\n                evaluator = self.evaluator(\n                    clustering_dataset,\n                    input_column_name=self.input_column_name,\n                    label_column_name=self.label_column_name,\n                    task_metadata=self.metadata,\n                    hf_split=hf_split,\n                    hf_subset=hf_subset,\n                    **kwargs,\n                )\n                clusters_assignment = evaluator(model, encode_kwargs=encode_kwargs)\n                clusters.append(clusters_assignment)\n                set_metrics = self._compute_metrics(\n                    clustering_dataset[self.label_column_name],\n                    clusters_assignment,\n                    v_measure_only=True,\n                )\n                all_metrics.append(set_metrics)\n\n            if prediction_folder:\n                self._save_task_predictions(\n                    clusters,\n                    model,\n                    prediction_folder,\n                    hf_subset=hf_subset,\n                    hf_split=hf_split,\n                )\n            v_measures = [m[\"v_measure\"] for m in all_metrics]\n            v_mean = np.mean(v_measures)\n            v_std = np.std(v_measures)\n            scores = {\n                \"v_measure\": v_mean,\n                \"v_measure_std\": v_std,\n                \"v_measures\": v_measures,\n            }\n            return scores\n\n        evaluator = self.evaluator(\n            data_split,\n            input_column_name=self.input_column_name,\n            label_column_name=self.label_column_name,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        evaluate_clusters = evaluator(\n            model,\n            encode_kwargs=encode_kwargs,\n            num_proc=num_proc,\n        )\n        if prediction_folder:\n            self._save_task_predictions(\n                evaluate_clusters,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._compute_metrics(\n            data_split[self.label_column_name],\n            evaluate_clusters,\n        )\n\n    def _compute_metrics(\n        self,\n        labels: list[int],\n        cluster_assignment: list[int],\n        v_measure_only: bool = False,\n    ) -&gt; ClusteringMetrics:\n        logger.info(\"Running clustering - Evaluating clustering...\")\n        v_measure = metrics.cluster.v_measure_score(labels, cluster_assignment)\n        if v_measure_only:\n            return ClusteringMetrics(\n                v_measure=v_measure,\n            )\n        nmi = metrics.cluster.normalized_mutual_info_score(labels, cluster_assignment)\n        ari = metrics.cluster.adjusted_rand_score(labels, cluster_assignment)\n\n        matrix = metrics.confusion_matrix(labels, cluster_assignment)\n        # get linear sum assignment\n        row_ind, col_ind = linear_sum_assignment(matrix, maximize=True)\n        total_correct = matrix[row_ind, col_ind].sum()\n        clustering_accuracy = total_correct / len(labels)\n        return ClusteringMetrics(\n            v_measure=float(v_measure),\n            nmi=float(nmi),\n            ari=float(ari),\n            cluster_accuracy=float(clustering_accuracy),\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ClusteringDescriptiveStatistics:\n        if hf_subset:\n            inputs = self.dataset[hf_subset][split][self.input_column_name]\n            labels = self.dataset[hf_subset][split][self.label_column_name]\n        elif compute_overall:\n            inputs = []\n            labels = []\n            for hf_subset in self.metadata.eval_langs:\n                inputs.extend(self.dataset[hf_subset][split][self.input_column_name])\n                labels.extend(self.dataset[hf_subset][split][self.label_column_name])\n        else:\n            inputs = self.dataset[split][self.input_column_name]\n            labels = self.dataset[split][self.label_column_name]\n\n        if isinstance(inputs[0], list):\n            inputs = [item for sublist in inputs for item in sublist]\n        if isinstance(labels[0], list):\n            labels = [item for sublist in labels for item in sublist]\n\n        text_statistics, image_statistics, audio_statistics = None, None, None\n        if \"image\" in self.metadata.modalities:\n            image_statistics = calculate_image_statistics(inputs)\n\n        if \"text\" in self.metadata.modalities:\n            text_statistics = calculate_text_statistics(inputs)\n\n        if \"audio\" in self.metadata.modalities:\n            audio_statistics = calculate_audio_statistics(inputs)\n\n        label_statistics = calculate_label_statistics(labels)\n\n        return ClusteringDescriptiveStatistics(\n            num_samples=len(inputs),\n            text_statistics=text_statistics,\n            image_statistics=image_statistics,\n            audio_statistics=audio_statistics,\n            label_statistics=label_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input_column_name,\n                self.label_column_name,\n            ],\n            num_proc=num_proc,\n        )\n</code></pre>"},{"location":"api/task/#text-tasks","title":"Text Tasks","text":""},{"location":"api/task/#mteb.abstasks.text.bitext_mining.AbsTaskBitextMining","title":"<code>mteb.abstasks.text.bitext_mining.AbsTaskBitextMining</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for BitextMining tasks</p> <p>The similarity is computed between pairs and the results are ranked.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace dataset containing the data for the task. It must contain the following columns sentence1 and sentence2 for the two texts to be compared.</p> <code>parallel_subsets</code> <p>If true task language pairs should be in one split as column names, otherwise each language pair should be a subset.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/text/bitext_mining.py</code> <pre><code>class AbsTaskBitextMining(AbsTask):\n    \"\"\"Abstract class for BitextMining tasks\n\n    The similarity is computed between pairs and the results are ranked.\n\n    Attributes:\n        dataset: A HuggingFace dataset containing the data for the task. It must contain the following columns sentence1 and sentence2 for the two texts to be compared.\n        parallel_subsets: If true task language pairs should be in one split as column names, otherwise each language pair should be a subset.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    parallel_subsets = False\n    abstask_prompt = \"Retrieve parallel sentences.\"\n    _DEFAULT_PAIR: ClassVar[list[tuple[str, str]]] = [(\"sentence1\", \"sentence2\")]\n\n    def evaluate(\n        self,\n        model: MTEBModels,\n        split: str = \"test\",\n        subsets_to_run: list[HFSubset] | None = None,\n        *,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; dict[HFSubset, ScoresDict]:\n        \"\"\"Added load for \"parallel\" datasets\"\"\"\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        if not self.data_loaded:\n            self.load_data(num_proc=num_proc)\n\n        hf_subsets = self.hf_subsets\n\n        # If subsets_to_run is specified, filter the hf_subsets accordingly\n        if subsets_to_run is not None:\n            hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n        encoder_model = cast(\"EncoderProtocol\", model)\n\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not loaded.\")\n\n        scores: dict[str, BitextMiningMetrics] = {}\n        if self.parallel_subsets:\n            scores = self._evaluate_subset(  # type: ignore[assignment]\n                encoder_model,\n                self.dataset[split],\n                parallel=True,\n                hf_split=split,\n                hf_subset=\"parallel\",\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                num_proc=num_proc,\n                **kwargs,\n            )\n        else:\n            for hf_subset in hf_subsets:\n                logger.info(\n                    f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n                )\n\n                if hf_subset not in self.dataset and hf_subset == \"default\":\n                    data_split = self.dataset[split]\n                else:\n                    data_split = self.dataset[hf_subset][split]\n                scores[hf_subset] = self._evaluate_subset(  # type: ignore[assignment]\n                    encoder_model,\n                    data_split,\n                    hf_split=split,\n                    hf_subset=hf_subset,\n                    encode_kwargs=encode_kwargs,\n                    prediction_folder=prediction_folder,\n                    num_proc=num_proc,\n                    **kwargs,\n                )\n\n        return cast(\"dict[HFSubset, ScoresDict]\", scores)\n\n    def _get_pairs(self, parallel: bool) -&gt; list[tuple[str, str]]:\n        pairs = self._DEFAULT_PAIR\n        if parallel:\n            pairs = [langpair.split(\"-\") for langpair in self.hf_subsets]  # type: ignore[misc]\n        return pairs\n\n    def _evaluate_subset(  # type: ignore[override]\n        self,\n        model: EncoderProtocol,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        parallel: bool = False,\n        num_proc: int | None = None,\n        **kwargs,\n    ) -&gt; BitextMiningMetrics | dict[str, BitextMiningMetrics]:\n        pairs = self._get_pairs(parallel)\n\n        evaluator = BitextMiningEvaluator(\n            data_split,\n            task_metadata=self.metadata,\n            pair_columns=pairs,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        # NOTE: used only by BUCC\n        gold = (\n            list(zip(range(len(data_split)), range(len(data_split))))\n            if \"gold\" not in data_split\n            else data_split[\"gold\"]\n        )\n\n        neighbours = evaluator(model, encode_kwargs=encode_kwargs, num_proc=num_proc)\n\n        if prediction_folder:\n            self._save_task_predictions(\n                neighbours,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        if parallel:\n            parallel_metrics = {}\n            for keys, nearest_neighbors in neighbours.items():\n                parallel_metrics[keys] = self._compute_metrics(nearest_neighbors, gold)\n\n            for v in parallel_metrics.values():\n                self._add_main_score(v)\n            return parallel_metrics\n        def_pair_str = \"-\".join(self._DEFAULT_PAIR[0])\n        metrics = self._compute_metrics(neighbours[def_pair_str], gold)\n        self._add_main_score(metrics)\n        return metrics\n\n    def _compute_metrics(\n        self,\n        nearest_neighbors: list[dict[str, float]],\n        gold: list[tuple[int, int]],\n    ) -&gt; BitextMiningMetrics:\n        logger.info(\"Computing metrics...\")\n        labels = []\n        predictions = []\n        for i, x in enumerate(nearest_neighbors):\n            j = x[\"corpus_id\"]\n            predictions.append(j)\n            labels.append(gold[i][1])\n\n        return BitextMiningMetrics(\n            precision=precision_score(\n                labels, predictions, zero_division=0, average=\"weighted\"\n            ),\n            recall=recall_score(\n                labels, predictions, zero_division=0, average=\"weighted\"\n            ),\n            f1=f1_score(labels, predictions, zero_division=0, average=\"weighted\"),\n            accuracy=accuracy_score(labels, predictions),\n        )\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; BitextDescriptiveStatistics:\n        pairs_cols = self._get_pairs(self.parallel_subsets)\n        if hf_subset:\n            if self.parallel_subsets:\n                sent_1, sent_2 = hf_subset.split(\"-\")\n                sentence1 = self.dataset[split][sent_1]\n                sentence2 = self.dataset[split][sent_2]\n            else:\n                sent_1, sent_2 = pairs_cols[0]\n                sentence1 = self.dataset[hf_subset][split][sent_1]\n                sentence2 = self.dataset[hf_subset][split][sent_2]\n        elif compute_overall:\n            sentence1, sentence2 = [], []\n            if self.parallel_subsets:\n                for hf_subset in self.metadata.eval_langs:\n                    sent_1, sent_2 = hf_subset.split(\"-\")\n                    sentence1.extend(self.dataset[split][sent_1])\n                    sentence2.extend(self.dataset[split][sent_2])\n            else:\n                sent_1, sent_2 = pairs_cols[0]\n                for hf_subset in self.metadata.eval_langs:\n                    sentence1.extend(self.dataset[hf_subset][split][sent_1])\n                    sentence2.extend(self.dataset[hf_subset][split][sent_2])\n        else:\n            sent_1, sent_2 = pairs_cols[0]\n            sentence1 = self.dataset[split][sent_1]\n            sentence2 = self.dataset[split][sent_2]\n\n        text1_statistics = calculate_text_statistics(sentence1)\n        text2_statistics = calculate_text_statistics(sentence2)\n        unique_pairs = len(set(zip(sentence1, sentence2)))\n\n        return BitextDescriptiveStatistics(\n            num_samples=len(sentence1),\n            number_of_characters=(\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n            ),\n            unique_pairs=unique_pairs,\n            sentence1_statistics=text1_statistics,\n            sentence2_statistics=text2_statistics,\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not loaded.\")\n\n        if self.metadata.is_multilingual:\n            dataset: dict[str, dict[str, list[str]]] = defaultdict(dict)\n            for config in self.metadata.eval_langs:\n                logger.info(f\"Converting {config} of {self.metadata.name}\")\n\n                if self.parallel_subsets:\n                    for split in self.dataset:\n                        sent_1, sent_2 = config.split(\"-\")\n                        dataset[split][sent_1] = self.dataset[split][sent_1]\n                        dataset[split][sent_2] = self.dataset[split][sent_2]\n                else:\n                    sent_1, sent_2 = self._get_pairs(self.parallel_subsets)[0]\n                    lang_1, lang_2 = config.split(\"-\")\n                    for split in self.dataset[config]:\n                        dataset[split][lang_1] = self.dataset[config][split][sent_1]\n                        dataset[split][lang_2] = self.dataset[config][split][sent_2]\n            dataset_dict = DatasetDict(\n                {split: Dataset.from_dict(dataset[split]) for split in dataset}\n            )\n            dataset_dict.push_to_hub(repo_name, num_proc=num_proc)\n        else:\n            sentences = {}\n            for split in self.dataset:\n                sent_1, sent_2 = self._get_pairs(self.parallel_subsets)[0]\n                sentences[split] = Dataset.from_dict(\n                    {\n                        \"sentence1\": self.dataset[split][sent_1],\n                        \"sentence2\": self.dataset[split][sent_2],\n                    }\n                )\n            sentences = DatasetDict(sentences)\n            sentences.push_to_hub(repo_name, num_proc=num_proc)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.bitext_mining.AbsTaskBitextMining.evaluate","title":"<code>evaluate(model, split='test', subsets_to_run=None, *, encode_kwargs, prediction_folder=None, num_proc=None, **kwargs)</code>","text":"<p>Added load for \"parallel\" datasets</p> Source code in <code>mteb/abstasks/text/bitext_mining.py</code> <pre><code>def evaluate(\n    self,\n    model: MTEBModels,\n    split: str = \"test\",\n    subsets_to_run: list[HFSubset] | None = None,\n    *,\n    encode_kwargs: EncodeKwargs,\n    prediction_folder: Path | None = None,\n    num_proc: int | None = None,\n    **kwargs: Any,\n) -&gt; dict[HFSubset, ScoresDict]:\n    \"\"\"Added load for \"parallel\" datasets\"\"\"\n    if not isinstance(model, EncoderProtocol):\n        raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n    if not self.data_loaded:\n        self.load_data(num_proc=num_proc)\n\n    hf_subsets = self.hf_subsets\n\n    # If subsets_to_run is specified, filter the hf_subsets accordingly\n    if subsets_to_run is not None:\n        hf_subsets = [s for s in hf_subsets if s in subsets_to_run]\n\n    encoder_model = cast(\"EncoderProtocol\", model)\n\n    if self.dataset is None:\n        raise ValueError(\"Dataset is not loaded.\")\n\n    scores: dict[str, BitextMiningMetrics] = {}\n    if self.parallel_subsets:\n        scores = self._evaluate_subset(  # type: ignore[assignment]\n            encoder_model,\n            self.dataset[split],\n            parallel=True,\n            hf_split=split,\n            hf_subset=\"parallel\",\n            encode_kwargs=encode_kwargs,\n            prediction_folder=prediction_folder,\n            num_proc=num_proc,\n            **kwargs,\n        )\n    else:\n        for hf_subset in hf_subsets:\n            logger.info(\n                f\"Task: {self.metadata.name}, split: {split}, subset: {hf_subset}. Running...\"\n            )\n\n            if hf_subset not in self.dataset and hf_subset == \"default\":\n                data_split = self.dataset[split]\n            else:\n                data_split = self.dataset[hf_subset][split]\n            scores[hf_subset] = self._evaluate_subset(  # type: ignore[assignment]\n                encoder_model,\n                data_split,\n                hf_split=split,\n                hf_subset=hf_subset,\n                encode_kwargs=encode_kwargs,\n                prediction_folder=prediction_folder,\n                num_proc=num_proc,\n                **kwargs,\n            )\n\n    return cast(\"dict[HFSubset, ScoresDict]\", scores)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.pair_classification.AbsTaskPairClassification","title":"<code>mteb.abstasks.pair_classification.AbsTaskPairClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for PairClassificationTasks</p> <p>The similarity is computed between pairs and the results are ranked. Average precision is computed to measure how well the methods can be used for pairwise pair classification.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace dataset containing the data for the task. Should contain the following columns: sentence1, sentence2, labels.</p> <code>input1_column_name</code> <code>str</code> <p>The name of the column containing the first sentence in the pair.</p> <code>input2_column_name</code> <code>str</code> <p>The name of the column containing the second sentence in the pair.</p> <code>label_column_name</code> <code>str</code> <p>The name of the column containing the labels for the pairs. Labels should be 0 or 1.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> <code>input1_prompt_type</code> <code>PromptType | None</code> <p>Type of prompt of first input. Used for asymmetric tasks.</p> <code>input2_prompt_type</code> <code>PromptType | None</code> <p>Type of prompt of second input. Used for asymmetric tasks.</p> Source code in <code>mteb/abstasks/pair_classification.py</code> <pre><code>class AbsTaskPairClassification(AbsTask):\n    \"\"\"Abstract class for PairClassificationTasks\n\n    The similarity is computed between pairs and the results are ranked. Average precision\n    is computed to measure how well the methods can be used for pairwise pair classification.\n\n    Attributes:\n        dataset: A HuggingFace dataset containing the data for the task. Should contain the following columns: sentence1, sentence2, labels.\n        input1_column_name: The name of the column containing the first sentence in the pair.\n        input2_column_name: The name of the column containing the second sentence in the pair.\n        label_column_name: The name of the column containing the labels for the pairs. Labels should be 0 or 1.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n        input1_prompt_type: Type of prompt of first input. Used for asymmetric tasks.\n        input2_prompt_type: Type of prompt of second input. Used for asymmetric tasks.\n    \"\"\"\n\n    abstask_prompt = \"Retrieve text that are semantically similar to the given text.\"\n    input1_column_name: str = \"sentence1\"\n    input2_column_name: str = \"sentence2\"\n    label_column_name: str = \"labels\"\n    input1_prompt_type: PromptType | None = None\n    input2_prompt_type: PromptType | None = None\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs,\n    ) -&gt; dict[str, float]:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        if self.metadata.modalities == [\"text\"]:\n            # for compatibility with v1 version where datasets were stored in a single row\n            data_split = (\n                Dataset.from_dict(data_split[0]) if len(data_split) == 1 else data_split\n            )\n        evaluator = PairClassificationEvaluator(\n            data_split,\n            self.input1_column_name,\n            self.input2_column_name,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            input1_prompt_type=self.input1_prompt_type,\n            input2_prompt_type=self.input2_prompt_type,\n            **kwargs,\n        )\n        similarity_scores = evaluator(\n            model,\n            encode_kwargs=encode_kwargs,\n            num_proc=num_proc,\n        )\n\n        if prediction_folder:\n            self._save_task_predictions(\n                similarity_scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n        return self._compute_metrics(\n            similarity_scores, data_split[self.label_column_name]\n        )\n\n    def _compute_metrics(\n        self, similarity_scores: PairClassificationDistances, labels: list[int]\n    ) -&gt; dict[str, float]:\n        logger.info(\"Computing metrics...\")\n        np_labels: NDArray[np.int64] = np.asarray(labels, dtype=np.int64)\n        output_scores = {}\n        max_scores = defaultdict(list)\n        for short_name, scores, reverse in [\n            [\n                \"similarity\",\n                similarity_scores[\"similarity_scores\"],\n                True,\n            ],\n            [ScoringFunction.COSINE.value, similarity_scores[\"cosine_scores\"], True],\n            [\n                ScoringFunction.MANHATTAN.value,\n                similarity_scores[\"manhattan_distances\"],\n                False,\n            ],\n            [\n                ScoringFunction.EUCLIDEAN.value,\n                similarity_scores[\"euclidean_distances\"],\n                False,\n            ],\n            [ScoringFunction.DOT_PRODUCT.value, similarity_scores[\"dot_scores\"], True],\n        ]:\n            metrics = self._compute_metrics_values(scores, np_labels, reverse)  # type: ignore[arg-type]\n            for metric_name, metric_value in metrics.items():\n                output_scores[f\"{short_name}_{metric_name}\"] = metric_value\n                max_scores[metric_name].append(metric_value)\n\n        for metric in max_scores:\n            if metric in [\"f1\", \"ap\", \"precision\", \"recall\", \"accuracy\"]:\n                output_scores[f\"max_{metric}\"] = max(max_scores[metric])\n        return output_scores\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; PairClassificationDescriptiveStatistics:\n        if hf_subset:\n            dataset = self.dataset[hf_subset][split]\n        elif compute_overall:\n            dataset = defaultdict(list)\n            for hf_subset in self.metadata.eval_langs:\n                cur_dataset = self.dataset[hf_subset][split]\n                # for compatibility with v1 version where datasets were stored in a single row\n                if isinstance(cur_dataset, list) or len(cur_dataset) == 1:\n                    cur_dataset = cur_dataset[0]\n                if isinstance(cur_dataset, Dataset):\n                    for row in cur_dataset:\n                        for k, v in row.items():\n                            dataset[k].append(v)\n                else:\n                    for key, value in cur_dataset.items():\n                        dataset[key].extend(value[0] if len(value) == 1 else value)\n        else:\n            dataset = self.dataset[split]\n\n        if isinstance(dataset, list):\n            dataset = dataset[0]\n\n        input1 = (\n            dataset[self.input1_column_name][0]\n            if len(dataset[self.input1_column_name]) == 1\n            else dataset[self.input1_column_name]\n        )\n        input2 = (\n            dataset[self.input2_column_name][0]\n            if len(dataset[self.input2_column_name]) == 1\n            else dataset[self.input2_column_name]\n        )\n        labels = (\n            dataset[self.label_column_name][0]\n            if len(dataset[self.label_column_name]) == 1\n            else dataset[self.label_column_name]\n        )\n\n        text1_statistics = None\n        text2_statistics = None\n        image1_statistics = None\n        image2_statistics = None\n        number_of_characters = None\n        audio1_statistics = None\n        audio2_statistics = None\n        unique_pairs = None\n        if self.metadata.modalities == [\"text\"]:\n            text1_statistics = calculate_text_statistics(input1)\n            text2_statistics = calculate_text_statistics(input2)\n            number_of_characters = (\n                text1_statistics[\"total_text_length\"]\n                + text2_statistics[\"total_text_length\"]\n            )\n            unique_pairs = len(set(zip(input1, input2)))\n\n        if self.metadata.modalities == [\"image\"]:\n            image1_statistics = calculate_image_statistics(input1)\n            image2_statistics = calculate_image_statistics(input2)\n\n            def _compute_image_hash(inputs: list) -&gt; list[str]:\n                hashes = set()\n                for img in inputs:\n                    img_bytes = img.tobytes()\n                    img_hash = hashlib.md5(img_bytes).hexdigest()\n                    hashes.add(img_hash)\n                return list(hashes)\n\n            image_1_hashes = _compute_image_hash(input1)\n            image_2_hashes = _compute_image_hash(input2)\n            unique_pairs = len(set(zip(image_1_hashes, image_2_hashes)))\n\n        if self.metadata.modalities == [\"audio\"]:\n            audio1_statistics = calculate_audio_statistics(input1)\n            audio2_statistics = calculate_audio_statistics(input2)\n\n            def _compute_audio_hash(inputs: list) -&gt; list[str]:\n                hashes = set()\n                for audio in inputs:\n                    array = audio[\"array\"]\n                    audio_bytes = array.tobytes()\n                    audio_hash = hashlib.md5(audio_bytes).hexdigest()\n                    hashes.add(audio_hash)\n                return list(hashes)\n\n            audio_1_hashes = _compute_audio_hash(input1)\n            audio_2_hashes = _compute_audio_hash(input2)\n            unique_pairs = len(set(zip(audio_1_hashes, audio_2_hashes)))\n\n        return PairClassificationDescriptiveStatistics(\n            num_samples=len(input1),\n            unique_pairs=unique_pairs,\n            number_of_characters=number_of_characters,\n            text1_statistics=text1_statistics,\n            image1_statistics=image1_statistics,\n            audio1_statistics=audio1_statistics,\n            text2_statistics=text2_statistics,\n            image2_statistics=image2_statistics,\n            audio2_statistics=audio2_statistics,\n            labels_statistics=calculate_label_statistics(labels),\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        # previously pair classification datasets were stored in a single row\n        if self.dataset is None:\n            # overall this shouldn't happen as we check for dataset before pushing to hub\n            # added here for type checking purposes\n            raise RuntimeError(\n                \"Dataset not loaded. To load dataset run `task.load_data()`.\"\n            )\n        if self.metadata.is_multilingual:\n            for subset in self.dataset:\n                for split in self.dataset[subset]:\n                    if len(self.dataset[subset][split]) == 1:\n                        self.dataset[subset][split] = self.dataset[subset][split][0]\n        else:\n            for split in self.dataset:\n                if len(self.dataset[split]) == 1:\n                    self.dataset[split] = self.dataset[split][0]\n        self._upload_dataset_to_hub(\n            repo_name,\n            [\n                self.input1_column_name,\n                self.input2_column_name,\n                self.label_column_name,\n            ],\n            num_proc=num_proc,\n        )\n\n    def _compute_metrics_values(\n        self,\n        scores: list[float],\n        labels: NDArray[np.int64],\n        high_score_more_similar: bool,\n    ) -&gt; dict[str, float]:\n        \"\"\"Compute the metrics for the given scores and labels.\n\n        Args:\n            scores: The similarity/dissimilarity scores for the pairs, specified as an array of shape (n_pairs, ).\n            labels: The labels for the pairs, specified as an array of shape (n_pairs, ).\n            high_score_more_similar: If true, then the higher the score, the more similar the pairs are.\n\n        Returns:\n            The metrics for the given scores and labels.\n        \"\"\"\n        acc, acc_threshold = self._find_best_acc_and_threshold(\n            scores, labels, high_score_more_similar\n        )\n        (\n            f1,\n            precision,\n            recall,\n            f1_threshold,\n        ) = self._find_best_f1_and_threshold(scores, labels, high_score_more_similar)\n        ap = average_precision_score(\n            labels, np.array(scores) * (1 if high_score_more_similar else -1)\n        )\n\n        return dict(\n            accuracy=float(acc),\n            f1=float(f1),\n            precision=float(precision),\n            recall=float(recall),\n            ap=float(ap),\n        )\n\n    def _find_best_acc_and_threshold(\n        self,\n        scores: list[float],\n        labels: NDArray[np.int64],\n        high_score_more_similar: bool,\n    ) -&gt; tuple[float, float]:\n        rows = list(zip(scores, labels))\n        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n\n        max_acc = 0\n        best_threshold = -1.0\n        positive_so_far = 0\n        remaining_negatives = sum(labels == 0)\n\n        for i in range(len(rows) - 1):\n            score, label = rows[i]\n            if label == 1:\n                positive_so_far += 1\n            else:\n                remaining_negatives -= 1\n\n            acc = (positive_so_far + remaining_negatives) / len(labels)\n            if acc &gt; max_acc:\n                max_acc = acc\n                best_threshold = (rows[i][0] + rows[i + 1][0]) / 2\n        return max_acc, best_threshold\n\n    def _find_best_f1_and_threshold(\n        self, scores, labels: NDArray[np.int64], high_score_more_similar: bool\n    ) -&gt; tuple[float, float, float, float]:\n        scores = np.asarray(scores)\n\n        rows = list(zip(scores, labels))\n\n        rows = sorted(rows, key=lambda x: x[0], reverse=high_score_more_similar)\n\n        best_f1 = best_precision = best_recall = 0.0\n        threshold = 0\n        nextract = 0\n        ncorrect = 0\n        total_num_duplicates = sum(labels)\n\n        for i in range(len(rows) - 1):\n            score, label = rows[i]\n            nextract += 1\n\n            if label == 1:\n                ncorrect += 1\n\n            if ncorrect &gt; 0:\n                precision = ncorrect / nextract\n                recall = ncorrect / total_num_duplicates\n                f1 = 2 * precision * recall / (precision + recall)\n                if f1 &gt; best_f1:\n                    best_f1 = f1\n                    best_precision = precision\n                    best_recall = recall\n                    threshold = (rows[i][0] + rows[i + 1][0]) / 2\n\n        return best_f1, best_precision, best_recall, threshold\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.summarization.AbsTaskSummarization","title":"<code>mteb.abstasks.text.summarization.AbsTaskSummarization</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for summarization experiments.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>HuggingFace dataset containing the data for the task. Should have columns: - text: The original text to be summarized. - human_summaries: A list of human-written summaries for the text. - machine_summaries: A list of machine-generated summaries for the text. - relevance: A list of relevance scores (integers) corresponding to each machine summary, indicating how relevant each summary is to the original text.</p> <code>min_score</code> <code>int</code> <p>Minimum possible relevance score (inclusive).</p> <code>max_score</code> <code>int</code> <p>Maximum possible relevance score (inclusive).</p> <code>human_summaries_column_name</code> <code>str</code> <p>Name of the column containing human summaries.</p> <code>machine_summaries_column_name</code> <code>str</code> <p>Name of the column containing machine summaries.</p> <code>text_column_name</code> <code>str</code> <p>Name of the column containing the original text.</p> <code>relevancy_column_name</code> <code>str</code> <p>Name of the column containing relevance scores.</p> <code>abstask_prompt</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/text/summarization.py</code> <pre><code>class AbsTaskSummarization(AbsTask):\n    \"\"\"Abstract class for summarization experiments.\n\n    Attributes:\n        dataset: HuggingFace dataset containing the data for the task. Should have columns:\n            - text: The original text to be summarized.\n            - human_summaries: A list of human-written summaries for the text.\n            - machine_summaries: A list of machine-generated summaries for the text.\n            - relevance: A list of relevance scores (integers) corresponding to each machine summary, indicating how relevant each summary is to the original text.\n        min_score: Minimum possible relevance score (inclusive).\n        max_score: Maximum possible relevance score (inclusive).\n        human_summaries_column_name: Name of the column containing human summaries.\n        machine_summaries_column_name: Name of the column containing machine summaries.\n        text_column_name: Name of the column containing the original text.\n        relevancy_column_name: Name of the column containing relevance scores.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    min_score: int\n    max_score: int\n\n    abstask_prompt = (\n        \"Given a news summary, retrieve other semantically similar summaries.\"\n    )\n    # SummEval has DeprecatedSummarizationEvaluator\n    evaluator = SummarizationEvaluator\n    text_column_name: str = \"text\"\n    human_summaries_column_name: str = \"human_summaries\"\n    machine_summaries_column_name: str = \"machine_summaries\"\n    relevancy_column_name: str = \"relevance\"\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        hf_split: str,\n        hf_subset: str,\n        encode_kwargs: EncodeKwargs,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs,\n    ) -&gt; SummarizationMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n\n        normalized_scores = [\n            (\n                (np.array(x) - self.min_score) / (self.max_score - self.min_score)\n            ).tolist()\n            for x in data_split[self.relevancy_column_name]\n        ]\n        evaluator = self.evaluator(\n            machine_summaries=data_split[self.machine_summaries_column_name],\n            human_summaries=data_split[self.human_summaries_column_name],\n            texts=data_split[self.text_column_name],\n            gold_scores=normalized_scores,\n            task_metadata=self.metadata,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        scores = evaluator(model, encode_kwargs=encode_kwargs, num_proc=num_proc)\n        if prediction_folder:\n            self._save_task_predictions(\n                scores,\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n        return evaluator._calculate_metrics(scores)\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; SummarizationDescriptiveStatistics:\n        if hf_subset:\n            text = self.dataset[hf_subset][split][self.text_column_name]\n            human_summaries = self.dataset[hf_subset][split][\n                self.human_summaries_column_name\n            ]\n            machine_summaries = self.dataset[hf_subset][split][\n                self.machine_summaries_column_name\n            ]\n            relevance = self.dataset[hf_subset][split][self.relevancy_column_name]\n        elif compute_overall:\n            text = []\n            human_summaries = []\n            machine_summaries = []\n            relevance = []\n\n            for hf_subset in self.metadata.eval_langs:\n                text.extend(self.dataset[hf_subset][split][self.text_column_name])\n                human_summaries.extend(\n                    self.dataset[hf_subset][split][self.human_summaries_column_name]\n                )\n                machine_summaries.extend(\n                    self.dataset[hf_subset][split][self.machine_summaries_column_name]\n                )\n                relevance.extend(\n                    self.dataset[hf_subset][split][self.relevancy_column_name]\n                )\n        else:\n            text = self.dataset[split][self.text_column_name]\n            human_summaries = self.dataset[split][self.human_summaries_column_name]\n            machine_summaries = self.dataset[split][self.machine_summaries_column_name]\n            relevance = self.dataset[split][self.relevancy_column_name]\n\n        all_human_summaries = []\n        for s in human_summaries:\n            all_human_summaries.extend(s)\n\n        all_machine_summaries = []\n        for s in machine_summaries:\n            all_machine_summaries.extend(s)\n\n        text_statistics = calculate_text_statistics(text)\n        human_summaries_statistics = calculate_text_statistics(all_human_summaries)\n        machine_summaries_statistics = calculate_text_statistics(all_machine_summaries)\n\n        relevance = [item for sublist in relevance for item in sublist]\n\n        return SummarizationDescriptiveStatistics(\n            num_samples=len(text),\n            number_of_characters=(\n                text_statistics[\"total_text_length\"]\n                + human_summaries_statistics[\"total_text_length\"]\n                + machine_summaries_statistics[\"total_text_length\"]\n            ),\n            text_statistics=text_statistics,\n            human_summaries_statistics=human_summaries_statistics,\n            machine_summaries_statistics=machine_summaries_statistics,\n            score_statistics=calculate_score_statistics(relevance),\n        )\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking","title":"<code>mteb.abstasks.text.reranking.AbsTaskReranking</code>","text":"<p>               Bases: <code>AbsTaskRetrieval</code></p> <p>Reranking task class.</p> Deprecated <p>This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead. You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format. You can reupload it using <code>task.push_dataset_to_hub('your/repository')</code> after loading the data. For dataformat and other information, see AbsTaskRetrieval.</p> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>@deprecated(\n    \"This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead. \"\n    \"You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format. \"\n    \"You can reupload it using `task.push_dataset_to_hub('your/repository')` after loading the data.\"\n)\nclass AbsTaskReranking(AbsTaskRetrieval):\n    \"\"\"Reranking task class.\n\n    Warning: Deprecated\n        This class is deprecated and will be removed in future versions. Please use the updated retrieval tasks instead.\n        You can add your task name to mteb.abstasks.text.reranking.OLD_FORMAT_RERANKING_TASKS to load it in new format.\n        You can reupload it using `task.push_dataset_to_hub('your/repository')` after loading the data.\n        For dataformat and other information, see [AbsTaskRetrieval][mteb.abstasks.retrieval.AbsTaskRetrieval].\n    \"\"\"\n\n    def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n        \"\"\"Load the dataset.\"\"\"\n        if self.data_loaded:\n            return\n\n        if self.metadata.name in OLD_FORMAT_RERANKING_TASKS:\n            self.transform_old_dataset_format()\n        else:\n            # use AbsTaskRetrieval default to load the data\n            return super().load_data(num_proc=num_proc)\n\n    def _process_example(self, example: dict, split: str, query_idx: int) -&gt; dict:\n        \"\"\"Process a single example from the dataset.\n\n        Args:\n            example: A single example from the dataset containing 'query', 'positive', and 'negative' fields.\n            split: The dataset split (e.g., 'train', 'validation', 'test').\n            query_idx: The index of the query in the dataset split.\n\n        Returns:\n            A dictionary containing the processed example with query_id, query text, document ids, document texts, and relevance scores.\n        \"\"\"\n        query = example[\"query\"]\n        positive_docs = example[\"positive\"]\n        negative_docs = example[\"negative\"]\n\n        query_id = f\"{split}_query{query_idx}\"\n\n        # Initialize the structures for this example\n        example_data = {\n            \"query_id\": query_id,\n            \"query\": query,\n            \"doc_ids\": [],\n            \"doc_texts\": [],\n            \"relevance_scores\": [],\n        }\n\n        for i, pos_doc in enumerate(positive_docs):\n            # format i as a five digit number\n            formatted_i = str(i).zfill(5)\n            # have \"a\" in front so that positives are first, then negatives\n            #   this shouldn't matter except for ties, and the previous reranking results\n            #   had the positives first\n            doc_id = f\"apositive_{query_id}_{formatted_i}\"\n            example_data[\"doc_ids\"].append(doc_id)\n            example_data[\"doc_texts\"].append(pos_doc)\n            example_data[\"relevance_scores\"].append(1)\n\n        for i, neg_doc in enumerate(negative_docs):\n            formatted_i = str(i).zfill(5)\n            doc_id = f\"negative_{query_id}_{formatted_i}\"\n            example_data[\"doc_ids\"].append(doc_id)\n            example_data[\"doc_texts\"].append(neg_doc)\n            example_data[\"relevance_scores\"].append(0)\n\n        return example_data\n\n    def transform_old_dataset_format(self, given_dataset: Dataset | None = None):\n        \"\"\"Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.\n\n        Args:\n            given_dataset (Dataset, optional): The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.\n        \"\"\"\n        if self.metadata.name not in OLD_FORMAT_RERANKING_TASKS:\n            return\n\n        logger.info(\n            f\"Transforming old format to standard format for {self.metadata.name}\"\n        )\n\n        given_dataset = copy(given_dataset)\n        self.dataset: dict[str, dict[str, RetrievalSplitData]] = defaultdict(\n            lambda: defaultdict(dict)  # type: ignore[arg-type]\n        )\n\n        hf_subsets = self.hf_subsets\n\n        for hf_subset in hf_subsets:\n            if given_dataset:\n                cur_dataset = given_dataset\n                if hf_subset in cur_dataset:\n                    cur_dataset = cur_dataset[hf_subset]\n            elif \"name\" in self.metadata.dataset:\n                cur_dataset = datasets.load_dataset(**self.metadata.dataset)\n                assert hf_subset == \"default\", (\n                    f\"Only default subset is supported for {self.metadata.name} since `name` is given in the metadata.\"\n                )\n            else:\n                cur_dataset = datasets.load_dataset(\n                    **self.metadata.dataset, name=hf_subset\n                )\n\n            for split in cur_dataset:\n                corpus = []\n                queries = []\n                relevant_docs: dict[str, dict[str, int]] = defaultdict(dict)\n                top_ranked = defaultdict(list)\n\n                # Create an enumerated dataset to pass indices\n                enumerated_dataset = Dataset.from_dict(\n                    {\n                        \"index\": range(len(cur_dataset[split])),\n                        \"query\": cur_dataset[split][\"query\"],\n                        \"positive\": cur_dataset[split][\"positive\"],\n                        \"negative\": cur_dataset[split][\"negative\"],\n                    }\n                )\n\n                # first, filter out the ones that have no positive or no negatives\n                enumerated_dataset = enumerated_dataset.filter(\n                    lambda example: len(example[\"positive\"]) &gt; 0\n                    and len(example[\"negative\"]) &gt; 0\n                )\n\n                logger.info(\n                    f\"Filtered out {len(cur_dataset[split]) - len(enumerated_dataset)} examples with no positive or no negative examples. {len(enumerated_dataset)} examples remaining.\"\n                )\n\n                # Map the transformation function over the dataset\n                processed_dataset = enumerated_dataset.map(\n                    lambda example, idx: self._process_example(example, split, idx),\n                    with_indices=True,\n                    remove_columns=enumerated_dataset.column_names,\n                )\n\n                # Populate the data structures\n                for item in processed_dataset:\n                    query_id = item[\"query_id\"]\n                    queries.append({\"id\": query_id, \"text\": item[\"query\"]})\n\n                    # Add documents and relevance information\n                    for doc_id, doc_text, relevance in zip(\n                        item[\"doc_ids\"], item[\"doc_texts\"], item[\"relevance_scores\"]\n                    ):\n                        corpus.append(\n                            {\n                                \"title\": \"\",\n                                \"text\": doc_text,\n                                \"id\": doc_id,\n                            }\n                        )\n                        top_ranked[query_id].append(doc_id)\n                        relevant_docs[query_id][doc_id] = relevance\n\n                self.dataset[hf_subset][split] = RetrievalSplitData(\n                    corpus=Dataset.from_list(corpus),\n                    queries=Dataset.from_list(queries),\n                    relevant_docs=relevant_docs,\n                    top_ranked=top_ranked,\n                )\n        self.data_loaded = True\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking.load_data","title":"<code>load_data(num_proc=None, **kwargs)</code>","text":"<p>Load the dataset.</p> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n    \"\"\"Load the dataset.\"\"\"\n    if self.data_loaded:\n        return\n\n    if self.metadata.name in OLD_FORMAT_RERANKING_TASKS:\n        self.transform_old_dataset_format()\n    else:\n        # use AbsTaskRetrieval default to load the data\n        return super().load_data(num_proc=num_proc)\n</code></pre>"},{"location":"api/task/#mteb.abstasks.text.reranking.AbsTaskReranking.transform_old_dataset_format","title":"<code>transform_old_dataset_format(given_dataset=None)</code>","text":"<p>Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.</p> <p>Parameters:</p> Name Type Description Default <code>given_dataset</code> <code>Dataset</code> <p>The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.</p> <code>None</code> Source code in <code>mteb/abstasks/text/reranking.py</code> <pre><code>def transform_old_dataset_format(self, given_dataset: Dataset | None = None):\n    \"\"\"Transform the old format to the new format using HF datasets mapping. This is a one-time transformation for datasets which are in the old format.\n\n    Args:\n        given_dataset (Dataset, optional): The dataset to transform. Defaults to None. This is helpful for some older datasets which are loaded with custom code, but need to be transformed still.\n    \"\"\"\n    if self.metadata.name not in OLD_FORMAT_RERANKING_TASKS:\n        return\n\n    logger.info(\n        f\"Transforming old format to standard format for {self.metadata.name}\"\n    )\n\n    given_dataset = copy(given_dataset)\n    self.dataset: dict[str, dict[str, RetrievalSplitData]] = defaultdict(\n        lambda: defaultdict(dict)  # type: ignore[arg-type]\n    )\n\n    hf_subsets = self.hf_subsets\n\n    for hf_subset in hf_subsets:\n        if given_dataset:\n            cur_dataset = given_dataset\n            if hf_subset in cur_dataset:\n                cur_dataset = cur_dataset[hf_subset]\n        elif \"name\" in self.metadata.dataset:\n            cur_dataset = datasets.load_dataset(**self.metadata.dataset)\n            assert hf_subset == \"default\", (\n                f\"Only default subset is supported for {self.metadata.name} since `name` is given in the metadata.\"\n            )\n        else:\n            cur_dataset = datasets.load_dataset(\n                **self.metadata.dataset, name=hf_subset\n            )\n\n        for split in cur_dataset:\n            corpus = []\n            queries = []\n            relevant_docs: dict[str, dict[str, int]] = defaultdict(dict)\n            top_ranked = defaultdict(list)\n\n            # Create an enumerated dataset to pass indices\n            enumerated_dataset = Dataset.from_dict(\n                {\n                    \"index\": range(len(cur_dataset[split])),\n                    \"query\": cur_dataset[split][\"query\"],\n                    \"positive\": cur_dataset[split][\"positive\"],\n                    \"negative\": cur_dataset[split][\"negative\"],\n                }\n            )\n\n            # first, filter out the ones that have no positive or no negatives\n            enumerated_dataset = enumerated_dataset.filter(\n                lambda example: len(example[\"positive\"]) &gt; 0\n                and len(example[\"negative\"]) &gt; 0\n            )\n\n            logger.info(\n                f\"Filtered out {len(cur_dataset[split]) - len(enumerated_dataset)} examples with no positive or no negative examples. {len(enumerated_dataset)} examples remaining.\"\n            )\n\n            # Map the transformation function over the dataset\n            processed_dataset = enumerated_dataset.map(\n                lambda example, idx: self._process_example(example, split, idx),\n                with_indices=True,\n                remove_columns=enumerated_dataset.column_names,\n            )\n\n            # Populate the data structures\n            for item in processed_dataset:\n                query_id = item[\"query_id\"]\n                queries.append({\"id\": query_id, \"text\": item[\"query\"]})\n\n                # Add documents and relevance information\n                for doc_id, doc_text, relevance in zip(\n                    item[\"doc_ids\"], item[\"doc_texts\"], item[\"relevance_scores\"]\n                ):\n                    corpus.append(\n                        {\n                            \"title\": \"\",\n                            \"text\": doc_text,\n                            \"id\": doc_id,\n                        }\n                    )\n                    top_ranked[query_id].append(doc_id)\n                    relevant_docs[query_id][doc_id] = relevance\n\n            self.dataset[hf_subset][split] = RetrievalSplitData(\n                corpus=Dataset.from_list(corpus),\n                queries=Dataset.from_list(queries),\n                relevant_docs=relevant_docs,\n                top_ranked=top_ranked,\n            )\n    self.data_loaded = True\n</code></pre>"},{"location":"api/task/#image-tasks","title":"Image Tasks","text":""},{"location":"api/task/#mteb.abstasks.image.image_text_pair_classification.AbsTaskImageTextPairClassification","title":"<code>mteb.abstasks.image.image_text_pair_classification.AbsTaskImageTextPairClassification</code>","text":"<p>               Bases: <code>AbsTask</code></p> <p>Abstract class for Image Text Pair Classification tasks (Compositionality evaluation).</p> <p>The similarity is computed between pairs and the results are ranked. Note that the number of images and the number of captions can be different.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>dict[HFSubset, DatasetDict] | None</code> <p>A HuggingFace Dataset containing the data for the ImageTextPairClassification task. Should have columns: - images: List of images. - captions: List of captions.</p> <code>images_column_names</code> <code>str | Sequence[str]</code> <p>Name of the column(s) containing the images.</p> <code>texts_column_names</code> <code>str | Sequence[str]</code> <p>Name of the column(s) containing the captions.</p> <code>abstask_prompt</code> <code>str</code> <p>Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.</p> Source code in <code>mteb/abstasks/image/image_text_pair_classification.py</code> <pre><code>class AbsTaskImageTextPairClassification(AbsTask):\n    \"\"\"Abstract class for Image Text Pair Classification tasks (Compositionality evaluation).\n\n    The similarity is computed between pairs and the results are ranked.\n    Note that the number of images and the number of captions can be different.\n\n    Attributes:\n        dataset: A HuggingFace Dataset containing the data for the ImageTextPairClassification task. Should have columns:\n            - images: List of images.\n            - captions: List of captions.\n        images_column_names: Name of the column(s) containing the images.\n        texts_column_names: Name of the column(s) containing the captions.\n        abstask_prompt: Prompt to use for the task for instruction model if not prompt is provided in TaskMetadata.prompt.\n    \"\"\"\n\n    # it can be [\"image_0\", \"image_1\"]; [\"text_0\", \"text_1\"] for datasets like WinoGround\n    images_column_names: str | Sequence[str] = \"image\"\n    texts_column_names: str | Sequence[str] = \"caption\"\n\n    def _calculate_descriptive_statistics_from_split(\n        self, split: str, hf_subset: str | None = None, compute_overall: bool = False\n    ) -&gt; ImageTextPairClassificationDescriptiveStatistics:\n        if compute_overall:\n            dataset = concatenate_datasets(\n                [\n                    self.dataset[hf_subset][split]\n                    for hf_subset in self.metadata.eval_langs\n                ]\n            )\n        else:\n            dataset = (\n                self.dataset[split]\n                if hf_subset is None\n                else self.dataset[hf_subset][split]\n            )\n        num_samples = len(dataset)\n\n        images = None\n        texts = None\n\n        if isinstance(self.images_column_names, str):\n            images = list(dataset[self.images_column_names])\n        elif isinstance(self.images_column_names, Sequence):\n            images = [\n                img\n                for img_column in self.images_column_names\n                for img in dataset[img_column]\n            ]\n\n        if isinstance(self.texts_column_names, str):\n            texts = list(dataset[self.texts_column_names])\n        elif isinstance(self.texts_column_names, Sequence):\n            texts = [\n                text\n                for text_column in self.texts_column_names\n                for text in dataset[text_column]\n            ]\n\n        return ImageTextPairClassificationDescriptiveStatistics(\n            num_samples=num_samples,\n            text_statistics=calculate_text_statistics(texts),\n            image_statistics=calculate_image_statistics(images),\n        )\n\n    def _evaluate_subset(\n        self,\n        model: MTEBModels,\n        data_split: Dataset,\n        *,\n        encode_kwargs: EncodeKwargs,\n        hf_split: str,\n        hf_subset: str,\n        prediction_folder: Path | None = None,\n        num_proc: int | None = None,\n        **kwargs: Any,\n    ) -&gt; ImageTextPairClassificationMetrics:\n        if not isinstance(model, EncoderProtocol):\n            raise TypeError(\"Expected model to be an instance of EncoderProtocol\")\n        select_columns = []\n        for columns in (self.images_column_names, self.texts_column_names):\n            if isinstance(columns, str):\n                select_columns.append(columns)\n            else:\n                select_columns.extend(columns)\n\n        data_split = data_split.select_columns(select_columns)\n        num_images_per_sample = (\n            1\n            if isinstance(self.images_column_names, str)\n            else len(self.images_column_names)\n        )\n        num_texts_per_sample = (\n            1\n            if isinstance(self.texts_column_names, str)\n            else len(self.texts_column_names)\n        )\n        evaluator = ImageTextPairClassificationEvaluator(\n            data_split,\n            images_column_names=self.images_column_names,\n            texts_column_names=self.texts_column_names,\n            task_metadata=self.metadata,\n            num_texts_per_sample=num_texts_per_sample,\n            num_images_per_sample=num_images_per_sample,\n            hf_split=hf_split,\n            hf_subset=hf_subset,\n            **kwargs,\n        )\n        scores: list[torch.Tensor] = evaluator(\n            model, encode_kwargs=encode_kwargs, num_proc=num_proc\n        )  # type: ignore[assignment]\n        if prediction_folder:\n            self._save_task_predictions(\n                [score.tolist() for score in scores],\n                model,\n                prediction_folder,\n                hf_subset=hf_subset,\n                hf_split=hf_split,\n            )\n\n        return self._compute_metrics(\n            scores,\n            num_images_per_sample,\n            num_texts_per_sample,\n        )\n\n    def _compute_metrics(\n        self,\n        scores: list[torch.Tensor],\n        num_images_per_sample: int,\n        num_texts_per_sample: int,\n    ) -&gt; ImageTextPairClassificationMetrics:\n        image_score = []\n        text_score = []\n        all_correct_scores = []\n        img_ground_truths = torch.arange(num_images_per_sample)\n        caption_ground_truths = torch.arange(num_texts_per_sample)\n\n        for score in scores:\n            image_closest_text = score.argmax(dim=1)  # shape = (num_images_per_sample)\n            text_closest_image = score.argmax(dim=0)  # shape = (num_texts_per_sample)\n            pred_text_is_correct = (\n                (image_closest_text == img_ground_truths).all().item()\n            )\n            pred_image_is_correct = (\n                (text_closest_image == caption_ground_truths).all().item()\n            )\n            all_correct = pred_text_is_correct and pred_image_is_correct\n            image_score.append(pred_image_is_correct)\n            text_score.append(pred_text_is_correct)\n            all_correct_scores.append(all_correct)\n\n        return ImageTextPairClassificationMetrics(\n            image_acc=torch.Tensor(image_score).float().mean().item(),\n            text_acc=torch.Tensor(text_score).float().mean().item(),\n            accuracy=torch.Tensor(all_correct_scores).float().mean().item(),\n        )\n\n    def _push_dataset_to_hub(\n        self,\n        repo_name: str,\n        num_proc: int | None = None,\n    ) -&gt; None:\n        text_columns = (\n            [self.texts_column_names]\n            if isinstance(self.texts_column_names, str)\n            else self.texts_column_names\n        )\n        image_columns = (\n            [self.images_column_names]\n            if isinstance(self.images_column_names, str)\n            else self.images_column_names\n        )\n\n        self._upload_dataset_to_hub(\n            repo_name,\n            [*text_columns, *image_columns],\n            num_proc=num_proc,\n        )\n</code></pre>"},{"location":"api/types/","title":"Additional Types","text":"<p>MTEB implements a variety of utility types to allow us and you to better know what a model returns. This page documents some of these types.</p>"},{"location":"api/types/#mteb.types._encoder_io","title":"Encoder Input/Output types","text":""},{"location":"api/types/#mteb.types._encoder_io.Array","title":"<code>Array = NDArray[np.floating | np.integer | np.bool_] | torch.Tensor</code>  <code>module-attribute</code>","text":"<p>General array type, can be a numpy array (float, int, or bool) or a torch tensor.</p>"},{"location":"api/types/#mteb.types._encoder_io.Conversation","title":"<code>Conversation = list[ConversationTurn]</code>  <code>module-attribute</code>","text":"<p>A conversation, consisting of a list of messages.</p>"},{"location":"api/types/#mteb.types._encoder_io.BatchedInput","title":"<code>BatchedInput = TextInput | CorpusInput | QueryInput | ImageInput | AudioInput | MultimodalInput</code>  <code>module-attribute</code>","text":"<p>Represents the input format accepted by the encoder for a batch of data.</p> <p>The encoder can process several input types depending on the task or modality. Each type is defined as a separate structured input with its own fields.</p>"},{"location":"api/types/#mteb.types._encoder_io.BatchedInput--supported-input-types","title":"Supported input types","text":"<ol> <li><code>TextInput</code>    For pure text inputs.</li> </ol> <p><pre><code>{\"text\": [\"This is a sample text.\", \"Another text.\"]}\n</code></pre> 2. <code>CorpusInput</code>    For corpus-style inputs with titles and bodies.</p> <p><pre><code>{\"text\": [\"Title 1 Body 1\", \"Title 2 Body 2\"], \"title\": [\"Title 1\", \"Title 2\"], \"body\": [\"Body 1\", \"Body 2\"]}\n</code></pre> 3. <code>QueryInput</code>    For query\u2013instruction pairs, typically used in retrieval or question answering tasks. Queries and instructions are combined with the model's instruction template.</p> <p><pre><code>{\n    \"text\": [\"Instruction: Your task is to find document for this query. Query: What is AI?\", \"Instruction: Your task is to find term for definition. Query: Define machine learning.\"],\n    \"query\": [\"What is AI?\", \"Define machine learning.\"],\n    \"instruction\": [\"Your task is find document for this query.\", \"Your task is to find term for definition.\"]\n}\n</code></pre> 4. <code>ImageInput</code>    For visual inputs consisting of images.</p> <p><pre><code>{\"image\": [PIL.Image1, PIL.Image2]}\n</code></pre> 5. <code>MultimodalInput</code>    For combined text\u2013image (multimodal) inputs.</p> <pre><code>{\"text\": [\"This is a sample text.\"], \"image\": [PIL.Image1]}\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.TextBatchedInput","title":"<code>TextBatchedInput = TextInput | CorpusInput | QueryInput</code>  <code>module-attribute</code>","text":"<p>The input to the encoder for a batch of text data.</p>"},{"location":"api/types/#mteb.types._encoder_io.QueryDatasetType","title":"<code>QueryDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval query dataset, containing queries. Should have columns: 1. <code>id</code>, <code>text</code>, <code>instruction</code> (optionally) for text queries 2. <code>id</code>, <code>image</code> for image queries 3. <code>id</code>, <code>audio</code> for audio queries or a combination of these for multimodal queries.</p>"},{"location":"api/types/#mteb.types._encoder_io.CorpusDatasetType","title":"<code>CorpusDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval corpus dataset, containing documents. Should have columns: 1. <code>id</code>, <code>title</code> (optionally), <code>body</code> for text corpus 2. <code>id</code>, <code>image</code> for image corpus 3. <code>id</code>, <code>audio</code> for audio corpus or a combination of these for multimodal corpus.</p>"},{"location":"api/types/#mteb.types._encoder_io.InstructionDatasetType","title":"<code>InstructionDatasetType = Dataset</code>  <code>module-attribute</code>","text":"<p>Retrieval instruction dataset, containing instructions. Should have columns <code>query-id</code>, <code>instruction</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.RelevantDocumentsType","title":"<code>RelevantDocumentsType = Mapping[str, Mapping[str, int]]</code>  <code>module-attribute</code>","text":"<p>Relevant documents for each query, mapping query IDs to a mapping of document IDs and their relevance scores. Should have columns <code>query-id</code>, <code>corpus-id</code>, <code>score</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.TopRankedDocumentsType","title":"<code>TopRankedDocumentsType = Mapping[str, list[str]]</code>  <code>module-attribute</code>","text":"<p>Top-ranked documents for each query, mapping query IDs to a list of document IDs. Should have columns <code>query-id</code>, <code>corpus-ids</code>.</p>"},{"location":"api/types/#mteb.types._encoder_io.RetrievalOutputType","title":"<code>RetrievalOutputType = dict[str, dict[str, float]]</code>  <code>module-attribute</code>","text":"<p>Retrieval output, containing the scores for each query-document pair.</p>"},{"location":"api/types/#mteb.types._encoder_io.EncodeKwargs","title":"<code>EncodeKwargs</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Keyword arguments for encoding methods.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>NotRequired[int]</code> <p>The batch size to use for encoding.</p> <code>show_progress_bar</code> <code>NotRequired[bool]</code> <p>Whether to show a progress bar during encoding.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class EncodeKwargs(TypedDict):\n    \"\"\"Keyword arguments for encoding methods.\n\n    Attributes:\n        batch_size: The batch size to use for encoding.\n        show_progress_bar: Whether to show a progress bar during encoding.\n    \"\"\"\n\n    batch_size: NotRequired[int]\n    show_progress_bar: NotRequired[bool]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.PromptType","title":"<code>PromptType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>The type of prompt used in the input for retrieval models. Used to differentiate between queries and documents.</p> <p>Attributes:</p> Name Type Description <code>query</code> <p>A prompt that is a query.</p> <code>document</code> <p>A prompt that is a document.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class PromptType(str, Enum):\n    \"\"\"The type of prompt used in the input for retrieval models. Used to differentiate between queries and documents.\n\n    Attributes:\n        query: A prompt that is a query.\n        document: A prompt that is a document.\n    \"\"\"\n\n    query = \"query\"\n    document = \"document\"\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.ConversationTurn","title":"<code>ConversationTurn</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A conversation, consisting of a list of messages.</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>The role of the message sender.</p> <code>content</code> <code>str</code> <p>The content of the message.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class ConversationTurn(TypedDict):\n    \"\"\"A conversation, consisting of a list of messages.\n\n    Attributes:\n        role: The role of the message sender.\n        content: The content of the message.\n    \"\"\"\n\n    role: str\n    content: str\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.TextInput","title":"<code>TextInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for text.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>list[str]</code> <p>The text to encode. Can be a list of texts or a list of lists of texts.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class TextInput(TypedDict):\n    \"\"\"The input to the encoder for text.\n\n    Attributes:\n        text: The text to encode. Can be a list of texts or a list of lists of texts.\n    \"\"\"\n\n    text: list[str]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.CorpusInput","title":"<code>CorpusInput</code>","text":"<p>               Bases: <code>TextInput</code></p> <p>The input to the encoder for retrieval corpus.</p> <p>Attributes:</p> Name Type Description <code>title</code> <code>list[str]</code> <p>The title of the text to encode. Can be a list of titles or a list of lists of titles.</p> <code>body</code> <code>list[str]</code> <p>The body of the text to encode. Can be a list of bodies or a list of lists of bodies.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class CorpusInput(TextInput):\n    \"\"\"The input to the encoder for retrieval corpus.\n\n    Attributes:\n        title: The title of the text to encode. Can be a list of titles or a\n            list of lists of titles.\n        body: The body of the text to encode. Can be a list of bodies or a\n            list of lists of bodies.\n    \"\"\"\n\n    title: list[str]\n    body: list[str]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.QueryInput","title":"<code>QueryInput</code>","text":"<p>               Bases: <code>TextInput</code></p> <p>The input to the encoder for queries.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>list[str]</code> <p>The query to encode. Can be a list of queries or a list of lists of queries.</p> <code>conversation</code> <code>NotRequired[list[Conversation]]</code> <p>Optional. A list of conversations, each conversation is a list of messages.</p> <code>instruction</code> <code>NotRequired[list[str]]</code> <p>Optional. A list of instructions to encode.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class QueryInput(TextInput):\n    \"\"\"The input to the encoder for queries.\n\n    Attributes:\n        query: The query to encode. Can be a list of queries or a list of lists of queries.\n        conversation: Optional. A list of conversations, each conversation is a list of messages.\n        instruction: Optional. A list of instructions to encode.\n    \"\"\"\n\n    query: list[str]\n    conversation: NotRequired[list[Conversation]]\n    instruction: NotRequired[list[str]]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.ImageInput","title":"<code>ImageInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for images.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>list[Image]</code> <p>The image to encode. Can be a list of images or a list of lists of images.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class ImageInput(TypedDict):\n    \"\"\"The input to the encoder for images.\n\n    Attributes:\n        image: The image to encode. Can be a list of images or a list of lists of images.\n    \"\"\"\n\n    image: list[Image.Image]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.AudioInputItem","title":"<code>AudioInputItem</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>An audio item for the AudioInput.</p> <p>Dataset based on <code>datasets.Audio</code> will be converted to this format during encoding.</p> <p>Attributes:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>The audio array as bytes.</p> <code>sampling_rate</code> <code>int</code> <p>The sampling rate of the audio.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class AudioInputItem(TypedDict):\n    \"\"\"An audio item for the AudioInput.\n\n    Dataset based on `datasets.Audio` will be converted to this format during encoding.\n\n    Attributes:\n        array: The audio array as bytes.\n        sampling_rate: The sampling rate of the audio.\n    \"\"\"\n\n    array: np.ndarray\n    sampling_rate: int\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.AudioInput","title":"<code>AudioInput</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The input to the encoder for audio.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>list[AudioInputItem]</code> <p>The audio to encode. Can be a list of audio files or a list of lists of audio files.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class AudioInput(TypedDict):\n    \"\"\"The input to the encoder for audio.\n\n    Attributes:\n        audio: The audio to encode. Can be a list of audio files or a list of lists of audio files.\n    \"\"\"\n\n    audio: list[AudioInputItem]\n</code></pre>"},{"location":"api/types/#mteb.types._encoder_io.MultimodalInput","title":"<code>MultimodalInput</code>","text":"<p>               Bases: <code>TextInput</code>, <code>CorpusInput</code>, <code>QueryInput</code>, <code>ImageInput</code>, <code>AudioInput</code></p> <p>The input to the encoder for multimodal data.</p> Source code in <code>mteb/types/_encoder_io.py</code> <pre><code>class MultimodalInput(TextInput, CorpusInput, QueryInput, ImageInput, AudioInput):  # type: ignore[misc]\n    \"\"\"The input to the encoder for multimodal data.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/types/#mteb.types._metadata","title":"Metadata types","text":""},{"location":"api/types/#mteb.types._metadata.ISOLanguageScript","title":"<code>ISOLanguageScript = str</code>  <code>module-attribute</code>","text":"<p>A string representing the language and script. Language is denoted as a 3-letter ISO 639-3 language code and the script is denoted by a 4-letter ISO 15924 script code (e.g. \"eng-Latn\").</p>"},{"location":"api/types/#mteb.types._metadata.ISOLanguage","title":"<code>ISOLanguage = str</code>  <code>module-attribute</code>","text":"<p>A string representing the language. Language is denoted as a 3-letter ISO 639-3 language code (e.g. \"eng\").</p>"},{"location":"api/types/#mteb.types._metadata.ISOScript","title":"<code>ISOScript = str</code>  <code>module-attribute</code>","text":"<p>A string representing the script. The script is denoted by a 4-letter ISO 15924 script code (e.g. \"Latn\").</p>"},{"location":"api/types/#mteb.types._metadata.Languages","title":"<code>Languages = list[ISOLanguageScript] | Mapping[HFSubset, list[ISOLanguageScript]]</code>  <code>module-attribute</code>","text":"<p>A list of languages or a mapping from HFSubset to a list of languages. E.g. [\"eng-Latn\", \"deu-Latn\"] or {\"en-de\": [\"eng-Latn\", \"deu-Latn\"], \"fr-it\": [\"fra-Latn\", \"ita-Latn\"]}.</p>"},{"location":"api/types/#mteb.types._metadata.Licenses","title":"<code>Licenses = Literal['not specified', 'mit', 'cc-by-2.0', 'cc-by-3.0', 'cc-by-4.0', 'cc-by-sa-3.0', 'cc-by-sa-4.0', 'cc-by-nc-3.0', 'cc-by-nc-4.0', 'cc-by-nc-sa-3.0', 'cc-by-nc-sa-4.0', 'cc-by-nc-nd-4.0', 'cc-by-nd-4.0', 'openrail', 'openrail++', 'odc-by', 'afl-3.0', 'apache-2.0', 'cc-by-nd-2.1-jp', 'cc0-1.0', 'bsd-3-clause', 'gpl-3.0', 'lgpl', 'lgpl-3.0', 'cdla-sharing-1.0', 'mpl-2.0', 'msr-la-nc', 'multiple', 'gemma']</code>  <code>module-attribute</code>","text":"<p>The different licenses that a dataset or model can have. This list can be extended as needed.</p>"},{"location":"api/types/#mteb.types._metadata.ModelName","title":"<code>ModelName = str</code>  <code>module-attribute</code>","text":"<p>The name of a model, typically as found on HuggingFace e.g. <code>sentence-transformers/all-MiniLM-L6-v2</code>.</p>"},{"location":"api/types/#mteb.types._metadata.Revision","title":"<code>Revision = str</code>  <code>module-attribute</code>","text":"<p>The revision of a model, typically a git commit hash. For APIs this can be a version string e.g. <code>1</code>.</p>"},{"location":"api/types/#mteb.types._metadata.Modalities","title":"<code>Modalities = Literal['text', 'image', 'audio']</code>  <code>module-attribute</code>","text":"<p>The different modalities that a model can support.</p>"},{"location":"api/types/#mteb.types._result","title":"Results types","text":""},{"location":"api/types/#mteb.types._result.HFSubset","title":"<code>HFSubset = str</code>  <code>module-attribute</code>","text":"<p>The name of a HuggingFace dataset subset, e.g. 'en-de', 'en', 'default' (default is used when there is no subset).</p>"},{"location":"api/types/#mteb.types._result.SplitName","title":"<code>SplitName = str</code>  <code>module-attribute</code>","text":"<p>The name of a data split, e.g. 'test', 'validation', 'train'.</p>"},{"location":"api/types/#mteb.types._result.Score","title":"<code>Score = Any</code>  <code>module-attribute</code>","text":"<p>A score value, could e.g. be accuracy. Normally it is a float or int, but it can take on any value. Should be json serializable.</p>"},{"location":"api/types/#mteb.types._result.ScoresDict","title":"<code>ScoresDict = Mapping[str, Score]</code>  <code>module-attribute</code>","text":"<p>A dictionary of scores, typically also include metadata, e.g {'main_score': 0.5, 'accuracy': 0.5, 'f1': 0.6, 'hf_subset': 'en-de', 'languages': ['eng-Latn', 'deu-Latn']}</p>"},{"location":"api/types/#mteb.types._result.RetrievalEvaluationResult","title":"<code>RetrievalEvaluationResult</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Holds the results of retrieval evaluation metrics.</p> Source code in <code>mteb/types/_result.py</code> <pre><code>class RetrievalEvaluationResult(NamedTuple):\n    \"\"\"Holds the results of retrieval evaluation metrics.\"\"\"\n\n    all_scores: dict[str, dict[str, float]]\n    ndcg: dict[str, float]\n    map: dict[str, float]\n    recall: dict[str, float]\n    precision: dict[str, float]\n    naucs: dict[str, float]\n    mrr: dict[str, float]\n    naucs_mrr: dict[str, float]\n    hit_rate: dict[str, float]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics","title":"Statistics types","text":""},{"location":"api/types/#mteb.types.statistics.SplitDescriptiveStatistics","title":"<code>SplitDescriptiveStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Base class for descriptive statistics for the subset.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class SplitDescriptiveStatistics(TypedDict):\n    \"\"\"Base class for descriptive statistics for the subset.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.DescriptiveStatistics","title":"<code>DescriptiveStatistics</code>","text":"<p>               Bases: <code>TypedDict</code>, <code>SplitDescriptiveStatistics</code></p> <p>Class for descriptive statistics for the full task.</p> <p>Attributes:</p> Name Type Description <code>num_samples</code> <code>int</code> <p>Total number of samples</p> <code>hf_subset_descriptive_stats</code> <code>NotRequired[dict[HFSubset, SplitDescriptiveStatistics]]</code> <p>HFSubset descriptive statistics (only for multilingual datasets)</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class DescriptiveStatistics(TypedDict, SplitDescriptiveStatistics):\n    \"\"\"Class for descriptive statistics for the full task.\n\n    Attributes:\n        num_samples: Total number of samples\n        hf_subset_descriptive_stats: HFSubset descriptive statistics (only for multilingual datasets)\n    \"\"\"\n\n    num_samples: int\n    hf_subset_descriptive_stats: NotRequired[dict[HFSubset, SplitDescriptiveStatistics]]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.TextStatistics","title":"<code>TextStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>total_text_length</code> <code>int</code> <p>Total length of all texts</p> <code>min_text_length</code> <code>int</code> <p>Minimum length of text</p> <code>average_text_length</code> <code>float</code> <p>Average length of text</p> <code>max_text_length</code> <code>int</code> <p>Maximum length of text</p> <code>unique_texts</code> <code>int</code> <p>Number of unique texts</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class TextStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        total_text_length: Total length of all texts\n        min_text_length: Minimum length of text\n        average_text_length: Average length of text\n        max_text_length: Maximum length of text\n        unique_texts: Number of unique texts\n    \"\"\"\n\n    total_text_length: int\n    min_text_length: int\n    average_text_length: float\n    max_text_length: int\n    unique_texts: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.ImageStatistics","title":"<code>ImageStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for images.</p> <p>Attributes:</p> Name Type Description <code>min_image_width</code> <code>float</code> <p>Minimum width of images</p> <code>average_image_width</code> <code>float</code> <p>Average width of images</p> <code>max_image_width</code> <code>float</code> <p>Maximum width of images</p> <code>min_image_height</code> <code>float</code> <p>Minimum height of images</p> <code>average_image_height</code> <code>float</code> <p>Average height of images</p> <code>max_image_height</code> <code>float</code> <p>Maximum height of images</p> <code>unique_images</code> <code>int</code> <p>Number of unique images</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class ImageStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for images.\n\n    Attributes:\n        min_image_width: Minimum width of images\n        average_image_width: Average width of images\n        max_image_width: Maximum width of images\n\n        min_image_height: Minimum height of images\n        average_image_height: Average height of images\n        max_image_height: Maximum height of images\n\n        unique_images: Number of unique images\n    \"\"\"\n\n    min_image_width: float\n    average_image_width: float\n    max_image_width: float\n\n    min_image_height: float\n    average_image_height: float\n    max_image_height: float\n\n    unique_images: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.AudioStatistics","title":"<code>AudioStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for audio.</p> <p>Attributes:</p> Name Type Description <code>total_duration_seconds</code> <code>float</code> <p>Total length of all audio clips in total frames</p> <code>min_duration_seconds</code> <code>float</code> <p>Minimum length of audio clip in seconds</p> <code>average_duration_seconds</code> <code>float</code> <p>Average length of audio clip in seconds</p> <code>max_duration_seconds</code> <code>float</code> <p>Maximum length of audio clip in seconds</p> <code>unique_audios</code> <code>int</code> <p>Number of unique audio clips</p> <code>average_sampling_rate</code> <code>float</code> <p>Average sampling rate</p> <code>sampling_rates</code> <code>dict[int, int]</code> <p>Dict of unique sampling rates and their frequencies</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class AudioStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for audio.\n\n    Attributes:\n        total_duration_seconds: Total length of all audio clips in total frames\n        min_duration_seconds: Minimum length of audio clip in seconds\n        average_duration_seconds: Average length of audio clip in seconds\n        max_duration_seconds: Maximum length of audio clip in seconds\n        unique_audios: Number of unique audio clips\n        average_sampling_rate: Average sampling rate\n        sampling_rates: Dict of unique sampling rates and their frequencies\n    \"\"\"\n\n    total_duration_seconds: float\n\n    min_duration_seconds: float\n    average_duration_seconds: float\n    max_duration_seconds: float\n\n    unique_audios: int\n\n    average_sampling_rate: float\n    sampling_rates: dict[int, int]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.LabelStatistics","title":"<code>LabelStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>min_labels_per_text</code> <code>int</code> <p>Minimum number of labels per text</p> <code>average_label_per_text</code> <code>float</code> <p>Average number of labels per text</p> <code>max_labels_per_text</code> <code>int</code> <p>Maximum number of labels per text</p> <code>unique_labels</code> <code>int</code> <p>Number of unique labels</p> <code>labels</code> <code>dict[str, dict[str, int]]</code> <p>dict of label frequencies</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class LabelStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        min_labels_per_text: Minimum number of labels per text\n        average_label_per_text: Average number of labels per text\n        max_labels_per_text: Maximum number of labels per text\n\n        unique_labels: Number of unique labels\n        labels: dict of label frequencies\n    \"\"\"\n\n    min_labels_per_text: int\n    average_label_per_text: float\n    max_labels_per_text: int\n\n    unique_labels: int\n    labels: dict[str, dict[str, int]]\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.ScoreStatistics","title":"<code>ScoreStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Class for descriptive statistics for texts.</p> <p>Attributes:</p> Name Type Description <code>min_score</code> <code>int | float</code> <p>Minimum score</p> <code>avg_score</code> <code>float</code> <p>Average score</p> <code>max_score</code> <code>int | float</code> <p>Maximum score</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class ScoreStatistics(TypedDict):\n    \"\"\"Class for descriptive statistics for texts.\n\n    Attributes:\n        min_score: Minimum score\n        avg_score: Average score\n        max_score: Maximum score\n    \"\"\"\n\n    min_score: int | float\n    avg_score: float\n    max_score: int | float\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.TopRankedStatistics","title":"<code>TopRankedStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Statistics for top ranked documents in a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>num_top_ranked</code> <code>int</code> <p>Total number of top ranked documents across all queries.</p> <code>min_top_ranked_per_query</code> <code>int</code> <p>Minimum number of top ranked documents for any query.</p> <code>average_top_ranked_per_query</code> <code>float</code> <p>Average number of top ranked documents per query.</p> <code>max_top_ranked_per_query</code> <code>int</code> <p>Maximum number of top ranked documents for any query.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class TopRankedStatistics(TypedDict):\n    \"\"\"Statistics for top ranked documents in a retrieval task.\n\n    Attributes:\n        num_top_ranked: Total number of top ranked documents across all queries.\n        min_top_ranked_per_query: Minimum number of top ranked documents for any query.\n        average_top_ranked_per_query: Average number of top ranked documents per query.\n        max_top_ranked_per_query: Maximum number of top ranked documents for any query.\n    \"\"\"\n\n    num_top_ranked: int\n    min_top_ranked_per_query: int\n    average_top_ranked_per_query: float\n    max_top_ranked_per_query: int\n</code></pre>"},{"location":"api/types/#mteb.types.statistics.RelevantDocsStatistics","title":"<code>RelevantDocsStatistics</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>Statistics for relevant documents in a retrieval task.</p> <p>Attributes:</p> Name Type Description <code>num_relevant_docs</code> <code>int</code> <p>Total number of relevant documents across all queries.</p> <code>min_relevant_docs_per_query</code> <code>int</code> <p>Minimum number of relevant documents for any query.</p> <code>average_relevant_docs_per_query</code> <code>float</code> <p>Average number of relevant documents per query.</p> <code>max_relevant_docs_per_query</code> <code>float</code> <p>Maximum number of relevant documents for any query.</p> <code>unique_relevant_docs</code> <code>int</code> <p>Number of unique relevant documents across all queries.</p> Source code in <code>mteb/types/statistics.py</code> <pre><code>class RelevantDocsStatistics(TypedDict):\n    \"\"\"Statistics for relevant documents in a retrieval task.\n\n    Attributes:\n        num_relevant_docs: Total number of relevant documents across all queries.\n        min_relevant_docs_per_query: Minimum number of relevant documents for any query.\n        average_relevant_docs_per_query: Average number of relevant documents per query.\n        max_relevant_docs_per_query: Maximum number of relevant documents for any query.\n        unique_relevant_docs: Number of unique relevant documents across all queries.\n    \"\"\"\n\n    num_relevant_docs: int\n    min_relevant_docs_per_query: int\n    average_relevant_docs_per_query: float\n    max_relevant_docs_per_query: float\n    unique_relevant_docs: int\n</code></pre>"},{"location":"contributing/adding_a_benchmark/","title":"Adding a Benchmark","text":""},{"location":"contributing/adding_a_benchmark/#adding-a-benchmark","title":"Adding a benchmark","text":"<p>The MTEB has a growing list of benchmarks, and we are always looking to add more. MTEB include both benchmark that displayed on the leaderboard and benchmarks that are not displayed on the leaderboard but are still available for evaluation. These non-leaderboard benchmarks are available in the  <code>mteb.get_benchmark(s)</code> function and are e.g. useful for evaluating models during development or benchmark that are too specific to be added to the leaderboard.</p>"},{"location":"contributing/adding_a_benchmark/#implement-a-new-benchmark","title":"Implement a new benchmark","text":"<p>To implement a new benchmark <code>Benchmark</code> object, and select the MTEB tasks that will be in the benchmark. If some of the tasks do not exist in MTEB, follow the \"add a dataset\" instructions to add them.</p> <p>Once you have selected the tasks, you can create a new benchmark as follows:</p> <pre><code>import mteb\n\ncustom_bench = Benchmark(\n    name=\"MTEB(custom, v1)\", # set the name \n    tasks=mteb.get_tasks( # (1)\n        tasks=[\"AmazonCounterfactualClassification\", \"AmazonPolarityClassification\"],\n        languages=[\"eng\"],\n    ),\n    # give a short description of the benchmark of what the benchmarks \n    # seeks to test for:\n    description=(\n        \"My custom Amazon benchmark, \"\n        \"which seeks to test for the ability of models \"\n        \"to classify Amazon reviews based on their embeddings.\"\n    ),\n)\n</code></pre> <ol> <li>Select the tasks that will be in the benchmark. See selecting tasks for more details on how to select tasks.</li> </ol> Selecting high-quality tasks <p>When selecting tasks for a benchmark, it is important to select high-quality tasks that reflects what you seeks to measure. To facilitate this process each task in mteb comes with metadata (<code>task.metadata</code>) that includes a description of the task, the construction and annotation process, licensing and more. We additionally also include descriptive statistics (<code>task.metadata.descriptive_stats</code>) which includes information about the number of samples, minimum length and other statistics that can be useful to select the right tasks for your benchmark.</p> <p>Generally we recommend selecting tasks that are well established in the community, are not machine translated, are not too small and that are not too similar to other tasks in the benchmark. However, the selection of tasks will depend on what you seek to measure with the benchmark, and thus we recommend carefully reading the metadata of the tasks and selecting the ones that best fit your needs.</p>"},{"location":"contributing/adding_a_benchmark/#submitting-a-benchmark","title":"Submitting a Benchmark","text":"<p>To submit a benchmark to MTEB, you need to add your benchmark to benchmarks.py and then open a pull request (PR).</p> <p>Once submitted the PR will be reviewed by one of the organizers or contributors who might ask you to change things. The reviewer review both the implementation of the benchmark, but also quality and relevance of the tasks. Once the PR is approved the benchmark will be added into mteb and will be fetchable using <code>mteb.get_benchmark(name)</code>. Note this does not automatically add the benchmark to the leaderboard, see next section for instructions on how to do that.</p>"},{"location":"contributing/adding_a_benchmark/#submitting-a-benchmark-to-the-leaderboard","title":"Submitting a Benchmark to the Leaderboard","text":"<p>To submit a benchmark to the leaderboard, you need to:</p> <ol> <li>Have added the benchmark to MTEB as described in the previous section</li> <li>Evalaute a set of models on the benchmark and submit a PR with the results to the results repository with the results of the models on the benchmark.</li> <li>When your PR with benchmarks results is merged, you can add your benchmark to the most fitting section in benchmark_selector.py to be shown on the leaderboard. You can check that the leaderboard looks correctly by running the leaderboard locally.</li> <li>When PRs are merged, your benchmark will be added to the leaderboard automatically after the next workflow trigger (every day at midnight Pacific Time (8 AM UTC)).</li> </ol> <p>Not all benchmarks becomes leaderboards</p> <p>A benchmark is a selection of tasks that intends to test for a specific purpose. Some benchmarks are very specific, are intended for development or are superseded by newer benchmark. We continually try to keep the benchmarks on the leaderboard relevant and thus we might remove benchmarks from the leaderboard if they are no longer relevant. However, these benchmarks will still be available in MTEB and can be used for evaluation, but they just won't be shown on the leaderboard.</p>"},{"location":"contributing/adding_a_dataset/","title":"Adding a Dataset","text":"<p>To add a new dataset to MTEB, you need to do four things:</p> <ol> <li>Implement a new task: Implement a task with the desired dataset, by subclassing an abstract task</li> <li>Fill out the metadata describing the task, main scores, languages etc.</li> <li>Calculate descriptive statistics, which is used to detect duplicates, few number of samples or very short documents</li> <li>Submit a Pull Request of the edits to the MTEB repository</li> </ol> <p>We go through these steps below, but if you have any questions regarding this process feel free to open a discussion thread. It is also reasonable to look at tasks already implemented to get an idea about the structure.</p>"},{"location":"contributing/adding_a_dataset/#implement-a-new-task","title":"Implement a new task","text":""},{"location":"contributing/adding_a_dataset/#overview-of-the-task","title":"Overview of the task","text":"<p>Implementing a task in <code>mteb</code>, typically has the following structure:</p> <pre><code>from datasets import load_dataset\n\nfrom mteb.abstasks.task_metadata import TaskMetadata\nfrom mteb.abstasks.classification import AbsTaskClassification\n\nclass MyNewTask(AbsTaskClassification):\n    # metadata contains information such as title, description, metrics etc.\n    metadata = TaskMetadata(...)\n\n    # task specific setting e.g. specifying the column names\n    label_column_name = \"label\"\n    input_column_name = \"text\"\n\n    # This is the function that loads the dataset, this is typically untouched\n    def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n        self.dataset = load_dataset(\n            **self.metadata.dataset,\n        )\n        self.dataset_transform() # optional processing\n        self.data_loaded = True\n\n    # dataset transform, which allow you to process the dataset\n    # including downsampling, filtering etc.\n    def dataset_transform(self, num_proc: int | None = None, **kwargs) -&gt; None:\n        # some processing\n        ...\n</code></pre>"},{"location":"contributing/adding_a_dataset/#select-an-appropriate-task","title":"Select an appropriate task","text":"<p>To add a dataset you first need to figure out which type of task is the best suited for the dataset. Below we will give you an overview of the most common, but do see abstasks for an overview of all the tasks available.</p> Task (Abstask) Description Common Metric Classification, <code>AbsTaskClassification</code> Fits a classifier on the embeddings derived from the model. The goal is to predict the labels correctly. This does not change the weights of the model itself. See also classes for multi label classification, regression and pair classification. accuracy Clustering, <code>AbsTaskClustering</code> Cluster documents based on their embeddings. The goal is to cluster documents according to predefined categories. Support clustering in multiple hierarchies. v_measure Retrieval, <code>AbsTaskRetrieval</code> Retrieval tasks include a corpus from which you retrieve from using a query. The goal is to retrieve relevant documents. See also <code>convert_to_reranking</code> for how to convert a retrieval task into a reranking task. ndcg_at_10 Semantic Text Similarity, <code>AbsTaskSTS</code> Compares the (semantic) similarity of pairs of documents. The goal is to embed documents such that semantically similar statements appear close. spearman <p>Note</p> <p>While each task has a main score we compute multiple and it is possible to select any of these are the main metric for your task.</p>"},{"location":"contributing/adding_a_dataset/#implementing-the-task","title":"Implementing the task","text":"<p>Once we have decided on task, we can implement them as follows:</p> ClassificationClusteringRetrievalMultilingual Semantic Similarity <p>For our classification task we use the poem sentiment dataset, which consist of verses with four labels 0 (negative), 1 (positive), 2 (no_impact) and 3 (mixed). Let us say that we just want to look at the labels 0 and 1.</p> <p>We can then implement the task as follows:</p> <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskClassification\n\n\nclass MyClassificationtask(AbsTaskClassification):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MyClassificationTask\",\n        description=\"A dummy classification task based on poems sentiment classification.\",\n        main_score=\"accuracy\",\n        eval_langs=[\"eng-Latn\"],\n        eval_splits=[\"test\"],\n        type=\"Classification\",\n        dataset={\n            \"path\": \"mteb/poem_sentiment\",\n            \"revision\": \"9fdc57b89ccc09a8d9256f376112d626878e51a7\",\n        },\n        prompt=\"Classify poem verses as positive or negative\"\n    )\n\n    label_column_name = \"label\"\n    input_column_name = \"text\"\n\n    def dataset_transform(self, num_proc=None, **kwargs) -&gt; None:\n        # filter to only include label 1 positive and label 0 negative\n        self.dataset = self.dataset.filter(\n            lambda x: x[self.label_column_name] in [0, 1], num_proc=num_proc\n        )\n</code></pre> <p>Once we have the task we can then test to make sure that everything works as intended:</p> <pre><code># ensure that the dataset can be loaded and transformed properly\ntask = MyClassificationtask()\ntask.load_data()\n\nprint(task.dataset[\"test\"][0]) # check one of the samples:\n# {'id': 1, 'text': 'shall yet be glad for him, and he shall bless', 'label': 1}\n\n# ensure that we can evaluate the model on the task\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\nprint(results[0].get_score())  # print the accuracy score of the random baseline\n# 0.49428571428571433\n</code></pre> <p>For our classification task we use the swedn dataset of Swedish newspapers along four main categories. It contains three splits \"headlines\", \"articles\" and \"summaries\", here we will look at the headlines only.</p> <p>The clustering task performs multiple experiments to get a more consistent estimate and reports the average. Here we set it to 10 experiments using 1000 samples, but typically you would set it higher. Using the default is also a reasonable idea.</p> <p>We can then implement the task as follows:</p> <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskClustering\n\nclass MyClusteringTask(AbsTaskClustering):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MyClusteringTask\",\n        description=\"A dummy clustering task on swedish news articles\",\n        main_score=\"v_measure\",\n        eval_langs=[\"swe-Latn\"],\n        eval_splits=[\"headlines\"],  # just use the headlines split for evaluation\n        type=\"Clustering\",\n        dataset={\n            \"path\": \"mteb/SwednClusteringP2P\",\n            \"revision\": \"f8dbf10ec231cc25e9f63454d5cd2d90af95e5f8\",\n        },\n    )\n\n    label_column_name = \"labels\"\n    input_column_name = \"sentences\"\n    n_clusters = 10  # number of clustering experiments to run\n    max_documents_per_cluster = (\n        1000  # numberof documents to sample per each clustering experiment\n    )\n</code></pre> <p>Once we have the task we can then test to make sure that everything works as intended:</p> <pre><code># ensure that the dataset can be loaded and transformed properly\ntask = MyClusteringTask()\ntask.load_data()\n\nprint(task.dataset[\"headlines\"][0])  # check one of the samples:\n# {'sentences': 'Ryssland kritiserar ut\u00f6kade Irak-sanktioner', 'labels': 'domestic news'}\n\n# ensure that we can evaluate the model on the task\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\nprint(results[0].get_score())  # print the v_measure score of the random baseline\n# 0.006444026234557669\n</code></pre> <p>For our retrieval dataset we use the a dataset consisting of android-related questions and answer. The dataset consist of a corpus of questions and a set of queries, where each query has a set of relevant documents in the corpus. The goal is to retrieve the relevant documents for each query.</p> <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskRetrieval\n\n\nclass MyRetrievalTask(AbsTaskRetrieval):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MyRetrievalTask\",\n        description=\"A dummy retrieval task on a community question-answering dataset on android-related questions.\",\n        main_score=\"ndcg_at_10\",\n        eval_langs=[\"eng-Latn\"],\n        eval_splits=[\"test\"],\n        type=\"Retrieval\",\n        dataset={\n            \"path\": \"mteb/CQADupstackAndroidRetrieval\",\n            \"revision\": \"9be4c0e46342e8e3aff577a89b9a1ec9bc6b4af3\",\n        },\n        prompt=\"Given a question, retrieve the most relevant question from the corpus.\",\n    )\n</code></pre> <p>Once we have the task we can then test to make sure that everything works as intended:</p> <pre><code># ensure that the dataset can be loaded without errors\ntask = MyRetrievalTask()\ntask.load_data()\n\ntest_set = task.dataset[\"default\"][\"test\"]  # default unless there are multiple subsets\nprint(test_set[\"corpus\"][0])  # print the first item in the corpus\n# {'id': '51829', 'title': 'How can show android tablet as a external storage to PC?', 'text': \"I want to send files to android tablet ...\"}\nprint(test_set[\"queries\"][0])  # print the first query\n# {'id': '11546', 'text': 'Android chroot ubuntu - is it possible to get ubuntu to recognise usb devices'}\ntest_set[\"relevant_docs\"][\"11546\"]  # print the relevant documents for the first query\n# {'18572': 1}\n\n# ensure that we can evaluate the model on the task\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\nprint(results[0].get_score())  # print the ndcg_at_10 score of the random baseline\n# 0.021194685839832323\n</code></pre> <p>We kick up the notch here an do a multilingual example using a Indic Cross-lingual semantic similarity corpus. We will use just three of the languages as an example, but we can easily add more. The subsets on huggingface we will be using is \"en-as\", \"en-bn\", and \"en-gu\". The dataset consist of pairs of sentences between English and an Indic language which are rated based on their similarity on a score from 0-5.</p> <p>We will implement this as follows:</p> <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskSTS\n\n\nclass MySTSTask(AbsTaskSTS):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MySTSTask\",\n        description=\"A dummy STS task on a cross-lingual dataset between English and 3 indic languages\",\n        main_score=\"spearman\",\n        # for multilingual tasks, we need to specify the eval_langs as a mapping between the huggingface subset (e.g. \"en-as\")\n        # and their respective language codes in the dataset.\n        eval_langs={\n            \"en-as\": [\"eng-Latn\", \"asm-Beng\"],\n            \"en-bn\": [\"eng-Latn\", \"ben-Beng\"],\n            \"en-gu\": [\"eng-Latn\", \"guj-Gujr\"],\n        },\n        eval_splits=[\"test\"],\n        type=\"STS\",\n        dataset={\n            \"path\": \"mteb/IndicCrosslingualSTS\",\n            \"revision\": \"217bb120770a619b091d77aa06421a770821b22b\",\n        },\n    )\n\n    column_names = (\"sentence1\", \"sentence2\")\n    min_score = 0\n    max_score = 5\n</code></pre> <p>Once we have the task we can then test to make sure that everything works as intended:</p> <pre><code># ensure that the dataset can be loaded without errors\ntask = MySTSTask()\ntask.load_data()\n\nprint(task.dataset[\"en-gu\"][\"test\"][0])  # check one of the samples:\n# {'sentence1': 'Akrund is a small village in Dhansura Taluka of Aravalli district of northern Gujarat in western India.', 'sentence2': '\u0ab0\u0abe\u0ab9\u0aa4\u0ab2\u0abe\u0ab5 \u0aad\u0abe\u0ab0\u0aa4 \u0aa6\u0ac7\u0ab6\u0aa8\u0abe \u0aaa\u0ab6\u0acd\u0a9a\u0abf\u0aae \u0aad\u0abe\u0a97\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac7\u0ab2\u0abe \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4 \u0ab0\u0abe\u0a9c\u0acd\u0aaf\u0aa8\u0abe \u0a9a\u0ab0\u0acb\u0aa4\u0ab0 \u0aaa\u0acd\u0ab0\u0aa6\u0ac7\u0ab6\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac7\u0ab2\u0abe \u0a86\u0aa3\u0a82\u0aa6 \u0a9c\u0abf\u0ab2\u0acd\u0ab2\u0abe\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac7\u0ab2\u0abe \u0a86\u0aa3\u0a82\u0aa6 \u0aa4\u0abe\u0ab2\u0ac1\u0a95\u0abe\u0aae\u0abe\u0a82 \u0a86\u0ab5\u0ac7\u0ab2\u0ac1\u0a82 \u0a8f\u0a95 \u0a97\u0abe\u0aae \u0a9b\u0ac7.', 'score': 1.0}\n\n# ensure that we can evaluate the model on the task\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\nprint(results[0].get_score())  # print the spearman score of the random baseline\n# 0.021194685839832323\n</code></pre> Overwriting <code>load_data</code> <p>While we do not recommend overwriting <code>load_data</code> it can often be useful, when developing tasks and can also be used in conjunction with <code>push_dataset_to_hub</code> to push datasets to the hub in the correct format.</p> ClassificationRetrieval <p>For our classification task we use a dummy dataset consisting of two samples with labels 0 and 1. We can implement the task as follows: <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskClassification\n\n\nclass MyClassificationtask(AbsTaskClassification):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MyClassificationTask\",\n        description=\"A dummy classification just for testing\",\n        main_score=\"accuracy\",\n        eval_langs=[\"eng-Latn\"],\n        eval_splits=[\"test\"],\n        type=\"Classification\",\n        dataset={\"path\": \"na\", \"revision\": \"na\"},\n    )\n\n    label_column_name = \"label\"\n    input_column_name = \"text\"\n\n    def load_data(self, num_proc=None, **kwargs) -&gt; None:\n        self.dataset = {\n            \"test\": [\n                {\"text\": \"sample text 1\", \"label\": 1},\n                {\"text\": \"sample text 2\", \"label\": 0},\n            ]\n        }\n        self.data_loaded = True\n</code></pre> which can then be run as follows: <pre><code>task = MyClassificationtask()\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\n</code></pre></p> <p>Retrieval tasks often have a more complex structure, with a corpus, queries and relevant documents. Here we implement a dummy retrieval task with a small corpus and a few queries. <pre><code>import mteb\nfrom mteb.abstasks import AbsTaskRetrieval\nfrom mteb.abstasks.retrieval_dataset_loaders import RetrievalSplitData\nfrom datasets import Dataset\n\nclass MyRetrievalTask(AbsTaskRetrieval):\n    metadata = mteb.TaskMetadata(  # minimal metadata\n        name=\"MyRetrievalTask\",\n        description=\"A dummy retrieval task on a community question-answering dataset on android-related questions.\",\n        main_score=\"ndcg_at_10\",\n        eval_langs=[\"eng-Latn\"],\n        eval_splits=[\"test\"],\n        type=\"Retrieval\",\n        dataset={\n            \"path\": \"mteb/CQADupstackAndroidRetrieval\",\n            \"revision\": \"9be4c0e46342e8e3aff577a89b9a1ec9bc6b4af3\",\n        },\n        prompt=\"Given a question, retrieve the most relevant question from the corpus.\",\n    )\n\n    def load_data(self, num_proc: int | None = None, **kwargs) -&gt; None:\n        # corpus should have `id` and (`text` + `title`)/`image`/`audio` or any combination of these columns\n        corpus = Dataset.from_dict(\n            {\n                \"id\": [\"doc1\", \"doc2\", \"doc3\"],\n                \"text\": [\n                    \"This is the first document.\",\n                    \"This is the second document.\",\n                    \"This is the third document.\",\n                ],\n                \"title\": [\"Doc 1\", \"Doc 2\", \"Doc 3\"],\n            }\n        )\n\n        # queries should have `id` and `text`/`image`/`audio` or any combination of these columns\n        queries = Dataset.from_dict(\n            {\n                \"id\": [\"query1\", \"query2\"],\n                \"text\": [\n                    \"What is the first document about?\",\n                    \"What is the second document about?\",\n                ],\n            }\n        )\n\n        # qrels should be a dictionary `dict[query_id][document_id] = relevance_score`\n        qrels = {\n            \"query1\": {\"doc1\": 1, \"doc2\": 0, \"doc3\": 0},\n            \"query2\": {\"doc1\": 0, \"doc2\": 1, \"doc3\": 0},\n        }\n\n        # for reranking only\n        # should be a dictionary `dict[query_id] = [document_id1, document_id2, ...]` with the top ranked documents\n        top_ranked = {\n            \"query1\": [\"doc1\", \"doc2\", \"doc3\"],\n            \"query2\": [\"doc2\", \"doc1\", \"doc3\"],\n        }\n\n        self.dataset[\"default\"][\"test\"] = RetrievalSplitData(\n            corpus=corpus,\n            queries=queries,\n            relevant_docs=qrels,\n            top_ranked=top_ranked,  # only for reranking\n        )\n</code></pre> which can then be run as follows: <pre><code>task = MyRetrievalTask()\nmdl = mteb.get_model(\"mteb/baseline-random-encoder\")\nresults = mteb.evaluate(mdl, task)\n</code></pre></p>"},{"location":"contributing/adding_a_dataset/#filling-out-the-taskmetadata","title":"Filling out the TaskMetadata","text":"<p>To run a task locally you do not necessarily need to fill out all the fields in the <code>TaskMetadata</code>, but if you want to include the dataset in <code>mteb</code> we require that all fields are filled out. This is to ensure that we have enough information about the dataset to be able to include it in the benchmark and to make it easier for users to understand the dataset and its characteristics.</p> <p>If you are making a PR, feel free to leave fields and <code>None</code> if you are unsure about how to fill. You can always ask about it during the PR.</p> <p>Here is an example of how to fill out the <code>TaskMetadata</code> for the poem sentiment classification dataset, we implemented above and which is also available within <code>mteb</code> as <code>PoemSentimentClassification.v2</code>, for more details about the fields see the TaskMetadata documentation.</p> <pre><code>TaskMetadata(\n    name=\"PoemSentimentClassification.v2\",\n    # description should provide a concise overview of the dataset\n    description=\"Poem Sentiment consists of poem verses from Project Gutenberg annotated for sentiment using the labels negative (0), positive (1), no_impact (2), and mixed (3). This version was corrected as part of [pull request](https://github.com/embeddings-benchmark/mteb/pull/2900) to fix common issues in the original dataset, including removing duplicates and train-test leakage.\",\n    reference=\"https://arxiv.org/abs/2011.02686\",\n    dataset={\n        \"path\": \"mteb/poem_sentiment\",\n        \"revision\": \"9fdc57b89ccc09a8d9256f376112d626878e51a7\",\n    },\n    type=\"Classification\",\n    category=\"t2c\", # text-2-class\n    modalities=[\"text\"],\n    eval_splits=[\"validation\", \"test\"],\n    eval_langs=[\"eng-Latn\"],\n    main_score=\"accuracy\",\n    date=(\"1700-01-01\", \"1900-01-01\"), # a very rough guess of the date range of the poems, we do not have exact dates for all poems\n    domains=[\"Written\", \"Fiction\", \"Poetry\"],\n    task_subtypes=[\"Sentiment/Hate speech\"], # if no subtypes match then just use []\n    license=\"cc-by-4.0\",\n    annotations_creators=\"human-annotated\",\n    dialect=[\"eng-Latn-US\", \"en-Latn-GB\"], # dialects is often unknown if so just use []\n    sample_creation=\"found\", # the text was not created for the purpose of the dataset, but rather found and annotated\n    adapted_from=[\"PoemSentimentClassification\"], # Previous version of the dataset, can be None\n    bibtex_citation=r\"\"\"\n@misc{sheng2020investigating,\n  archiveprefix = {arXiv},\n  author = {Emily Sheng and David Uthus},\n  eprint = {2011.02686},\n  primaryclass = {cs.CL},\n  title = {Investigating Societal Biases in a Poetry Composition System},\n  year = {2020},\n}\n\"\"\",\n)\n</code></pre>"},{"location":"contributing/adding_a_dataset/#pushing-the-dataset-to-the-hub","title":"Pushing the dataset to the hub","text":"<p>Once a dataset is added to the repository, it can be pushed to the hub using the <code>push_dataset_to_hub</code> method. This will reupload the dataset to the hub in the correct format and with all the processing applied. This means that if you have done some processing in the <code>dataset_transform</code> method, this will be reflected in the dataset pushed to the hub. You can then update class to utilize the new dataset on the hub and remove the <code>dataset_transform</code> method. This avoid the need to do the processing every time the dataset is loaded, which can be time consuming for large datasets.</p> <pre><code>import mteb\n\nclass MyTask(...):\n    ...\n\ntask = MyTask()\nrepo_name = f\"myorg/{task.metadata.name}\"\n# Push the dataset to the Hub\ntask.push_dataset_to_hub(repo_name)\n</code></pre>"},{"location":"contributing/adding_a_dataset/#create-a-pull-request","title":"Create a Pull Request","text":"<p>Once you have your task you can create a pull request (PR) to the main repository. To do so place task inside the <code>mteb/tasks</code> directory, and make sure to import the task in the <code>__init__.py</code> file in the same directory.</p>"},{"location":"contributing/adding_a_dataset/#calculate-descriptive-statistics","title":"Calculate descriptive statistics","text":"<p>Before creating a pull request, it is important to calculate some descriptive statistics about the dataset. This is to ensure that the dataset is not too small, does not contain duplicates and that the documents are not too short. This is important to ensure that the dataset is of high quality and that it can be used to evaluate models in a meaningful way.</p> <p>To calculate the descriptive statistics, you can simply run <code>task.calculate_descriptive_statistics()</code>.</p>"},{"location":"contributing/adding_a_dataset/#submit-a-pull-request","title":"Submit a Pull Request","text":"<p>Once added, here is a checklist to ensure that everything works before you submit the PR:</p> <pre><code>- [ ] I have outlined why this dataset is filling an existing gap in `mteb`\n- [ ] I have tested that the dataset runs with the `mteb` package.\n- [ ] I have run the following models on the task (adding the results to the pr). These can be run using the `mteb run -m {model_name} -t {task_name}` command.\n  - [ ] `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`\n  - [ ] `intfloat/multilingual-e5-small`\n- [ ] I have checked that the performance is neither trivial (both models gain close to perfect scores) nor random (both models gain close to random scores).\n- [ ] I have considered the size of the dataset and reduced it if it is too big (2048 examples is typically large enough for most tasks)\n</code></pre> <p>An easy way to test it is using:</p> PythonCLI <pre><code>import mteb\n# sample model:\nmodel = mteb.get_model(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\ntask = mteb.get_task(\"{name of your task}\")\n\nresults = mteb.evaluate(model, task)\n</code></pre> <pre><code>mteb run -m sentence-transformers/paraphrase-multilingual-MiniLM -t {name of your task}\n</code></pre> <p>Once submitted the PR will be reviewed by one of the organizers or contributors who might ask you to change things. Once the PR is approved the dataset will be added into the main repository.</p>"},{"location":"contributing/adding_a_model/","title":"Adding a Model","text":""},{"location":"contributing/adding_a_model/#adding-a-model-to-the-leaderboard","title":"Adding a model to the Leaderboard","text":"<p>The MTEB Leaderboard is available here. To submit to it:</p> <ol> <li>Add the model meta to <code>mteb</code></li> <li>Evaluate the desired model using <code>mteb</code> on the benchmarks</li> <li>Push the results to the results repository via a PR. Once merged they will appear on the leaderboard after a day.</li> </ol>"},{"location":"contributing/adding_a_model/#adding-a-model-implementation","title":"Adding a model implementation","text":"<p>Adding a model implementation to <code>mteb</code> is quite straightforward. Typically, it only requires that you fill in metadata about the model and add it to the model directory:</p> Adding a ModelMeta object <pre><code>from mteb.models import ModelMeta, sentence_transformers_loader\n\nmy_model = ModelMeta(\n    name=\"model_name\",\n    loader=sentence_transformers_loader,\n    languages=[\"eng-Latn\"], # follows ISO 639-3 and BCP-47\n    open_weights=True,\n    revision=\"5617a9f61b028005a4858fdac845db406aefb181\",\n    release_date=\"2025-01-01\",\n    n_parameters=568_000_000,\n    memory_usage_mb=2167,\n    embed_dim=4096,\n    license=\"mit\",\n    max_tokens=8194,\n    reference=\"https://huggingface.co/user-or-org/model-name\",\n    similarity_fn_name=\"cosine\",\n    framework=[\"Sentence Transformers\", \"PyTorch\"],\n    use_instructions=False,\n    public_training_code=\"https://github.com/user-or-org/my-training-code\",\n    public_training_data=\"https://huggingface.co/datasets/user-or-org/full-dataset\",\n    training_datasets={\"MSMARCO\"}, # if you trained on the MSMARCO training set\n)\n</code></pre> <p>This works for all Sentence Transformers compatible models. Once filled out, you can submit your model to <code>mteb</code> by submitting a PR.</p> <p>You can generate it automatically by using:</p> General model from hubFor Sentence transformers modelFor CrossEncoder <pre><code>from mteb.models import ModelMeta\n\nmeta = ModelMeta.from_hub(\"Qwen/Qwen3-Embedding-0.6B\")\nprint(meta.to_python())\n</code></pre> <pre><code>from mteb.models import ModelMeta\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\", device=\"cpu\")\nmeta = ModelMeta.from_sentence_transformer_model(model)\nprint(meta.to_python())\n</code></pre> <pre><code>from mteb.models import ModelMeta\nfrom sentence_transformers import CrossEncoder\n\nmodel = SentenceTransformer(\"Qwen/Qwen3-Reranker-0.6B\", device=\"cpu\")\nmeta = ModelMeta.from_cross_encoder(model)\nprint(meta.to_python())\n</code></pre>"},{"location":"contributing/adding_a_model/#calculating-the-memory-usage","title":"Calculating the Memory Usage","text":"<p>To calculate <code>memory_usage_mb</code>, run:</p> <pre><code>model_meta = mteb.get_model_meta(\"model_name\")\nmodel_meta.calculate_memory_usage_mb()\n</code></pre>"},{"location":"contributing/adding_a_model/#adding-instruction-models","title":"Adding instruction models","text":"<p>Some models, such as the [E5 models]<sup>1</sup>, use instructions or prefixes. You can directly add the prompts when saving and uploading your model to the Hub. Refer to this configuration file as an example.</p> <p>However, you can also add these directly to the model configuration:</p> <pre><code>model = ModelMeta(\n    loader=sentence_transformers_loader\n    loader_kwargs=dict(\n        model_prompts={\n           \"query\": \"query: \",\n           \"passage\": \"passage: \",\n        },\n    ),\n    ...\n)\n</code></pre>"},{"location":"contributing/adding_a_model/#using-a-custom-implementation","title":"Using a custom Implementation","text":"<p>If you need to use a custom implementation, you can specify the <code>loader</code> parameter in the <code>ModelMeta</code> class. It should implement one of the following protocols: Encoder, CrossEncoder, or Search.</p> Custom Model Implementation <pre><code>from mteb.types import PromptType, Array\nimport numpy as np\n\nclass CustomModel:\n    def __init__(self, model_name: str, revision: str, **kwargs):\n        pass # your initialization of model here\n\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        *,\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs,\n    ) -&gt; Array:\n\n        arrays = []\n        for batch in inputs:\n            documents = batch[\"text\"]\n            # embed documents:\n            embed_dim = 100\n            embedding = np.zeros((len(documents), embed_dim))\n\n        embeddings = np.concat(arrays)\n        return embeddings\n</code></pre> <p>Then you can specify the <code>loader</code> parameter in the <code>ModelMeta</code> class:</p> <pre><code>your_model = ModelMeta(\n    loader=CustomModel,\n    loader_kwargs={...},\n    ...\n)\n</code></pre>"},{"location":"contributing/adding_a_model/#adding-model-dependencies","title":"Adding model dependencies","text":"<p>If you are adding a model that requires additional dependencies, you can add them to the <code>pyproject.toml</code> file, under optional dependencies:</p> <pre><code>voyageai = [\"voyageai&gt;=1.0.0,&lt;2.0.0\"]\n</code></pre> <p>This ensures that the implementation does not break if a package is updated.</p> <p>As it is an optional dependency, you can't use top-level dependencies, but will instead have to use import inside the wrapper scope:</p> Adding optional dependencies <pre><code>from mteb._requires_package import requires_package\n\nclass VoyageAIModel:\n    def __init__(self, model_name: str, revision: str, **kwargs) -&gt; None:\n        requires_package(self, \"voyageai\", model_name, \"pip install 'mteb[voyageai]'\")\n        import voyageai\n        ...\n</code></pre>"},{"location":"contributing/adding_a_model/#submitting-your-model-as-a-pr","title":"Submitting your model as a PR","text":"<p>When submitting you models as a PR, please copy and paste the following checklist into the pull request message:</p> <pre><code>- [ ] I have filled out the ModelMeta object to the extent possible\n- [ ] I have ensured that my model can be loaded using\n  - [ ] `mteb.get_model(model_name, revision)` and\n  - [ ] `mteb.get_model_meta(model_name, revision)`\n- [ ] I have tested the implementation works on a representative set of tasks.\n- [ ] The model is public, i.e., is available either as an API or the weights are publicly available to download\n</code></pre> <ol> <li> <p>Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Multilingual e5 text embeddings: a technical report. arXiv preprint arXiv:2402.05672, 2024.\u00a0\u21a9</p> </li> </ol>"},{"location":"overview/","title":"Overview","text":"<p>This section provides an overview of available tasks, models and benchmarks in MTEB.</p>"},{"location":"overview/available_benchmarks/","title":"Available Benchmarks","text":""},{"location":"overview/available_benchmarks/#beir","title":"BEIR","text":"<p>BEIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark.</p> <p>Learn more \u2192</p> Tasks name type modalities languages TRECCOVID Retrieval text eng NFCorpus Retrieval text eng NQ Retrieval text eng HotpotQA Retrieval text eng FiQA2018 Retrieval text eng ArguAna Retrieval text eng Touche2020 Retrieval text eng CQADupstackRetrieval Retrieval text eng QuoraRetrieval Retrieval text eng DBPedia Retrieval text eng SCIDOCS Retrieval text eng FEVER Retrieval text eng ClimateFEVER Retrieval text eng SciFact Retrieval text eng MSMARCO Retrieval text eng Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#beir-nl","title":"BEIR-NL","text":"<p>BEIR-NL is a Dutch adaptation of the publicly available BEIR benchmark, created through automated translation.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ArguAna-NL Retrieval text nld CQADupstack-NL Retrieval text nld FEVER-NL Retrieval text nld NQ-NL Retrieval text nld Touche2020-NL Retrieval text nld FiQA2018-NL Retrieval text nld Quora-NL Retrieval text nld HotpotQA-NL Retrieval text nld SCIDOCS-NL Retrieval text nld ClimateFEVER-NL Retrieval text nld mMARCO-NL Retrieval text nld SciFact-NL Retrieval text nld DBPedia-NL Retrieval text nld NFCorpus-NL Retrieval text nld TRECCOVID-NL Retrieval text nld Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#bright","title":"BRIGHT","text":"<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval.     BRIGHT is the first text retrieval     benchmark that requires intensive reasoning to retrieve relevant documents with     a dataset consisting of 1,384 real-world queries spanning diverse domains, such as     economics, psychology, mathematics, and coding. These queries are drawn from     naturally occurring and carefully curated human data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BrightRetrieval Retrieval text eng Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#bright-long","title":"BRIGHT (long)","text":"<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval. BRIGHT is the first text retrieval benchmark that requires intensive reasoning to retrieve relevant documents with a dataset consisting of 1,384 real-world queries spanning diverse domains, such as economics, psychology, mathematics, and coding. These queries are drawn from naturally occurring and carefully curated human data.</p> <p>This is the long version of the benchmark, which only filter longer documents.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BrightLongRetrieval Retrieval text eng Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#brightv11","title":"BRIGHT(v1.1)","text":"<p>v1.1 refactors the BRIGHT into a different tasks and added prompt to individual tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BrightBiologyRetrieval Retrieval text eng BrightEarthScienceRetrieval Retrieval text eng BrightEconomicsRetrieval Retrieval text eng BrightPsychologyRetrieval Retrieval text eng BrightRoboticsRetrieval Retrieval text eng BrightStackoverflowRetrieval Retrieval text eng BrightSustainableLivingRetrieval Retrieval text eng BrightPonyRetrieval Retrieval text eng BrightLeetcodeRetrieval Retrieval text eng BrightAopsRetrieval Retrieval text eng BrightTheoremQATheoremsRetrieval Retrieval text eng BrightTheoremQAQuestionsRetrieval Retrieval text eng BrightBiologyLongRetrieval Retrieval text eng BrightEarthScienceLongRetrieval Retrieval text eng BrightEconomicsLongRetrieval Retrieval text eng BrightPsychologyLongRetrieval Retrieval text eng BrightRoboticsLongRetrieval Retrieval text eng BrightStackoverflowLongRetrieval Retrieval text eng BrightSustainableLivingLongRetrieval Retrieval text eng BrightPonyLongRetrieval Retrieval text eng Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#builtbencheng","title":"BuiltBench(eng)","text":"<p>\"Built-Bench\" is an ongoing effort aimed at evaluating text embedding models in the context of built asset management, spanning over various disciplines such as architecture, engineering, construction, and operations management of the built environment.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BuiltBenchClusteringP2P Clustering text eng BuiltBenchClusteringS2S Clustering text eng BuiltBenchRetrieval Retrieval text eng BuiltBenchReranking Reranking text eng Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#chemteb","title":"ChemTEB","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PubChemSMILESBitextMining BitextMining text eng SDSEyeProtectionClassification Classification text eng SDSGlovesClassification Classification text eng WikipediaBioMetChemClassification Classification text eng WikipediaGreenhouseEnantiopureClassification Classification text eng WikipediaSolidStateColloidalClassification Classification text eng WikipediaOrganicInorganicClassification Classification text eng WikipediaCryobiologySeparationClassification Classification text eng WikipediaChemistryTopicsClassification Classification text eng WikipediaTheoreticalAppliedClassification Classification text eng WikipediaChemFieldsClassification Classification text eng WikipediaLuminescenceClassification Classification text eng WikipediaIsotopesFissionClassification Classification text eng WikipediaSaltsSemiconductorsClassification Classification text eng WikipediaBiolumNeurochemClassification Classification text eng WikipediaCrystallographyAnalyticalClassification Classification text eng WikipediaCompChemSpectroscopyClassification Classification text eng WikipediaChemEngSpecialtiesClassification Classification text eng WikipediaChemistryTopicsClustering Clustering text eng WikipediaSpecialtiesInChemistryClustering Clustering text eng PubChemAISentenceParaphrasePC PairClassification text eng PubChemSMILESPC PairClassification text eng PubChemSynonymPC PairClassification text eng PubChemWikiParagraphsPC PairClassification text eng PubChemWikiPairClassification PairClassification text ces, deu, eng, fra, hin, ... (13) ChemNQRetrieval Retrieval text eng ChemHotpotQARetrieval Retrieval text eng Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#chemtebv11","title":"ChemTEB(v1.1)","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version adds the ChemRxivRetrieval task.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PubChemSMILESBitextMining BitextMining text eng SDSEyeProtectionClassification Classification text eng SDSGlovesClassification Classification text eng WikipediaBioMetChemClassification Classification text eng WikipediaGreenhouseEnantiopureClassification Classification text eng WikipediaSolidStateColloidalClassification Classification text eng WikipediaOrganicInorganicClassification Classification text eng WikipediaCryobiologySeparationClassification Classification text eng WikipediaChemistryTopicsClassification Classification text eng WikipediaTheoreticalAppliedClassification Classification text eng WikipediaChemFieldsClassification Classification text eng WikipediaLuminescenceClassification Classification text eng WikipediaIsotopesFissionClassification Classification text eng WikipediaSaltsSemiconductorsClassification Classification text eng WikipediaBiolumNeurochemClassification Classification text eng WikipediaCrystallographyAnalyticalClassification Classification text eng WikipediaCompChemSpectroscopyClassification Classification text eng WikipediaChemEngSpecialtiesClassification Classification text eng WikipediaChemistryTopicsClustering Clustering text eng WikipediaSpecialtiesInChemistryClustering Clustering text eng PubChemAISentenceParaphrasePC PairClassification text eng PubChemSMILESPC PairClassification text eng PubChemSynonymPC PairClassification text eng PubChemWikiParagraphsPC PairClassification text eng PubChemWikiPairClassification PairClassification text ces, deu, eng, fra, hin, ... (13) ChemNQRetrieval Retrieval text eng ChemHotpotQARetrieval Retrieval text eng ChemRxivRetrieval Retrieval text eng Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kasmaee2025chembed,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Astaraki, Mahdi and Saloot, Mohammad Arshi and Sherck, Nicholas and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2508.01643},\n  title = {Chembed: Enhancing chemical literature search through domain-specific text embeddings},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#coir","title":"CoIR","text":"<p>CoIR: A Comprehensive Benchmark for Code Information Retrieval Models</p> <p>Learn more \u2192</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python CodeFeedbackMT Retrieval text eng CodeFeedbackST Retrieval text eng CodeSearchNetCCRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeTransOceanContest Retrieval text c++, python CodeTransOceanDL Retrieval text python CosQA Retrieval text eng, python COIRCodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) StackOverflowQA Retrieval text eng SyntheticText2SQL Retrieval text eng, sql Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#coderag","title":"CodeRAG","text":"<p>A benchmark for evaluating code retrieval augmented generation, testing models' ability to retrieve relevant programming solutions, tutorials and documentation.</p> <p>Learn more \u2192</p> Tasks name type modalities languages CodeRAGLibraryDocumentationSolutions Reranking text python CodeRAGOnlineTutorials Reranking text python CodeRAGProgrammingSolutions Reranking text python CodeRAGStackoverflowPosts Reranking text python Citation <pre><code>@misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#encodechka","title":"Encodechka","text":"<p>A benchmark for evaluating text embedding models on Russian data.</p> <p>Learn more \u2192</p> Tasks name type modalities languages RUParaPhraserSTS STS text rus SentiRuEval2016 Classification text rus RuToxicOKMLCUPClassification Classification text rus InappropriatenessClassificationv2 Classification text rus RuNLUIntentClassification Classification text rus XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) RuSTSBenchmarkSTS STS text rus Citation <pre><code>@misc{dale_encodechka,\n  author = {Dale, David},\n  editor = {habr.com},\n  month = {June},\n  note = {[Online; posted 12-June-2022]},\n  title = {Russian rating of sentence encoders},\n  url = {https://habr.com/ru/articles/669674/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#followir","title":"FollowIR","text":"<p>Retrieval w/Instructions is the task of finding relevant documents for a query that has detailed instructions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Robust04InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Core17InstructionRetrieval InstructionReranking text eng Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#humev1","title":"HUME(v1)","text":"<p>The HUME benchmark is designed to evaluate the performance of text embedding models and humans on a comparable set of tasks. This captures areas where models perform better than human annotators and the reverse. In the paper, we go further into the analysis and what conclusions can be drawn.</p> Tasks name type modalities languages HUMEEmotionClassification Classification text eng HUMEToxicConversationsClassification Classification text eng HUMETweetSentimentExtractionClassification Classification text eng HUMEMultilingualSentimentClassification Classification text ara, eng, nob, rus HUMEArxivClusteringP2P Clustering text eng HUMERedditClusteringP2P Clustering text eng HUMEWikiCitiesClustering Clustering text eng HUMESIB200ClusteringS2S Clustering text ara, dan, eng, fra, rus HUMECore17InstructionReranking Reranking text eng HUMENews21InstructionReranking Reranking text eng HUMERobust04InstructionReranking Reranking text eng HUMEWikipediaRerankingMultilingual Reranking text dan, eng, nob HUMESICK-R STS text eng HUMESTS12 STS text eng HUMESTSBenchmark STS text eng HUMESTS22 STS text ara, eng, fra, rus"},{"location":"overview/available_benchmarks/#jmtebv2","title":"JMTEB(v2)","text":"<p>JMTEB is a benchmark for evaluating Japanese text embedding models. In v2, we have extended the benchmark to 28 datasets, enabling more comprehensive evaluation compared with v1 (MTEB(jpn, v1)).</p> <p>Learn more \u2192</p> Tasks name type modalities languages LivedoorNewsClustering.v2 Clustering text jpn MewsC16JaClustering Clustering text jpn SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) JapaneseSentimentClassification Classification text jpn SIB200Classification Classification text ace, acm, acq, aeb, afr, ... (197) WRIMEClassification Classification text jpn JSTS STS text jpn JSICK STS text jpn JaqketRetrieval Retrieval text jpn MrTidyRetrieval Retrieval text ara, ben, eng, fin, ind, ... (11) JaGovFaqsRetrieval Retrieval text jpn NLPJournalTitleAbsRetrieval.V2 Retrieval text jpn NLPJournalTitleIntroRetrieval.V2 Retrieval text jpn NLPJournalAbsIntroRetrieval.V2 Retrieval text jpn NLPJournalAbsArticleRetrieval.V2 Retrieval text jpn JaCWIRRetrieval Retrieval text jpn MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) MintakaRetrieval Retrieval text ara, deu, fra, hin, ita, ... (8) MultiLongDocRetrieval Retrieval text ara, cmn, deu, eng, fra, ... (13) ESCIReranking Reranking text eng, jpn, spa JQaRAReranking Reranking text jpn JaCWIRReranking Reranking text jpn MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) MultiLongDocReranking Reranking text ara, deu, eng, fra, hin, ... (13) Citation <pre><code>@article{li2025jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide and Kawahara, Daisuke},\n  issue = {3},\n  journal = {Vol.2025-NL-265,No.3,1-15},\n  month = {sep},\n  title = {{JMTEB and JMTEB-lite: Japanese Massive Text Embedding Benchmark and Its Lightweight Version}},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#jmteb-litev1","title":"JMTEB-lite(v1)","text":"<p>JMTEB-lite is a lightweight version of JMTEB. It makes agile evaluation possible by reaching an average of 5x faster evaluation comparing with JMTEB, as 6 heavy datasets in JMTEB are optimized with hard negative pooling strategy, making them much smaller. The result of JMTEB-lite is proved to be highly relevant with that of JMTEB, making it a faithful preview of JMTEB.</p> <p>Learn more \u2192</p> Tasks name type modalities languages LivedoorNewsClustering.v2 Clustering text jpn MewsC16JaClustering Clustering text jpn SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) JapaneseSentimentClassification Classification text jpn SIB200Classification Classification text ace, acm, acq, aeb, afr, ... (197) WRIMEClassification Classification text jpn JSTS STS text jpn JSICK STS text jpn JaqketRetrievalLite Retrieval text jpn MrTyDiJaRetrievalLite Retrieval text jpn JaGovFaqsRetrieval Retrieval text jpn NLPJournalTitleAbsRetrieval.V2 Retrieval text jpn NLPJournalTitleIntroRetrieval.V2 Retrieval text jpn NLPJournalAbsIntroRetrieval.V2 Retrieval text jpn NLPJournalAbsArticleRetrieval.V2 Retrieval text jpn JaCWIRRetrievalLite Retrieval text jpn MIRACLJaRetrievalLite Retrieval text jpn MintakaRetrieval Retrieval text ara, deu, fra, hin, ita, ... (8) MultiLongDocRetrieval Retrieval text ara, cmn, deu, eng, fra, ... (13) ESCIReranking Reranking text eng, jpn, spa JQaRARerankingLite Reranking text jpn JaCWIRRerankingLite Reranking text jpn MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) MultiLongDocReranking Reranking text ara, deu, eng, fra, hin, ... (13) Citation <pre><code>@article{li2025jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide and Kawahara, Daisuke},\n  issue = {3},\n  journal = {Vol.2025-NL-265,No.3,1-15},\n  month = {sep},\n  title = {{JMTEB and JMTEB-lite: Japanese Massive Text Embedding Benchmark and Its Lightweight Version}},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#jinavdr","title":"JinaVDR","text":"<p>Multilingual, domain-diverse and layout-rich document retrieval benchmark.</p> <p>Learn more \u2192</p> Tasks name type modalities languages JinaVDRMedicalPrescriptionsRetrieval DocumentUnderstanding text, image eng JinaVDRStanfordSlideRetrieval DocumentUnderstanding text, image eng JinaVDRDonutVQAISynHMPRetrieval DocumentUnderstanding text, image eng JinaVDRTableVQARetrieval DocumentUnderstanding text, image eng JinaVDRChartQARetrieval DocumentUnderstanding text, image eng JinaVDRTQARetrieval DocumentUnderstanding text, image eng JinaVDROpenAINewsRetrieval DocumentUnderstanding text, image eng JinaVDREuropeanaDeNewsRetrieval DocumentUnderstanding text, image deu JinaVDREuropeanaEsNewsRetrieval DocumentUnderstanding text, image spa JinaVDREuropeanaItScansRetrieval DocumentUnderstanding text, image ita JinaVDREuropeanaNlLegalRetrieval DocumentUnderstanding text, image nld JinaVDRHindiGovVQARetrieval DocumentUnderstanding text, image hin JinaVDRAutomobileCatelogRetrieval DocumentUnderstanding text, image jpn JinaVDRBeveragesCatalogueRetrieval DocumentUnderstanding text, image rus JinaVDRRamensBenchmarkRetrieval DocumentUnderstanding text, image jpn JinaVDRJDocQARetrieval DocumentUnderstanding text, image jpn JinaVDRHungarianDocQARetrieval DocumentUnderstanding text, image hun JinaVDRArabicChartQARetrieval DocumentUnderstanding text, image ara JinaVDRArabicInfographicsVQARetrieval DocumentUnderstanding text, image ara JinaVDROWIDChartsRetrieval DocumentUnderstanding text, image eng JinaVDRMPMQARetrieval DocumentUnderstanding text, image eng JinaVDRJina2024YearlyBookRetrieval DocumentUnderstanding text, image eng JinaVDRWikimediaCommonsMapsRetrieval DocumentUnderstanding text, image eng JinaVDRPlotQARetrieval DocumentUnderstanding text, image eng JinaVDRMMTabRetrieval DocumentUnderstanding text, image eng JinaVDRCharXivOCRRetrieval DocumentUnderstanding text, image eng JinaVDRStudentEnrollmentSyntheticRetrieval DocumentUnderstanding text, image eng JinaVDRGitHubReadmeRetrieval DocumentUnderstanding text, image ara, ben, deu, eng, fra, ... (17) JinaVDRTweetStockSyntheticsRetrieval DocumentUnderstanding text, image ara, deu, eng, fra, hin, ... (10) JinaVDRAirbnbSyntheticRetrieval DocumentUnderstanding text, image ara, deu, eng, fra, hin, ... (10) JinaVDRShanghaiMasterPlanRetrieval DocumentUnderstanding text, image zho JinaVDRWikimediaCommonsDocumentsRetrieval DocumentUnderstanding text, image ara, ben, deu, eng, fra, ... (20) JinaVDREuropeanaFrNewsRetrieval DocumentUnderstanding text, image fra JinaVDRDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng JinaVDRDocQAAI DocumentUnderstanding text, image eng JinaVDRShiftProjectRetrieval DocumentUnderstanding text, image eng JinaVDRTatQARetrieval DocumentUnderstanding text, image eng JinaVDRInfovqaRetrieval DocumentUnderstanding text, image eng JinaVDRDocVQARetrieval DocumentUnderstanding text, image eng JinaVDRDocQAGovReportRetrieval DocumentUnderstanding text, image eng JinaVDRTabFQuadRetrieval DocumentUnderstanding text, image eng JinaVDRDocQAEnergyRetrieval DocumentUnderstanding text, image eng JinaVDRArxivQARetrieval DocumentUnderstanding text, image eng Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#kovidorev2","title":"KoViDoRe(v2)","text":"<p>KoViDoRe v2 sets a new industry gold standard for multi-modal, enterprise document visual retrieval evaluation. It addresses a critical challenge in production RAG systems: retrieving accurate information from complex, visually-rich documents.</p> <p>Learn more \u2192</p> Tasks name type modalities languages KoVidore2CybersecurityRetrieval DocumentUnderstanding text, image kor KoVidore2EconomicRetrieval DocumentUnderstanding text, image kor KoVidore2EnergyRetrieval DocumentUnderstanding text, image kor KoVidore2HrRetrieval DocumentUnderstanding text, image kor Citation <pre><code>@misc{choi2026kovidorev2,\n  author = {Yongbin Choi},\n  note = {A benchmark for evaluating Korean vision document retrieval with multi-page reasoning queries in practical domains},\n  title = {KoViDoRe v2: a comprehensive evaluation of vision document retrieval for enterprise use-cases},\n  url = {https://github.com/whybe-choi/kovidore-data-generator},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#longembed","title":"LongEmbed","text":"<p>LongEmbed is a benchmark oriented at exploring models' performance on long-context retrieval.     The benchmark comprises two synthetic tasks and four carefully chosen real-world tasks,     featuring documents of varying length and dispersed target information.</p> <p>Learn more \u2192</p> Tasks name type modalities languages LEMBNarrativeQARetrieval Retrieval text eng LEMBNeedleRetrieval Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng LEMBQMSumRetrieval Retrieval text eng LEMBSummScreenFDRetrieval Retrieval text eng LEMBWikimQARetrieval Retrieval text eng Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#maebbeta","title":"MAEB(beta)","text":"<p>MAEB is a comprehensive audio benchmark with 30 tasks spanning both audio-only and audio-text cross-modal evaluation. Tasks span 7 task types: retrieval (9), classification (10), clustering (3), multilabel classification (2), pair classification (3), reranking (1), and zero-shot classification (2). The benchmark is currently in beta as the paper has been submitted for review and will be released in its final version after the review process.</p> Tasks name type modalities languages ClothoT2ARetrieval Any2AnyRetrieval text, audio eng CommonVoiceMini21T2ARetrieval Any2AnyRetrieval text, audio abk, afr, amh, ara, asm, ... (114) FleursT2ARetrieval Any2AnyRetrieval text, audio afr, amh, ara, asm, ast, ... (102) GigaSpeechT2ARetrieval Any2AnyRetrieval text, audio eng JamAltArtistA2ARetrieval Any2AnyRetrieval audio deu, eng, fra, spa JamAltLyricA2TRetrieval Any2AnyRetrieval text, audio deu, eng, fra, spa MACST2ARetrieval Any2AnyRetrieval text, audio eng SpokenSQuADT2ARetrieval Any2AnyRetrieval text, audio eng UrbanSound8KT2ARetrieval Any2AnyRetrieval text, audio zxx BeijingOpera AudioClassification audio zxx BirdCLEF AudioClassification audio zxx CREMA_D AudioClassification audio eng CommonLanguageAgeDetection AudioClassification audio eng GTZANGenre AudioClassification audio zxx IEMOCAPGender AudioClassification audio eng MInDS14 AudioClassification audio ces, deu, eng, fra, ita, ... (12) MridinghamTonic AudioClassification audio zxx VoxCelebSA AudioClassification audio eng VoxPopuliLanguageID AudioClassification audio deu, eng, fra, pol, spa CREMA_DClustering AudioClustering audio eng VehicleSoundClustering AudioClustering audio zxx VoxPopuliGenderClustering AudioClustering audio deu, eng, fra, pol, spa FSD2019Kaggle AudioMultilabelClassification audio eng SIBFLEURS AudioMultilabelClassification audio afr, amh, arb, asm, ast, ... (101) CREMADPairClassification AudioPairClassification audio eng NMSQAPairClassification AudioPairClassification audio eng VoxPopuliAccentPairClassification AudioPairClassification audio eng GTZANAudioReranking AudioReranking audio zxx RavdessZeroshot AudioZeroshotClassification audio, text eng SpeechCommandsZeroshotv0.02 AudioZeroshotClassification audio, text eng"},{"location":"overview/available_benchmarks/#maebbeta-audio-only","title":"MAEB(beta, audio-only)","text":"<p>MAEB(audio-only) is the audio-only subset of MAEB with 19 tasks spanning 6 task types: classification (10), clustering (3), multilabel classification (1), pair classification (3), reranking (1), and retrieval (1). The benchmark is currently in beta as the paper has been submitted for review and will be released in its final version after the review process.</p> Tasks name type modalities languages JamAltArtistA2ARetrieval Any2AnyRetrieval audio deu, eng, fra, spa BeijingOpera AudioClassification audio zxx BirdCLEF AudioClassification audio zxx CREMA_D AudioClassification audio eng CommonLanguageAgeDetection AudioClassification audio eng GTZANGenre AudioClassification audio zxx IEMOCAPGender AudioClassification audio eng MInDS14 AudioClassification audio ces, deu, eng, fra, ita, ... (12) MridinghamTonic AudioClassification audio zxx VoxCelebSA AudioClassification audio eng VoxPopuliLanguageID AudioClassification audio deu, eng, fra, pol, spa CREMA_DClustering AudioClustering audio eng VehicleSoundClustering AudioClustering audio zxx VoxPopuliGenderClustering AudioClustering audio deu, eng, fra, pol, spa SIBFLEURS AudioMultilabelClassification audio afr, amh, arb, asm, ast, ... (101) CREMADPairClassification AudioPairClassification audio eng NMSQAPairClassification AudioPairClassification audio eng VoxPopuliAccentPairClassification AudioPairClassification audio eng GTZANAudioReranking AudioReranking audio zxx"},{"location":"overview/available_benchmarks/#maebbeta-extended","title":"MAEB(beta, extended)","text":"<p>MAEB(extended) is an intermediate benchmark used during task selection, containing 89 tasks that combine audio-only and audio-text evaluation before filtering to MAEB. Audio-only (53 tasks): classification (28), multilabel classification (4), reranking (5), clustering (10), pair classification (5), audio-to-audio retrieval (1). Audio-text (36 tasks): audio-text retrieval (31), zero-shot classification (5). The benchmark is currently in beta as the paper has been submitted for review and will be released in its final version after the review process.</p> Tasks name type modalities languages FSD50K AudioMultilabelClassification audio eng SIBFLEURS AudioMultilabelClassification audio afr, amh, arb, asm, ast, ... (101) FSD2019Kaggle AudioMultilabelClassification audio eng AudioSetMini AudioMultilabelClassification audio eng VoxPopuliAccentID AudioClassification audio eng MInDS14 AudioClassification audio ces, deu, eng, fra, ita, ... (12) VoxPopuliGenderID AudioClassification audio deu, eng, fra, pol, spa BeijingOpera AudioClassification audio zxx AmbientAcousticContext AudioClassification audio zxx CREMA_D AudioClassification audio eng VoxCelebSA AudioClassification audio eng TUTAcousticScenes AudioClassification audio zxx NSynth AudioClassification audio zxx VocalSound AudioClassification audio eng VoxLingua107_Top10 AudioClassification audio eng ESC50 AudioClassification audio zxx CommonLanguageAgeDetection AudioClassification audio eng IEMOCAPEmotion AudioClassification audio eng CommonLanguageLanguageDetection AudioClassification audio eng CommonLanguageGenderDetection AudioClassification audio eng IEMOCAPGender AudioClassification audio eng SpokeNEnglish AudioClassification audio eng FSDD AudioClassification audio eng LibriCount AudioClassification audio eng GTZANGenre AudioClassification audio zxx BirdCLEF AudioClassification audio zxx VoxPopuliLanguageID AudioClassification audio deu, eng, fra, pol, spa MridinghamStroke AudioClassification audio zxx GunshotTriangulation AudioClassification audio zxx SpeechCommands AudioClassification audio eng MridinghamTonic AudioClassification audio zxx BirdSet AudioMultilabelClassification audio zxx ESC50AudioReranking AudioReranking audio zxx UrbanSound8KAudioReranking AudioReranking audio zxx GTZANAudioReranking AudioReranking audio zxx FSDnoisy18kAudioReranking AudioReranking audio eng VocalSoundAudioReranking AudioReranking audio eng VoiceGenderClustering AudioClustering audio eng VoxPopuliAccentClustering AudioClustering audio eng AmbientAcousticContextClustering AudioClustering audio eng VoxCelebClustering AudioClustering audio eng VoxPopuliGenderClustering AudioClustering audio deu, eng, fra, pol, spa VehicleSoundClustering AudioClustering audio zxx MusicGenreClustering AudioClustering audio zxx ESC50Clustering AudioClustering audio zxx CREMA_DClustering AudioClustering audio eng GTZANGenreClustering AudioClustering audio zxx VoxPopuliAccentPairClassification AudioPairClassification audio eng ESC50PairClassification AudioPairClassification audio zxx NMSQAPairClassification AudioPairClassification audio eng VocalSoundPairClassification AudioPairClassification audio eng CREMADPairClassification AudioPairClassification audio eng JamAltArtistA2ARetrieval Any2AnyRetrieval audio deu, eng, fra, spa AudioCapsA2TRetrieval Any2AnyRetrieval text, audio eng, zxx AudioSetStrongA2TRetrieval Any2AnyRetrieval text, audio eng CMUArcticA2TRetrieval Any2AnyRetrieval text, audio eng EmoVDBA2TRetrieval Any2AnyRetrieval text, audio eng GigaSpeechA2TRetrieval Any2AnyRetrieval text, audio eng HiFiTTSA2TRetrieval Any2AnyRetrieval text, audio eng JLCorpusA2TRetrieval Any2AnyRetrieval text, audio eng JamAltLyricA2TRetrieval Any2AnyRetrieval text, audio deu, eng, fra, spa LibriTTSA2TRetrieval Any2AnyRetrieval text, audio eng MACSA2TRetrieval Any2AnyRetrieval text, audio eng MusicCapsA2TRetrieval Any2AnyRetrieval audio, text zxx SpokenSQuADT2ARetrieval Any2AnyRetrieval text, audio eng UrbanSound8KA2TRetrieval Any2AnyRetrieval text, audio zxx AudioCapsT2ARetrieval Any2AnyRetrieval text, audio eng, zxx AudioSetStrongT2ARetrieval Any2AnyRetrieval text, audio eng CMUArcticT2ARetrieval Any2AnyRetrieval text, audio eng EmoVDBT2ARetrieval Any2AnyRetrieval text, audio eng GigaSpeechT2ARetrieval Any2AnyRetrieval text, audio eng HiFiTTST2ARetrieval Any2AnyRetrieval text, audio eng JLCorpusT2ARetrieval Any2AnyRetrieval text, audio eng JamAltLyricT2ARetrieval Any2AnyRetrieval text, audio deu, eng, fra, spa LibriTTST2ARetrieval Any2AnyRetrieval text, audio eng MACST2ARetrieval Any2AnyRetrieval text, audio eng MusicCapsT2ARetrieval Any2AnyRetrieval audio, text zxx UrbanSound8KT2ARetrieval Any2AnyRetrieval text, audio zxx ESC50_Zeroshot AudioZeroshotClassification audio, text zxx RavdessZeroshot AudioZeroshotClassification audio, text eng SpeechCommandsZeroshotv0.01 AudioZeroshotClassification audio, text eng SpeechCommandsZeroshotv0.02 AudioZeroshotClassification audio, text eng UrbanSound8kZeroshot AudioZeroshotClassification audio, text zxx ClothoA2TRetrieval Any2AnyRetrieval audio, text eng ClothoT2ARetrieval Any2AnyRetrieval text, audio eng FleursA2TRetrieval Any2AnyRetrieval audio, text afr, amh, ara, asm, ast, ... (102) FleursT2ARetrieval Any2AnyRetrieval text, audio afr, amh, ara, asm, ast, ... (102) CommonVoiceMini21A2TRetrieval Any2AnyRetrieval text, audio abk, afr, amh, ara, asm, ... (114) CommonVoiceMini21T2ARetrieval Any2AnyRetrieval text, audio abk, afr, amh, ara, asm, ... (114)"},{"location":"overview/available_benchmarks/#maebbeta_1","title":"MAEB+(beta)","text":"<p>MAEB+ is the full Massive Audio Embedding Benchmark (v1), containing 98 tasks with audio modality across 6 task types: classification, clustering, pair classification, reranking, zero-shot classification, and retrieval. The benchmark is currently in beta as the paper has been submitted for review and will be released in its final version after the review process.</p> Tasks name type modalities languages AmbientAcousticContext AudioClassification audio zxx AudioSet AudioMultilabelClassification audio eng AudioSetMini AudioMultilabelClassification audio eng BeijingOpera AudioClassification audio zxx BirdCLEF AudioClassification audio zxx BirdSet AudioMultilabelClassification audio zxx CommonLanguageAgeDetection AudioClassification audio eng CommonLanguageGenderDetection AudioClassification audio eng CommonLanguageLanguageDetection AudioClassification audio eng CREMA_D AudioClassification audio eng ESC50 AudioClassification audio zxx FSD2019Kaggle AudioMultilabelClassification audio eng FSD50K AudioMultilabelClassification audio eng FSDD AudioClassification audio eng GTZANGenre AudioClassification audio zxx GunshotTriangulation AudioClassification audio zxx IEMOCAPEmotion AudioClassification audio eng IEMOCAPGender AudioClassification audio eng LibriCount AudioClassification audio eng MInDS14 AudioClassification audio ces, deu, eng, fra, ita, ... (12) MridinghamStroke AudioClassification audio zxx MridinghamTonic AudioClassification audio zxx NSynth AudioClassification audio zxx SIBFLEURS AudioMultilabelClassification audio afr, amh, arb, asm, ast, ... (101) SpeechCommands AudioClassification audio eng SpokeNEnglish AudioClassification audio eng SpokenQAForIC AudioClassification audio eng TUTAcousticScenes AudioClassification audio zxx UrbanSound8k AudioClassification audio zxx VocalSound AudioClassification audio eng VoxCelebSA AudioClassification audio eng VoxLingua107_Top10 AudioClassification audio eng VoxPopuliAccentID AudioClassification audio eng VoxPopuliGenderID AudioClassification audio deu, eng, fra, pol, spa VoxPopuliLanguageID AudioClassification audio deu, eng, fra, pol, spa AmbientAcousticContextClustering AudioClustering audio eng CREMA_DClustering AudioClustering audio eng ESC50Clustering AudioClustering audio zxx GTZANGenreClustering AudioClustering audio zxx MusicGenreClustering AudioClustering audio zxx VehicleSoundClustering AudioClustering audio zxx VoiceGenderClustering AudioClustering audio eng VoxCelebClustering AudioClustering audio eng VoxPopuliAccentClustering AudioClustering audio eng VoxPopuliGenderClustering AudioClustering audio deu, eng, fra, pol, spa CREMADPairClassification AudioPairClassification audio eng ESC50PairClassification AudioPairClassification audio zxx NMSQAPairClassification AudioPairClassification audio eng VocalSoundPairClassification AudioPairClassification audio eng VoxPopuliAccentPairClassification AudioPairClassification audio eng ESC50AudioReranking AudioReranking audio zxx FSDnoisy18kAudioReranking AudioReranking audio eng GTZANAudioReranking AudioReranking audio zxx UrbanSound8KAudioReranking AudioReranking audio zxx VocalSoundAudioReranking AudioReranking audio eng ESC50_Zeroshot AudioZeroshotClassification audio, text zxx RavdessZeroshot AudioZeroshotClassification audio, text eng SpeechCommandsZeroshotv0.01 AudioZeroshotClassification audio, text eng SpeechCommandsZeroshotv0.02 AudioZeroshotClassification audio, text eng UrbanSound8kZeroshot AudioZeroshotClassification audio, text zxx JamAltArtistA2ARetrieval Any2AnyRetrieval audio deu, eng, fra, spa AudioCapsA2TRetrieval Any2AnyRetrieval text, audio eng, zxx AudioSetStrongA2TRetrieval Any2AnyRetrieval text, audio eng ClothoA2TRetrieval Any2AnyRetrieval audio, text eng CMUArcticA2TRetrieval Any2AnyRetrieval text, audio eng CommonVoiceMini17A2TRetrieval Any2AnyRetrieval text, audio ara, ast, bel, ben, bre, ... (50) CommonVoiceMini21A2TRetrieval Any2AnyRetrieval text, audio abk, afr, amh, ara, asm, ... (114) EmoVDBA2TRetrieval Any2AnyRetrieval text, audio eng FleursA2TRetrieval Any2AnyRetrieval audio, text afr, amh, ara, asm, ast, ... (102) GigaSpeechA2TRetrieval Any2AnyRetrieval text, audio eng GoogleSVQA2TRetrieval Any2AnyRetrieval audio, text acm, apc, arq, arz, ben, ... (20) HiFiTTSA2TRetrieval Any2AnyRetrieval text, audio eng JamAltLyricA2TRetrieval Any2AnyRetrieval text, audio deu, eng, fra, spa JLCorpusA2TRetrieval Any2AnyRetrieval text, audio eng LibriTTSA2TRetrieval Any2AnyRetrieval text, audio eng MACSA2TRetrieval Any2AnyRetrieval text, audio eng MusicCapsA2TRetrieval Any2AnyRetrieval audio, text zxx SoundDescsA2TRetrieval Any2AnyRetrieval text, audio zxx UrbanSound8KA2TRetrieval Any2AnyRetrieval text, audio zxx AudioCapsT2ARetrieval Any2AnyRetrieval text, audio eng, zxx AudioSetStrongT2ARetrieval Any2AnyRetrieval text, audio eng ClothoT2ARetrieval Any2AnyRetrieval text, audio eng CMUArcticT2ARetrieval Any2AnyRetrieval text, audio eng CommonVoiceMini17T2ARetrieval Any2AnyRetrieval text, audio ara, ast, bel, ben, bre, ... (50) CommonVoiceMini21T2ARetrieval Any2AnyRetrieval text, audio abk, afr, amh, ara, asm, ... (114) EmoVDBT2ARetrieval Any2AnyRetrieval text, audio eng FleursT2ARetrieval Any2AnyRetrieval text, audio afr, amh, ara, asm, ast, ... (102) GigaSpeechT2ARetrieval Any2AnyRetrieval text, audio eng GoogleSVQT2ARetrieval Any2AnyRetrieval text, audio acm, apc, arq, arz, ben, ... (20) HiFiTTST2ARetrieval Any2AnyRetrieval text, audio eng JamAltLyricT2ARetrieval Any2AnyRetrieval text, audio deu, eng, fra, spa JLCorpusT2ARetrieval Any2AnyRetrieval text, audio eng LibriTTST2ARetrieval Any2AnyRetrieval text, audio eng MACST2ARetrieval Any2AnyRetrieval text, audio eng MusicCapsT2ARetrieval Any2AnyRetrieval audio, text zxx SoundDescsT2ARetrieval Any2AnyRetrieval text, audio zxx SpokenSQuADT2ARetrieval Any2AnyRetrieval text, audio eng UrbanSound8KT2ARetrieval Any2AnyRetrieval text, audio zxx"},{"location":"overview/available_benchmarks/#miebimg","title":"MIEB(Img)","text":"<p>A image-only version of MIEB(Multilingual) that consists of 49 tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages CUB200I2IRetrieval Any2AnyRetrieval image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng METI2IRetrieval Any2AnyRetrieval image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng VOC2007 ImageClassification image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng STS17MultilingualVisualSTS VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) STSBenchmarkMultilingualVisualSTS VisualSTS(multi) image cmn, deu, eng, fra, ita, ... (10) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#miebmultilingual","title":"MIEB(Multilingual)","text":"<p>MIEB(Multilingual) is a comprehensive image embeddings benchmark, spanning 10 task types, covering 130 tasks and a total of 39 languages.     In addition to image classification (zero shot and linear probing), clustering, retrieval, MIEB includes tasks in compositionality evaluation,     document understanding, visual STS, and CV-centric tasks. This benchmark consists of MIEB(eng) + 3 multilingual retrieval     datasets + the multilingual parts of VisualSTS-b and VisualSTS-16.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng VOC2007 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng BirdsnapZeroShot ZeroShotClassification image, text eng Caltech101ZeroShot ZeroShotClassification text, image eng CIFAR10ZeroShot ZeroShotClassification text, image eng CIFAR100ZeroShot ZeroShotClassification text, image eng CLEVRZeroShot ZeroShotClassification text, image eng CLEVRCountZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng DTDZeroShot ZeroShotClassification image, text eng EuroSATZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng GTSRBZeroShot ZeroShotClassification image eng Imagenet1kZeroShot ZeroShotClassification image, text eng MNISTZeroShot ZeroShotClassification image, text eng OxfordPetsZeroShot ZeroShotClassification text, image eng PatchCamelyonZeroShot ZeroShotClassification image, text eng RenderedSST2 ZeroShotClassification text, image eng RESISC45ZeroShot ZeroShotClassification image, text eng StanfordCarsZeroShot ZeroShotClassification image, text eng STL10ZeroShot ZeroShotClassification image, text eng SUN397ZeroShot ZeroShotClassification image, text eng UCF101ZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng BLINKIT2TMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng SugarCrepe Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng BLINKIT2IRetrieval Any2AnyRetrieval text, image eng BLINKIT2TRetrieval Any2AnyRetrieval text, image eng CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng EDIST2ITRetrieval Any2AnyRetrieval text, image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng Fashion200kT2IRetrieval Any2AnyRetrieval text, image eng FashionIQIT2IRetrieval Any2AnyRetrieval text, image eng Flickr30kI2TRetrieval Any2AnyRetrieval text, image eng Flickr30kT2IRetrieval Any2AnyRetrieval text, image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng GLDv2I2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesT2IRetrieval Any2AnyRetrieval text, image eng ImageCoDeT2IRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2ITRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng MemotionI2TRetrieval Any2AnyRetrieval text, image eng MemotionT2IRetrieval Any2AnyRetrieval text, image eng METI2IRetrieval Any2AnyRetrieval image eng MSCOCOI2TRetrieval Any2AnyRetrieval text, image eng MSCOCOT2IRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2ITRetrieval Any2AnyRetrieval image, text eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SciMMIRI2TRetrieval Any2AnyRetrieval text, image eng SciMMIRT2IRetrieval Any2AnyRetrieval text, image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng TUBerlinT2IRetrieval Any2AnyRetrieval text, image eng VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VisualNewsT2IRetrieval Any2AnyRetrieval image, text eng VizWizIT2TRetrieval Any2AnyRetrieval text, image eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WebQAT2TRetrieval Any2AnyRetrieval text eng WITT2IRetrieval Any2AnyMultilingualRetrieval text, image ara, bul, dan, ell, eng, ... (11) XFlickr30kCoT2IRetrieval Any2AnyMultilingualRetrieval text, image deu, eng, ind, jpn, rus, ... (8) XM3600T2IRetrieval Any2AnyMultilingualRetrieval text, image ara, ben, ces, dan, deu, ... (38) VisualSTS17Eng VisualSTS(eng) image eng VisualSTS-b-Eng VisualSTS(eng) image eng VisualSTS17Multilingual VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) VisualSTS-b-Multilingual VisualSTS(multi) image cmn, deu, fra, ita, nld, ... (9) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#miebeng","title":"MIEB(eng)","text":"<p>MIEB(eng) is a comprehensive image embeddings benchmark, spanning 8 task types, covering 125 tasks.     In addition to image classification (zero shot and linear probing), clustering, retrieval, MIEB includes tasks in compositionality evaluation,     document understanding, visual STS, and CV-centric tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Birdsnap ImageClassification image eng Caltech101 ImageClassification image eng CIFAR10 ImageClassification image eng CIFAR100 ImageClassification image eng Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng FER2013 ImageClassification image eng FGVCAircraft ImageClassification image eng Food101Classification ImageClassification image eng GTSRB ImageClassification image eng Imagenet1k ImageClassification image eng MNIST ImageClassification image eng OxfordFlowersClassification ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng StanfordCars ImageClassification image eng STL10 ImageClassification image eng SUN397 ImageClassification image eng UCF101 ImageClassification image eng VOC2007 ImageClassification image eng CIFAR10Clustering ImageClustering image eng CIFAR100Clustering ImageClustering image eng ImageNetDog15Clustering ImageClustering image eng ImageNet10Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng BirdsnapZeroShot ZeroShotClassification image, text eng Caltech101ZeroShot ZeroShotClassification text, image eng CIFAR10ZeroShot ZeroShotClassification text, image eng CIFAR100ZeroShot ZeroShotClassification text, image eng CLEVRZeroShot ZeroShotClassification text, image eng CLEVRCountZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng DTDZeroShot ZeroShotClassification image, text eng EuroSATZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng GTSRBZeroShot ZeroShotClassification image eng Imagenet1kZeroShot ZeroShotClassification image, text eng MNISTZeroShot ZeroShotClassification image, text eng OxfordPetsZeroShot ZeroShotClassification text, image eng PatchCamelyonZeroShot ZeroShotClassification image, text eng RenderedSST2 ZeroShotClassification text, image eng RESISC45ZeroShot ZeroShotClassification image, text eng StanfordCarsZeroShot ZeroShotClassification image, text eng STL10ZeroShot ZeroShotClassification image, text eng SUN397ZeroShot ZeroShotClassification image, text eng UCF101ZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng BLINKIT2TMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng SugarCrepe Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS12VisualSTS VisualSTS(eng) image eng STS13VisualSTS VisualSTS(eng) image eng STS14VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng STS16VisualSTS VisualSTS(eng) image eng BLINKIT2IRetrieval Any2AnyRetrieval text, image eng BLINKIT2TRetrieval Any2AnyRetrieval text, image eng CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng EDIST2ITRetrieval Any2AnyRetrieval text, image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng Fashion200kT2IRetrieval Any2AnyRetrieval text, image eng FashionIQIT2IRetrieval Any2AnyRetrieval text, image eng Flickr30kI2TRetrieval Any2AnyRetrieval text, image eng Flickr30kT2IRetrieval Any2AnyRetrieval text, image eng FORBI2IRetrieval Any2AnyRetrieval image eng GLDv2I2IRetrieval Any2AnyRetrieval image eng GLDv2I2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesT2IRetrieval Any2AnyRetrieval text, image eng ImageCoDeT2IRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2ITRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng MemotionI2TRetrieval Any2AnyRetrieval text, image eng MemotionT2IRetrieval Any2AnyRetrieval text, image eng METI2IRetrieval Any2AnyRetrieval image eng MSCOCOI2TRetrieval Any2AnyRetrieval text, image eng MSCOCOT2IRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2ITRetrieval Any2AnyRetrieval image, text eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng ROxfordEasyI2IRetrieval Any2AnyRetrieval image eng ROxfordMediumI2IRetrieval Any2AnyRetrieval image eng ROxfordHardI2IRetrieval Any2AnyRetrieval image eng RP2kI2IRetrieval Any2AnyRetrieval image eng RParisEasyI2IRetrieval Any2AnyRetrieval image eng RParisMediumI2IRetrieval Any2AnyRetrieval image eng RParisHardI2IRetrieval Any2AnyRetrieval image eng SciMMIRI2TRetrieval Any2AnyRetrieval text, image eng SciMMIRT2IRetrieval Any2AnyRetrieval text, image eng SketchyI2IRetrieval Any2AnyRetrieval image eng SOPI2IRetrieval Any2AnyRetrieval image eng StanfordCarsI2IRetrieval Any2AnyRetrieval image eng TUBerlinT2IRetrieval Any2AnyRetrieval text, image eng VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VisualNewsT2IRetrieval Any2AnyRetrieval image, text eng VizWizIT2TRetrieval Any2AnyRetrieval text, image eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WebQAT2TRetrieval Any2AnyRetrieval text eng VisualSTS17Eng VisualSTS(eng) image eng VisualSTS-b-Eng VisualSTS(eng) image eng Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mieblite","title":"MIEB(lite)","text":"<p>MIEB(lite) is a comprehensive image embeddings benchmark, spanning 10 task types, covering 51 tasks.     This is a lite version of MIEB(Multilingual), designed to be run at a fraction of the cost while maintaining     relative rank of models.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Country211 ImageClassification image eng DTD ImageClassification image eng EuroSAT ImageClassification image eng GTSRB ImageClassification image eng OxfordPets ImageClassification image eng PatchCamelyon ImageClassification image eng RESISC45 ImageClassification image eng SUN397 ImageClassification image eng ImageNetDog15Clustering ImageClustering image eng TinyImageNetClustering ImageClustering image eng CIFAR100ZeroShot ZeroShotClassification text, image eng Country211ZeroShot ZeroShotClassification image, text eng FER2013ZeroShot ZeroShotClassification image, text eng FGVCAircraftZeroShot ZeroShotClassification text, image eng Food101ZeroShot ZeroShotClassification text, image eng OxfordPetsZeroShot ZeroShotClassification text, image eng StanfordCarsZeroShot ZeroShotClassification image, text eng BLINKIT2IMultiChoice VisionCentricQA text, image eng CVBenchCount VisionCentricQA image, text eng CVBenchRelation VisionCentricQA text, image eng CVBenchDepth VisionCentricQA text, image eng CVBenchDistance VisionCentricQA text, image eng AROCocoOrder Compositionality text, image eng AROFlickrOrder Compositionality text, image eng AROVisualAttribution Compositionality text, image eng AROVisualRelation Compositionality text, image eng Winoground Compositionality text, image eng ImageCoDe Compositionality text, image eng STS13VisualSTS VisualSTS(eng) image eng STS15VisualSTS VisualSTS(eng) image eng VisualSTS17Multilingual VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9) VisualSTS-b-Multilingual VisualSTS(multi) image cmn, deu, fra, ita, nld, ... (9) CIRRIT2IRetrieval Any2AnyRetrieval text, image eng CUB200I2IRetrieval Any2AnyRetrieval image eng Fashion200kI2TRetrieval Any2AnyRetrieval text, image eng HatefulMemesI2TRetrieval Any2AnyRetrieval text, image eng InfoSeekIT2TRetrieval Any2AnyRetrieval text, image eng NIGHTSI2IRetrieval Any2AnyRetrieval image eng OVENIT2TRetrieval Any2AnyRetrieval text, image eng RP2kI2IRetrieval Any2AnyRetrieval image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VisualNewsI2TRetrieval Any2AnyRetrieval image, text eng VQA2IT2TRetrieval Any2AnyRetrieval text, image eng WebQAT2ITRetrieval Any2AnyRetrieval image, text eng WITT2IRetrieval Any2AnyMultilingualRetrieval text, image ara, bul, dan, ell, eng, ... (11) XM3600T2IRetrieval Any2AnyMultilingualRetrieval text, image ara, ben, ces, dan, deu, ... (38) Citation <pre><code>@article{xiao2025mieb,\n  author = {Chenghao Xiao and Isaac Chung and Imene Kerboua and Jamie Stirling and Xin Zhang and M\u00e1rton Kardos and Roman Solomatin and Noura Al Moubayed and Kenneth Enevoldsen and Niklas Muennighoff},\n  doi = {10.48550/ARXIV.2504.10471},\n  journal = {arXiv preprint arXiv:2504.10471},\n  publisher = {arXiv},\n  title = {MIEB: Massive Image Embedding Benchmark},\n  url = {https://arxiv.org/abs/2504.10471},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#minersbitextmining","title":"MINERSBitextMining","text":"<p>Bitext Mining texts from the MINERS benchmark, a benchmark designed to evaluate the     ability of multilingual LMs in semantic retrieval tasks,     including bitext mining and classification via retrieval-augmented contexts.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BUCC BitextMining text cmn, deu, eng, fra, rus LinceMTBitextMining BitextMining text eng, hin NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) PhincBitextMining BitextMining text eng, hin Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) Citation <pre><code>@article{winata2024miners,\n  author = {Winata, Genta Indra and Zhang, Ruochen and Adelani, David Ifeoluwa},\n  journal = {arXiv preprint arXiv:2406.07424},\n  title = {MINERS: Multilingual Language Models as Semantic Retrievers},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebcode-v1","title":"MTEB(Code, v1)","text":"<p>A massive code embedding benchmark covering retrieval tasks in a miriad of popular programming languages.</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python CodeEditSearchRetrieval Retrieval text c, c++, go, java, javascript, ... (13) CodeFeedbackMT Retrieval text eng CodeFeedbackST Retrieval text eng CodeSearchNetCCRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) CodeTransOceanContest Retrieval text c++, python CodeTransOceanDL Retrieval text python CosQA Retrieval text eng, python COIRCodeSearchNetRetrieval Retrieval text go, java, javascript, php, python, ... (6) StackOverflowQA Retrieval text eng SyntheticText2SQL Retrieval text eng, sql Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeurope-v1","title":"MTEB(Europe, v1)","text":"<p>A regional geopolitical text embedding benchmark targeting embedding performance on European languages.</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicChatClassification Classification text eng ToxicConversationsClassification Classification text eng EstonianValenceClassification Classification text est ItaCaseholdClassification Classification text ita AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita TweetSentimentClassification Classification text ara, deu, eng, fra, hin, ... (8) CBD Classification text pol PolEmo2.0-OUT Classification text pol CSFDSKMovieReviewSentimentClassification Classification text slk DalajClassification Classification text swe WikiCitiesClustering Clustering text eng RomaniBibleClustering Clustering text rom BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan LegalQuAD Retrieval text deu ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng WinoGrande Retrieval text eng AlloprofRetrieval Retrieval text fra BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PSC PairClassification text pol WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SICK-R STS text eng STS12 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FinParaSTS STS text fin STS17 STS text ara, deu, eng, fra, ita, ... (9) SICK-R-PL STS text pol STSES STS text spa Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebindic-v1","title":"MTEB(Indic, v1)","text":"<p>A regional geopolitical text embedding benchmark targeting embedding performance on Indic languages.</p> Tasks name type modalities languages IN22ConvBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) BengaliSentimentAnalysis Classification text ben GujaratiNewsClassification Classification text guj HindiDiscourseClassification Classification text hin SentimentAnalysisHindi Classification text hin MalayalamNewsClassification Classification text mal MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) TweetSentimentClassification Classification text ara, deu, eng, fra, hin, ... (8) NepaliNewsClassification Classification text nep PunjabiNewsClassification Classification text pan SanskritShlokasClassification Classification text san UrduRomanSentimentClassification Classification text urd XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) XQuADRetrieval Retrieval text arb, deu, ell, eng, hin, ... (12) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mteblaw-v1","title":"MTEB(Law, v1)","text":"<p>A benchmark of retrieval tasks in the legal domain.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng GerDaLIRSmall Retrieval text deu LeCaRDv2 Retrieval text zho LegalBenchConsumerContractsQA Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LegalQuAD Retrieval text deu"},{"location":"overview/available_benchmarks/#mtebmedical-v1","title":"MTEB(Medical, v1)","text":"<p>A curated set of MTEB tasks designed to evaluate systems in the context of medical information retrieval.</p> Tasks name type modalities languages CUREv1 Retrieval text eng, fra, spa NFCorpus Retrieval text eng TRECCOVID Retrieval text eng TRECCOVID-PL Retrieval text pol SciFact Retrieval text eng SciFact-PL Retrieval text pol MedicalQARetrieval Retrieval text eng PublicHealthQA Retrieval text ara, eng, fra, kor, rus, ... (8) MedrxivClusteringP2P.v2 Clustering text eng MedrxivClusteringS2S.v2 Clustering text eng CmedqaRetrieval Retrieval text cmn CMedQAv2-reranking Reranking text cmn"},{"location":"overview/available_benchmarks/#mtebmultilingual-v1","title":"MTEB(Multilingual, v1)","text":"<p>A large-scale multilingual expansion of MTEB, driven mainly by highly-curated community contributions covering 250+ languages. This benhcmark has been replaced by MTEB(Multilingual, v2) as one of the datasets (SNLHierarchicalClustering) included in v1 was removed from the Hugging Face Hub.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IndicGenBenchFloresBitextMining BitextMining text asm, awa, ben, bgc, bho, ... (30) NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicConversationsClassification Classification text eng TweetTopicSingleClassification Classification text eng EstonianValenceClassification Classification text est FilipinoShopeeReviewsClassification Classification text fil GujaratiNewsClassification Classification text guj SentimentAnalysisHindi Classification text hin IndonesianIdClickbaitClassification Classification text ind ItaCaseholdClassification Classification text ita KorSarcasmClassification Classification text kor KurdishSentimentClassification Classification text kur MacedonianTweetSentimentClassification Classification text mkd AfriSentiClassification Classification text amh, arq, ary, hau, ibo, ... (12) AmazonCounterfactualClassification Classification text deu, eng, jpn CataloniaTweetClassification Classification text cat, spa CyrillicTurkicLangClassification Classification text bak, chv, kaz, kir, krc, ... (9) IndicLangClassification Classification text asm, ben, brx, doi, gom, ... (22) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NusaParagraphEmotionClassification Classification text bbc, bew, bug, jav, mad, ... (10) NusaX-senti Classification text ace, ban, bbc, bjn, bug, ... (12) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita NepaliNewsClassification Classification text nep OdiaNewsClassification Classification text ory PunjabiNewsClassification Classification text pan PolEmo2.0-OUT Classification text pol PAC Classification text pol SinhalaNewsClassification Classification text sin CSFDSKMovieReviewSentimentClassification Classification text slk SiswatiNewsClassification Classification text ssw SlovakMovieReviewSentimentClassification Classification text slk SwahiliNewsClassification Classification text swa DalajClassification Classification text swe TswanaNewsClassification Classification text tsn IsiZuluNewsClassification Classification text zul WikiCitiesClustering Clustering text eng MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) RomaniBibleClustering Clustering text rom ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng MedrxivClusteringP2P.v2 Clustering text eng StackExchangeClustering.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) PlscClusteringP2P.v2 Clustering text pol SwednClusteringP2P Clustering text swe CLSClusteringP2P.v2 Clustering text cmn StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan AILAStatutes Retrieval text eng ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TRECCOVID Retrieval text eng WinoGrande Retrieval text eng BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) MLQARetrieval Retrieval text ara, deu, eng, hin, spa, ... (7) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) CovidRetrieval Retrieval text cmn Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng KorHateSpeechMLClassification MultilabelClassification text kor MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) BrazilianToxicTweetsClassification MultilabelClassification text por CEDRClassification MultilabelClassification text rus CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng TwitterURLCorpus PairClassification text eng ArmenianParaphrasePC PairClassification text hye indonli PairClassification text ind OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PpcPC PairClassification text pol TERRa PairClassification text rus WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra VoyageMMarcoReranking Reranking text jpn WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) RuBQReranking Reranking text rus T2Reranking Reranking text cmn GermanSTSBenchmark STS text deu SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FaroeseSTS STS text fao FinParaSTS STS text fin JSICK STS text jpn IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) SemRel24STS STS text afr, amh, arb, arq, ary, ... (12) STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) STSES STS text spa STSB STS text cmn MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) SNLHierarchicalClusteringP2P Clustering text nob Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebmultilingual-v2","title":"MTEB(Multilingual, v2)","text":"<p>A large-scale multilingual expansion of MTEB, driven mainly by highly-curated community contributions covering 250+ languages. </p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan BibleNLPBitextMining BitextMining text aai, aak, aau, aaz, abt, ... (829) BUCC.v2 BitextMining text cmn, deu, eng, fra, rus DiaBlaBitextMining BitextMining text eng, fra FloresBitextMining BitextMining text ace, acm, acq, aeb, afr, ... (196) IN22GenBitextMining BitextMining text asm, ben, brx, doi, eng, ... (23) IndicGenBenchFloresBitextMining BitextMining text asm, awa, ben, bgc, bho, ... (30) NollySentiBitextMining BitextMining text eng, hau, ibo, pcm, yor NorwegianCourtsBitextMining BitextMining text nno, nob NTREXBitextMining BitextMining text afr, amh, arb, aze, bak, ... (119) NusaTranslationBitextMining BitextMining text abs, bbc, bew, bhp, ind, ... (12) NusaXBitextMining BitextMining text ace, ban, bbc, bjn, bug, ... (12) Tatoeba BitextMining text afr, amh, ang, ara, arq, ... (113) BulgarianStoreReviewSentimentClassfication Classification text bul CzechProductReviewSentimentClassification Classification text ces GreekLegalCodeClassification Classification text ell DBpediaClassification Classification text eng FinancialPhrasebankClassification Classification text eng PoemSentimentClassification Classification text eng ToxicConversationsClassification Classification text eng TweetTopicSingleClassification Classification text eng EstonianValenceClassification Classification text est FilipinoShopeeReviewsClassification Classification text fil GujaratiNewsClassification Classification text guj SentimentAnalysisHindi Classification text hin IndonesianIdClickbaitClassification Classification text ind ItaCaseholdClassification Classification text ita KorSarcasmClassification Classification text kor KurdishSentimentClassification Classification text kur MacedonianTweetSentimentClassification Classification text mkd AfriSentiClassification Classification text amh, arq, ary, hau, ibo, ... (12) AmazonCounterfactualClassification Classification text deu, eng, jpn CataloniaTweetClassification Classification text cat, spa CyrillicTurkicLangClassification Classification text bak, chv, kaz, kir, krc, ... (9) IndicLangClassification Classification text asm, ben, brx, doi, gom, ... (22) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NusaParagraphEmotionClassification Classification text bbc, bew, bug, jav, mad, ... (10) NusaX-senti Classification text ace, ban, bbc, bjn, bug, ... (12) ScalaClassification Classification text dan, nno, nob, swe SwissJudgementClassification Classification text deu, fra, ita NepaliNewsClassification Classification text nep OdiaNewsClassification Classification text ory PunjabiNewsClassification Classification text pan PolEmo2.0-OUT Classification text pol PAC Classification text pol SinhalaNewsClassification Classification text sin CSFDSKMovieReviewSentimentClassification Classification text slk SiswatiNewsClassification Classification text ssw SlovakMovieReviewSentimentClassification Classification text slk SwahiliNewsClassification Classification text swa DalajClassification Classification text swe TswanaNewsClassification Classification text tsn IsiZuluNewsClassification Classification text zul WikiCitiesClustering Clustering text eng MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) RomaniBibleClustering Clustering text rom ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng BigPatentClustering.v2 Clustering text eng BiorxivClusteringP2P.v2 Clustering text eng MedrxivClusteringP2P.v2 Clustering text eng StackExchangeClustering.v2 Clustering text eng AlloProfClusteringS2S.v2 Clustering text fra HALClusteringS2S.v2 Clustering text fra SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) WikiClusteringP2P.v2 Clustering text bos, cat, ces, dan, eus, ... (14) PlscClusteringP2P.v2 Clustering text pol SwednClusteringP2P Clustering text swe CLSClusteringP2P.v2 Clustering text cmn StackOverflowQA Retrieval text eng TwitterHjerneRetrieval Retrieval text dan AILAStatutes Retrieval text eng ArguAna Retrieval text eng HagridRetrieval Retrieval text eng LegalBenchCorporateLobbying Retrieval text eng LEMBPasskeyRetrieval Retrieval text eng SCIDOCS Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TRECCOVID Retrieval text eng WinoGrande Retrieval text eng BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) MLQARetrieval Retrieval text ara, deu, eng, hin, spa, ... (7) StatcanDialogueDatasetRetrieval Retrieval text eng, fra WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) CovidRetrieval Retrieval text cmn Core17InstructionRetrieval InstructionReranking text eng News21InstructionRetrieval InstructionReranking text eng Robust04InstructionRetrieval InstructionReranking text eng KorHateSpeechMLClassification MultilabelClassification text kor MalteseNewsClassification MultilabelClassification text mlt MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) BrazilianToxicTweetsClassification MultilabelClassification text por CEDRClassification MultilabelClassification text rus CTKFactsNLI PairClassification text ces SprintDuplicateQuestions PairClassification text eng TwitterURLCorpus PairClassification text eng ArmenianParaphrasePC PairClassification text hye indonli PairClassification text ind OpusparcusPC PairClassification text deu, eng, fin, fra, rus, ... (6) PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) RTE3 PairClassification text deu, eng, fra, ita XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) PpcPC PairClassification text pol TERRa PairClassification text rus WebLINXCandidatesReranking Reranking text eng AlloprofReranking Reranking text fra VoyageMMarcoReranking Reranking text jpn WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) RuBQReranking Reranking text rus T2Reranking Reranking text cmn GermanSTSBenchmark STS text deu SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng FaroeseSTS STS text fao FinParaSTS STS text fin JSICK STS text jpn IndicCrosslingualSTS STS text asm, ben, eng, guj, hin, ... (13) SemRel24STS STS text afr, amh, arb, arq, ary, ... (12) STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) STSES STS text spa STSB STS text cmn MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebscandinavian-v1","title":"MTEB(Scandinavian, v1)","text":"<p>A curated selection of tasks coverering the Scandinavian languages; Danish, Swedish and Norwegian, including Bokm\u00e5l and Nynorsk.</p> <p>Learn more \u2192</p> Tasks name type modalities languages BornholmBitextMining BitextMining text dan NorwegianCourtsBitextMining BitextMining text nno, nob AngryTweetsClassification Classification text dan DanishPoliticalCommentsClassification Classification text dan DalajClassification Classification text swe DKHateClassification Classification text dan LccSentimentClassification Classification text dan MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) NordicLangClassification Classification text dan, fao, isl, nno, nob, ... (6) NoRecClassification Classification text nob NorwegianParliamentClassification Classification text nob ScalaClassification Classification text dan, nno, nob, swe SwedishSentimentClassification Classification text swe SweRecClassification Classification text swe DanFeverRetrieval Retrieval text dan NorQuadRetrieval Retrieval text nob SNLRetrieval Retrieval text nob SwednRetrieval Retrieval text swe SweFaqRetrieval Retrieval text swe TV2Nordretrieval Retrieval text dan TwitterHjerneRetrieval Retrieval text dan SNLHierarchicalClusteringS2S Clustering text nob SNLHierarchicalClusteringP2P Clustering text nob SwednClusteringP2P Clustering text swe SwednClusteringS2S Clustering text swe VGHierarchicalClusteringS2S Clustering text nob VGHierarchicalClusteringP2P Clustering text nob Citation <pre><code>@article{enevoldsenScandinavianEmbeddingBenchmarks2024,\n  author = {Enevoldsen, Kenneth and Kardos, M\u00e1rton and Muennighoff, Niklas and Nielbo, Kristoffer},\n  language = {en},\n  month = feb,\n  shorttitle = {The {Scandinavian} {Embedding} {Benchmarks}},\n  title = {The {Scandinavian} {Embedding} {Benchmarks}: {Comprehensive} {Assessment} of {Multilingual} and {Monolingual} {Text} {Embedding}},\n  url = {https://openreview.net/forum?id=pJl_i7HIA72},\n  urldate = {2024-04-12},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebcmn-v1","title":"MTEB(cmn, v1)","text":"<p>The Chinese Massive Text Embedding Benchmark (C-MTEB) is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages T2Retrieval Retrieval text cmn MMarcoRetrieval Retrieval text cmn DuRetrieval Retrieval text cmn CovidRetrieval Retrieval text cmn CmedqaRetrieval Retrieval text cmn EcomRetrieval Retrieval text cmn MedicalRetrieval Retrieval text cmn VideoRetrieval Retrieval text cmn T2Reranking Reranking text cmn MMarcoReranking Reranking text cmn CMedQAv1-reranking Reranking text cmn CMedQAv2-reranking Reranking text cmn Ocnli PairClassification text cmn Cmnli PairClassification text cmn CLSClusteringS2S Clustering text cmn CLSClusteringP2P Clustering text cmn ThuNewsClusteringS2S Clustering text cmn ThuNewsClusteringP2P Clustering text cmn LCQMC STS text cmn PAWSX STS text cmn AFQMC STS text cmn QBQTC STS text cmn TNews Classification text cmn IFlyTek Classification text cmn Waimai Classification text cmn OnlineShopping Classification text cmn JDReview Classification text cmn MultilingualSentiment Classification text cmn ATEC STS text cmn BQ STS text cmn STSB STS text cmn MultilingualSentiment Classification text cmn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebdeu-v1","title":"MTEB(deu, v1)","text":"<p>A benchmark for text-embedding performance in German.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AmazonCounterfactualClassification Classification text deu, eng, jpn AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) BlurbsClusteringP2P Clustering text deu BlurbsClusteringS2S Clustering text deu TenKGnadClusteringP2P Clustering text deu TenKGnadClusteringS2S Clustering text deu FalseFriendsGermanEnglish PairClassification text deu PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) GermanQuAD-Retrieval Retrieval text deu GermanDPR Retrieval text deu XMarket Retrieval text deu, eng, spa GerDaLIR Retrieval text deu GermanSTSBenchmark STS text deu STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@misc{wehrli2024germantextembeddingclustering,\n  archiveprefix = {arXiv},\n  author = {Silvan Wehrli and Bert Arnrich and Christopher Irrgang},\n  eprint = {2401.02709},\n  primaryclass = {cs.CL},\n  title = {German Text Embedding Clustering Benchmark},\n  url = {https://arxiv.org/abs/2401.02709},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeng-v1","title":"MTEB(eng, v1)","text":"<p>The original English benchmark by Muennighoff et al., (2023). This page is an adaptation of the old MTEB leaderboard. We recommend that you use MTEB(eng, v2) instead, as it uses updated versions of the task, making it notably faster to run and resolving a known bug in existing tasks. This benchmark also removes datasets common for fine-tuning, such as MSMARCO, which makes model performance scores more comparable. However, generally, both benchmarks provide similar estimates.</p> Tasks name type modalities languages AmazonPolarityClassification Classification text eng AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) ArguAna Retrieval text eng ArxivClusteringP2P Clustering text eng ArxivClusteringS2S Clustering text eng AskUbuntuDupQuestions Reranking text eng BIOSSES STS text eng Banking77Classification Classification text eng BiorxivClusteringP2P Clustering text eng BiorxivClusteringS2S Clustering text eng CQADupstackRetrieval Retrieval text eng ClimateFEVER Retrieval text eng DBPedia Retrieval text eng EmotionClassification Classification text eng FEVER Retrieval text eng FiQA2018 Retrieval text eng HotpotQA Retrieval text eng ImdbClassification Classification text eng MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MedrxivClusteringP2P Clustering text eng MedrxivClusteringS2S Clustering text eng MindSmallReranking Reranking text eng NFCorpus Retrieval text eng NQ Retrieval text eng QuoraRetrieval Retrieval text eng RedditClustering Clustering text eng RedditClusteringP2P Clustering text eng SCIDOCS Retrieval text eng SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STS16 STS text eng STSBenchmark STS text eng SciDocsRR Reranking text eng SciFact Retrieval text eng SprintDuplicateQuestions PairClassification text eng StackExchangeClustering Clustering text eng StackExchangeClusteringP2P Clustering text eng StackOverflowDupQuestions Reranking text eng SummEval Summarization text eng TRECCOVID Retrieval text eng Touche2020 Retrieval text eng ToxicConversationsClassification Classification text eng TweetSentimentExtractionClassification Classification text eng TwentyNewsgroupsClustering Clustering text eng TwitterSemEval2015 PairClassification text eng TwitterURLCorpus PairClassification text eng MSMARCO Retrieval text eng AmazonCounterfactualClassification Classification text deu, eng, jpn STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo\u00efc and Reimers, Nils},\n  doi = {10.48550/ARXIV.2210.07316},\n  journal = {arXiv preprint arXiv:2210.07316},\n  publisher = {arXiv},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2210.07316},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebeng-v2","title":"MTEB(eng, v2)","text":"<p>The new English Massive Text Embedding Benchmark. This benchmark was created to account for the fact that many models have now been finetuned to tasks in the original MTEB, and contains tasks that are not as frequently used for model training. This way the new benchmark and leaderboard can give our users a more realistic expectation of models' generalization performance.</p> <p>The original MTEB leaderboard is available under the MTEB(eng, v1) tab.</p> Tasks name type modalities languages ArguAna Retrieval text eng ArXivHierarchicalClusteringP2P Clustering text eng ArXivHierarchicalClusteringS2S Clustering text eng AskUbuntuDupQuestions Reranking text eng BIOSSES STS text eng Banking77Classification Classification text eng BiorxivClusteringP2P.v2 Clustering text eng CQADupstackGamingRetrieval Retrieval text eng CQADupstackUnixRetrieval Retrieval text eng ClimateFEVERHardNegatives Retrieval text eng FEVERHardNegatives Retrieval text eng FiQA2018 Retrieval text eng HotpotQAHardNegatives Retrieval text eng ImdbClassification Classification text eng MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MedrxivClusteringP2P.v2 Clustering text eng MedrxivClusteringS2S.v2 Clustering text eng MindSmallReranking Reranking text eng SCIDOCS Retrieval text eng SICK-R STS text eng STS12 STS text eng STS13 STS text eng STS14 STS text eng STS15 STS text eng STSBenchmark STS text eng SprintDuplicateQuestions PairClassification text eng StackExchangeClustering.v2 Clustering text eng StackExchangeClusteringP2P.v2 Clustering text eng TRECCOVID Retrieval text eng Touche2020Retrieval.v3 Retrieval text eng ToxicConversationsClassification Classification text eng TweetSentimentExtractionClassification Classification text eng TwentyNewsgroupsClustering.v2 Clustering text eng TwitterSemEval2015 PairClassification text eng TwitterURLCorpus PairClassification text eng SummEvalSummarization.v2 Summarization text eng AmazonCounterfactualClassification Classification text deu, eng, jpn STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22.v2 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{enevoldsen2025mmtebmassivemultilingualtext,\n  author = {Kenneth Enevoldsen and Isaac Chung and Imene Kerboua and M\u00e1rton Kardos and Ashwin Mathur and David Stap and Jay Gala and Wissam Siblini and Dominik Krzemi\u0144ski and Genta Indra Winata and Saba Sturua and Saiteja Utpala and Mathieu Ciancone and Marion Schaeffer and Gabriel Sequeira and Diganta Misra and Shreeya Dhakal and Jonathan Rystr\u00f8m and Roman Solomatin and \u00d6mer \u00c7a\u011fatan and Akash Kundu and Martin Bernstorff and Shitao Xiao and Akshita Sukhlecha and Bhavish Pahwa and Rafa\u0142 Po\u015bwiata and Kranthi Kiran GV and Shawon Ashraf and Daniel Auras and Bj\u00f6rn Pl\u00fcster and Jan Philipp Harries and Lo\u00efc Magne and Isabelle Mohr and Mariya Hendriksen and Dawei Zhu and Hippolyte Gisserot-Boukhlef and Tom Aarsen and Jan Kostkan and Konrad Wojtasik and Taemin Lee and Marek \u0160uppa and Crystina Zhang and Roberta Rocca and Mohammed Hamdy and Andrianos Michail and John Yang and Manuel Faysse and Aleksei Vatolin and Nandan Thakur and Manan Dey and Dipam Vasani and Pranjal Chitale and Simone Tedeschi and Nguyen Tai and Artem Snegirev and Michael G\u00fcnther and Mengzhou Xia and Weijia Shi and Xing Han L\u00f9 and Jordan Clive and Gayatri Krishnakumar and Anna Maksimova and Silvan Wehrli and Maria Tikhonova and Henil Panchal and Aleksandr Abramov and Malte Ostendorff and Zheng Liu and Simon Clematide and Lester James Miranda and Alena Fenogenova and Guangyu Song and Ruqiya Bin Safi and Wen-Ding Li and Alessia Borghini and Federico Cassano and Hongjin Su and Jimmy Lin and Howard Yen and Lasse Hansen and Sara Hooker and Chenghao Xiao and Vaibhav Adlakha and Orion Weller and Siva Reddy and Niklas Muennighoff},\n  doi = {10.48550/arXiv.2502.13595},\n  journal = {arXiv preprint arXiv:2502.13595},\n  publisher = {arXiv},\n  title = {MMTEB: Massive Multilingual Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2502.13595},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfas-v1","title":"MTEB(fas, v1)","text":"<p>The Persian Massive Text Embedding Benchmark (FaMTEB) is a comprehensive benchmark for Persian text embeddings covering 7 tasks and 60+ datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PersianFoodSentimentClassification Classification text fas SynPerChatbotConvSAClassification Classification text fas SynPerChatbotConvSAToneChatbotClassification Classification text fas SynPerChatbotConvSAToneUserClassification Classification text fas SynPerChatbotSatisfactionLevelClassification Classification text fas SynPerChatbotRAGToneChatbotClassification Classification text fas SynPerChatbotRAGToneUserClassification Classification text fas SynPerChatbotToneChatbotClassification Classification text fas SynPerChatbotToneUserClassification Classification text fas SynPerTextToneClassification Classification text fas SIDClassification Classification text fas DeepSentiPers Classification text fas PersianTextEmotion Classification text fas SentimentDKSF Classification text fas NLPTwitterAnalysisClassification Classification text fas DigikalamagClassification Classification text fas MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) BeytooteClustering Clustering text fas DigikalamagClustering Clustering text fas HamshahriClustring Clustering text fas NLPTwitterAnalysisClustering Clustering text fas SIDClustring Clustering text fas FarsTail PairClassification text fas CExaPPC PairClassification text fas SynPerChatbotRAGFAQPC PairClassification text fas FarsiParaphraseDetection PairClassification text fas SynPerTextKeywordsPC PairClassification text fas SynPerQAPC PairClassification text fas ParsinluEntail PairClassification text fas ParsinluQueryParaphPC PairClassification text fas MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SynPerQARetrieval Retrieval text fas SynPerChatbotTopicsRetrieval Retrieval text fas SynPerChatbotRAGTopicsRetrieval Retrieval text fas SynPerChatbotRAGFAQRetrieval Retrieval text fas PersianWebDocumentRetrieval Retrieval text fas WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) ClimateFEVER-Fa Retrieval text fas DBPedia-Fa Retrieval text fas HotpotQA-Fa Retrieval text fas MSMARCO-Fa Retrieval text fas NQ-Fa Retrieval text fas ArguAna-Fa Retrieval text fas CQADupstackRetrieval-Fa Retrieval text fas FiQA2018-Fa Retrieval text fas NFCorpus-Fa Retrieval text fas QuoraRetrieval-Fa Retrieval text fas SCIDOCS-Fa Retrieval text fas SciFact-Fa Retrieval text fas TRECCOVID-Fa Retrieval text fas Touche2020-Fa Retrieval text fas Farsick STS text fas SynPerSTS STS text fas Query2Query STS text fas SAMSumFa BitextMining text fas SynPerChatbotSumSRetrieval BitextMining text fas SynPerChatbotRAGSumSRetrieval BitextMining text fas Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfas-v2","title":"MTEB(fas, v2)","text":"<p>The Persian Massive Text Embedding Benchmark (FaMTEB) is a comprehensive benchmark for Persian text embeddings covering 7 tasks and 50+ datasets. In version 2, we have optimized large datasets to make them more manageable and accessible, removed low-quality datasets, and added higher-quality data to improve the overall benchmark. For more details on the improvements, see the main PR comment: main PR.</p> <p>Learn more \u2192</p> Tasks name type modalities languages PersianFoodSentimentClassification Classification text fas SynPerChatbotConvSAClassification Classification text fas SynPerChatbotConvSAToneChatbotClassification Classification text fas SynPerChatbotConvSAToneUserClassification Classification text fas SynPerChatbotSatisfactionLevelClassification Classification text fas SynPerTextToneClassification.v3 Classification text fas SIDClassification.v2 Classification text fas DeepSentiPers.v2 Classification text fas PersianTextEmotion.v2 Classification text fas NLPTwitterAnalysisClassification.v2 Classification text fas DigikalamagClassification Classification text fas MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) StyleClassification Classification text fas PerShopDomainClassification Classification text fas PerShopIntentClassification Classification text fas BeytooteClustering Clustering text fas DigikalamagClustering Clustering text fas HamshahriClustring Clustering text fas NLPTwitterAnalysisClustering Clustering text fas SIDClustring Clustering text fas FarsTail PairClassification text fas SynPerChatbotRAGFAQPC PairClassification text fas FarsiParaphraseDetection PairClassification text fas SynPerTextKeywordsPC PairClassification text fas SynPerQAPC PairClassification text fas ParsinluEntail PairClassification text fas ParsinluQueryParaphPC PairClassification text fas MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) SynPerQARetrieval Retrieval text fas SynPerChatbotRAGFAQRetrieval Retrieval text fas PersianWebDocumentRetrieval Retrieval text fas WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) HotpotQA-FaHardNegatives Retrieval text fas MSMARCO-FaHardNegatives Retrieval text fas NQ-FaHardNegatives Retrieval text fas ArguAna-Fa.v2 Retrieval text fas FiQA2018-Fa.v2 Retrieval text fas QuoraRetrieval-Fa.v2 Retrieval text fas SCIDOCS-Fa.v2 Retrieval text fas SciFact-Fa.v2 Retrieval text fas TRECCOVID-Fa.v2 Retrieval text fas FEVER-FaHardNegatives Retrieval text fas NeuCLIR2023RetrievalHardNegatives Retrieval text fas, rus, zho WebFAQRetrieval Retrieval text ara, aze, ben, bul, cat, ... (51) Farsick STS text fas SynPerSTS STS text fas SAMSumFa BitextMining text fas SynPerChatbotSumSRetrieval BitextMining text fas SynPerChatbotRAGSumSRetrieval BitextMining text fas Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebfra-v1","title":"MTEB(fra, v1)","text":"<p>MTEB-French, a French expansion of the original benchmark with high-quality native French datasets.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) MasakhaNEWSClassification Classification text amh, eng, fra, hau, ibo, ... (16) MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) MTOPDomainClassification Classification text deu, eng, fra, hin, spa, ... (6) MTOPIntentClassification Classification text deu, eng, fra, hin, spa, ... (6) AlloProfClusteringP2P Clustering text fra AlloProfClusteringS2S Clustering text fra HALClusteringS2S Clustering text fra MasakhaNEWSClusteringP2P Clustering text amh, eng, fra, hau, ibo, ... (16) MasakhaNEWSClusteringS2S Clustering text amh, eng, fra, hau, ibo, ... (16) MLSUMClusteringP2P Clustering text deu, fra, rus, spa MLSUMClusteringS2S Clustering text deu, fra, rus, spa PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) AlloprofReranking Reranking text fra SyntecReranking Reranking text fra AlloprofRetrieval Retrieval text fra BSARDRetrieval Retrieval text fra MintakaRetrieval Retrieval text ara, deu, fra, hin, ita, ... (8) SyntecRetrieval Retrieval text fra XPQARetrieval Retrieval text ara, cmn, deu, eng, fra, ... (13) SICKFr STS text fra STSBenchmarkMultilingualSTS STS text cmn, deu, eng, fra, ita, ... (10) SummEvalFr Summarization text fra STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@misc{ciancone2024mtebfrenchresourcesfrenchsentence,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {MTEB-French: Resources for French Sentence Embedding Evaluation and Analysis},\n  url = {https://arxiv.org/abs/2405.20468},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebjpn-v1","title":"MTEB(jpn, v1)","text":"<p>JMTEB is a benchmark for evaluating Japanese text embedding models.</p> <p>Learn more \u2192</p> Tasks name type modalities languages LivedoorNewsClustering.v2 Clustering text jpn MewsC16JaClustering Clustering text jpn AmazonReviewsClassification Classification text cmn, deu, eng, fra, jpn, ... (6) AmazonCounterfactualClassification Classification text deu, eng, jpn MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) JSTS STS text jpn JSICK STS text jpn PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) JaqketRetrieval Retrieval text jpn MrTidyRetrieval Retrieval text ara, ben, eng, fin, ind, ... (11) JaGovFaqsRetrieval Retrieval text jpn NLPJournalTitleAbsRetrieval Retrieval text jpn NLPJournalAbsIntroRetrieval Retrieval text jpn NLPJournalTitleIntroRetrieval Retrieval text jpn ESCIReranking Reranking text eng, jpn, spa"},{"location":"overview/available_benchmarks/#mtebkor-v1","title":"MTEB(kor, v1)","text":"<p>A benchmark and leaderboard for evaluation of text embedding in Korean.</p> Tasks name type modalities languages KLUE-TC Classification text kor MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) Ko-StrategyQA Retrieval text kor KLUE-STS STS text kor KorSTS STS text kor"},{"location":"overview/available_benchmarks/#mtebnld-v1","title":"MTEB(nld, v1)","text":"<p>MTEB-NL</p> <p>Learn more \u2192</p> Tasks name type modalities languages DutchBookReviewSentimentClassification.v2 Classification text nld MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) SIB200Classification Classification text ace, acm, acq, aeb, afr, ... (197) MultiHateClassification Classification text ara, cmn, deu, eng, fra, ... (11) VaccinChatNLClassification Classification text nld DutchColaClassification Classification text nld DutchGovernmentBiasClassification Classification text nld DutchSarcasticHeadlinesClassification Classification text nld DutchNewsArticlesClassification Classification text nld OpenTenderClassification Classification text nld IconclassClassification Classification text nld SICKNLPairClassification PairClassification text nld XLWICNLPairClassification PairClassification text nld CovidDisinformationNLMultiLabelClassification MultilabelClassification text nld MultiEURLEXMultilabelClassification MultilabelClassification text bul, ces, dan, deu, ell, ... (23) VABBMultiLabelClassification MultilabelClassification text nld DutchNewsArticlesClusteringS2S Clustering text nld DutchNewsArticlesClusteringP2P Clustering text nld SIB200ClusteringS2S Clustering text ace, acm, acq, aeb, afr, ... (197) VABBClusteringS2S Clustering text nld VABBClusteringP2P Clustering text nld OpenTenderClusteringS2S Clustering text nld OpenTenderClusteringP2P Clustering text nld IconclassClusteringS2S Clustering text nld WikipediaRerankingMultilingual Reranking text ben, bul, ces, dan, deu, ... (18) ArguAna-NL.v2 Retrieval text nld SCIDOCS-NL.v2 Retrieval text nld SciFact-NL.v2 Retrieval text nld NFCorpus-NL.v2 Retrieval text nld BelebeleRetrieval Retrieval text acm, afr, als, amh, apc, ... (115) WebFAQRetrieval Retrieval text ara, aze, ben, bul, cat, ... (51) DutchNewsArticlesRetrieval Retrieval text nld bBSARDNLRetrieval Retrieval text nld LegalQANLRetrieval Retrieval text nld OpenTenderRetrieval Retrieval text nld VABBRetrieval Retrieval text nld WikipediaRetrievalMultilingual Retrieval text ben, bul, ces, dan, deu, ... (16) SICK-NL-STS STS text nld STSBenchmarkMultilingualSTS STS text cmn, deu, eng, fra, ita, ... (10) Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {22509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebpol-v1","title":"MTEB(pol, v1)","text":"<p>Polish Massive Text Embedding Benchmark (PL-MTEB), a comprehensive benchmark for text embeddings in Polish. The PL-MTEB consists of 28 diverse NLP tasks from 5 task types. With tasks adapted based on previously used datasets by the Polish NLP community. In addition, a new PLSC (Polish Library of Science Corpus) dataset was created consisting of titles and abstracts of scientific publications in Polish, which was used as the basis for two novel clustering tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages AllegroReviews Classification text pol CBD Classification text pol MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) PolEmo2.0-IN Classification text pol PolEmo2.0-OUT Classification text pol PAC Classification text pol EightTagsClustering Clustering text pol PlscClusteringS2S Clustering text pol PlscClusteringP2P Clustering text pol CDSC-E PairClassification text pol PpcPC PairClassification text pol PSC PairClassification text pol SICK-E-PL PairClassification text pol CDSC-R STS text pol SICK-R-PL STS text pol STS22 STS text ara, cmn, deu, eng, fra, ... (10) Citation <pre><code>@article{poswiata2024plmteb,\n  author = {Rafa\u0142 Po\u015bwiata and S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz},\n  journal = {arXiv preprint arXiv:2405.10138},\n  title = {PL-MTEB: Polish Massive Text Embedding Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebrus-v1","title":"MTEB(rus, v1)","text":"<p>A Russian version of the Massive Text Embedding Benchmark with a number of novel Russian tasks in all task categories of the original MTEB.</p> <p>Learn more \u2192</p> Tasks name type modalities languages GeoreviewClassification Classification text rus HeadlineClassification Classification text rus InappropriatenessClassification Classification text rus KinopoiskClassification Classification text rus MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) RuReviewsClassification Classification text rus RuSciBenchGRNTIClassification Classification text rus RuSciBenchOECDClassification Classification text rus GeoreviewClusteringP2P Clustering text rus RuSciBenchGRNTIClusteringP2P Clustering text rus RuSciBenchOECDClusteringP2P Clustering text rus CEDRClassification MultilabelClassification text rus SensitiveTopicsClassification MultilabelClassification text rus TERRa PairClassification text rus MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) RuBQReranking Reranking text rus MIRACLRetrieval Retrieval text ara, ben, deu, eng, fas, ... (18) RiaNewsRetrieval Retrieval text rus RuBQRetrieval Retrieval text rus RUParaPhraserSTS STS text rus STS22 STS text ara, cmn, deu, eng, fra, ... (10) RuSTSBenchmarkSTS STS text rus Citation <pre><code>@misc{snegirev2024russianfocusedembeddersexplorationrumteb,\n  archiveprefix = {arXiv},\n  author = {Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},\n  eprint = {2408.12503},\n  primaryclass = {cs.CL},\n  title = {The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},\n  url = {https://arxiv.org/abs/2408.12503},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebrus-v11","title":"MTEB(rus, v1.1)","text":"<p>A Russian version of the Massive Text Embedding Benchmark covering the task categories of classification, clustering, reranking, pair classification, retrieval, and semantic similarity. In v1.1, MIRACLRetrieval and RiaNewsRetrieval were replaced with their HardNegatives variants for improved time-optimization measurement. MIRACLRetrievalHardNegatives and RiaNewsRetrievalHardNegatives are used in their updated versions (v2), both of which include improved default prompts.</p> <p>Learn more \u2192</p> Tasks name type modalities languages GeoreviewClassification Classification text rus HeadlineClassification Classification text rus InappropriatenessClassification Classification text rus KinopoiskClassification Classification text rus MassiveIntentClassification Classification text afr, amh, ara, aze, ben, ... (50) MassiveScenarioClassification Classification text afr, amh, ara, aze, ben, ... (50) RuReviewsClassification Classification text rus RuSciBenchGRNTIClassification Classification text rus RuSciBenchOECDClassification Classification text rus GeoreviewClusteringP2P Clustering text rus RuSciBenchGRNTIClusteringP2P Clustering text rus RuSciBenchOECDClusteringP2P Clustering text rus CEDRClassification MultilabelClassification text rus SensitiveTopicsClassification MultilabelClassification text rus TERRa PairClassification text rus MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) RuBQReranking Reranking text rus MIRACLRetrievalHardNegatives.v2 Retrieval text ara, ben, deu, eng, fas, ... (18) RiaNewsRetrievalHardNegatives.v2 Retrieval text rus RuBQRetrieval Retrieval text rus RUParaPhraserSTS STS text rus STS22 STS text ara, cmn, deu, eng, fra, ... (10) RuSTSBenchmarkSTS STS text rus Citation <pre><code>@misc{snegirev2024russianfocusedembeddersexplorationrumteb,\n  archiveprefix = {arXiv},\n  author = {Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},\n  eprint = {2408.12503},\n  primaryclass = {cs.CL},\n  title = {The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},\n  url = {https://arxiv.org/abs/2408.12503},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#mtebspa-v1","title":"MTEB(spa, v1)","text":"<p>Spanish text embedding benchmark covering classification, clustering, pair classification, reranking, retrieval, and semantic textual similarity tasks. For a discussion on the benchmark construction see the original submission.</p> Tasks name type modalities languages SpanishNewsClassification.v2 Classification text spa SpanishSentimentClassification.v2 Classification text spa MLSUMClusteringP2P Clustering text deu, fra, rus, spa MLSUMClusteringS2S Clustering text deu, fra, rus, spa PawsXPairClassification PairClassification text cmn, deu, eng, fra, jpn, ... (7) XNLI PairClassification text ara, bul, deu, ell, eng, ... (14) MIRACLReranking Reranking text ara, ben, deu, eng, fas, ... (18) MIRACLRetrievalHardNegatives.v2 Retrieval text ara, ben, deu, eng, fas, ... (18) MintakaRetrieval Retrieval text ara, deu, fra, hin, ita, ... (8) SpanishPassageRetrievalS2P Retrieval text spa SpanishPassageRetrievalS2S Retrieval text spa XPQARetrieval Retrieval text ara, cmn, deu, eng, fra, ... (13) STSES STS text spa STSBenchmarkMultilingualSTS STS text cmn, deu, eng, fra, ita, ... (10) STS17 STS text ara, deu, eng, fra, ita, ... (9) STS22 STS text ara, cmn, deu, eng, fra, ... (10)"},{"location":"overview/available_benchmarks/#nanobeir","title":"NanoBEIR","text":"<p>A benchmark to evaluate with subsets of BEIR datasets to use less computational power</p> <p>Learn more \u2192</p> Tasks name type modalities languages NanoArguAnaRetrieval Retrieval text eng NanoClimateFeverRetrieval Retrieval text eng NanoDBPediaRetrieval Retrieval text eng NanoFEVERRetrieval Retrieval text eng NanoFiQA2018Retrieval Retrieval text eng NanoHotpotQARetrieval Retrieval text eng NanoMSMARCORetrieval Retrieval text eng NanoNFCorpusRetrieval Retrieval text eng NanoNQRetrieval Retrieval text eng NanoQuoraRetrieval Retrieval text eng NanoSCIDOCSRetrieval Retrieval text eng NanoSciFactRetrieval Retrieval text eng NanoTouche2020Retrieval Retrieval text eng"},{"location":"overview/available_benchmarks/#r2med","title":"R2MED","text":"<p>R2MED: First Reasoning-Driven Medical Retrieval Benchmark.     R2MED is a high-quality, high-resolution information retrieval (IR) dataset designed for medical scenarios.     It contains 876 queries with three retrieval tasks, five medical scenarios, and twelve body systems.</p> <p>Learn more \u2192</p> Tasks name type modalities languages R2MEDBiologyRetrieval Retrieval text eng R2MEDBioinformaticsRetrieval Retrieval text eng R2MEDMedicalSciencesRetrieval Retrieval text eng R2MEDMedXpertQAExamRetrieval Retrieval text eng R2MEDMedQADiagRetrieval Retrieval text eng R2MEDPMCTreatmentRetrieval Retrieval text eng R2MEDPMCClinicalRetrieval Retrieval text eng R2MEDIIYiClinicalRetrieval Retrieval text eng Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rar-b","title":"RAR-b","text":"<p>A benchmark to evaluate reasoning capabilities of retrievers.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ARCChallenge Retrieval text eng AlphaNLI Retrieval text eng HellaSwag Retrieval text eng WinoGrande Retrieval text eng PIQA Retrieval text eng SIQA Retrieval text eng Quail Retrieval text eng SpartQA Retrieval text eng TempReasonL1 Retrieval text eng TempReasonL2Pure Retrieval text eng TempReasonL2Fact Retrieval text eng TempReasonL2Context Retrieval text eng TempReasonL3Pure Retrieval text eng TempReasonL3Fact Retrieval text eng TempReasonL3Context Retrieval text eng RARbCode Retrieval text eng RARbMath Retrieval text eng Citation <pre><code>@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Al Moubayed, Noura},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebcode-beta","title":"RTEB(Code, beta)","text":"<p>RTEB Code is a subset of RTEB containing retrieval tasks specifically focused on programming and code domains including algorithmic problems, data science tasks, code evaluation, SQL retrieval, and multilingual code retrieval. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python Code1Retrieval Retrieval text eng JapaneseCode1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebhealth-beta","title":"RTEB(Health, beta)","text":"<p>RTEB Healthcare is a subset of RTEB containing retrieval tasks specifically focused on healthcare and medical domains including medical Q&amp;A, healthcare information retrieval, cross-lingual medical retrieval, and multilingual medical consultation. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa EnglishHealthcare1Retrieval Retrieval text eng GermanHealthcare1Retrieval Retrieval text deu Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rteblaw-beta","title":"RTEB(Law, beta)","text":"<p>RTEB Legal is a subset of RTEB containing retrieval tasks specifically focused on legal domain including case documents, statutes, legal summarization, and multilingual legal Q&amp;A. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng LegalQuAD Retrieval text deu FrenchLegal1Retrieval Retrieval text fra GermanLegal1Retrieval Retrieval text deu JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebbeta","title":"RTEB(beta)","text":"<p>RTEB (ReTrieval Embedding Benchmark) is a comprehensive benchmark for evaluating text retrieval models across multiple specialized domains including legal, finance, code, and healthcare. It contains diverse retrieval tasks designed to test models' ability to understand domain-specific terminology and retrieve relevant documents in specialized contexts across multiple languages. The dataset includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng LegalQuAD Retrieval text deu FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa MIRACLRetrievalHardNegatives Retrieval text ara, ben, deu, eng, fas, ... (18) Code1Retrieval Retrieval text eng JapaneseCode1Retrieval Retrieval text jpn EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng EnglishHealthcare1Retrieval Retrieval text eng French1Retrieval Retrieval text fra FrenchLegal1Retrieval Retrieval text fra German1Retrieval Retrieval text deu GermanHealthcare1Retrieval Retrieval text deu GermanLegal1Retrieval Retrieval text deu JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebdeu-beta","title":"RTEB(deu, beta)","text":"<p>RTEB German is a subset of RTEB containing retrieval tasks in German across legal, healthcare, and business domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages LegalQuAD Retrieval text deu German1Retrieval Retrieval text deu GermanHealthcare1Retrieval Retrieval text deu GermanLegal1Retrieval Retrieval text deu Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebeng-beta","title":"RTEB(eng, beta)","text":"<p>RTEB English is a subset of RTEB containing retrieval tasks in English across legal, finance, code, and healthcare domains. Includes diverse tasks covering specialized domains such as healthcare and finance. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages AILACasedocs Retrieval text eng AILAStatutes Retrieval text eng LegalSummarization Retrieval text eng FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng AppsRetrieval Retrieval text eng, python DS1000Retrieval Retrieval text eng, python HumanEvalRetrieval Retrieval text eng, python MBPPRetrieval Retrieval text eng, python WikiSQLRetrieval Retrieval text eng, sql FreshStackRetrieval Retrieval text eng, go, javascript, python ChatDoctorRetrieval Retrieval text eng CUREv1 Retrieval text eng, fra, spa Code1Retrieval Retrieval text eng EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng EnglishHealthcare1Retrieval Retrieval text eng Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebfin-beta","title":"RTEB(fin, beta)","text":"<p>RTEB Finance is a subset of RTEB  containing retrieval tasks specifically focused on financial domain including finance benchmarks, Q&amp;A, financial document retrieval, and corporate governance. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages FinanceBenchRetrieval Retrieval text eng HC3FinanceRetrieval Retrieval text eng FinQARetrieval Retrieval text eng EnglishFinance1Retrieval Retrieval text eng EnglishFinance2Retrieval Retrieval text eng EnglishFinance3Retrieval Retrieval text eng EnglishFinance4Retrieval Retrieval text eng Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebfra-beta","title":"RTEB(fra, beta)","text":"<p>RTEB French is a subset of RTEB containing retrieval tasks in French across legal and general knowledge domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages CUREv1 Retrieval text eng, fra, spa French1Retrieval Retrieval text fra FrenchLegal1Retrieval Retrieval text fra Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#rtebjpn-beta","title":"RTEB(jpn, beta)","text":"<p>RTEB Japanese is a subset of RTEB  containing retrieval tasks in Japanese across legal and code domains. The benchmark includes both open and closed datasets, providing a robust evaluation framework for real-world applications. To submit results on private tasks, please create open an issue.</p> <p>Note: We have temporarily removed the 'Private' column to read more about this decision out the announcement.</p> Tasks name type modalities languages JapaneseCode1Retrieval Retrieval text jpn JapaneseLegal1Retrieval Retrieval text jpn Citation <pre><code>@article{rteb2025,\n  author = {Liu, Frank and Enevoldsen, Kenneth and Solomatin, Roman and Chung, Isaac and Aarsen, Tom and F\u0151di, Zolt\u00e1n},\n  title = {Introducing RTEB: A New Standard for Retrieval Evaluation},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#ruscibench","title":"RuSciBench","text":"<p>RuSciBench is a benchmark designed for evaluating sentence encoders and language models on scientific texts in both Russian and English. The data is sourced from eLibrary (www.elibrary.ru), Russia's largest electronic library of scientific publications. This benchmark facilitates the evaluation and comparison of models on various research-related tasks.</p> <p>Learn more \u2192</p> Tasks name type modalities languages RuSciBenchBitextMining.v2 BitextMining text eng, rus RuSciBenchCoreRiscClassification Classification text eng, rus RuSciBenchGRNTIClassification.v2 Classification text eng, rus RuSciBenchOECDClassification.v2 Classification text eng, rus RuSciBenchPubTypeClassification Classification text eng, rus RuSciBenchCiteRetrieval Retrieval text eng, rus RuSciBenchCociteRetrieval Retrieval text eng, rus RuSciBenchCitedCountRegression Regression text eng, rus RuSciBenchYearPublRegression Regression text eng, rus Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vn-mteb-vie-v1","title":"VN-MTEB (vie, v1)","text":"<p>A benchmark for text-embedding performance in Vietnamese.</p> <p>Learn more \u2192</p> Tasks name type modalities languages ArguAna-VN Retrieval text vie SciFact-VN Retrieval text vie ClimateFEVER-VN Retrieval text vie FEVER-VN Retrieval text vie DBPedia-VN Retrieval text vie NQ-VN Retrieval text vie HotpotQA-VN Retrieval text vie MSMARCO-VN Retrieval text vie TRECCOVID-VN Retrieval text vie FiQA2018-VN Retrieval text vie NFCorpus-VN Retrieval text vie SCIDOCS-VN Retrieval text vie Touche2020-VN Retrieval text vie Quora-VN Retrieval text vie CQADupstackAndroid-VN Retrieval text vie CQADupstackGis-VN Retrieval text vie CQADupstackMathematica-VN Retrieval text vie CQADupstackPhysics-VN Retrieval text vie CQADupstackProgrammers-VN Retrieval text vie CQADupstackStats-VN Retrieval text vie CQADupstackTex-VN Retrieval text vie CQADupstackUnix-VN Retrieval text vie CQADupstackWebmasters-VN Retrieval text vie CQADupstackWordpress-VN Retrieval text vie Banking77VNClassification Classification text vie EmotionVNClassification Classification text vie AmazonCounterfactualVNClassification Classification text vie MTOPDomainVNClassification Classification text vie TweetSentimentExtractionVNClassification Classification text vie ToxicConversationsVNClassification Classification text vie ImdbVNClassification Classification text vie MTOPIntentVNClassification Classification text vie MassiveScenarioVNClassification Classification text vie MassiveIntentVNClassification Classification text vie AmazonReviewsVNClassification Classification text vie AmazonPolarityVNClassification Classification text vie SprintDuplicateQuestions-VN PairClassification text vie TwitterSemEval2015-VN PairClassification text vie TwitterURLCorpus-VN PairClassification text vie TwentyNewsgroupsClustering-VN Clustering text vie RedditClusteringP2P-VN Clustering text vie StackExchangeClusteringP2P-VN Clustering text vie StackExchangeClustering-VN Clustering text vie RedditClustering-VN Clustering text vie SciDocsRR-VN Reranking text vie AskUbuntuDupQuestions-VN Reranking text vie StackOverflowDupQuestions-VN Reranking text vie BIOSSES-VN STS text vie SICK-R-VN STS text vie STSBenchmark-VN STS text vie Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev1v2","title":"ViDoRe(v1&amp;v2)","text":"<p>A benchmark for evaluating visual document retrieval, combining ViDoRe v1 and v2.</p> <p>Learn more \u2192</p> Tasks name type modalities languages VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng Vidore2ESGReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2EconomicsReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2BioMedicalLecturesRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2ESGReportsHLRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev1","title":"ViDoRe(v1)","text":"<p>Retrieve associated pages according to questions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages VidoreArxivQARetrieval DocumentUnderstanding text, image eng VidoreDocVQARetrieval DocumentUnderstanding text, image eng VidoreInfoVQARetrieval DocumentUnderstanding text, image eng VidoreTabfquadRetrieval DocumentUnderstanding text, image eng VidoreTatdqaRetrieval DocumentUnderstanding text, image eng VidoreShiftProjectRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAAIRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAEnergyRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAGovernmentReportsRetrieval DocumentUnderstanding text, image eng VidoreSyntheticDocQAHealthcareIndustryRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev2","title":"ViDoRe(v2)","text":"<p>Retrieve associated pages according to questions.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Vidore2ESGReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2EconomicsReportsRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2BioMedicalLecturesRetrieval DocumentUnderstanding text, image deu, eng, fra, spa Vidore2ESGReportsHLRetrieval DocumentUnderstanding text, image eng Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_benchmarks/#vidorev3","title":"ViDoRe(v3)","text":"<p>ViDoRe V3 sets a new industry gold standard for multi-modal, enterprise document visual retrieval evaluation. It addresses a critical challenge in production RAG systems: retrieving accurate information from complex, visually-rich documents. The benchmark includes both open and closed datasets: to submit results on private tasks, please open an issue.</p> <p>Learn more \u2192</p> Tasks name type modalities languages Vidore3FinanceEnRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3IndustrialRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3ComputerScienceRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3PharmaceuticalsRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3HrRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3FinanceFrRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3PhysicsRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3EnergyRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3TelecomRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Vidore3NuclearRetrieval DocumentUnderstanding text, image deu, eng, fra, ita, por, ... (6) Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_models/audio/","title":"Audio Model","text":"<ul> <li>Number of models: 41</li> </ul>"},{"location":"overview/available_models/audio/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/audio/#mitast-finetuned-audioset-10-10-04593","title":"<code>MIT/ast-finetuned-audioset-10-10-0.4593</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 86.6M 330.0 MB 2021-07-08 eng-Latn Citation <pre><code>@misc{gong2021astaudiospectrogramtransformer,\n      title={AST: Audio Spectrogram Transformer},\n      author={Yuan Gong and Yu-An Chung and James Glass},\n      year={2021},\n      eprint={2104.01778},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2104.01778},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#asappsew-d-base-plus-400k-ft-ls100h","title":"<code>asapp/sew-d-base-plus-400k-ft-ls100h</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 95.0M 675.0 MB 2021-09-14 eng-Latn Citation <pre><code>@misc{wu2021performanceefficiencytradeoffsunsupervisedpretraining,\n      title={Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition},\n      author={Felix Wu and Kwangyoun Kim and Jing Pan and Kyu Han and Kilian Q. Weinberger and Yoav Artzi},\n      year={2021},\n      eprint={2109.06870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2109.06870},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#asappsew-d-mid-400k-ft-ls100h","title":"<code>asapp/sew-d-mid-400k-ft-ls100h</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 139.0M 530.0 MB 2021-09-14 eng-Latn Citation <pre><code>@misc{wu2021performanceefficiencytradeoffsunsupervisedpretraining,\n      title={Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition},\n      author={Felix Wu and Kwangyoun Kim and Jing Pan and Kyu Han and Kilian Q. Weinberger and Yoav Artzi},\n      year={2021},\n      eprint={2109.06870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2109.06870},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#asappsew-d-tiny-100k-ft-ls100h","title":"<code>asapp/sew-d-tiny-100k-ft-ls100h</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 19.7M 92.0 MB 2021-09-14 eng-Latn Citation <pre><code>@misc{wu2021performanceefficiencytradeoffsunsupervisedpretraining,\n      title={Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition},\n      author={Felix Wu and Kwangyoun Kim and Jing Pan and Kyu Han and Kilian Q. Weinberger and Yoav Artzi},\n      year={2021},\n      eprint={2109.06870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2109.06870},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookdata2vec-audio-base-960h","title":"<code>facebook/data2vec-audio-base-960h</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 93.2M 355.0 MB 2022-02-07 eng-Latn Citation <pre><code>@misc{baevski2022data2vecgeneralframeworkselfsupervised,\n    title={data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},\n    author={Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},\n    year={2022},\n    eprint={2202.03555},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2202.03555},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookdata2vec-audio-large-960h","title":"<code>facebook/data2vec-audio-large-960h</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 313.3M 1.2 GB 2022-02-07 eng-Latn Citation <pre><code>@misc{baevski2022data2vecgeneralframeworkselfsupervised,\n    title={data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},\n    author={Alexei Baevski and Wei-Ning Hsu and Qiantong Xu and Arun Babu and Jiatao Gu and Michael Auli},\n    year={2022},\n    eprint={2202.03555},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG},\n    url={https://arxiv.org/abs/2202.03555},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookencodec_24khz","title":"<code>facebook/encodec_24khz</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 128 23.3M 88.0 MB 2022-10-25 eng-Latn Citation <pre><code>@misc{d\u00e9fossez2022highfidelityneuralaudio,\n      title={High Fidelity Neural Audio Compression},\n      author={Alexandre D\u00e9fossez and Jade Copet and Gabriel Synnaeve and Yossi Adi},\n      year={2022},\n      eprint={2210.13438},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2210.13438},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookhubert-base-ls960","title":"<code>facebook/hubert-base-ls960</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 95.0M 360.0 MB 2021-06-14 eng-Latn Citation <pre><code>@misc{hsu2021hubertselfsupervisedspeechrepresentation,\n    title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},\n    author={Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},\n    year={2021},\n    eprint={2106.07447},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2106.07447},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookhubert-large-ls960-ft","title":"<code>facebook/hubert-large-ls960-ft</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 317.0M 1.2 GB 2021-06-14 eng-Latn Citation <pre><code>@misc{hsu2021hubertselfsupervisedspeechrepresentation,\n    title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},\n    author={Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},\n    year={2021},\n    eprint={2106.07447},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2106.07447},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookmms-1b-all","title":"<code>facebook/mms-1b-all</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 1.0B 3.6 GB 2023-05-22 ara-Arab, cmn-Hans, deu-Latn, eng-Latn, fra-Latn, ... (7) Citation <pre><code>@misc{pratap2023scalingspeechtechnology1000,\n  title={Scaling Speech Technology to 1,000+ Languages},\n  author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},\n  year={2023},\n  eprint={2305.13516},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2305.13516},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookmms-1b-fl102","title":"<code>facebook/mms-1b-fl102</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 1.0B 3.6 GB 2023-05-22 eng-Latn Citation <pre><code>@misc{pratap2023scalingspeechtechnology1000,\n  title={Scaling Speech Technology to 1,000+ Languages},\n  author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},\n  year={2023},\n  eprint={2305.13516},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2305.13516},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookmms-1b-l1107","title":"<code>facebook/mms-1b-l1107</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 1.0B 3.6 GB 2023-05-22 eng-Latn Citation <pre><code>@misc{pratap2023scalingspeechtechnology1000,\n  title={Scaling Speech Technology to 1,000+ Languages},\n  author={Vineel Pratap and Andros Tjandra and Bowen Shi and Paden Tomasello and Arun Babu and Sayani Kundu and Ali Elkahky and Zhaoheng Ni and Apoorv Vyas and Maryam Fazel-Zarandi and Alexei Baevski and Yossi Adi and Xiaohui Zhang and Wei-Ning Hsu and Alexis Conneau and Michael Auli},\n  year={2023},\n  eprint={2305.13516},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2305.13516},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookseamless-m4t-v2-large","title":"<code>facebook/seamless-m4t-v2-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 2.3B 8.6 GB 2023-11-06 eng-Latn Citation <pre><code>@misc{communication2023seamlessmultilingualexpressivestreaming,\n      title={Seamless: Multilingual Expressive and Streaming Speech Translation},\n      author={Seamless Communication and Lo\u00efc Barrault and Yu-An Chung and Mariano Coria Meglioli and David Dale and Ning Dong and Mark Duppenthaler and Paul-Ambroise Duquenne and Brian Ellis and Hady Elsahar and Justin Haaheim and John Hoffman and Min-Jae Hwang and Hirofumi Inaguma and Christopher Klaiber and Ilia Kulikov and Pengwei Li and Daniel Licht and Jean Maillard and Ruslan Mavlyutov and Alice Rakotoarison and Kaushik Ram Sadagopan and Abinesh Ramakrishnan and Tuan Tran and Guillaume Wenzek and Yilin Yang and Ethan Ye and Ivan Evtimov and Pierre Fernandez and Cynthia Gao and Prangthip Hansanti and Elahe Kalbassi and Amanda Kallet and Artyom Kozhevnikov and Gabriel Mejia Gonzalez and Robin San Roman and Christophe Touret and Corinne Wong and Carleigh Wood and Bokai Yu and Pierre Andrews and Can Balioglu and Peng-Jen Chen and Marta R. Costa-juss\u00e0 and Maha Elbayad and Hongyu Gong and Francisco Guzm\u00e1n and Kevin Heffernan and Somya Jain and Justine Kao and Ann Lee and Xutai Ma and Alex Mourachko and Benjamin Peloquin and Juan Pino and Sravya Popuri and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Anna Sun and Paden Tomasello and Changhan Wang and Jeff Wang and Skyler Wang and Mary Williamson},\n      year={2023},\n      eprint={2312.05187},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2312.05187},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-base","title":"<code>facebook/wav2vec2-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 95.0M 362.0 MB 2020-10-26 eng-Latn Citation <pre><code>@misc{baevski2020wav2vec20frameworkselfsupervised,\n      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},\n      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},\n      year={2020},\n      eprint={2006.11477},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2006.11477},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-base-960h","title":"<code>facebook/wav2vec2-base-960h</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 95.0M 360.0 MB 2020-10-26 eng-Latn Citation <pre><code>@misc{baevski2020wav2vec20frameworkselfsupervised,\n      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},\n      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},\n      year={2020},\n      eprint={2006.11477},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2006.11477},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-large","title":"<code>facebook/wav2vec2-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 317.0M 1.2 GB 2020-10-26 eng-Latn Citation <pre><code>@misc{baevski2020wav2vec20frameworkselfsupervised,\n      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},\n      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},\n      year={2020},\n      eprint={2006.11477},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2006.11477},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-large-xlsr-53","title":"<code>facebook/wav2vec2-large-xlsr-53</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 317.0M 1.2 GB 2020-10-26 eng-Latn Citation <pre><code>@misc{conneau2020unsupervisedcrosslingualrepresentationlearning,\n      title={Unsupervised Cross-lingual Representation Learning for Speech Recognition},\n      author={Alexis Conneau and Alexei Baevski and Ronan Collobert and Abdelrahman Mohamed and Michael Auli},\n      year={2020},\n      eprint={2006.13979},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2006.13979},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-lv-60-espeak-cv-ft","title":"<code>facebook/wav2vec2-lv-60-espeak-cv-ft</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 317.0M 1.2 GB 2020-10-26 eng-Latn Citation <pre><code>@misc{baevski2020wav2vec20frameworkselfsupervised,\n      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},\n      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},\n      year={2020},\n      eprint={2006.11477},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2006.11477},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-xls-r-1b","title":"<code>facebook/wav2vec2-xls-r-1b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 1.0B 4.4 GB 2024-09-10 abk-Cyrl, afr-Latn, amh-Latn, ara-Latn, asm-Latn, ... (55) Citation <pre><code>@misc{babu2021xlsrselfsupervisedcrosslingualspeech,\n      title={XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},\n      author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},\n      year={2021},\n      eprint={2111.09296},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.09296},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-xls-r-2b","title":"<code>facebook/wav2vec2-xls-r-2b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 2.0B 8.8 GB 2024-09-10 abk-Cyrl, afr-Latn, amh-Latn, ara-Latn, asm-Latn, ... (55) Citation <pre><code>@misc{babu2021xlsrselfsupervisedcrosslingualspeech,\n      title={XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},\n      author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},\n      year={2021},\n      eprint={2111.09296},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.09296},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-xls-r-2b-21-to-en","title":"<code>facebook/wav2vec2-xls-r-2b-21-to-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 2.0B 9.0 GB 2024-09-10 abk-Cyrl, afr-Latn, amh-Latn, ara-Latn, asm-Latn, ... (55) Citation <pre><code>@misc{babu2021xlsrselfsupervisedcrosslingualspeech,\n      title={XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},\n      author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},\n      year={2021},\n      eprint={2111.09296},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.09296},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#facebookwav2vec2-xls-r-300m","title":"<code>facebook/wav2vec2-xls-r-300m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 300.0M 1.2 GB 2021-10-13 abk-Cyrl, afr-Latn, amh-Latn, ara-Latn, asm-Latn, ... (55) Citation <pre><code>@misc{babu2021xlsrselfsupervisedcrosslingualspeech,\n      title={XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},\n      author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},\n      year={2021},\n      eprint={2111.09296},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.09296},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#googlevggish","title":"<code>google/vggish</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 128 72.1M 275.0 MB 2019-06-13 eng-Latn Citation <pre><code>@inproceedings{hershey2017cnn,\n    author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},\n    title = {CNN architectures for large-scale audio classification},\n    year = {2017},\n    publisher = {IEEE Press},\n    url = {https://doi.org/10.1109/ICASSP.2017.7952132},\n    doi = {10.1109/ICASSP.2017.7952132},\n    booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n    pages = {131\u2013135},\n    numpages = {5},\n    location = {New Orleans, LA, USA}\n}\n</code></pre>"},{"location":"overview/available_models/audio/#googleyamnet","title":"<code>google/yamnet</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 3.8M 14.0 MB 2020-10-06 eng-Latn Citation <pre><code>@inproceedings{audioset,\n  title={Audio set: An ontology and human-labeled dataset for audio events},\n  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},\n  booktitle={2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},\n  pages={776--780},\n  year={2017},\n  organization={IEEE}\n}\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftspeecht5_asr","title":"<code>microsoft/speecht5_asr</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 151.6M 578.0 MB 2022-05-16 eng-Latn Citation <pre><code>@misc{ao2022speecht5unifiedmodalencoderdecoderpretraining,\n      title={SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n      author={Junyi Ao and Rui Wang and Long Zhou and Chengyi Wang and Shuo Ren and Yu Wu and Shujie Liu and Tom Ko and Qing Li and Yu Zhang and Zhihua Wei and Yao Qian and Jinyu Li and Furu Wei},\n      year={2022},\n      eprint={2110.07205},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2110.07205},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftunispeech-sat-base-100h-libri-ft","title":"<code>microsoft/unispeech-sat-base-100h-libri-ft</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.0M 359.0 MB 2021-10-12 eng-Latn Citation <pre><code>@misc{chen2021unispeechsatuniversalspeechrepresentation,\n      title={UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware Pre-Training},\n      author={Sanyuan Chen and Yu Wu and Chengyi Wang and Zhengyang Chen and Zhuo Chen and Shujie Liu and Jian Wu and Yao Qian and Furu Wei and Jinyu Li and Xiangzhan Yu},\n      year={2021},\n      eprint={2110.05752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2110.05752},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base","title":"<code>microsoft/wavlm-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base-plus","title":"<code>microsoft/wavlm-base-plus</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base-plus-sd","title":"<code>microsoft/wavlm-base-plus-sd</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base-plus-sv","title":"<code>microsoft/wavlm-base-plus-sv</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base-sd","title":"<code>microsoft/wavlm-base-sd</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-base-sv","title":"<code>microsoft/wavlm-base-sv</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 94.7M 361.0 MB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#microsoftwavlm-large","title":"<code>microsoft/wavlm-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 316.6M 1.2 GB 2022-07-19 eng-Latn Citation <pre><code>@article{Chen_2022,\n   title={WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing},\n   volume={16},\n   ISSN={1941-0484},\n   url={http://dx.doi.org/10.1109/JSTSP.2022.3188113},\n   DOI={10.1109/jstsp.2022.3188113},\n   number={6},\n   journal={IEEE Journal of Selected Topics in Signal Processing},\n   publisher={Institute of Electrical and Electronics Engineers (IEEE)},\n   author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},\n   year={2022},\n   month=oct, pages={1505\u20131518} }\n</code></pre>"},{"location":"overview/available_models/audio/#openaiwhisper-base","title":"<code>openai/whisper-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 74.0M 277.0 MB 2022-09-27 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (99) Citation <pre><code>@misc{radford2022robustspeechrecognitionlargescale,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision},\n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2212.04356},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#openaiwhisper-large-v3","title":"<code>openai/whisper-large-v3</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1280 1.6B 5.7 GB 2022-09-27 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (99) Citation <pre><code>@misc{radford2022robustspeechrecognitionlargescale,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision},\n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2212.04356},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#openaiwhisper-medium","title":"<code>openai/whisper-medium</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 769.0M 2.8 GB 2022-09-27 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (99) Citation <pre><code>@misc{radford2022robustspeechrecognitionlargescale,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision},\n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2212.04356},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#openaiwhisper-small","title":"<code>openai/whisper-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 768 244.0M 922.0 MB 2022-09-27 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (99) Citation <pre><code>@misc{radford2022robustspeechrecognitionlargescale,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision},\n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2212.04356},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#openaiwhisper-tiny","title":"<code>openai/whisper-tiny</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 39.0M 144.0 MB 2022-09-27 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (99) Citation <pre><code>@misc{radford2022robustspeechrecognitionlargescale,\n      title={Robust Speech Recognition via Large-Scale Weak Supervision},\n      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},\n      year={2022},\n      eprint={2212.04356},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2212.04356},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#speechbraincnn14-esc50","title":"<code>speechbrain/cnn14-esc50</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2048 80.8M 308.0 MB 2022-11-26 eng-Latn Citation <pre><code>@inproceedings{wang2022CRL,\n    title={Learning Representations for New Sound Classes With Continual Self-Supervised Learning},\n    author={Zhepei Wang, Cem Subakan, Xilin Jiang, Junkai Wu, Efthymios Tzinis, Mirco Ravanelli, Paris Smaragdis},\n    year={2022},\n    booktitle={Accepted to IEEE Signal Processing Letters}\n}\n</code></pre>"},{"location":"overview/available_models/audio/#speechbrainm-ctc-t-large","title":"<code>speechbrain/m-ctc-t-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 3.9 GB 2022-01-10 abk-Cyrl, ara-Arab, asm-Beng, bre-Latn, cat-Latn, ... (58) Citation <pre><code>@misc{lugosch2022pseudolabelingmassivelymultilingualspeech,\n      title={Pseudo-Labeling for Massively Multilingual Speech Recognition},\n      author={Loren Lugosch and Tatiana Likhomanenko and Gabriel Synnaeve and Ronan Collobert},\n      year={2022},\n      eprint={2111.00161},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.00161},\n}\n</code></pre>"},{"location":"overview/available_models/audio/#vitouphywav2vec2-xls-r-300m-phoneme","title":"<code>vitouphy/wav2vec2-xls-r-300m-phoneme</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 300.0M 1.2 GB 2022-05-19 eng-Latn Citation <pre><code>@misc{babu2021xlsrselfsupervisedcrosslingualspeech,\n      title={XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale},\n      author={Arun Babu and Changhan Wang and Andros Tjandra and Kushal Lakhotia and Qiantong Xu and Naman Goyal and Kritika Singh and Patrick von Platen and Yatharth Saraf and Juan Pino and Alexei Baevski and Alexis Conneau and Michael Auli},\n      year={2021},\n      eprint={2111.09296},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2111.09296},\n}\n</code></pre>"},{"location":"overview/available_models/audio_image_text/","title":"Audio-image-text Model","text":"<ul> <li>Number of models: 2</li> </ul>"},{"location":"overview/available_models/audio_image_text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/audio_image_text/#mtebbaseline-random-cross-encoder","title":"mteb/baseline-random-cross-encoder","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 32 0 0.0 MB not specified not specified"},{"location":"overview/available_models/audio_image_text/#mtebbaseline-random-encoder","title":"mteb/baseline-random-encoder","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 32 0 0.0 MB not specified not specified"},{"location":"overview/available_models/audio_text/","title":"Audio-text Model","text":"<ul> <li>Number of models: 10</li> </ul>"},{"location":"overview/available_models/audio_text/#instruction-model","title":"Instruction Model","text":""},{"location":"overview/available_models/audio_text/#lco-embeddinglco-embedding-omni-3b","title":"<code>LCO-Embedding/LCO-Embedding-Omni-3B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 4.7B 8.8 GB 2025-10-23 eng-Latn Citation <pre><code>@misc{xiao2025scalinglanguagecentricomnimodalrepresentation,\n  title={Scaling Language-Centric Omnimodal Representation Learning},\n  author={Chenghao Xiao and Hou Pong Chan and Hao Zhang and Weiwen Xu and Mahani Aljunied and Yu Rong},\n  year={2025},\n  eprint={2510.11693},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2510.11693},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#lco-embeddinglco-embedding-omni-7b","title":"<code>LCO-Embedding/LCO-Embedding-Omni-7B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 8.9B 16.6 GB 2025-10-15 eng-Latn Citation <pre><code>@misc{xiao2025scalinglanguagecentricomnimodalrepresentation,\n  title={Scaling Language-Centric Omnimodal Representation Learning},\n  author={Chenghao Xiao and Hou Pong Chan and Hao Zhang and Weiwen Xu and Mahani Aljunied and Yu Rong},\n  year={2025},\n  eprint={2510.11693},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2510.11693},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#qwenqwen2-audio-7b","title":"<code>Qwen/Qwen2-Audio-7B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 1280 7.0B not specified 2024-08-09 eng-Latn Citation <pre><code>@misc{chu2024qwen2audiotechnicalreport,\n      title={Qwen2-Audio Technical Report},\n      author={Yunfei Chu and Jin Xu and Qian Yang and Haojie Wei and Xipin Wei and Zhifang Guo and Yichong Leng and Yuanjun Lv and Jinzheng He and Junyang Lin and Chang Zhou and Jingren Zhou},\n      year={2024},\n      eprint={2407.10759},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2407.10759},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/audio_text/#openmuqmuq-mulan-large","title":"<code>OpenMuQ/MuQ-MuLan-large</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 512 630.0M 2.5 GB 2025-01-01 eng-Latn, zho-Hans Citation <pre><code>@misc{zhu2025muqselfsupervisedmusicrepresentation,\n  title={MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization},\n  author={Haina Zhu and Yizhi Zhou and Hangting Chen and Jianwei Yu and Ziyang Ma and Rongzhi Gu and Yi Luo and Wei Tan and Xie Chen},\n  year={2025},\n  eprint={2501.01108},\n  archivePrefix={arXiv},\n  primaryClass={cs.SD},\n  url={https://arxiv.org/abs/2501.01108},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#laionclap-htsat-fused","title":"<code>laion/clap-htsat-fused</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 153.5M 586.0 MB 2023-05-22 eng-Latn Citation <pre><code>@misc{wu2024largescalecontrastivelanguageaudiopretraining,\n      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},\n      year={2024},\n      eprint={2211.06687},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2211.06687},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#laionclap-htsat-unfused","title":"<code>laion/clap-htsat-unfused</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 153.5M 586.0 MB 2023-05-22 eng-Latn Citation <pre><code>@misc{wu2024largescalecontrastivelanguageaudiopretraining,\n      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},\n      year={2024},\n      eprint={2211.06687},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2211.06687},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#laionlarger_clap_general","title":"<code>laion/larger_clap_general</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 193.9M 740.0 MB 2023-05-22 eng-Latn Citation <pre><code>@misc{wu2024largescalecontrastivelanguageaudiopretraining,\n      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},\n      year={2024},\n      eprint={2211.06687},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2211.06687},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#laionlarger_clap_music","title":"<code>laion/larger_clap_music</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 193.9M 740.0 MB 2023-05-22 eng-Latn Citation <pre><code>@misc{wu2024largescalecontrastivelanguageaudiopretraining,\n      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},\n      year={2024},\n      eprint={2211.06687},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2211.06687},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#laionlarger_clap_music_and_speech","title":"<code>laion/larger_clap_music_and_speech</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 512 193.9M 740.0 MB 2023-05-22 eng-Latn Citation <pre><code>@misc{wu2024largescalecontrastivelanguageaudiopretraining,\n      title={Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},\n      author={Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},\n      year={2024},\n      eprint={2211.06687},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2211.06687},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#lyrebirdwav2clip","title":"<code>lyrebird/wav2clip</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 512 163.0M 622.0 MB 2022-03-15 eng-Latn Citation <pre><code>@misc{wu2022wav2cliplearningrobustaudio,\n  title={Wav2CLIP: Learning Robust Audio Representations From CLIP},\n  author={Ho-Hsiang Wu and Prem Seetharaman and Kundan Kumar and Juan Pablo Bello},\n  year={2022},\n  eprint={2110.11499},\n  archivePrefix={arXiv},\n  primaryClass={cs.SD},\n  url={https://arxiv.org/abs/2110.11499},\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#microsoftmsclap-2022","title":"<code>microsoft/msclap-2022</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 196.0M 750.0 MB 2022-12-01 eng-Latn Citation <pre><code>@inproceedings{CLAP2022,\n  title={Clap learning audio concepts from natural language supervision},\n  author={Elizalde, Benjamin and Deshmukh, Soham and Al Ismail, Mahmoud and Wang, Huaming},\n  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  pages={1--5},\n  year={2023},\n  organization={IEEE}\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#microsoftmsclap-2023","title":"<code>microsoft/msclap-2023</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 160.0M 610.0 MB 2023-09-01 eng-Latn Citation <pre><code>@misc{CLAP2023,\n      title={Natural Language Supervision for General-Purpose Audio Representations},\n      author={Benjamin Elizalde and Soham Deshmukh and Huaming Wang},\n      year={2023},\n      eprint={2309.05767},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD},\n      url={https://arxiv.org/abs/2309.05767}\n}\n</code></pre>"},{"location":"overview/available_models/audio_text/#microsoftspeecht5_multimodal","title":"<code>microsoft/speecht5_multimodal</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 297.9M 1.1 GB 2022-05-16 eng-Latn Citation <pre><code>@misc{ao2022speecht5unifiedmodalencoderdecoderpretraining,\n      title={SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n      author={Junyi Ao and Rui Wang and Long Zhou and Chengyi Wang and Shuo Ren and Yu Wu and Shujie Liu and Tom Ko and Qing Li and Yu Zhang and Zhihua Wei and Yao Qian and Jinyu Li and Furu Wei},\n      year={2022},\n      eprint={2110.07205},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2110.07205},\n}\n</code></pre>"},{"location":"overview/available_models/image/","title":"Image Model","text":"<ul> <li>Number of models: 21</li> </ul>"},{"location":"overview/available_models/image/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/image/#facebookdinov2-base","title":"<code>facebook/dinov2-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 86.6M 330.0 MB 2023-07-18 eng-Latn Citation <pre><code>@misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision},\n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookdinov2-giant","title":"<code>facebook/dinov2-giant</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 4.2 GB 2023-07-18 eng-Latn Citation <pre><code>@misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision},\n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookdinov2-large","title":"<code>facebook/dinov2-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.4M 1.1 GB 2023-07-18 eng-Latn Citation <pre><code>@misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision},\n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookdinov2-small","title":"<code>facebook/dinov2-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 384 22.1M 84.0 MB 2023-07-18 eng-Latn Citation <pre><code>@misc{oquab2023dinov2,\n      title={DINOv2: Learning Robust Visual Features without Supervision},\n      author={Maxime Oquab and Timoth\u00e9e Darcet and Th\u00e9o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herv\u00e9 Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},\n      year={2023},\n      eprint={2304.07193},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino1b-full2b-224","title":"<code>facebook/webssl-dino1b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 4.2 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-full2b-224","title":"<code>facebook/webssl-dino2b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.1B 7.8 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-heavy2b-224","title":"<code>facebook/webssl-dino2b-heavy2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.1B 7.8 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino2b-light2b-224","title":"<code>facebook/webssl-dino2b-light2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 2688 2.1B 7.8 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino300m-full2b-224","title":"<code>facebook/webssl-dino300m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 303.7M 1.1 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-full2b-224","title":"<code>facebook/webssl-dino3b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 2.9B 11.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-heavy2b-224","title":"<code>facebook/webssl-dino3b-heavy2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 2.9B 11.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino3b-light2b-224","title":"<code>facebook/webssl-dino3b-light2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3072 2.9B 11.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino5b-full2b-224","title":"<code>facebook/webssl-dino5b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 3584 4.9B 18.4 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-224","title":"<code>facebook/webssl-dino7b-full8b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 6.5B 24.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-378","title":"<code>facebook/webssl-dino7b-full8b-378</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 6.5B 24.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-dino7b-full8b-518","title":"<code>facebook/webssl-dino7b-full8b-518</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 6.5B 24.0 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-mae1b-full2b-224","title":"<code>facebook/webssl-mae1b-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1536 1.1B 4.2 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-mae300m-full2b-224","title":"<code>facebook/webssl-mae300m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.4M 1.1 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#facebookwebssl-mae700m-full2b-224","title":"<code>facebook/webssl-mae700m-full2b-224</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 632.4M 2.4 GB 2025-04-24 eng-Latn Citation <pre><code>@article{fan2025scaling,\n  title={Scaling Language-Free Visual Representation Learning},\n  author={David Fan and Shengbang Tong and Jiachen Zhu and Koustuv Sinha and Zhuang Liu and Xinlei Chen and Michael Rabbat and Nicolas Ballas and Yann LeCun and Amir Bar and Saining Xie},\n  year={2025},\n  eprint={2504.01017},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image/#nyu-visionxmoco-v3-vit-b","title":"<code>nyu-visionx/moco-v3-vit-b</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 86.6M 330.0 MB 2024-06-03 eng-Latn Citation <pre><code>@Article{chen2021mocov3,\n    author  = {Xinlei Chen* and Saining Xie* and Kaiming He},\n    title   = {An Empirical Study of Training Self-Supervised Vision Transformers},\n    journal = {arXiv preprint arXiv:2104.02057},\n    year    = {2021},\n}\n</code></pre>"},{"location":"overview/available_models/image/#nyu-visionxmoco-v3-vit-l","title":"<code>nyu-visionx/moco-v3-vit-l</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 304.0M 1.1 GB 2024-06-03 eng-Latn Citation <pre><code>@Article{chen2021mocov3,\n    author  = {Xinlei Chen* and Saining Xie* and Kaiming He},\n    title   = {An Empirical Study of Training Self-Supervised Vision Transformers},\n    journal = {arXiv preprint arXiv:2104.02057},\n    year    = {2021},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/","title":"Image-text Model","text":"<ul> <li>Number of models: 46</li> </ul>"},{"location":"overview/available_models/image_text/#instruction-model","title":"Instruction Model","text":""},{"location":"overview/available_models/image_text/#alibaba-nlpgme-qwen2-vl-2b-instruct","title":"<code>Alibaba-NLP/gme-Qwen2-VL-2B-Instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 2.2B 8.2 GB 2024-12-24 cmn-Hans, eng-Latn Citation <pre><code>@misc{zhang2024gme,\n      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs},\n      author={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},\n      year={2024},\n      eprint={2412.16855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={http://arxiv.org/abs/2412.16855}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#alibaba-nlpgme-qwen2-vl-7b-instruct","title":"<code>Alibaba-NLP/gme-Qwen2-VL-7B-Instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.7B 30.9 GB 2024-12-24 cmn-Hans, eng-Latn Citation <pre><code>@misc{zhang2024gme,\n      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs},\n      author={Zhang, Xin and Zhang, Yanzhao and Xie, Wen and Li, Mingxin and Dai, Ziqi and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Li, Wenjie and Zhang, Min},\n      year={2024},\n      eprint={2412.16855},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={http://arxiv.org/abs/2412.16855}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#apsarastackmaasevoqwen25-vl-retriever-3b-v1","title":"<code>ApsaraStackMaaS/EvoQwen2.5-VL-Retriever-3B-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 7.0 GB 2025-11-04 eng-Latn"},{"location":"overview/available_models/image_text/#apsarastackmaasevoqwen25-vl-retriever-7b-v1","title":"<code>ApsaraStackMaaS/EvoQwen2.5-VL-Retriever-7B-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 7.0B 14.1 GB 2025-11-04 eng-Latn"},{"location":"overview/available_models/image_text/#opensearch-aiops-colqwen3-4b","title":"<code>OpenSearch-AI/Ops-Colqwen3-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.8B 9.0 GB 2026-01-24 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{ops_colqwen3_4b,\n  author       = {OpenSearch-AI},\n  title        = {Ops-ColQwen3: State-of-the-Art Multimodal Embedding Model for Visual Document Retrieval},\n  year         = {2026},\n  url          = {https://huggingface.co/OpenSearch-AI/Ops-ColQwen3-4B},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tiger-labvlm2vec-full","title":"<code>TIGER-Lab/VLM2Vec-Full</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 3072 4.1B 7.7 GB 2024-10-08 eng-Latn Citation <pre><code>@article{jiang2024vlm2vec,\n  title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks},\n  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},\n  journal={arXiv preprint arXiv:2410.05160},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tiger-labvlm2vec-lora","title":"<code>TIGER-Lab/VLM2Vec-LoRA</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 3072 4.2B not specified 2024-10-08 eng-Latn Citation <pre><code>@article{jiang2024vlm2vec,\n  title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks},\n  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},\n  journal={arXiv preprint arXiv:2410.05160},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tomoroaitomoro-colqwen3-embed-4b","title":"<code>TomoroAI/tomoro-colqwen3-embed-4b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 320 4.0B 8.3 GB 2025-11-26 eng-Latn Citation <pre><code>@misc{huang2025tomoro_colqwen3_embed,\n  title={TomoroAI/tomoro-colqwen3-embed},\n  author={Xin Huang and Kye Min Tan and Albert Phelps},\n  year={2025},\n  url={https://huggingface.co/TomoroAI/tomoro-colqwen3-embed-8b}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#tomoroaitomoro-colqwen3-embed-8b","title":"<code>TomoroAI/tomoro-colqwen3-embed-8b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 320 8.0B 16.3 GB 2025-11-26 eng-Latn Citation <pre><code>@misc{huang2025tomoro_colqwen3_embed,\n  title={TomoroAI/tomoro-colqwen3-embed},\n  author={Xin Huang and Kye Min Tan and Albert Phelps},\n  year={2025},\n  url={https://huggingface.co/TomoroAI/tomoro-colqwen3-embed-8b}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-collfm2-450m-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColLFM2-450M-v0.1</code>","text":"<p>License: https://huggingface.co/LiquidAI/LFM2-VL-450M/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 128 451.0M 860.0 MB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-colministral3-3b-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColMinistral3-3b-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 128 4.3B 7.9 GB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-colqwen3-17b-turbo-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColQwen3-1.7b-Turbo-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 128 1.8B 3.3 GB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-colqwen3-2b-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColQwen3-2b-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 128 2.1B 4.0 GB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-colqwen3-4b-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColQwen3-4b-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 128 4.4B 8.3 GB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vagosolutionssauerkrautlm-colqwen3-8b-v01","title":"<code>VAGOsolutions/SauerkrautLM-ColQwen3-8b-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 128 8.1B 15.2 GB 2025-12-20 deu-Latn, eng-Latn, fra-Latn, ita-Latn, por-Latn, ... (6) Citation <pre><code>@misc{sauerkrautlm-colpali-2025,\n  title={SauerkrautLM-ColPali: Multi-Vector Vision Retrieval Models},\n  author={David Golchinfar},\n  organization={VAGO Solutions},\n  year={2025},\n  url={https://github.com/VAGOsolutions/sauerkrautlm-colpali}\n}\n\n@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#eagerworkseager-embed-v1","title":"<code>eagerworks/eager-embed-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 2560 4.4B 16.5 GB 2025-11-20 deu-Latn, eng-Latn, fra-Latn, spa-Latn Citation <pre><code>@article{EagerEmbed,\n  title={Eager Embed V1: Multimodal Dense Embeddings for Retrieval},\n  author={Juan Pablo Balarini},\n  year={2025},\n  publisher={Eagerworks},\n  url={https://github.com/eagerworks/eager-embed},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#ibm-granitegranite-vision-33-2b-embedding","title":"<code>ibm-granite/granite-vision-3.3-2b-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 11.1 GB 2025-06-11 eng-Latn Citation <pre><code>@article{karlinsky2025granitevision,\n  title={Granite Vision: a lightweight, open-source multimodal model for enterprise Intelligence},\n  author={Granite Vision Team and Karlinsky, Leonid and Arbelle, Assaf and Daniels, Abraham and Nassar, Ahmed and Alfassi, Amit and Wu, Bo and Schwartz, Eli and Joshi, Dhiraj and Kondic, Jovana and Shabtay, Nimrod and Li, Pengyuan and Herzig, Roei and Abedin, Shafiq and Perek, Shaked and Harary, Sivan and Barzelay, Udi and Raz Goldfarb, Adi and Oliva, Aude and Wieles, Ben and Bhattacharjee, Bishwaranjan and Huang, Brandon and Auer, Christoph and Gutfreund, Dan and Beymer, David and Wood, David and Kuehne, Hilde and Hansen, Jacob and Shtok, Joseph and Wong, Ken and Bathen, Luis Angel and Mishra, Mayank and Lysak, Maksym and Dolfi, Michele and Yurochkin, Mikhail and Livathinos, Nikolaos and Harel, Nimrod and Azulai, Ophir and Naparstek, Oshri and de Lima, Rafael Teixeira and Panda, Rameswar and Doveh, Sivan and Gupta, Shubham and Das, Subhro and Zawad, Syed and Kim, Yusik and He, Zexue and Brooks, Alexander and Goodhart, Gabe and Govindjee, Anita and Leist, Derek and Ibrahim, Ibrahim and Soffer, Aya and Cox, David and Soule, Kate and Lastras, Luis and Desai, Nirmit and Ofek-koifman, Shila and Raghavan, Sriram and Syeda-Mahmood, Tanveer and Staar, Peter and Drory, Tal and Feris, Rogerio},\n  journal={arXiv preprint arXiv:2502.09927},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#intfloatmme5-mllama-11b-instruct","title":"<code>intfloat/mmE5-mllama-11b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 4096 10.6B 19.8 GB 2025-02-12 eng-Latn Citation <pre><code>@article{chen2025mmE5,\n  title={mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data},\n  author={Chen, Haonan and Wang, Liang and Yang, Nan and Zhu, Yutao and Zhao, Ziliang and Wei, Furu and Dou, Zhicheng},\n  journal={arXiv preprint arXiv:2502.08468},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#jinaaijina-clip-v1","title":"<code>jinaai/jina-clip-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 222.6M 849.0 MB 2024-05-30 eng-Latn Citation <pre><code>@article{koukounas2024jinaclip,\n  title={Jina CLIP: Your CLIP Model Is Also Your Text Retriever},\n  author={Koukounas, Andreas and Mastrapas, Georgios and G\u00fcnther, Michael and Wang, Bo and Martens, Scott and Mohr, Isabelle and Sturua, Saba and Akram, Mohammad Kalim and Mart\u00ednez, Joan Fontanals and Ognawala, Saahil and Guzman, Susana and Werk, Maximilian and Wang, Nan and Xiao, Han},\n  journal={arXiv preprint arXiv:2405.20204},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#jinaaijina-embeddings-v4","title":"<code>jinaai/jina-embeddings-v4</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 3.9B 7.3 GB 2025-06-24 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n      title={jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n      author={Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Sedigheh Eslami and Scott Martens and Bo Wang and Nan Wang and Han Xiao},\n      year={2025},\n      eprint={2506.18902},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2506.18902},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-b-16","title":"<code>microsoft/LLM2CLIP-Openai-B-16</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 360.6M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-l-14-224","title":"<code>microsoft/LLM2CLIP-Openai-L-14-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 578.0M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#microsoftllm2clip-openai-l-14-336","title":"<code>microsoft/LLM2CLIP-Openai-L-14-336</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1280 578.6M not specified 2024-11-07 eng-Latn Citation <pre><code>@misc{huang2024llm2clippowerfullanguagemodel,\n  title={LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation},\n  author={Weiquan Huang and Aoqi Wu and Yifan Yang and Xufang Luo and Yuqing Yang and Liang Hu and Qi Dai and Xiyang Dai and Dongdong Chen and Chong Luo and Lili Qiu},\n  year={2024},\n  eprint={2411.04997},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV},\n  url={https://arxiv.org/abs/2411.04997}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-aicolnomic-embed-multimodal-3b","title":"<code>nomic-ai/colnomic-embed-multimodal-3b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 7.0 GB 2025-03-31 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{nomicembedmultimodal2025,\n  title={Nomic Embed Multimodal: Interleaved Text, Image, and Screenshots for Visual Document Retrieval},\n  author={Nomic Team},\n  year={2025},\n  publisher={Nomic AI},\n  url={https://nomic.ai/blog/posts/nomic-embed-multimodal}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-aicolnomic-embed-multimodal-7b","title":"<code>nomic-ai/colnomic-embed-multimodal-7b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 7.0B 14.1 GB 2025-03-31 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{nomicembedmultimodal2025,\n  title={Nomic Embed Multimodal: Interleaved Text, Image, and Screenshots for Visual Document Retrieval},\n  author={Nomic Team},\n  year={2025},\n  publisher={Nomic AI},\n  url={https://nomic.ai/blog/posts/nomic-embed-multimodal}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nomic-ainomic-embed-vision-v15","title":"<code>nomic-ai/nomic-embed-vision-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 92.9M 355.0 MB 2024-06-08 eng-Latn Citation <pre><code>@article{nussbaum2024nomicembedvision,\n      title={Nomic Embed Vision: Expanding the Latent Space},\n      author={Nussbaum, Zach and Duderstadt, Brandon and Mulyar, Andriy},\n      journal={arXiv preprint arXiv:2406.18587},\n      year={2024},\n      eprint={2406.18587},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2406.18587}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidiallama-nemoretriever-colembed-1b-v1","title":"<code>nvidia/llama-nemoretriever-colembed-1b-v1</code>","text":"<p>License: https://huggingface.co/nvidia/llama-nemoretriever-colembed-1b-v1/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.4B 4.5 GB 2025-06-27 eng-Latn Citation <pre><code>@misc{xu2025llamanemoretrievercolembedtopperforming,\n      title={Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model},\n      author={Mengyao Xu and Gabriel Moreira and Ronay Ak and Radek Osmulski and Yauhen Babakhin and Zhiding Yu and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2507.05513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.05513}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidiallama-nemoretriever-colembed-3b-v1","title":"<code>nvidia/llama-nemoretriever-colembed-3b-v1</code>","text":"<p>License: https://huggingface.co/nvidia/llama-nemoretriever-colembed-1b-v1/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3072 4.4B 8.2 GB 2025-06-27 eng-Latn Citation <pre><code>@misc{xu2025llamanemoretrievercolembedtopperforming,\n      title={Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model},\n      author={Mengyao Xu and Gabriel Moreira and Ronay Ak and Radek Osmulski and Yauhen Babakhin and Zhiding Yu and Benedikt Schifferer and Even Oldridge},\n      year={2025},\n      eprint={2507.05513},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2507.05513}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidiallama-nemotron-colembed-vl-3b-v2","title":"<code>nvidia/llama-nemotron-colembed-vl-3b-v2</code>","text":"<p>License: https://huggingface.co/nvidia/llama-nemotron-colembed-vl-3b-v2/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3072 4.4B 8.2 GB 2026-01-21 eng-Latn Citation <pre><code>@misc{moreira2026nemotroncolembedv2topperforming,\n    title={Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval},\n    author={Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Oliver Holworthy and Benedikt Schifferer and Zhiding Yu and Yauhen Babakhin and Radek Osmulski and Jiarui Cai and Ryan Chesler and Bo Liu and Even Oldridge},\n    year={2026},\n    eprint={2602.03992},\n    archivePrefix={arXiv},\n    primaryClass={cs.IR},\n    url={https://arxiv.org/abs/2602.03992},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidianemotron-colembed-vl-4b-v2","title":"<code>nvidia/nemotron-colembed-vl-4b-v2</code>","text":"<p>License: https://huggingface.co/nvidia/nemotron-colembed-vl-4b-v2/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 2560 4.8B 9.0 GB 2026-01-07 eng-Latn Citation <pre><code>@misc{moreira2026nemotroncolembedv2topperforming,\n    title={Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval},\n    author={Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Oliver Holworthy and Benedikt Schifferer and Zhiding Yu and Yauhen Babakhin and Radek Osmulski and Jiarui Cai and Ryan Chesler and Bo Liu and Even Oldridge},\n    year={2026},\n    eprint={2602.03992},\n    archivePrefix={arXiv},\n    primaryClass={cs.IR},\n    url={https://arxiv.org/abs/2602.03992},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#nvidianemotron-colembed-vl-8b-v2","title":"<code>nvidia/nemotron-colembed-vl-8b-v2</code>","text":"<p>License: https://huggingface.co/nvidia/nemotron-colembed-vl-8b-v2/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 262.1K 4096 8.7B 16.3 GB 2026-01-07 eng-Latn Citation <pre><code>@misc{moreira2026nemotroncolembedv2topperforming,\n    title={Nemotron ColEmbed V2: Top-Performing Late Interaction embedding models for Visual Document Retrieval},\n    author={Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Oliver Holworthy and Benedikt Schifferer and Zhiding Yu and Yauhen Babakhin and Radek Osmulski and Jiarui Cai and Ryan Chesler and Bo Liu and Even Oldridge},\n    year={2026},\n    eprint={2602.03992},\n    archivePrefix={arXiv},\n    primaryClass={cs.IR},\n    url={https://arxiv.org/abs/2602.03992},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#royokonge5-v","title":"<code>royokong/e5-v</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.4B 15.6 GB 2024-07-17 eng-Latn Citation <pre><code>@article{jiang2024e5v,\n      title={E5-V: Universal Embeddings with Multimodal Large Language Models},\n      author={Jiang, Ting and Song, Minghui and Zhang, Zihan and Huang, Haizhen and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},\n      journal={arXiv preprint arXiv:2407.12580},\n      year={2024},\n      eprint={2407.12580},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.12580}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolsmol-256m","title":"<code>vidore/colSmol-256M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 128 256.0M 800.0 MB 2025-01-22 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolsmol-500m","title":"<code>vidore/colSmol-500M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 128 500.0M 1.2 GB 2025-01-22 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v11","title":"<code>vidore/colpali-v1.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-08-21 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v12","title":"<code>vidore/colpali-v1.2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-08-26 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolpali-v13","title":"<code>vidore/colpali-v1.3</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.4K 128 2.9B 4.6 GB 2024-11-01 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolqwen2-v10","title":"<code>vidore/colqwen2-v1.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 128 2.2B 7.0 GB 2025-11-03 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#vidorecolqwen25-v02","title":"<code>vidore/colqwen2.5-v0.2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 128 3.0B 7.0 GB 2025-01-31 eng-Latn Citation <pre><code>@misc{faysse2024colpali,\n  title={ColPali: Efficient Document Retrieval with Vision Language Models},\n  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C'eline and Colombo, Pierre},\n  year={2024},\n  eprint={2407.01449},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/image_text/#baaibge-visualized-base","title":"<code>BAAI/bge-visualized-base</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 196.0M 1.6 GB 2024-06-06 eng-Latn Citation <pre><code>@article{zhou2024vista,\n  title={VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval},\n  author={Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping},\n  journal={arXiv preprint arXiv:2406.04292},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#baaibge-visualized-m3","title":"<code>BAAI/bge-visualized-m3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 872.9M 4.2 GB 2024-06-06 eng-Latn Citation <pre><code>@article{zhou2024vista,\n  title={VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval},\n  author={Zhou, Junjie and Liu, Zheng and Xiao, Shitao and Zhao, Bo and Xiong, Yongping},\n  journal={arXiv preprint arXiv:2406.04292},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40","title":"<code>Cohere/Cohere-embed-v4.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40-output_dtypebinary","title":"<code>Cohere/Cohere-embed-v4.0 (output_dtype=binary)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#coherecohere-embed-v40-output_dtypeint8","title":"<code>Cohere/Cohere-embed-v4.0 (output_dtype=int8)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1536 not specified not specified 2024-12-01 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/image_text/#quansuneva02-clip-b-16","title":"<code>QuanSun/EVA02-CLIP-B-16</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 149.0M 568.0 MB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-l-14","title":"<code>QuanSun/EVA02-CLIP-L-14</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 428.0M 1.6 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-bige-14","title":"<code>QuanSun/EVA02-CLIP-bigE-14</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 4.7B 17.5 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#quansuneva02-clip-bige-14-plus","title":"<code>QuanSun/EVA02-CLIP-bigE-14-plus</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 5.0B 18.6 GB 2023-04-26 eng-Latn Citation <pre><code>@article{EVA-CLIP,\n      title={EVA-CLIP: Improved Training Techniques for CLIP at Scale},\n      author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},\n      journal={arXiv preprint arXiv:2303.15389},\n      year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-image-captioning-base","title":"<code>Salesforce/blip-image-captioning-base</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 224.7M 942.0 MB 2023-08-01 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-image-captioning-large","title":"<code>Salesforce/blip-image-captioning-large</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 446.1M 1.8 GB 2023-12-07 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-itm-base-coco","title":"<code>Salesforce/blip-itm-base-coco</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 223.7M 942.0 MB 2023-08-01 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-itm-base-flickr","title":"<code>Salesforce/blip-itm-base-flickr</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 223.7M 942.0 MB 2023-08-01 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-itm-large-coco","title":"<code>Salesforce/blip-itm-large-coco</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 446.1M 1.8 GB 2023-08-01 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-itm-large-flickr","title":"<code>Salesforce/blip-itm-large-flickr</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 446.1M 1.8 GB 2023-08-01 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-vqa-base","title":"<code>Salesforce/blip-vqa-base</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 384.7M 1.4 GB 2023-12-07 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip-vqa-capfilt-large","title":"<code>Salesforce/blip-vqa-capfilt-large</code>","text":"<p>License: bsd-3-clause</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 384.7M 942.0 MB 2023-01-22 eng-Latn Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2201.12086,\n    doi = {10.48550/ARXIV.2201.12086},\n    url = {https://arxiv.org/abs/2201.12086},\n    author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n    keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n    title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n    publisher = {arXiv},\n    year = {2022},\n    copyright = {Creative Commons Attribution 4.0 International}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip2-opt-27b","title":"<code>Salesforce/blip2-opt-2.7b</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 3.7B 14.0 GB 2024-03-22 eng-Latn Citation <pre><code>@inproceedings{li2023blip2,\n    title={{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},\n    author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},\n    year={2023},\n    booktitle={ICML},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#salesforceblip2-opt-67b-coco","title":"<code>Salesforce/blip2-opt-6.7b-coco</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 7.8B 28.9 GB 2024-03-31 eng-Latn Citation <pre><code>@inproceedings{li2023blip2,\n    title={{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},\n    author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},\n    year={2023},\n    booktitle={ICML},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#cohereembed-english-v30","title":"<code>cohere/embed-english-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 not specified not specified 2024-10-24 eng-Latn"},{"location":"overview/available_models/image_text/#cohereembed-multilingual-v30","title":"<code>cohere/embed-multilingual-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 not specified not specified 2024-10-24 not specified"},{"location":"overview/available_models/image_text/#facebookmetaclip-2-mt5-worldwide-b32","title":"<code>facebook/metaclip-2-mt5-worldwide-b32</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 254.0M 969.0 MB 2025-11-12 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{xu2025metaclip2,\n  title={MetaCLIP 2: A Worldwide Scaling Recipe},\n  author={Xu, Hu and Xie, Saining and Ghosh, Gargi and Kira, Zsolt and Darrell, Trevor},\n  journal={arXiv preprint arXiv:2507.22062},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-224","title":"<code>google/siglip-base-patch16-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.2M 775.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-256","title":"<code>google/siglip-base-patch16-256</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.2M 775.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-256-multilingual","title":"<code>google/siglip-base-patch16-256-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 370.6M 1.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-384","title":"<code>google/siglip-base-patch16-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.4M 776.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-base-patch16-512","title":"<code>google/siglip-base-patch16-512</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 203.8M 777.0 MB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-large-patch16-256","title":"<code>google/siglip-large-patch16-256</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1024 652.2M 2.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-large-patch16-384","title":"<code>google/siglip-large-patch16-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1024 652.5M 2.4 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch14-224","title":"<code>google/siglip-so400m-patch14-224</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16 1152 877.4M 3.3 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch14-384","title":"<code>google/siglip-so400m-patch14-384</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1152 878.0M 3.3 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#googlesiglip-so400m-patch16-256-i18n","title":"<code>google/siglip-so400m-patch16-256-i18n</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 1152 1.1B 4.2 GB 2024-01-08 eng-Latn Citation <pre><code>@misc{zhai2023sigmoid,\n      title={Sigmoid Loss for Language Image Pre-Training},\n      author={Xiaohua Zhai and Basil Mustafa and Alexander Kolesnikov and Lucas Beyer},\n      year={2023},\n      eprint={2303.15343},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#jinaaijina-clip-v2","title":"<code>jinaai/jina-clip-v2</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 865.3M 1.6 GB 2024-10-09 eng-Latn Citation <pre><code>@misc{koukounas2024jinaclipv2multilingualmultimodalembeddings,\n      title={jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images},\n      author={Andreas Koukounas and Georgios Mastrapas and Bo Wang and Mohammad Kalim Akram and Sedigheh Eslami and Michael G\u00fcnther and Isabelle Mohr and Saba Sturua and Scott Martens and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2412.08802},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.08802},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#kakaobrainalign-base","title":"<code>kakaobrain/align-base</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 64 768 172.1M 671.0 MB 2023-02-24 eng-Latn Citation <pre><code>@misc{kakaobrain2022coyo-align,\n    title         = {COYO-ALIGN},\n    author        = {Yoon, Boogeo and Lee, Youhan and Baek, Woonhyuk},\n    year          = {2022},\n    howpublished  = {https://github.com/kakaobrain/coyo-align},\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-b-16-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-B-16-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 150.0M 572.0 MB 2023-04-26 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-b-32-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-B-32-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.0M 576.0 MB 2023-04-26 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-b-32-laion2b-s34b-b79k","title":"<code>laion/CLIP-ViT-B-32-laion2B-s34B-b79K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.3M 577.0 MB 2022-09-15 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-h-14-laion2b-s32b-b79k","title":"<code>laion/CLIP-ViT-H-14-laion2B-s32B-b79K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 986.1M 3.7 GB 2022-09-15 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-l-14-datacompxl-s13b-b90k","title":"<code>laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 427.6M 1.6 GB 2023-04-26 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-l-14-laion2b-s32b-b82k","title":"<code>laion/CLIP-ViT-L-14-laion2B-s32B-b82K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 427.6M 1.6 GB 2022-09-15 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-bigg-14-laion2b-39b-b160k","title":"<code>laion/CLIP-ViT-bigG-14-laion2B-39B-b160k</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1280 2.5B 9.5 GB 2023-01-23 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#laionclip-vit-g-14-laion2b-s34b-b88k","title":"<code>laion/CLIP-ViT-g-14-laion2B-s34B-b88K</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 1024 1.4B 5.1 GB 2023-03-06 eng-Latn Citation <pre><code>@inproceedings{cherti2023reproducible,\n    title={Reproducible scaling laws for contrastive language-image learning},\n    author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n    pages={2818--2829},\n    year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#openaiclip-vit-base-patch16","title":"<code>openai/clip-vit-base-patch16</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 149.6M 576.0 MB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#openaiclip-vit-base-patch32","title":"<code>openai/clip-vit-base-patch32</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 512 151.3M 576.0 MB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#openaiclip-vit-large-patch14","title":"<code>openai/clip-vit-large-patch14</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 77 768 427.6M 1.6 GB 2021-02-26 eng-Latn Citation <pre><code>@article{radford2021learning,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},\n  journal={arXiv preprint arXiv:2103.00020},\n  year={2021}\n}\n</code></pre>"},{"location":"overview/available_models/image_text/#voyageaivoyage-multimodal-3","title":"<code>voyageai/voyage-multimodal-3</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 not specified not specified 2024-11-10 not specified"},{"location":"overview/available_models/text/","title":"Text Model","text":"<ul> <li>Number of models: 240</li> </ul>"},{"location":"overview/available_models/text/#instruction-model","title":"Instruction Model","text":""},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen15-7b-instruct","title":"<code>Alibaba-NLP/gte-Qwen1.5-7B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 28.8 GB 2024-04-20 eng-Latn Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen2-15b-instruct","title":"<code>Alibaba-NLP/gte-Qwen2-1.5B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 8960 1.5B 6.6 GB 2024-07-29 eng-Latn Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#alibaba-nlpgte-qwen2-7b-instruct","title":"<code>Alibaba-NLP/gte-Qwen2-7B-instruct</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.1B 28.4 GB 2024-06-15 not specified Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-en","title":"<code>BAAI/bge-base-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-08-05 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-en-v15","title":"<code>BAAI/bge-base-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-09-11 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-zh","title":"<code>BAAI/bge-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 390.0 MB 2023-08-05 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-base-zh-v15","title":"<code>BAAI/bge-base-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 416.0 MB 2023-09-11 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-large-en","title":"<code>BAAI/bge-large-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-08-05 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-large-en-v15","title":"<code>BAAI/bge-large-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-09-12 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-large-zh","title":"<code>BAAI/bge-large-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-08-02 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-large-zh-v15","title":"<code>BAAI/bge-large-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-09-12 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-small-en","title":"<code>BAAI/bge-small-en</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-08-05 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-small-en-v15","title":"<code>BAAI/bge-small-en-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-09-12 eng-Latn Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-small-zh","title":"<code>BAAI/bge-small-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 127.0 MB 2023-08-05 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-small-zh-v15","title":"<code>BAAI/bge-small-zh-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 33.4M 91.0 MB 2023-09-12 zho-Hans Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-1b","title":"<code>BMRetriever/BMRetriever-1B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 908.8M 3.4 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-2b","title":"<code>BMRetriever/BMRetriever-2B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.5B 9.3 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-410m","title":"<code>BMRetriever/BMRetriever-410M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 353.8M 1.3 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#bmretrieverbmretriever-7b","title":"<code>BMRetriever/BMRetriever-7B</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-04-29 eng-Latn Citation <pre><code>@inproceedings{xu-etal-2024-bmretriever,\n    title = \"{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers\",\n    author = \"Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"November\",\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    pages = \"22234--22254\",\n    url = \"https://aclanthology.org/2024.emnlp-main.1241/\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#beastyze5-r-mistral-7b","title":"<code>BeastyZ/e5-R-mistral-7b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 27.0 GB 2024-06-28 eng-Latn"},{"location":"overview/available_models/text/#bytedance-seedseed15-embedding","title":"<code>ByteDance-Seed/Seed1.5-Embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 not specified not specified 2025-04-25 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#bytedanceseed16-embedding","title":"<code>Bytedance/Seed1.6-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 not specified not specified 2025-06-18 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#bytedanceseed16-embedding-1215","title":"<code>Bytedance/Seed1.6-embedding-1215</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 not specified not specified 2025-12-15 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)"},{"location":"overview/available_models/text/#coherecohere-embed-english-light-v30","title":"<code>Cohere/Cohere-embed-english-light-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#coherecohere-embed-english-v30","title":"<code>Cohere/Cohere-embed-english-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#coherecohere-embed-multilingual-light-v30","title":"<code>Cohere/Cohere-embed-multilingual-light-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#coherecohere-embed-multilingual-v30","title":"<code>Cohere/Cohere-embed-multilingual-v3.0</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 512 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#geogpt-research-projectgeoembedding","title":"<code>GeoGPT-Research-Project/GeoEmbedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 27.0 GB 2025-04-22 eng-Latn"},{"location":"overview/available_models/text/#gritlmgritlm-7b","title":"<code>GritLM/GritLM-7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 13.5 GB 2024-02-15 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{muennighoff2024generative,\n      title={Generative Representational Instruction Tuning},\n      author={Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},\n      year={2024},\n      eprint={2402.09906},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#gritlmgritlm-8x7b","title":"<code>GritLM/GritLM-8x7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 32768 57.9B 87.0 GB 2024-02-15 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>@misc{muennighoff2024generative,\n      title={Generative Representational Instruction Tuning},\n      author={Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},\n      year={2024},\n      eprint={2402.09906},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v1","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-10-23 eng-Latn, zho-Hans Citation <pre><code>@misc{hu2025kalmembedding,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n  year={2025},\n  eprint={2501.01028},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v15","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-12-26 eng-Latn, zho-Hans Citation <pre><code>@misc{hu2025kalmembedding,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n  year={2025},\n  eprint={2501.01028},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-instruct-v2","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 942.0 MB 2025-06-25 eng-Latn, zho-Hans Citation <pre><code>@misc{hu2025kalmembedding,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n  year={2025},\n  eprint={2501.01028},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#ict-time-and-queritboom_4b_v1","title":"<code>ICT-TIME-and-Querit/BOOM_4B_v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.0B 7.5 GB 2026-01-31 ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (18)"},{"location":"overview/available_models/text/#ieityuanyuan-embedding-20-en","title":"<code>IEITYuan/Yuan-embedding-2.0-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 595.8M 2.2 GB 2025-11-27 eng-Latn"},{"location":"overview/available_models/text/#kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25","title":"<code>KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2025-09-30 eng-Latn, zho-Hans Citation <pre><code>@misc{zhao2025kalmembeddingv2,\n      title={KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model},\n      author={Xinping Zhao and Xinshuo Hu and Zifei Shan and Shouzheng Huang and Yao Zhou and Xin Zhang and Zetian Sun and Zhenyu Liu and Dongfang Li and Xinyuan Wei and Youcheng Pan and Yang Xiang and Meishan Zhang and Haofen Wang and Jun Yu and Baotian Hu and Min Zhang},\n      year={2025},\n      eprint={2506.20923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2506.20923},\n}\n\n@misc{hu2025kalmembedding,\n      title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n      author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n      year={2025},\n      eprint={2501.01028},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#kingsoft-llmqzhou-embedding","title":"<code>Kingsoft-LLM/QZhou-Embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3584 7.1B 14.1 GB 2025-08-24 eng-Latn, zho-Hans Citation <pre><code>@misc{yu2025qzhouembeddingtechnicalreport,\n      title={QZhou-Embedding Technical Report},\n      author={Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu},\n      year={2025},\n      eprint={2508.21632},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.21632},\n}\n</code></pre>"},{"location":"overview/available_models/text/#kingsoft-llmqzhou-embedding-zh","title":"<code>Kingsoft-LLM/QZhou-Embedding-Zh</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1792 7.6B 28.7 GB 2025-09-28 zho-Hans Citation <pre><code>@misc{yu2025qzhouembeddingtechnicalreport,\n      title={QZhou-Embedding Technical Report},\n      author={Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu},\n      year={2025},\n      eprint={2508.21632},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.21632},\n}\n</code></pre>"},{"location":"overview/available_models/text/#linq-ai-researchlinq-embed-mistral","title":"<code>Linq-AI-Research/Linq-Embed-Mistral</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-05-29 eng-Latn Citation <pre><code>@misc{LinqAIResearch2024,\n  title={Linq-Embed-Mistral:Elevating Text Retrieval with Improved GPT Data Through Task-Specific Control and Quality Refinement},\n  author={Junseong Kim and Seolhwa Lee and Jihoon Kwon and Sangmo Gu and Yejin Kim and Minkyung Cho and Jy-yong Sohn and Chanyeol Choi},\n  howpublished={Linq AI Research Blog},\n  year={2024},\n  url={https://getlinq.com/blog/linq-embed-mistral/}\n}\n</code></pre>"},{"location":"overview/available_models/text/#maniaclabsminiac-embed","title":"<code>ManiacLabs/miniac-embed</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 33.4M 127.0 MB 2026-02-19 eng-Latn"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 6.6B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 6.6B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 7.5B 28.0 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 7.5B 28.0 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.2B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-sheared-llama-mntp-supervised","title":"<code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 1.3B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse","title":"<code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 1.3B 26.5 GB 2024-04-09 eng-Latn Citation <pre><code>@misc{behnamghader2024llm2veclargelanguagemodels,\n      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},\n      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},\n      year={2024},\n      eprint={2404.05961},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2404.05961},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mongodbmdbr-leaf-ir","title":"<code>MongoDB/mdbr-leaf-ir</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 22.7M 86.0 MB 2025-08-27 eng-Latn Citation <pre><code>@misc{mdbr_leaf,\n  title={LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations},\n  author={Robin Vujanic and Thomas Rueckstiess},\n  year={2025},\n  eprint={2509.12539},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR},\n  url={https://arxiv.org/abs/2509.12539}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mongodbmdbr-leaf-mt","title":"<code>MongoDB/mdbr-leaf-mt</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 22.7M 86.0 MB 2025-08-27 eng-Latn Citation <pre><code>@misc{mdbr_leaf,\n  title={LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations},\n  author={Robin Vujanic and Thomas Rueckstiess},\n  year={2025},\n  eprint={2509.12539},\n  archivePrefix={arXiv},\n  primaryClass={cs.IR},\n  url={https://arxiv.org/abs/2509.12539}\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchjasper_en_vision_language_v1","title":"<code>NovaSearch/jasper_en_vision_language_v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 8960 1.6B 3.7 GB 2024-12-11 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchstella_en_15b_v5","title":"<code>NovaSearch/stella_en_1.5B_v5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 8960 1.5B 5.7 GB 2024-07-12 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#novasearchstella_en_400m_v5","title":"<code>NovaSearch/stella_en_400M_v5</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 435.0M 1.6 GB 2024-07-12 eng-Latn Citation <pre><code>@misc{zhang2025jasperstelladistillationsota,\n      title={Jasper and Stella: distillation of SOTA embedding models},\n      author={Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang},\n      year={2025},\n      eprint={2412.19048},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2412.19048},\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-06b","title":"<code>Qwen/Qwen3-Embedding-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 595.8M 1.1 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-4b","title":"<code>Qwen/Qwen3-Embedding-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.0B 7.5 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#qwenqwen3-embedding-8b","title":"<code>Qwen/Qwen3-Embedding-8B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.6B 14.1 GB 2025-06-05 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#reasonirreasonir-8b","title":"<code>ReasonIR/ReasonIR-8B</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K 4096 7.5B not specified 2025-04-29 eng-Latn Citation <pre><code>@article{shao2025reasonir,\n      title={ReasonIR: Training Retrievers for Reasoning Tasks},\n      author={Rulin Shao and Rui Qiao and Varsha Kishore and Niklas Muennighoff and Xi Victoria Lin and Daniela Rus and Bryan Kian Hsiang Low and Sewon Min and Wen-tau Yih and Pang Wei Koh and Luke Zettlemoyer},\n      year={2025},\n      journal={arXiv preprint arXiv:2504.20595},\n      url={https://arxiv.org/abs/2504.20595},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sailesh97hinvec","title":"<code>Sailesh97/Hinvec</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 939.6M 3.6 GB 2025-06-19 eng-Latn, hin-Deva"},{"location":"overview/available_models/text/#salesforcesfr-embedding-2_r","title":"<code>Salesforce/SFR-Embedding-2_R</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-06-14 eng-Latn Citation <pre><code>@misc{SFR-embedding-2,\n      title={SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training},\n      author={Rui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},\n      year={2024},\n      url={https://huggingface.co/Salesforce/SFR-Embedding-2_R}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#salesforcesfr-embedding-code-2b_r","title":"<code>Salesforce/SFR-Embedding-Code-2B_R</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2304 2.6B 4.9 GB 2025-01-17 eng-Latn Citation <pre><code>@article{liu2024codexembed,\n  title={CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval},\n  author={Liu, Ye and Meng, Rui and Jot, Shafiq and Savarese, Silvio and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih},\n  journal={arXiv preprint arXiv:2411.12644},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#salesforcesfr-embedding-mistral","title":"<code>Salesforce/SFR-Embedding-Mistral</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-01-24 eng-Latn Citation <pre><code>    @misc{SFRAIResearch2024,\n  title={SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning},\n  author={Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz},\n  howpublished={Salesforce AI Research Blog},\n  year={2024},\n  url={https://www.salesforce.com/blog/sfr-embedding/}\n}\n</code></pre>"},{"location":"overview/available_models/text/#samilpwc-axnode-genaipwc-embedding_expr","title":"<code>SamilPwC-AXNode-GenAI/PwC-Embedding_expr</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2025-08-12 kor-Hang"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-l","title":"<code>Snowflake/snowflake-arctic-embed-l</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 1.2 GB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-l-v20","title":"<code>Snowflake/snowflake-arctic-embed-l-v2.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-12-04 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74) Citation <pre><code>@article{yu2024arctic,\n      title={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\n      author={Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel},\n      journal={arXiv preprint arXiv:2412.04506},\n      year={2024},\n      eprint={2412.04506},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2412.04506}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m","title":"<code>Snowflake/snowflake-arctic-embed-m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 415.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-long","title":"<code>Snowflake/snowflake-arctic-embed-m-long</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 137.0M 522.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-v15","title":"<code>Snowflake/snowflake-arctic-embed-m-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 415.0 MB 2024-07-08 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-m-v20","title":"<code>Snowflake/snowflake-arctic-embed-m-v2.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 305.0M 1.1 GB 2024-12-04 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74) Citation <pre><code>@article{yu2024arctic,\n      title={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\n      author={Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel},\n      journal={arXiv preprint arXiv:2412.04506},\n      year={2024},\n      eprint={2412.04506},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2412.04506}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-s","title":"<code>Snowflake/snowflake-arctic-embed-s</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-04-12 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#snowflakesnowflake-arctic-embed-xs","title":"<code>Snowflake/snowflake-arctic-embed-xs</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 86.0 MB 2024-07-08 eng-Latn Citation <pre><code>@article{merrick2024embedding,\n      title={Embedding And Clustering Your Data Can Improve Contrastive Pretraining},\n      author={Merrick, Luke},\n      journal={arXiv preprint arXiv:2407.18887},\n      year={2024},\n      eprint={2407.18887},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2407.18887}\n}\n</code></pre>"},{"location":"overview/available_models/text/#tarka-airtarka-embedding-150m-v1","title":"<code>Tarka-AIR/Tarka-Embedding-150M-V1</code>","text":"<p>License: gemma</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 155.7M 576.0 MB 2025-11-04 arb-Arab, deu-Latn, eng-Latn, fra-Latn, jpn-Jpan, ... (8) Citation <pre><code>@misc{tarka_ai_research_2025,\n    author       = { Tarka AI Research },\n    title        = { Tarka-Embedding-150M-V1 (Revision c5f4f43) },\n    year         = 2025,\n    url          = { https://huggingface.co/Tarka-AIR/Tarka-Embedding-150M-V1 },\n    doi          = { 10.57967/hf/6875 },\n    publisher    = { Hugging Face }\n}\n</code></pre>"},{"location":"overview/available_models/text/#tarka-airtarka-embedding-350m-v1","title":"<code>Tarka-AIR/Tarka-Embedding-350M-V1</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128.0K 1024 354.5M 676.0 MB 2025-11-11 arb-Arab, deu-Latn, eng-Latn, fra-Latn, jpn-Jpan, ... (8) Citation <pre><code>@misc{tarka_ai_research_2025,\n    author       = { Tarka AI Research },\n    title        = { Tarka-Embedding-350M-V1 (Revision f4b5de8) },\n    year         = 2025,\n    url          = { https://huggingface.co/Tarka-AIR/Tarka-Embedding-350M-V1 },\n    doi          = { 10.57967/hf/6979 },\n    publisher    = { Hugging Face }\n}\n</code></pre>"},{"location":"overview/available_models/text/#tencentbacconan-embedding-v2","title":"<code>TencentBAC/Conan-embedding-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 not specified not specified 2025-04-10 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#vplabssearchmap_preview","title":"<code>VPLabs/SearchMap_Preview</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 435.0M 1.6 GB 2025-03-05 eng-Latn Citation <pre><code>@misc{vectorpath2025searchmap,\n  title={SearchMap: Conversational E-commerce Search Embedding Model},\n  author={VectorPath Research Team},\n  year={2025},\n  publisher={Hugging Face},\n  journal={HuggingFace Model Hub},\n}\n</code></pre>"},{"location":"overview/available_models/text/#whereisaiuae-large-v1","title":"<code>WhereIsAI/UAE-Large-V1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 1.2 GB 2023-12-04 eng-Latn Citation <pre><code>    @article{li2023angle,\n      title={AnglE-optimized Text Embeddings},\n      author={Li, Xianming and Li, Jing},\n      journal={arXiv preprint arXiv:2309.12871},\n      year={2023}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#ai-foreverfrida","title":"<code>ai-forever/FRIDA</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 823.0M 3.1 GB 2024-12-29 rus-Cyrl"},{"location":"overview/available_models/text/#ai-foreverru-en-rosberta","title":"<code>ai-forever/ru-en-RoSBERTa</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 404.0M 1.5 GB 2024-07-29 rus-Cyrl Citation <pre><code>@misc{snegirev2024russianfocusedembeddersexplorationrumteb,\n      title={The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design},\n      author={Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov},\n      year={2024},\n      eprint={2408.12503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2408.12503},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#ai-sagegiga-embeddings-instruct","title":"<code>ai-sage/Giga-Embeddings-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 2048 3.2B 12.6 GB 2025-09-23 eng-Latn, rus-Cyrl"},{"location":"overview/available_models/text/#annamodelslgai-embedding-preview","title":"<code>annamodels/LGAI-Embedding-Preview</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2025-06-11 eng-Latn Citation <pre><code>@misc{choi2025lgaiembeddingpreviewtechnicalreport,\n      title={LGAI-EMBEDDING-Preview Technical Report},\n      author={Jooyoung Choi and Hyun Kim and Hansol Jang and Changwook Jun and Kyunghoon Bae and Hyewon Choi and Stanley Jungkyu Choi and Honglak Lee and Chulmin Yun},\n      year={2025},\n      eprint={2506.07438},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2506.07438},\n}\n</code></pre>"},{"location":"overview/available_models/text/#bedrockcohere-embed-english-v3","title":"<code>bedrock/cohere-embed-english-v3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 eng-Latn"},{"location":"overview/available_models/text/#bedrockcohere-embed-multilingual-v3","title":"<code>bedrock/cohere-embed-multilingual-v3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2023-11-02 afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)"},{"location":"overview/available_models/text/#bflhcmod-embedding","title":"<code>bflhc/MoD-Embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.0B 7.5 GB 2025-12-14 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{mod-embedding-2025,\n  title={MoD-Embedding: A Fine-tuned Multilingual Text Embedding Model},\n  author={MoD Team},\n  year={2025},\n  url={https://huggingface.co/bflhc/MoD-Embedding}\n}\n</code></pre>"},{"location":"overview/available_models/text/#bflhcocten-embedding-06b","title":"<code>bflhc/Octen-Embedding-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 595.8M 1.1 GB 2026-01-10 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{octen-embedding-2025,\n  title={Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model},\n  author={Octen Team},\n  year={2025},\n  url={https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}\n}\n</code></pre>"},{"location":"overview/available_models/text/#bflhcocten-embedding-4b","title":"<code>bflhc/Octen-Embedding-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2560 4.0B 7.5 GB 2025-12-30 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{octen-embedding-2025,\n  title={Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model},\n  author={Octen Team},\n  year={2025},\n  url={https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}\n}\n</code></pre>"},{"location":"overview/available_models/text/#bflhcocten-embedding-8b","title":"<code>bflhc/Octen-Embedding-8B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.6B 14.1 GB 2025-12-23 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{octen-embedding-2025,\n  title={Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model},\n  author={Octen Team},\n  year={2025},\n  url={https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}\n}\n</code></pre>"},{"location":"overview/available_models/text/#castorinirepllama-v1-7b-lora-passage","title":"<code>castorini/repllama-v1-7b-lora-passage</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 27.0 MB 2023-10-11 eng-Latn Citation <pre><code>@article{rankllama,\n      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval},\n      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},\n      year={2023},\n      journal={arXiv:2310.08319},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-base","title":"<code>cl-nagoya/ruri-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 111.2M 212.0 MB 2024-08-28 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-base-v2","title":"<code>cl-nagoya/ruri-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 111.2M 424.0 MB 2024-12-05 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-large","title":"<code>cl-nagoya/ruri-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 337.4M 644.0 MB 2024-08-28 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-large-v2","title":"<code>cl-nagoya/ruri-large-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 337.4M 1.3 GB 2024-12-06 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-small","title":"<code>cl-nagoya/ruri-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 68.1M 130.0 MB 2024-08-28 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-small-v2","title":"<code>cl-nagoya/ruri-small-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 68.1M 260.0 MB 2024-12-05 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-v3-130m","title":"<code>cl-nagoya/ruri-v3-130m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 132.1M 504.0 MB 2025-04-09 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-v3-30m","title":"<code>cl-nagoya/ruri-v3-30m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 256 36.7M 140.0 MB 2025-04-07 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-v3-310m","title":"<code>cl-nagoya/ruri-v3-310m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 314.6M 1.2 GB 2025-04-09 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#cl-nagoyaruri-v3-70m","title":"<code>cl-nagoya/ruri-v3-70m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 256 70.0M 140.0 MB 2025-04-09 jpn-Jpan Citation <pre><code>@misc{Ruri,\n  title={{Ruri: Japanese General Text Embeddings}},\n  author={Hayato Tsukagoshi and Ryohei Sasano},\n  year={2024},\n  eprint={2409.07737},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2409.07737},\n}\n</code></pre>"},{"location":"overview/available_models/text/#clipse5-base-trm-nl","title":"<code>clips/e5-base-trm-nl</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 124.4M 237.0 MB 2025-09-23 nld-Latn Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#clipse5-large-trm-nl","title":"<code>clips/e5-large-trm-nl</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 355.0M 1.3 GB 2025-09-23 nld-Latn Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#clipse5-small-trm-nl","title":"<code>clips/e5-small-trm-nl</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 40.8M 78.0 MB 2025-09-23 nld-Latn Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#codefuse-aic2llm-05b","title":"<code>codefuse-ai/C2LLM-0.5B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 896 497.3M 948.0 MB 2025-12-22 eng-Latn, go-Code, java-Code, javascript-Code, php-Code, ... (8)"},{"location":"overview/available_models/text/#codefuse-aic2llm-7b","title":"<code>codefuse-ai/C2LLM-7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.7B 14.3 GB 2025-12-22 eng-Latn, go-Code, java-Code, javascript-Code, php-Code, ... (8)"},{"location":"overview/available_models/text/#codefuse-aif2llm-06b","title":"<code>codefuse-ai/F2LLM-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 596.0M 1.1 GB 2025-09-18 eng-Latn Citation <pre><code>@article{2025F2LLM,\n    title={F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data},\n    author={Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang},\n    journal={CoRR},\n    volume={abs/2510.02294},\n    year={2025},\n    url={https://doi.org/10.48550/arXiv.2510.02294},\n    doi={10.48550/ARXIV.2510.02294},\n    eprinttype={arXiv},\n    eprint={2510.02294}\n}\n</code></pre>"},{"location":"overview/available_models/text/#codefuse-aif2llm-17b","title":"<code>codefuse-ai/F2LLM-1.7B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2560 1.7B 3.2 GB 2025-09-18 eng-Latn Citation <pre><code>@article{2025F2LLM,\n    title={F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data},\n    author={Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang},\n    journal={CoRR},\n    volume={abs/2510.02294},\n    year={2025},\n    url={https://doi.org/10.48550/arXiv.2510.02294},\n    doi={10.48550/ARXIV.2510.02294},\n    eprinttype={arXiv},\n    eprint={2510.02294}\n}\n</code></pre>"},{"location":"overview/available_models/text/#codefuse-aif2llm-4b","title":"<code>codefuse-ai/F2LLM-4B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2560 4.0B 7.5 GB 2025-09-18 eng-Latn Citation <pre><code>@article{2025F2LLM,\n    title={F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data},\n    author={Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang},\n    journal={CoRR},\n    volume={abs/2510.02294},\n    year={2025},\n    url={https://doi.org/10.48550/arXiv.2510.02294},\n    doi={10.48550/ARXIV.2510.02294},\n    eprinttype={arXiv},\n    eprint={2510.02294}\n}\n</code></pre>"},{"location":"overview/available_models/text/#deepvkuser-base","title":"<code>deepvk/USER-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 427.0M 473.0 MB 2024-06-10 rus-Cyrl Citation <pre><code>@misc{deepvk2024user,\n        title={USER: Universal Sentence Encoder for Russian},\n        author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},\n        url={https://huggingface.co/datasets/deepvk/USER-base},\n        publisher={Hugging Face}\n        year={2024},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deepvkuser2-base","title":"<code>deepvk/USER2-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2025-04-19 rus-Cyrl Citation <pre><code>@misc{deepvk2025user,\n    title={USER2},\n    author={Malashenko, Boris and Spirin, Egor and Sokolov Andrey},\n    url={https://huggingface.co/deepvk/USER2-base},\n    publisher={Hugging Face},\n    year={2025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#deepvkuser2-small","title":"<code>deepvk/USER2-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 384 34.4M 131.0 MB 2025-04-19 rus-Cyrl Citation <pre><code>@misc{deepvk2025user,\n    title={USER2},\n    author={Malashenko, Boris and Spirin, Egor and Sokolov Andrey},\n    url={https://huggingface.co/deepvk/USER2-small},\n    publisher={Hugging Face},\n    year={2025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#emillykkejensenembeddinggemma-scandi-300m","title":"<code>emillykkejensen/EmbeddingGemma-Scandi-300m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 307.6M 578.0 MB 2025-10-17 dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#emillykkejensenqwen3-embedding-scandi-06b","title":"<code>emillykkejensen/Qwen3-Embedding-Scandi-0.6B</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 595.8M 2.2 GB 2025-10-17 dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn"},{"location":"overview/available_models/text/#emillykkejensenmmbertscandi-base-embedding","title":"<code>emillykkejensen/mmBERTscandi-base-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 306.9M 1.1 GB 2025-10-17 dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#fyaronskiyenglish_code_retriever","title":"<code>fyaronskiy/english_code_retriever</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2025-07-10 eng-Latn"},{"location":"overview/available_models/text/#googleembeddinggemma-300m","title":"<code>google/embeddinggemma-300m</code>","text":"<p>License: gemma</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 307.6M 1.1 GB 2025-09-04 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19) Citation <pre><code>@misc{vera2025embeddinggemmapowerfullightweighttext,\n      title={EmbeddingGemma: Powerful and Lightweight Text Representations},\n      author={Henrique Schechter Vera and Sahil Dua and Biao Zhang and Daniel Salz and Ryan Mullins and Sindhu Raghuram Panyam and Sara Smoot and Iftekhar Naim and Joe Zou and Feiyang Chen and Daniel Cer and Alice Lisak and Min Choi and Lucas Gonzalez and Omar Sanseviero and Glenn Cameron and Ian Ballantyne and Kat Black and Kaifeng Chen and Weiyi Wang and Zhe Li and Gus Martins and Jinhyuk Lee and Mark Sherwood and Juyeong Ji and Renjie Wu and Jingxiao Zheng and Jyotinder Singh and Abheesht Sharma and Divyashree Sreepathihalli and Aashi Jain and Adham Elarabawy and AJ Co and Andreas Doumanoglou and Babak Samari and Ben Hora and Brian Potetz and Dahun Kim and Enrique Alfonseca and Fedor Moiseev and Feng Han and Frank Palma Gomez and Gustavo Hern\u00e1ndez \u00c1brego and Hesen Zhang and Hui Hui and Jay Han and Karan Gill and Ke Chen and Koert Chen and Madhuri Shanbhogue and Michael Boratko and Paul Suganthan and Sai Meher Karthik Duddu and Sandeep Mariserla and Setareh Ariafar and Shanfeng Zhang and Shijie Zhang and Simon Baumgartner and Sonam Goenka and Steve Qiu and Tanmaya Dabral and Trevor Walker and Vikram Rao and Waleed Khawaja and Wenlei Zhou and Xiaoqi Ren and Ye Xia and Yichang Chen and Yi-Ting Chen and Zhe Dong and Zhongli Ding and Francesco Visin and Ga\u00ebl Liu and Jiageng Zhang and Kathleen Kenealy and Michelle Casbon and Ravin Kumar and Thomas Mesnard and Zach Gleicher and Cormac Brick and Olivier Lacombe and Adam Roberts and Qin Yin and Yunhsuan Sung and Raphael Hoffmann and Tris Warkentin and Armand Joulin and Tom Duerig and Mojtaba Seyedhosseini},\n      year={2025},\n      eprint={2509.20354},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2509.20354},\n}\n</code></pre>"},{"location":"overview/available_models/text/#googlegemini-embedding-001","title":"<code>google/gemini-embedding-001</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 3072 not specified not specified 2025-03-07 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)"},{"location":"overview/available_models/text/#googletext-embedding-004","title":"<code>google/text-embedding-004</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-05-14 eng-Latn"},{"location":"overview/available_models/text/#googletext-embedding-005","title":"<code>google/text-embedding-005</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-11-18 eng-Latn"},{"location":"overview/available_models/text/#googletext-multilingual-embedding-002","title":"<code>google/text-multilingual-embedding-002</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 768 not specified not specified 2024-05-14 arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)"},{"location":"overview/available_models/text/#infgradjasper-token-compression-600m","title":"<code>infgrad/Jasper-Token-Compression-600M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 2048 607.3M 2.2 GB 2025-11-14 eng-Latn, zho-Hans Citation <pre><code>@misc{zhang2025jaspertokencompression600mtechnicalreport,\n      title={Jasper-Token-Compression-600M Technical Report},\n      author={Dun Zhang and Ziyang Zeng and Yudong Zhou and Shuyang Lu},\n      year={2025},\n      eprint={2511.14405},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2511.14405},\n}\n</code></pre>"},{"location":"overview/available_models/text/#inflyinf-retriever-v1","title":"<code>infly/inf-retriever-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.1B 13.2 GB 2024-12-24 eng-Latn, zho-Hans Citation <pre><code>@misc{infly-ai_2025,\n  author       = {Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi},\n  title        = {inf-retriever-v1 (Revision 5f469d7)},\n  year         = 2025,\n  url          = {https://huggingface.co/infly/inf-retriever-v1},\n  doi          = {10.57967/hf/4262},\n  publisher    = {Hugging Face}\n}\n</code></pre>"},{"location":"overview/available_models/text/#inflyinf-retriever-v1-15b","title":"<code>infly/inf-retriever-v1-1.5b</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 1.5B 2.9 GB 2025-02-08 eng-Latn, zho-Hans Citation <pre><code>@misc{infly-ai_2025,\n  author       = {Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi},\n  title        = {inf-retriever-v1 (Revision 5f469d7)},\n  year         = 2025,\n  url          = {https://huggingface.co/infly/inf-retriever-v1},\n  doi          = {10.57967/hf/4262},\n  publisher    = {Hugging Face}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-base","title":"<code>intfloat/e5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2022-12-26 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-base-v2","title":"<code>intfloat/e5-base-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-large","title":"<code>intfloat/e5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2022-12-26 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-large-v2","title":"<code>intfloat/e5-large-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 335.0M 1.2 GB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-mistral-7b-instruct","title":"<code>intfloat/e5-mistral-7b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-02-08 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn Citation <pre><code>    @article{wang2023improving,\n      title={Improving Text Embeddings with Large Language Models},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2401.00368},\n      year={2023}\n    }\n\n    @article{wang2022text,\n      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2212.03533},\n      year={2022}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-small","title":"<code>intfloat/e5-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.0M 127.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloate5-small-v2","title":"<code>intfloat/e5-small-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.0M 127.0 MB 2024-02-08 eng-Latn Citation <pre><code>@article{wang2022text,\n  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2212.03533},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-base","title":"<code>intfloat/multilingual-e5-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-large","title":"<code>intfloat/multilingual-e5-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 560.0M 2.1 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-large-instruct","title":"<code>intfloat/multilingual-e5-large-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 560.0M 1.0 GB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n      title={Multilingual E5 Text Embeddings: A Technical Report},\n      author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n      journal={arXiv preprint arXiv:2402.05672},\n      year={2024}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#intfloatmultilingual-e5-small","title":"<code>intfloat/multilingual-e5-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 118.0M 449.0 MB 2024-02-08 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{wang2024multilingual,\n  title={Multilingual E5 Text Embeddings: A Technical Report},\n  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},\n  journal={arXiv preprint arXiv:2402.05672},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v3","title":"<code>jinaai/jina-embeddings-v3</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 572.3M 1.1 GB 2024-09-18 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>    @misc{sturua2024jinaembeddingsv3multilingualembeddingstask,\n      title={jina-embeddings-v3: Multilingual Embeddings With Task LoRA},\n      author={Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael G\u00fcnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao},\n      year={2024},\n      eprint={2409.10173},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2409.10173},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v5-text-nano","title":"<code>jinaai/jina-embeddings-v5-text-nano</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 211.8M 404.0 MB 2026-02-17 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{akram2026jinaembeddingsv5texttasktargetedembeddingdistillation,\n      title={jina-embeddings-v5-text: Task-Targeted Embedding Distillation},\n      author={Mohammad Kalim Akram and Saba Sturua and Nastia Havriushenko and Quentin Herreros and Michael G\u00fcnther and Maximilian Werk and Han Xiao},\n      year={2026},\n      eprint={2602.15547},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2602.15547},\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v5-text-small","title":"<code>jinaai/jina-embeddings-v5-text-small</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1024 596.0M 1.1 GB 2026-02-17 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{akram2026jinaembeddingsv5texttasktargetedembeddingdistillation,\n      title={jina-embeddings-v5-text: Task-Targeted Embedding Distillation},\n      author={Mohammad Kalim Akram and Saba Sturua and Nastia Havriushenko and Quentin Herreros and Michael G\u00fcnther and Maximilian Werk and Han Xiao},\n      year={2026},\n      eprint={2602.15547},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2602.15547},\n}\n</code></pre>"},{"location":"overview/available_models/text/#jxmcde-small-v1","title":"<code>jxm/cde-small-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 281.0M 1.0 GB 2024-09-24 eng-Latn Citation <pre><code>@misc{morris2024contextualdocumentembeddings,\n    title={Contextual Document Embeddings},\n    author={John X. Morris and Alexander M. Rush},\n    year={2024},\n    eprint={2410.02525},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2410.02525},\n}\n</code></pre>"},{"location":"overview/available_models/text/#jxmcde-small-v2","title":"<code>jxm/cde-small-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 306.0M 1.1 GB 2025-01-13 eng-Latn Citation <pre><code>@misc{morris2024contextualdocumentembeddings,\n    title={Contextual Document Embeddings},\n    author={John X. Morris and Alexander M. Rush},\n    year={2024},\n    eprint={2410.02525},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n    url={https://arxiv.org/abs/2410.02525},\n}\n</code></pre>"},{"location":"overview/available_models/text/#llamaindexvdr-2b-multi-v1","title":"<code>llamaindex/vdr-2b-multi-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 2.2B 4.1 GB 2024-01-08 deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn"},{"location":"overview/available_models/text/#manveertambercadet-embed-base-v1","title":"<code>manveertamber/cadet-embed-base-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.0M 418.0 MB 2025-05-11 eng-Latn Citation <pre><code>@article{tamber2025conventionalcontrastivelearningfalls,\n    title={Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval with Cross-Encoder Listwise Distillation and Synthetic Data},\n    author={Manveer Singh Tamber and Suleman Kazi and Vivek Sourabh and Jimmy Lin},\n    journal={arXiv:2505.19274},\n    year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-2d-large-v1","title":"<code>mixedbread-ai/mxbai-embed-2d-large-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.1M not specified 2024-03-04 eng-Latn"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-large-v1","title":"<code>mixedbread-ai/mxbai-embed-large-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 639.0 MB 2024-03-07 eng-Latn Citation <pre><code>    @online{emb2024mxbai,\n      title={Open Source Strikes Bread - New Fluffy Embeddings Model},\n      author={Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp},\n      year={2024},\n      url={https://www.mixedbread.ai/blog/mxbai-embed-large-v1},\n    }\n\n    @article{li2023angle,\n      title={AnglE-optimized Text Embeddings},\n      author={Li, Xianming and Li, Jing},\n      journal={arXiv preprint arXiv:2309.12871},\n      year={2023}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-embed-xsmall-v1","title":"<code>mixedbread-ai/mxbai-embed-xsmall-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 24.1M not specified 2024-08-13 eng-Latn Citation <pre><code>@online{xsmall2024mxbai,\n  title={Every Byte Matters: Introducing mxbai-embed-xsmall-v1},\n  author={Sean Lee and Julius Lipp and Rui Huang and Darius Koenig},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-embed-xsmall-v1},\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-aimodernbert-embed-base","title":"<code>nomic-ai/modernbert-embed-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 568.0 MB 2024-12-29 eng-Latn Citation <pre><code>@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder},\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-ainomic-embed-code","title":"<code>nomic-ai/nomic-embed-code</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.1B 26.3 GB 2025-03-24 eng-Latn Citation <pre><code>@misc{suresh2025cornstackhighqualitycontrastivedata,\n      title={CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking},\n      author={Tarun Suresh and Revanth Gangi Reddy and Yifei Xu and Zach Nussbaum and Andriy Mulyar and Brandon Duderstadt and Heng Ji},\n      year={2025},\n      eprint={2412.01007},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.01007},\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1","title":"<code>nomic-ai/nomic-embed-text-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 136.7M 522.0 MB 2024-01-31 eng-Latn Citation <pre><code>@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder},\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1-ablated","title":"<code>nomic-ai/nomic-embed-text-v1-ablated</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 136.7M not specified 2024-01-15 eng-Latn"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v1-unsupervised","title":"<code>nomic-ai/nomic-embed-text-v1-unsupervised</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 136.7M not specified 2024-01-15 eng-Latn"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v15","title":"<code>nomic-ai/nomic-embed-text-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 136.7M 522.0 MB 2024-02-10 eng-Latn Citation <pre><code>@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder},\n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nomic-ainomic-embed-text-v2-moe","title":"<code>nomic-ai/nomic-embed-text-v2-moe</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 475.3M 1.8 GB 2025-02-07 amh-Ethi, arb-Arab, bel-Cyrl, ben-Beng, bul-Cyrl, ... (98) Citation <pre><code>@misc{nussbaum2025trainingsparsemixtureexperts,\n      title={Training Sparse Mixture Of Experts Text Embedding Models},\n      author={Zach Nussbaum and Brandon Duderstadt},\n      year={2025},\n      eprint={2502.07972},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2502.07972},\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidianv-embed-v1","title":"<code>nvidia/NV-Embed-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.8B 14.6 GB 2024-09-13 eng-Latn Citation <pre><code>@misc{lee2025nvembedimprovedtechniquestraining,\n      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},\n      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},\n      year={2025},\n      eprint={2405.17428},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2405.17428},\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidianv-embed-v2","title":"<code>nvidia/NV-Embed-v2</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.8B 14.6 GB 2024-09-09 eng-Latn Citation <pre><code>@misc{lee2025nvembedimprovedtechniquestraining,\n      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models},\n      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},\n      year={2025},\n      eprint={2405.17428},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2405.17428},\n}\n</code></pre>"},{"location":"overview/available_models/text/#nvidiallama-embed-nemotron-8b","title":"<code>nvidia/llama-embed-nemotron-8b</code>","text":"<p>License: https://huggingface.co/nvidia/llama-embed-nemotron-8b/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.5B 28.0 GB 2025-10-23 afr-Latn, amh-Ethi, ara-Arab, arq-Arab, ary-Arab, ... (66) Citation <pre><code>@misc{babakhin2025llamaembednemotron8buniversaltextembedding,\n      title={Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks},\n      author={Yauhen Babakhin and Radek Osmulski and Ronay Ak and Gabriel Moreira and Mengyao Xu and Benedikt Schifferer and Bo Liu and Even Oldridge},\n      year={2025},\n      eprint={2511.07025},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2511.07025},\n}\n</code></pre>"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v1","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 133.0M 507.0 MB 2024-03-07 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 66.4M 267.0 MB 2024-07-17 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 22.7M 86.0 MB 2024-07-18 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 30522 66.4M 267.0 MB 2025-03-28 eng-Latn"},{"location":"overview/available_models/text/#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte","title":"<code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 30522 136.8M 549.0 MB 2025-06-18 eng-Latn"},{"location":"overview/available_models/text/#samaya-airepllama-reproduced","title":"<code>samaya-ai/RepLLaMA-reproduced</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 27.0 MB 2024-09-15 eng-Latn Citation <pre><code>@article{rankllama,\n      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval},\n      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},\n      year={2023},\n      journal={arXiv:2310.08319},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama2-7b-v1","title":"<code>samaya-ai/promptriever-llama2-7b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 26.1 GB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama31-8b-instruct-v1","title":"<code>samaya-ai/promptriever-llama3.1-8b-instruct-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.0B 29.8 GB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-llama31-8b-v1","title":"<code>samaya-ai/promptriever-llama3.1-8b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 4096 8.0B 29.8 GB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#samaya-aipromptriever-mistral-v01-7b-v1","title":"<code>samaya-ai/promptriever-mistral-v0.1-7b-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 4096 7.0B 26.1 GB 2024-09-15 eng-Latn Citation <pre><code>@article{weller2024promptriever,\n      title={Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models},\n      author={Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel},\n      year={2024},\n      eprint={2409.11136},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.11136},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sbintuitionssarashina-embedding-v2-1b","title":"<code>sbintuitions/sarashina-embedding-v2-1b</code>","text":"<p>License: https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1792 1.2B 4.6 GB 2025-07-30 jpn-Jpan"},{"location":"overview/available_models/text/#sergeyzhberta","title":"<code>sergeyzh/BERTA</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 128.0M 489.0 MB 2025-03-10 rus-Cyrl"},{"location":"overview/available_models/text/#sergeyzhrubert-mini-frida","title":"<code>sergeyzh/rubert-mini-frida</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 32.3M 123.0 MB 2025-03-02 rus-Cyrl"},{"location":"overview/available_models/text/#telepixpixie-rune-v10","title":"<code>telepix/PIXIE-Rune-v1.0</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 6.1K 1024 567.8M 2.1 GB 2026-01-15 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74) Citation <pre><code>@misc{TelePIX-PIXIE-Rune-v1.0,\n  title        = {PIXIE-Rune-v1.0},\n  author       = {TelePIX AI Research Team and Bongmin Kim},\n  year         = {2026},\n  howpublished = {Hugging Face model card},\n  url          = {https://huggingface.co/telepix/PIXIE-Rune-v1.0}\n}\n\n@article{yu2024arctic,\n      title={Arctic-Embed 2.0: Multilingual Retrieval Without Compromise},\n      author={Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel},\n      journal={arXiv preprint arXiv:2412.04506},\n      year={2024},\n      eprint={2412.04506},\n      archivePrefix={arXiv},\n      url={https://arxiv.org/abs/2412.04506}\n}\n</code></pre>"},{"location":"overview/available_models/text/#tencentkalm-embedding-gemma3-12b-2511","title":"<code>tencent/KaLM-Embedding-Gemma3-12B-2511</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3840 11.8B 43.8 GB 2025-11-06 not specified Citation <pre><code>@misc{zhao2025kalmembeddingv2,\n      title={KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model},\n      author={Xinping Zhao and Xinshuo Hu and Zifei Shan and Shouzheng Huang and Yao Zhou and Xin Zhang and Zetian Sun and Zhenyu Liu and Dongfang Li and Xinyuan Wei and Youcheng Pan and Yang Xiang and Meishan Zhang and Haofen Wang and Jun Yu and Baotian Hu and Min Zhang},\n      year={2025},\n      eprint={2506.20923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2506.20923},\n}\n\n@misc{hu2025kalmembedding,\n      title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n      author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n      year={2025},\n      eprint={2501.01028},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#tencentyoutu-embedding","title":"<code>tencent/Youtu-Embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.7B not specified 2025-09-28 zho-Hans Citation <pre><code>@misc{zhang2025codiemb,\n  title={CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity},\n  author={Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing},\n  year={2025},\n  eprint={2508.11442},\n  archivePrefix={arXiv},\n  url={https://arxiv.org/abs/2508.11442},\n}\n</code></pre>"},{"location":"overview/available_models/text/#voyageaivoyage-2","title":"<code>voyageai/voyage-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.0K 1024 not specified not specified 2023-10-29 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3","title":"<code>voyageai/voyage-3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-09-18 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-large","title":"<code>voyageai/voyage-3-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-07 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-lite","title":"<code>voyageai/voyage-3-lite</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 512 not specified not specified 2024-09-18 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-3-m-exp","title":"<code>voyageai/voyage-3-m-exp</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 2048 6.9B not specified 2025-01-08 eng-Latn"},{"location":"overview/available_models/text/#voyageaivoyage-35","title":"<code>voyageai/voyage-3.5</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-35-output_dtypebinary","title":"<code>voyageai/voyage-3.5 (output_dtype=binary)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-35-output_dtypeint8","title":"<code>voyageai/voyage-3.5 (output_dtype=int8)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2025-01-21 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-4","title":"<code>voyageai/voyage-4</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2026-01-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-4-large","title":"<code>voyageai/voyage-4-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2026-01-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-4-large-embed_dim2048","title":"<code>voyageai/voyage-4-large (embed_dim=2048)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 2048 not specified not specified 2026-01-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-4-lite","title":"<code>voyageai/voyage-4-lite</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2026-01-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-4-nano","title":"<code>voyageai/voyage-4-nano</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 2048 346.5M 661.0 MB 2026-01-15 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)"},{"location":"overview/available_models/text/#voyageaivoyage-code-2","title":"<code>voyageai/voyage-code-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1536 not specified not specified 2024-01-23 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-code-3","title":"<code>voyageai/voyage-code-3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-12-04 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-finance-2","title":"<code>voyageai/voyage-finance-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-05-30 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-large-2","title":"<code>voyageai/voyage-large-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1536 not specified not specified 2023-10-29 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-large-2-instruct","title":"<code>voyageai/voyage-large-2-instruct</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1024 not specified not specified 2024-05-05 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-law-2","title":"<code>voyageai/voyage-law-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 16.0K 1024 not specified not specified 2024-04-15 not specified"},{"location":"overview/available_models/text/#voyageaivoyage-multilingual-2","title":"<code>voyageai/voyage-multilingual-2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.0K 1024 not specified not specified 2024-06-10 not specified"},{"location":"overview/available_models/text/#yibinleilens-d4000","title":"<code>yibinlei/LENS-d4000</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4000 7.1B 26.5 GB 2025-01-17 not specified Citation <pre><code>@article{lei2025lens,\n  title={Enhancing Lexicon-Based Text Embeddings with Large Language Models},\n  author={Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew},\n  journal={arXiv preprint arXiv:2501.09749},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#yibinleilens-d8000","title":"<code>yibinlei/LENS-d8000</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 8000 7.1B 26.5 GB 2025-01-17 not specified Citation <pre><code>@article{lei2025lens,\n  title={Enhancing Lexicon-Based Text Embeddings with Large Language Models},\n  author={Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew},\n  journal={arXiv preprint arXiv:2501.09749},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#zeta-alpha-aizeta-alpha-e5-mistral","title":"<code>zeta-alpha-ai/Zeta-Alpha-E5-Mistral</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 13.2 GB 2024-08-30 eng-Latn"},{"location":"overview/available_models/text/#non-instruction-model","title":"Non-instruction Model","text":""},{"location":"overview/available_models/text/#aiteamvnvietnamese_embedding","title":"<code>AITeamVN/Vietnamese_Embedding</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-03-17 vie-Latn Citation <pre><code>@misc{Vietnamese_Embedding,\n  title={Vietnamese_Embedding: Embedding model in Vietnamese language.},\n  author={Nguyen Nho Trung, Nguyen Nhat Quang, Nguyen Van Huy},\n  year={2025},\n  publisher={Huggingface},\n}\n</code></pre>"},{"location":"overview/available_models/text/#alibaba-nlpgte-base-en-v15","title":"<code>Alibaba-NLP/gte-base-en-v1.5</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 136.8M not specified 2024-06-20 eng-Latn Citation <pre><code>@misc{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Xin Zhang and Yanzhao Zhang and Dingkun Long and Wen Xie and Ziqi Dai and Jialong Tang and Huan Lin and Baosong Yang and Pengjun Xie and Fei Huang and Meishan Zhang and Wenjie Li and Min Zhang},\n  year={2024},\n  eprint={2407.19669},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2407.19669},\n}\n@misc{li2023towards,\n  title={Towards General Text Embeddings with Multi-stage Contrastive Learning},\n  author={Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},\n  year={2023},\n  eprint={2308.03281},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2308.03281},\n}\n</code></pre>"},{"location":"overview/available_models/text/#alibaba-nlpgte-modernbert-base","title":"<code>Alibaba-NLP/gte-modernbert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 284.0 MB 2025-01-21 eng-Latn Citation <pre><code>@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n\n@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#alibaba-nlpgte-multilingual-base","title":"<code>Alibaba-NLP/gte-multilingual-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 305.4M 582.0 MB 2024-07-20 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@inproceedings{zhang2024mgte,\n  title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},\n  author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},\n  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},\n  pages={1393--1412},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-en-icl","title":"<code>BAAI/bge-en-icl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 4096 7.1B 26.5 GB 2024-07-25 eng-Latn Citation <pre><code>    @misc{li2024makingtextembeddersfewshot,\n      title={Making Text Embedders Few-Shot Learners},\n      author={Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2409.15700},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2409.15700},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-m3","title":"<code>BAAI/bge-m3</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-06-28 afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29) Citation <pre><code>@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-m3-unsupervised","title":"<code>BAAI/bge-m3-unsupervised</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 568.0M 2.1 GB 2024-01-30 afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29) Citation <pre><code>@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-multilingual-gemma2","title":"<code>BAAI/bge-multilingual-gemma2</code>","text":"<p>License: https://ai.google.dev/gemma/terms</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3584 9.2B 34.4 GB 2024-07-25 eng-Latn, fra-Latn, jpn-Jpan, jpn-Latn, kor-Hang, ... (7) Citation <pre><code>@misc{bge-m3,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n\n@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#baaibge-reranker-v2-m3","title":"BAAI/bge-reranker-v2-m3","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 2.1 GB 2024-06-24 ara-Arab, ben-Beng, dan-Latn, deu-Latn, eng-Latn, ... (32) Citation <pre><code>    @misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval},\n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n    }\n    @misc{bge-m3,\n      archiveprefix = {arXiv},\n      author = {Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      eprint = {2402.03216},\n      primaryclass = {cs.CL},\n      title = {BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n      year = {2024},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#bytedancelistconranker","title":"<code>ByteDance/ListConRanker</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 401.0M 1.2 GB 2024-12-11 zho-Hans Citation <pre><code>@article{liu2025listconranker,\n  title={ListConRanker: A Contrastive Text Reranker with Listwise Encoding},\n  author={Liu, Junlong and Ma, Yue and Zhao, Ruihui and Zheng, Junhao and Ma, Qianli and Kang, Yangyang},\n  journal={arXiv preprint arXiv:2501.07111},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#classicalyinka","title":"<code>Classical/Yinka</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 326.0M 1.2 GB 2024-01-09 zho-Hans"},{"location":"overview/available_models/text/#dmetasouldmeta-embedding-zh-small","title":"<code>DMetaSoul/Dmeta-embedding-zh-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 1.0K 768 74.2M 283.0 MB 2024-03-25 zho-Hans"},{"location":"overview/available_models/text/#dmetasoulsbert-chinese-general-v1","title":"<code>DMetaSoul/sbert-chinese-general-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 128 102.3M not specified 2022-03-25 zho-Hans"},{"location":"overview/available_models/text/#deeppavlovdistilrubert-small-cased-conversational","title":"<code>DeepPavlov/distilrubert-small-cased-conversational</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 107.0M 408.0 MB 2022-06-28 rus-Cyrl Citation <pre><code>@misc{https://doi.org/10.48550/arxiv.2205.02340,\n      doi = {10.48550/ARXIV.2205.02340},\n      url = {https://arxiv.org/abs/2205.02340},\n      author = {Kolesnikova, Alina and Kuratov, Yuri and Konovalov, Vasily and Burtsev, Mikhail},\n      keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Knowledge Distillation of Russian Language Models with Reduction of Vocabulary},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {arXiv.org perpetual, non-exclusive license}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deeppavlovrubert-base-cased","title":"<code>DeepPavlov/rubert-base-cased</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 1.3B 4.8 GB 2020-03-04 rus-Cyrl Citation <pre><code>@misc{kuratov2019adaptationdeepbidirectionalmultilingual,\n      title={Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language},\n      author={Yuri Kuratov and Mikhail Arkhipov},\n      year={2019},\n      eprint={1905.07213},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/1905.07213},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#deeppavlovrubert-base-cased-sentence","title":"<code>DeepPavlov/rubert-base-cased-sentence</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 107.0M 408.0 MB 2020-03-04 rus-Cyrl"},{"location":"overview/available_models/text/#facebookaixlm-roberta-base","title":"<code>FacebookAI/xlm-roberta-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2019-11-05 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"overview/available_models/text/#facebookaixlm-roberta-large","title":"<code>FacebookAI/xlm-roberta-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 559.9M 2.1 GB 2019-11-05 afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99) Citation <pre><code>@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"overview/available_models/text/#gameselosts-multilingual-mpnet-base-v2","title":"<code>Gameselo/STS-multilingual-mpnet-base-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-07 not specified Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#greennodegreennode-embedding-large-vn-mixed-v1","title":"<code>GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-04-11 vie-Latn"},{"location":"overview/available_models/text/#greennodegreennode-embedding-large-vn-v1","title":"<code>GreenNode/GreenNode-Embedding-Large-VN-V1</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-04-11 vie-Latn"},{"location":"overview/available_models/text/#hit-tmgkalm-embedding-multilingual-mini-v1","title":"<code>HIT-TMG/KaLM-embedding-multilingual-mini-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 896 494.0M 1.8 GB 2024-08-27 eng-Latn, zho-Hans Citation <pre><code>@misc{hu2025kalmembedding,\n  title={KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model},\n  author={Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang},\n  year={2025},\n  eprint={2501.01028},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2501.01028},\n}\n</code></pre>"},{"location":"overview/available_models/text/#haon-chenspeed-embedding-7b-instruct","title":"<code>Haon-Chen/speed-embedding-7b-instruct</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K not specified 7.1B 13.2 GB 2024-10-31 eng-Latn Citation <pre><code>@article{chen2024little,\n    title={Little Giants: Synthesizing High-Quality Embedding Data at Scale},\n    author={Chen, Haonan and Wang, Liang and Yang, Nan and Zhu, Yutao and Zhao, Ziliang and Wei, Furu and Dou, Zhicheng},\n    journal={arXiv preprint arXiv:2410.18634},\n    year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hooshvarelabbert-base-parsbert-uncased","title":"<code>HooshvareLab/bert-base-parsbert-uncased</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2021-05-19 fas-Arab Citation <pre><code>    @article{ParsBERT,\n    title={ParsBERT: Transformer-based Model for Persian Language Understanding},\n    author={Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},\n    journal={ArXiv},\n    year={2020},\n    volume={abs/2005.12515}\n}\n</code></pre>"},{"location":"overview/available_models/text/#hum-workslodestone-base-4096-v1","title":"<code>Hum-Works/lodestone-base-4096-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 137.4M not specified 2023-08-25 eng-Latn"},{"location":"overview/available_models/text/#ieityuanyuan-embedding-20-zh","title":"<code>IEITYuan/Yuan-embedding-2.0-zh</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 327.4M 1.2 GB 2025-11-24 zho-Hans"},{"location":"overview/available_models/text/#jaumegemma-2b-embeddings","title":"<code>Jaume/gemma-2b-embeddings</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 2048 2.5B 9.3 GB 2024-06-29 not specified"},{"location":"overview/available_models/text/#kblabsentence-bert-swedish-cased","title":"<code>KBLab/sentence-bert-swedish-cased</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 384 768 124.7M 476.0 MB 2023-01-11 swe-Latn Citation <pre><code>@misc{rekathati2021introducing,\n  author = {Rekathati, Faton},\n  title = {The KBLab Blog: Introducing a Swedish Sentence Transformer},\n  url = {https://kb-labb.github.io/posts/2021-08-23-a-swedish-sentence-transformer/},\n  year = {2021}\n}\n</code></pre>"},{"location":"overview/available_models/text/#kfstxlmroberta-en-da-sv-nb","title":"<code>KFST/XLMRoberta-en-da-sv-nb</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2022-02-22 dan-Latn, eng-Latn, nno-Latn, nob-Latn, swe-Latn"},{"location":"overview/available_models/text/#kennethenevoldsendfm-sentence-encoder-large","title":"<code>KennethEnevoldsen/dfm-sentence-encoder-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 355.1M 1.5 GB 2023-07-12 dan-Latn Citation <pre><code>@article{enevoldsenScandinavianEmbeddingBenchmarks2024,\n    title = {The {Scandinavian} {Embedding} {Benchmarks}: {Comprehensive} {Assessment} of {Multilingual} and {Monolingual} {Text} {Embedding}},\n    shorttitle = {The {Scandinavian} {Embedding} {Benchmarks}},\n    url = {https://openreview.net/forum?id=pJl_i7HIA72},\n    language = {en},\n    urldate = {2024-04-12},\n    author = {Enevoldsen, Kenneth and Kardos, M\u00e1rton and Muennighoff, Niklas and Nielbo, Kristoffer},\n    month = feb,\n    year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#kennethenevoldsendfm-sentence-encoder-medium","title":"<code>KennethEnevoldsen/dfm-sentence-encoder-medium</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2023-07-12 dan-Latn Citation <pre><code>@article{enevoldsenScandinavianEmbeddingBenchmarks2024,\n    title = {The {Scandinavian} {Embedding} {Benchmarks}: {Comprehensive} {Assessment} of {Multilingual} and {Monolingual} {Text} {Embedding}},\n    shorttitle = {The {Scandinavian} {Embedding} {Benchmarks}},\n    url = {https://openreview.net/forum?id=pJl_i7HIA72},\n    language = {en},\n    urldate = {2024-04-12},\n    author = {Enevoldsen, Kenneth and Kardos, M\u00e1rton and Muennighoff, Niklas and Nielbo, Kristoffer},\n    month = feb,\n    year = {2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#kowshik24bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2","title":"<code>Kowshik24/bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 128 768 278.0M 1.0 GB 2025-11-10 ben-Beng Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-base","title":"<code>Lajavaness/bilingual-embedding-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-26 not specified Citation <pre><code>    @article{DBLP:journals/corr/abs-1911-02116,\n      author    = {Alexis Conneau and\n                   Kartikay Khandelwal and\n                   Naman Goyal and\n                   Vishrav Chaudhary and\n                   Guillaume Wenzek and\n                   Francisco Guzm{'{a}}n and\n                   Edouard Grave and\n                   Myle Ott and\n                   Luke Zettlemoyer and\n                   Veselin Stoyanov},\n      title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n      journal   = {CoRR},\n      volume    = {abs/1911.02116},\n      year      = {2019},\n      url       = {http://arxiv.org/abs/1911.02116},\n      eprinttype = {arXiv},\n      eprint    = {1911.02116},\n      timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n      biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n      bibsource = {dblp computer science bibliography, https://dblp.org}\n    }\n\n@article{reimers2019sentence,\n   title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},\n   author={Nils Reimers, Iryna Gurevych},\n   journal={https://arxiv.org/abs/1908.10084},\n   year={2019}\n}\n\n@article{thakur2020augmented,\n  title={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks},\n  author={Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},\n  journal={arXiv e-prints},\n  pages={arXiv--2010},\n  year={2020}\n}\n</code></pre>"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-large","title":"<code>Lajavaness/bilingual-embedding-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2024-06-24 eng-Latn, fra-Latn Citation <pre><code>    @article{DBLP:journals/corr/abs-1911-02116,\n      author    = {Alexis Conneau and\n                   Kartikay Khandelwal and\n                   Naman Goyal and\n                   Vishrav Chaudhary and\n                   Guillaume Wenzek and\n                   Francisco Guzm{'{a}}n and\n                   Edouard Grave and\n                   Myle Ott and\n                   Luke Zettlemoyer and\n                   Veselin Stoyanov},\n      title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n      journal   = {CoRR},\n      volume    = {abs/1911.02116},\n      year      = {2019},\n      url       = {http://arxiv.org/abs/1911.02116},\n      eprinttype = {arXiv},\n      eprint    = {1911.02116},\n      timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n      biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n      bibsource = {dblp computer science bibliography, https://dblp.org}\n    }\n\n@article{reimers2019sentence,\n   title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},\n   author={Nils Reimers, Iryna Gurevych},\n   journal={https://arxiv.org/abs/1908.10084},\n   year={2019}\n}\n\n@article{thakur2020augmented,\n  title={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks},\n  author={Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},\n  journal={arXiv e-prints},\n  pages={arXiv--2010},\n  year={2020}\n}\n</code></pre>"},{"location":"overview/available_models/text/#lajavanessbilingual-embedding-small","title":"<code>Lajavaness/bilingual-embedding-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2024-07-17 eng-Latn, fra-Latn Citation <pre><code>@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{'{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n\n@article{reimers2019sentence,\n   title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},\n   author={Nils Reimers, Iryna Gurevych},\n   journal={https://arxiv.org/abs/1908.10084},\n   year={2019}\n}\n\n@article{thakur2020augmented,\n  title={Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks},\n  author={Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna},\n  journal={arXiv e-prints},\n  pages={arXiv--2010},\n  year={2020}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcinexthakim","title":"<code>MCINext/Hakim</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcinexthakim-small","title":"<code>MCINext/Hakim-small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 38.7M 148.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mcinexthakim-unsup","title":"<code>MCINext/Hakim-unsup</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2025-05-10 fas-Arab Citation <pre><code>@article{sarmadi2025hakim,\n      title={Hakim: Farsi Text Embedding Model},\n      author={Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra},\n      journal={arXiv preprint arXiv:2505.08435},\n      year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mihaiiibulbasaur","title":"<code>Mihaiii/Bulbasaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-27 not specified"},{"location":"overview/available_models/text/#mihaiiiivysaur","title":"<code>Mihaiii/Ivysaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-04-27 not specified"},{"location":"overview/available_models/text/#mihaiiisquirtle","title":"<code>Mihaiii/Squirtle</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 15.6M 60.0 MB 2024-04-30 not specified"},{"location":"overview/available_models/text/#mihaiiivenusaur","title":"<code>Mihaiii/Venusaur</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 15.6M 60.0 MB 2024-04-29 not specified"},{"location":"overview/available_models/text/#mihaiiiwartortle","title":"<code>Mihaiii/Wartortle</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-30 not specified"},{"location":"overview/available_models/text/#mihaiiigte-micro","title":"<code>Mihaiii/gte-micro</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 17.4M 66.0 MB 2024-04-21 not specified"},{"location":"overview/available_models/text/#mihaiiigte-micro-v4","title":"<code>Mihaiii/gte-micro-v4</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 19.2M 73.0 MB 2024-04-22 not specified"},{"location":"overview/available_models/text/#mira190euler-legal-embedding-v1","title":"<code>Mira190/Euler-Legal-Embedding-V1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 1.5K 4096 7.6B 15.3 GB 2025-11-06 eng-Latn Citation <pre><code>@misc{euler2025legal,\n      title={Euler-Legal-Embedding: Advanced Legal Representation Learning},\n      author={LawRank Team},\n      year={2025},\n      publisher={Hugging Face}\n}\n</code></pre>"},{"location":"overview/available_models/text/#nbailabnb-bert-base","title":"<code>NbAiLab/nb-bert-base</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 177.9M 681.0 MB 2021-01-13 nno-Latn, nob-Latn"},{"location":"overview/available_models/text/#nbailabnb-bert-large","title":"<code>NbAiLab/nb-bert-large</code>","text":"<p>License: cc-by-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 355.1M 1.3 GB 2021-04-29 nno-Latn, nob-Latn"},{"location":"overview/available_models/text/#nbailabnb-sbert-base","title":"<code>NbAiLab/nb-sbert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 75 4096 177.9M 678.0 MB 2022-11-23 dan-Latn, nno-Latn, nob-Latn, swe-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-100k","title":"<code>NeuML/pubmedbert-base-embeddings-100K</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 100.0K 0.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-1m","title":"<code>NeuML/pubmedbert-base-embeddings-1M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 1.0M 2.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-2m","title":"<code>NeuML/pubmedbert-base-embeddings-2M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 1.9M 7.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-500k","title":"<code>NeuML/pubmedbert-base-embeddings-500K</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 500.0K 2.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#neumlpubmedbert-base-embeddings-8m","title":"<code>NeuML/pubmedbert-base-embeddings-8M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.8M 30.0 MB 2025-01-03 eng-Latn"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 135.2M 516.0 MB 2024-06-16 ara-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet","title":"<code>Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2024-06-25 ara-Arab"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-triplet-matryoshka-v2","title":"<code>Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 768 768 135.2M 516.0 MB 2024-07-28 ara-Arab Citation <pre><code>    @article{nacar2025gate,\n    title={GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training},\n    author={Nacar, Omer and Koubaa, Anis and Sibaee, Serry and Al-Habashi, Yasser and Ammar, Adel and Boulila, Wadii},\n    journal={arXiv preprint arXiv:2505.24581},\n    year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-06-14 ara-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-labse-matryoshka","title":"<code>Omartificial-Intelligence-Space/Arabic-labse-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.9M 1.8 GB 2024-06-16 ara-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet","title":"<code>Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 109.5M 418.0 MB 2024-06-15 ara-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka","title":"<code>Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2024-06-17 ara-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#opensearch-aiops-moa-conan-embedding-v1","title":"<code>OpenSearch-AI/Ops-MoA-Conan-embedding-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 343.4M 1.3 GB 2025-03-26 zho-Hans"},{"location":"overview/available_models/text/#opensearch-aiops-moa-yuan-embedding-10","title":"<code>OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1536 343.4M 1.2 GB 2025-03-26 zho-Hans"},{"location":"overview/available_models/text/#ordalietechsolon-embeddings-large-01","title":"<code>OrdalieTech/Solon-embeddings-large-0.1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2023-12-09 fra-Latn"},{"location":"overview/available_models/text/#ordalietechsolon-embeddings-mini-beta-11","title":"<code>OrdalieTech/Solon-embeddings-mini-beta-1.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 211.8M 808.0 MB 2025-01-01 fra-Latn"},{"location":"overview/available_models/text/#partaitooka-sbert","title":"<code>PartAI/Tooka-SBERT</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 353.0M 1.3 GB 2024-12-07 fas-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#partaitooka-sbert-v2-large","title":"<code>PartAI/Tooka-SBERT-V2-Large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 353.0M 1.3 GB 2025-05-01 fas-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#partaitooka-sbert-v2-small","title":"<code>PartAI/Tooka-SBERT-V2-Small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 122.9M 496.0 MB 2025-05-01 fas-Arab Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#partaitookabert-base","title":"<code>PartAI/TookaBERT-Base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 122.9M 469.0 MB 2024-12-08 fas-Arab"},{"location":"overview/available_models/text/#qodoqodo-embed-1-15b","title":"<code>Qodo/Qodo-Embed-1-1.5B</code>","text":"<p>License: https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 1536 1.5B 6.6 GB 2025-02-19 c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)"},{"location":"overview/available_models/text/#qodoqodo-embed-1-7b","title":"<code>Qodo/Qodo-Embed-1-7B</code>","text":"<p>License: https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 32.8K 3584 7.1B 28.4 GB 2025-02-24 c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)"},{"location":"overview/available_models/text/#queritquerit","title":"<code>Querit/Querit</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 1024 4.9B 9.2 GB 2026-01-24 eng-Latn"},{"location":"overview/available_models/text/#shuu12121codesearch-modernbert-crow-plus","title":"<code>Shuu12121/CodeSearch-ModernBERT-Crow-Plus</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 1.0K 768 151.7M 607.0 MB 2025-04-21 eng-Latn"},{"location":"overview/available_models/text/#tencentbacconan-embedding-v1","title":"<code>TencentBAC/Conan-embedding-v1</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 326.0M 1.2 GB 2024-08-22 zho-Hans Citation <pre><code>@misc{li2024conanembeddinggeneraltextembedding,\n  title={Conan-embedding: General Text Embedding with More and Better Negative Samples},\n  author={Shiyu Li and Yang Tang and Shizhe Chen and Xi Chen},\n  year={2024},\n  eprint={2408.15710},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL},\n  url={https://arxiv.org/abs/2408.15710},\n}\n</code></pre>"},{"location":"overview/available_models/text/#vovanphucsup-simcse-vietnamese-phobert-base","title":"<code>VoVanPhuc/sup-SimCSE-VietNamese-phobert-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 768 135.0M 517.0 MB 2021-05-26 vie-Latn Citation <pre><code>@article{gao2021simcse,\n   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},\n   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},\n   journal={arXiv preprint arXiv:2104.08821},\n   year={2021}\n}\n\n@inproceedings{phobert,\ntitle     = {{PhoBERT: Pre-trained language models for Vietnamese}},\nauthor    = {Dat Quoc Nguyen and Anh Tuan Nguyen},\nbooktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},\nyear      = {2020},\npages     = {1037--1042}\n}\n</code></pre>"},{"location":"overview/available_models/text/#aari1995german_semantic_sts_v2","title":"<code>aari1995/German_Semantic_STS_V2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.7M 1.3 GB 2022-11-17 deu-Latn"},{"location":"overview/available_models/text/#abhinandmedembed-small-v01","title":"<code>abhinand/MedEmbed-small-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-10-20 eng-Latn Citation <pre><code>@software{balachandran2024medembed,\n  author = {Balachandran, Abhinand},\n  title = {MedEmbed: Medical-Focused Embedding Models},\n  year = {2024},\n  url = {https://github.com/abhinand5/MedEmbed}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ai-foreversbert_large_mt_nlu_ru","title":"<code>ai-forever/sbert_large_mt_nlu_ru</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 427.0M 1.6 GB 2021-05-18 rus-Cyrl"},{"location":"overview/available_models/text/#ai-foreversbert_large_nlu_ru","title":"<code>ai-forever/sbert_large_nlu_ru</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 427.0M 1.6 GB 2020-11-20 rus-Cyrl"},{"location":"overview/available_models/text/#amazontitan-text-embeddings-v2","title":"<code>amazon/Titan-text-embeddings-v2</code>","text":"<p>License: https://aws.amazon.com/service-terms/</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2024-04-30 eng-Latn"},{"location":"overview/available_models/text/#andersborgesmodel2vecdk","title":"<code>andersborges/model2vecdk</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 48.0M 183.0 MB 2025-11-21 dan-Latn Citation <pre><code>@article{minishlab2024model2vec,\n  author = {Tulkens, Stephan and {van Dongen}, Thomas},\n  title = {Model2Vec: Fast State-of-the-Art Static Embeddings},\n  year = {2024},\n  url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#andersborgesmodel2vecdk-stem","title":"<code>andersborges/model2vecdk-stem</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 48.6M 185.0 MB 2025-11-21 dan-Latn Citation <pre><code>@article{minishlab2024model2vec,\n  author = {Tulkens, Stephan and {van Dongen}, Thomas},\n  title = {Model2Vec: Fast State-of-the-Art Static Embeddings},\n  year = {2024},\n  url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#avsolatoriogist-embedding-v0","title":"<code>avsolatorio/GIST-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 418.0 MB 2024-01-31 eng-Latn Citation <pre><code>@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"overview/available_models/text/#avsolatoriogist-all-minilm-l6-v2","title":"<code>avsolatorio/GIST-all-MiniLM-L6-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-02-03 eng-Latn Citation <pre><code>@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"overview/available_models/text/#avsolatoriogist-large-embedding-v0","title":"<code>avsolatorio/GIST-large-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 1.2 GB 2024-02-14 eng-Latn Citation <pre><code>@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"overview/available_models/text/#avsolatoriogist-small-embedding-v0","title":"<code>avsolatorio/GIST-small-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-02-03 eng-Latn Citation <pre><code>@article{solatorio2024gistembed,\n    title={GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning},\n    author={Aivin V. Solatorio},\n    journal={arXiv preprint arXiv:2402.16829},\n    year={2024},\n    URL={https://arxiv.org/abs/2402.16829}\n    eprint={2402.16829},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"overview/available_models/text/#avsolatorionoinstruct-small-embedding-v0","title":"<code>avsolatorio/NoInstruct-small-Embedding-v0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2024-05-01 eng-Latn"},{"location":"overview/available_models/text/#baselinehuman","title":"baseline/Human","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified not specified ara-Arab, dan-Latn, eng-Latn, nob-Latn, rus-Cyrl"},{"location":"overview/available_models/text/#bedrockamazon-titan-embed-text-v1","title":"<code>bedrock/amazon-titan-embed-text-v1</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2023-09-27 not specified"},{"location":"overview/available_models/text/#bedrockamazon-titan-embed-text-v2","title":"<code>bedrock/amazon-titan-embed-text-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 not specified not specified 2024-04-30 not specified"},{"location":"overview/available_models/text/#bigsciencesgpt-bloom-7b1-msmarco","title":"<code>bigscience/sgpt-bloom-7b1-msmarco</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 4096 7.1B not specified 2022-08-26 not specified Citation <pre><code>@article{muennighoff2022sgpt,\n  title={SGPT: GPT Sentence Embeddings for Semantic Search},\n  author={Muennighoff, Niklas},\n  journal={arXiv preprint arXiv:2202.08904},\n  year={2022}\n}\n</code></pre>"},{"location":"overview/available_models/text/#bisectgroupbica-base","title":"<code>bisectgroup/BiCA-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 418.0 MB 2025-11-14 eng-Latn Citation <pre><code>@misc{sinha2025bicaeffectivebiomedicaldense,\n      title={BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives},\n      author={Aarush Sinha and Pavan Kumar S and Roshan Balaji and Nirav Pravinbhai Bhatt},\n      year={2025},\n      eprint={2511.08029},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2511.08029},\n}\n</code></pre>"},{"location":"overview/available_models/text/#bkai-foundation-modelsvietnamese-bi-encoder","title":"<code>bkai-foundation-models/vietnamese-bi-encoder</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 768 135.0M 515.0 MB 2023-09-09 vie-Latn Citation <pre><code>      @article{duc2024towards,\n    title={Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models},\n    author={Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang},\n    journal={arXiv preprint arXiv:2403.01616},\n    year={2024}\n  }\n</code></pre>"},{"location":"overview/available_models/text/#brahmairesearchslx-v01","title":"<code>brahmairesearch/slx-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2024-08-13 eng-Latn"},{"location":"overview/available_models/text/#castorinimonot5-3b-msmarco-10k","title":"<code>castorini/monot5-3b-msmarco-10k</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 3.0B not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-base-msmarco-10k","title":"castorini/monot5-base-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 296.9M not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-large-msmarco-10k","title":"castorini/monot5-large-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#castorinimonot5-small-msmarco-10k","title":"castorini/monot5-small-msmarco-10k","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-03-28 eng-Latn Citation <pre><code>@misc{rosa2022parameterleftbehinddistillation,\n      title={No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval},\n      author={Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira},\n      year={2022},\n      eprint={2206.02873},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2206.02873},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#codesagecodesage-base-v2","title":"<code>codesage/codesage-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 354.7M 1.3 GB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6) Citation <pre><code>@inproceedings{\n    zhang2024code,\n    title={{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}},\n    author={Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=vfzRRjumpX}\n}\n</code></pre>"},{"location":"overview/available_models/text/#codesagecodesage-large-v2","title":"<code>codesage/codesage-large-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 4.8 GB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6) Citation <pre><code>@inproceedings{\n    zhang2024code,\n    title={{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}},\n    author={Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=vfzRRjumpX}\n}\n</code></pre>"},{"location":"overview/available_models/text/#codesagecodesage-small-v2","title":"<code>codesage/codesage-small-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 128.0M 496.0 MB 2024-02-03 go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6) Citation <pre><code>@inproceedings{\n    zhang2024code,\n    title={{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}},\n    author={Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang},\n    booktitle={The Twelfth International Conference on Learning Representations},\n    year={2024},\n    url={https://openreview.net/forum?id=vfzRRjumpX}\n}\n</code></pre>"},{"location":"overview/available_models/text/#cointegratedlabse-en-ru","title":"<code>cointegrated/LaBSE-en-ru</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 129.0M 492.0 MB 2021-06-10 rus-Cyrl"},{"location":"overview/available_models/text/#cointegratedrubert-tiny","title":"<code>cointegrated/rubert-tiny</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 312 11.9M 45.0 MB 2021-05-24 rus-Cyrl"},{"location":"overview/available_models/text/#cointegratedrubert-tiny2","title":"<code>cointegrated/rubert-tiny2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 29.4M 112.0 MB 2021-10-28 rus-Cyrl"},{"location":"overview/available_models/text/#colbert-ircolbertv20","title":"<code>colbert-ir/colbertv2.0</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 180 not specified 109.5M 418.0 MB 2024-09-21 eng-Latn"},{"location":"overview/available_models/text/#consciousaicai-lunaris-text-embeddings","title":"<code>consciousAI/cai-lunaris-text-embeddings</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M not specified 2023-06-22 not specified"},{"location":"overview/available_models/text/#consciousaicai-stellaris-text-embeddings","title":"<code>consciousAI/cai-stellaris-text-embeddings</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 not specified not specified 2023-06-23 not specified"},{"location":"overview/available_models/text/#cross-encoderms-marco-minilm-l12-v2","title":"<code>cross-encoder/ms-marco-MiniLM-L12-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 127.0 MB 2021-04-16 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#cross-encoderms-marco-minilm-l2-v2","title":"<code>cross-encoder/ms-marco-MiniLM-L2-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 15.6M 60.0 MB 2021-04-16 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#cross-encoderms-marco-minilm-l4-v2","title":"<code>cross-encoder/ms-marco-MiniLM-L4-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 19.2M 73.0 MB 2021-04-16 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#cross-encoderms-marco-minilm-l6-v2","title":"<code>cross-encoder/ms-marco-MiniLM-L6-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2021-04-16 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#cross-encoderms-marco-tinybert-l2-v2","title":"<code>cross-encoder/ms-marco-TinyBERT-L2-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 128 4.4M 17.0 MB 2021-04-16 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#deepfileembedder-100p","title":"<code>deepfile/embedder-100p</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2023-07-24 not specified"},{"location":"overview/available_models/text/#deepvkuser-bge-m3","title":"<code>deepvk/USER-bge-m3</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 359.0M 1.3 GB 2024-07-05 rus-Cyrl Citation <pre><code>@misc{deepvk2024user,\n    title={USER: Universal Sentence Encoder for Russian},\n    author={Malashenko, Boris and  Zemerov, Anton and Spirin, Egor},\n    url={https://huggingface.co/datasets/deepvk/USER-base},\n    publisher={Hugging Face},\n    year={2024},\n}\n</code></pre>"},{"location":"overview/available_models/text/#deepvkdeberta-v1-base","title":"<code>deepvk/deberta-v1-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.0M 473.0 MB 2023-02-07 rus-Cyrl"},{"location":"overview/available_models/text/#dmedhipawanembd-68m","title":"<code>dmedhi/PawanEmbd-68M</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 67.8M 260.0 MB 2025-12-08 eng-Latn Citation <pre><code>@misc{medhi2025pawanembd,\n    title={PawanEmbd-68M: Distilled Embedding Model},\n    author={Medhi, D.},\n    year={2025},\n    url={https://huggingface.co/dmedhi/PawanEmbd-68M}\n}\n</code></pre>"},{"location":"overview/available_models/text/#dunzhangstella-large-zh-v3-1792d","title":"<code>dunzhang/stella-large-zh-v3-1792d</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 327.4M not specified 2024-02-17 zho-Hans"},{"location":"overview/available_models/text/#dunzhangstella-mrl-large-zh-v35-1792d","title":"<code>dunzhang/stella-mrl-large-zh-v3.5-1792d</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2024-02-27 zho-Hans"},{"location":"overview/available_models/text/#dwzhue5-base-4k","title":"<code>dwzhu/e5-base-4k</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K not specified 112.2M not specified 2024-03-28 eng-Latn Citation <pre><code>@article{zhu2024longembed,\n  title={LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  author={Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal={arXiv preprint arXiv:2404.12096},\n  year={2024}\n}\n</code></pre>"},{"location":"overview/available_models/text/#facebooksonar","title":"<code>facebook/SONAR</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2021-05-21 ace-Arab, ace-Latn, acm-Arab, acq-Arab, aeb-Arab, ... (204) Citation <pre><code>@misc{Duquenne:2023:sonar_arxiv,\n  author = {Paul-Ambroise Duquenne and Holger Schwenk and Benoit Sagot},\n  title = {{SONAR:} Sentence-Level Multimodal and Language-Agnostic Representations},\n  publisher = {arXiv},\n  year = {2023},\n  url = {https://arxiv.org/abs/2308.11466},\n}\n</code></pre>"},{"location":"overview/available_models/text/#facebookcontriever-msmarco","title":"<code>facebook/contriever-msmarco</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 572.0 MB 2022-06-25 eng-Latn Citation <pre><code>    @misc{izacard2021contriever,\n      title={Unsupervised Dense Information Retrieval with Contrastive Learning},\n      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},\n      year={2021},\n      url = {https://arxiv.org/abs/2112.09118},\n      doi = {10.48550/ARXIV.2112.09118},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#fangxqxyz-embedding","title":"<code>fangxq/XYZ-embedding</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 325.5M 1.2 GB 2024-09-13 zho-Hans"},{"location":"overview/available_models/text/#geoffseeauto-g-embed-st","title":"<code>geoffsee/auto-g-embed-st</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 22.7M 87.0 MB 2026-02-08 eng-Latn"},{"location":"overview/available_models/text/#googleflan-t5-base","title":"<code>google/flan-t5-base</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 247.6M 944.0 MB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-large","title":"<code>google/flan-t5-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 783.2M 2.9 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-xl","title":"google/flan-t5-xl","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 10.6 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#googleflan-t5-xxl","title":"google/flan-t5-xxl","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 42.0 GB 2022-10-21 eng-Latn Citation <pre><code>@misc{10.48550/arxiv.2210.11416,\n      doi = {10.48550/ARXIV.2210.11416},\n      url = {https://arxiv.org/abs/2210.11416},\n      author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n      keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n      title = {Scaling Instruction-Finetuned Language Models},\n      publisher = {arXiv},\n      year = {2022},\n      copyright = {Creative Commons Attribution 4.0 International}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#hiieuhalong_embedding","title":"<code>hiieu/halong_embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2024-07-06 vie-Latn Citation <pre><code>@misc{HalongEmbedding,\n  title={HalongEmbedding: A Vietnamese Text Embedding},\n  author={Ngo Hieu},\n  year={2024},\n  publisher={Huggingface},\n}\n</code></pre>"},{"location":"overview/available_models/text/#iampandazpoint_large_embedding_zh","title":"<code>iampanda/zpoint_large_embedding_zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2024-06-04 zho-Hans"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-107m-multilingual","title":"<code>ibm-granite/granite-embedding-107m-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 107.0M 204.0 MB 2024-12-18 ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13) Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-125m-english","title":"<code>ibm-granite/granite-embedding-125m-english</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.6M 238.0 MB 2024-12-18 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-278m-multilingual","title":"<code>ibm-granite/granite-embedding-278m-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 530.0 MB 2024-12-18 ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13) Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-30m-english","title":"<code>ibm-granite/granite-embedding-30m-english</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 30.3M 58.0 MB 2024-12-18 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-english-r2","title":"<code>ibm-granite/granite-embedding-english-r2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 149.0M 284.0 MB 2025-08-15 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#ibm-granitegranite-embedding-small-english-r2","title":"<code>ibm-granite/granite-embedding-small-english-r2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 384 47.7M 91.0 MB 2025-08-15 eng-Latn Citation <pre><code>@article{awasthy2025graniteembedding,\n  title={Granite Embedding Models},\n  author={Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu},\n  journal={arXiv preprint arXiv:2502.20204},\n  year={2025}\n}\n</code></pre>"},{"location":"overview/available_models/text/#infgradstella-base-en-v2","title":"<code>infgrad/stella-base-en-v2</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 not specified 109.5M not specified 2023-10-19 eng-Latn"},{"location":"overview/available_models/text/#infgradstella-base-zh-v3-1792d","title":"<code>infgrad/stella-base-zh-v3-1792d</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 104.0M not specified 2024-02-17 zho-Hans"},{"location":"overview/available_models/text/#izhxudever-bloom-1b1","title":"<code>izhx/udever-bloom-1b1</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 1.1B not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45) Citation <pre><code>@article{zhang2023language,\n  title={Language Models are Universal Embedders},\n  author={Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min},\n  journal={arXiv preprint arXiv:2310.08232},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#izhxudever-bloom-3b","title":"<code>izhx/udever-bloom-3b</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 3.0B not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45) Citation <pre><code>@article{zhang2023language,\n  title={Language Models are Universal Embedders},\n  author={Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min},\n  journal={arXiv preprint arXiv:2310.08232},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#izhxudever-bloom-560m","title":"<code>izhx/udever-bloom-560m</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 559.2M not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45) Citation <pre><code>@article{zhang2023language,\n  title={Language Models are Universal Embedders},\n  author={Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min},\n  journal={arXiv preprint arXiv:2310.08232},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#izhxudever-bloom-7b1","title":"<code>izhx/udever-bloom-7b1</code>","text":"<p>License: https://huggingface.co/spaces/bigscience/license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-10-24 aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45) Citation <pre><code>@article{zhang2023language,\n  title={Language Models are Universal Embedders},\n  author={Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min},\n  journal={arXiv preprint arXiv:2310.08232},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jhu-clspfollowir-7b","title":"<code>jhu-clsp/FollowIR-7B</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 7.1B 13.5 GB 2024-04-29 eng-Latn Citation <pre><code>    @misc{weller2024followir,\n      title={FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n      author={Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n      year={2024},\n      eprint={2403.15246},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n    }\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-colbert-v2","title":"<code>jinaai/jina-colbert-v2</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K not specified 559.4M 1.0 GB 2024-08-16 ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (22) Citation <pre><code>@inproceedings{xiao-etal-2024-jina,\n    title = \"{J}ina-{C}ol{BERT}-v2: A General-Purpose Multilingual Late Interaction Retriever\",\n    author = {Jha, Rohan  and\n      Wang, Bo  and\n      G{\"u}nther, Michael  and\n      Mastrapas, Georgios  and\n      Sturua, Saba  and\n      Mohr, Isabelle  and\n      Koukounas, Andreas  and\n      Wang, Mohammad Kalim  and\n      Wang, Nan  and\n      Xiao, Han},\n    editor = {S{\"a}lev{\"a}, Jonne  and\n      Owodunni, Abraham},\n    booktitle = \"Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.mrl-1.11/\",\n    doi = \"10.18653/v1/2024.mrl-1.11\",\n    pages = \"159--166\",\n    abstract = \"Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT`s late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce a novel architecture and a training framework to support long context window and multilingual retrieval. Leveraging Matryoshka Representation Loss, we further demonstrate that the reducing the embedding dimensionality from 128 to 64 has insignificant impact on the model`s retrieval performance and cut storage requirements by up to 50{\\%}. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks,\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embedding-b-en-v1","title":"<code>jinaai/jina-embedding-b-en-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.6M 420.0 MB 2023-07-07 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models},\n      author={Michael G\u00fcnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao},\n      year={2023},\n      eprint={2307.11224},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embedding-s-en-v1","title":"<code>jinaai/jina-embedding-s-en-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 35.3M 134.0 MB 2023-07-07 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models},\n      author={Michael G\u00fcnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao},\n      year={2023},\n      eprint={2307.11224},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v2-base-en","title":"<code>jinaai/jina-embeddings-v2-base-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 137.4M 262.0 MB 2023-09-27 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents},\n      author={Michael G\u00fcnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},\n      year={2023},\n      eprint={2310.19923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-embeddings-v2-small-en","title":"<code>jinaai/jina-embeddings-v2-small-en</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 32.7M 62.0 MB 2023-09-27 eng-Latn Citation <pre><code>@misc{g\u00fcnther2023jina,\n      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents},\n      author={Michael G\u00fcnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},\n      year={2023},\n      eprint={2310.19923},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#jinaaijina-reranker-v2-base-multilingual","title":"jinaai/jina-reranker-v2-base-multilingual","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified 531.0 MB 2024-09-26 eng-Latn"},{"location":"overview/available_models/text/#jinaaijina-reranker-v3","title":"<code>jinaai/jina-reranker-v3</code>","text":"<p>License: cc-by-nc-4.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 131.1K not specified 596.8M 1.1 GB 2025-09-18 afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73) Citation <pre><code>@misc{wang2025jinarerankerv3lateinteractionlistwise,\n      title={jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking},\n      author={Feng Wang and Yuqing Li and Han Xiao},\n      year={2025},\n      eprint={2509.25085},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2509.25085},}\n</code></pre>"},{"location":"overview/available_models/text/#keeeeenwmicrollama-text-embedding","title":"<code>keeeeenw/MicroLlama-text-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 1024 271.9M 1.0 GB 2024-11-10 eng-Latn"},{"location":"overview/available_models/text/#lier007xiaobu-embedding","title":"<code>lier007/xiaobu-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 326.0M 1.2 GB 2024-01-09 zho-Hans"},{"location":"overview/available_models/text/#lier007xiaobu-embedding-v2","title":"<code>lier007/xiaobu-embedding-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 326.0M 1.2 GB 2024-06-30 zho-Hans"},{"location":"overview/available_models/text/#lightonaigte-moderncolbert-v1","title":"<code>lightonai/GTE-ModernColBERT-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K not specified 149.0M not specified 2025-04-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\"\n}\n</code></pre>"},{"location":"overview/available_models/text/#llmrailsember-v1","title":"<code>llmrails/ember-v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.0M 1.2 GB 2023-10-10 eng-Latn Citation <pre><code>@misc{nur2024emberv1,\n      title={ember-v1: SOTA embedding model},\n      author={Enrike Nur and Anar Aliyev},\n      year={2023},\n}\n</code></pre>"},{"location":"overview/available_models/text/#m3hrdadfibert-zwnj-wnli-mean-tokens","title":"<code>m3hrdadfi/bert-zwnj-wnli-mean-tokens</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 118.3M 451.0 MB 2021-06-28 fas-Arab"},{"location":"overview/available_models/text/#m3hrdadfiroberta-zwnj-wnli-mean-tokens","title":"<code>m3hrdadfi/roberta-zwnj-wnli-mean-tokens</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 118.3M 451.0 MB 2021-06-28 fas-Arab"},{"location":"overview/available_models/text/#malenia1ternary-weight-embedding","title":"<code>malenia1/ternary-weight-embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 98.7M 158.0 MB 2024-10-23 not specified"},{"location":"overview/available_models/text/#manubge-m3-custom-fr","title":"<code>manu/bge-m3-custom-fr</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1024 567.8M 2.1 GB 2024-04-11 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v02","title":"<code>manu/sentence_croissant_alpha_v0.2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-03-15 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v03","title":"<code>manu/sentence_croissant_alpha_v0.3</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-04-26 not specified"},{"location":"overview/available_models/text/#manusentence_croissant_alpha_v04","title":"<code>manu/sentence_croissant_alpha_v0.4</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 2048 1.3B 2.4 GB 2024-04-27 eng-Latn, fra-Latn"},{"location":"overview/available_models/text/#meta-llamallama-2-7b-chat-hf","title":"<code>meta-llama/Llama-2-7b-chat-hf</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 7.0B not specified 2023-07-18 eng-Latn Citation <pre><code>@misc{touvron2023llama2openfoundation,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2307.09288},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#meta-llamallama-2-7b-hf","title":"meta-llama/Llama-2-7b-hf","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2023-07-18 eng-Latn Citation <pre><code>@misc{touvron2023llama2openfoundation,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models},\n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2307.09288},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#microsoftspeecht5_tts","title":"<code>microsoft/speecht5_tts</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 768 146.3M 558.0 MB 2022-05-16 eng-Latn Citation <pre><code>@misc{ao2022speecht5unifiedmodalencoderdecoderpretraining,\n      title={SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n      author={Junyi Ao and Rui Wang and Long Zhou and Chengyi Wang and Shuo Ren and Yu Wu and Shujie Liu and Tom Ko and Qing Li and Yu Zhang and Zhihua Wei and Yao Qian and Jinyu Li and Furu Wei},\n      year={2022},\n      eprint={2110.07205},\n      archivePrefix={arXiv},\n      primaryClass={eess.AS},\n      url={https://arxiv.org/abs/2110.07205},\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_glove","title":"<code>minishlab/M2V_base_glove</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 102.0M 391.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_glove_subword","title":"<code>minishlab/M2V_base_glove_subword</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 103.0M 391.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_base_output","title":"<code>minishlab/M2V_base_output</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.6M 29.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabm2v_multilingual_output","title":"<code>minishlab/M2V_multilingual_output</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 128.0M 489.0 MB 2024-09-21 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-2m","title":"<code>minishlab/potion-base-2M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 64 2.0M 7.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-4m","title":"<code>minishlab/potion-base-4M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 128 3.8M 14.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-base-8m","title":"<code>minishlab/potion-base-8M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 7.6M 29.0 MB 2024-10-29 eng-Latn Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#minishlabpotion-multilingual-128m","title":"<code>minishlab/potion-multilingual-128M</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 128.0M 489.0 MB 2025-05-23 afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (101) Citation <pre><code>@software{minishlab2024model2vec,\n      authors = {Stephan Tulkens, Thomas van Dongen},\n      title = {Model2Vec: Turn any Sentence Transformer into a Small Fast Model},\n      year = {2024},\n      url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mistralaimistral-7b-instruct-v02","title":"<code>mistralai/Mistral-7B-Instruct-v0.2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified 7.2B not specified 2023-12-11 eng-Latn Citation <pre><code>@misc{jiang2023mistral7b,\n      title={Mistral 7B},\n      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L\u00e9lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed},\n      year={2023},\n      eprint={2310.06825},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2310.06825},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-edge-colbert-v0-17m","title":"<code>mixedbread-ai/mxbai-edge-colbert-v0-17m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.0K not specified 17.0M 64.0 MB 2025-10-16 eng-Latn Citation <pre><code>@misc{takehi2025fantasticsmallretrieverstrain,\n      title={Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report},\n      author={Rikiya Takehi and Benjamin Clavi\u00e9 and Sean Lee and Aamir Shakir},\n      year={2025},\n      eprint={2510.14880},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2510.14880},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-edge-colbert-v0-32m","title":"<code>mixedbread-ai/mxbai-edge-colbert-v0-32m</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 511 not specified 32.0M 122.0 MB 2025-10-16 eng-Latn Citation <pre><code>@misc{takehi2025fantasticsmallretrieverstrain,\n      title={Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report},\n      author={Rikiya Takehi and Benjamin Clavi\u00e9 and Sean Lee and Aamir Shakir},\n      year={2025},\n      eprint={2510.14880},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2510.14880},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-rerank-base-v1","title":"<code>mixedbread-ai/mxbai-rerank-base-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 not specified 184.4M 352.0 MB 2024-02-29 eng-Latn Citation <pre><code>@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-rerank-large-v1","title":"<code>mixedbread-ai/mxbai-rerank-large-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 not specified 435.1M 830.0 MB 2024-02-29 eng-Latn Citation <pre><code>@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mixedbread-aimxbai-rerank-xsmall-v1","title":"<code>mixedbread-ai/mxbai-rerank-xsmall-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 not specified 70.8M 135.0 MB 2024-02-29 eng-Latn Citation <pre><code>@online{rerank2024mxbai,\n  title={Boost Your Search With The Crispy Mixedbread Rerank Models},\n  author={Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee},\n  year={2024},\n  url={https://www.mixedbread.ai/blog/mxbai-rerank-v1},\n}\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-base","title":"<code>moka-ai/m3e-base</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 102.3M 390.0 MB 2023-06-06 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-large","title":"<code>moka-ai/m3e-large</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 325.5M not specified 2023-06-21 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#moka-aim3e-small","title":"<code>moka-ai/m3e-small</code>","text":"<p>License: https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 512 24.0M not specified 2023-06-02 eng-Latn, zho-Hans Citation <pre><code>@software{MokaMassiveMixedEmbedding,\n  author = {Wang Yuxin and Sun Qingxuan and He Sicheng},\n  title = {M3E: Moka Massive Mixed Embedding Model},\n  year = {2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#mtebbaseline-bb25","title":"<code>mteb/baseline-bb25</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2026-02-06 not specified Citation <pre><code>@software{jeong2026bayesianbm25,\n  title={Bayesian BM25: A Probabilistic Framework for Hybrid Text and Vector Search},\n  author={Jeong, Jaepil},\n  year={2026},\n  doi={10.5281/zenodo.18414941},\n  url={https://doi.org/10.5281/zenodo.18414941},\n}\n@software{jeong2026neural,\n  title={From Bayesian Inference to Neural Computation: The Analytical Emergence of Neural Network Structure from Probabilistic Relevance Estimation},\n  author={Jeong, Jaepil},\n  year={2026},\n  doi={10.5281/zenodo.18512411},\n  url={https://doi.org/10.5281/zenodo.18512411},\n}\n</code></pre>"},{"location":"overview/available_models/text/#mtebbaseline-bm25s","title":"<code>mteb/baseline-bm25s</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2024-07-10 eng-Latn Citation <pre><code>@misc{bm25s,\n      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring},\n      author={Xing Han L\u00f9},\n      year={2024},\n      eprint={2407.03618},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2407.03618},\n}\n</code></pre>"},{"location":"overview/available_models/text/#myrkursentence-transformer-parsbert-fa","title":"<code>myrkur/sentence-transformer-parsbert-fa</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 162.8M 621.0 MB 2024-12-10 fas-Arab"},{"location":"overview/available_models/text/#nvidiallama-nemotron-rerank-1b-v2","title":"<code>nvidia/llama-nemotron-rerank-1b-v2</code>","text":"<p>License: https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 2048 1.2B 2.3 GB 2025-10-16 eng-Latn"},{"location":"overview/available_models/text/#omarelshehyarabic-english-sts-matryoshka","title":"<code>omarelshehy/arabic-english-sts-matryoshka</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2024-10-13 ara-Arab, eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#openaitext-embedding-3-large","title":"<code>openai/text-embedding-3-large</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 3072 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-large-embed_dim512","title":"<code>openai/text-embedding-3-large (embed_dim=512)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-small","title":"<code>openai/text-embedding-3-small</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-3-small-embed_dim512","title":"<code>openai/text-embedding-3-small (embed_dim=512)</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 512 not specified not specified 2024-01-25 not specified"},{"location":"overview/available_models/text/#openaitext-embedding-ada-002","title":"<code>openai/text-embedding-ada-002</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1536 not specified not specified 2022-12-15 not specified"},{"location":"overview/available_models/text/#openbmbminicpm-embedding","title":"<code>openbmb/MiniCPM-Embedding</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 2304 2.7B 5.1 GB 2024-09-04 eng-Latn, zho-Hans"},{"location":"overview/available_models/text/#panalexeuxlm-roberta-ua-distilled","title":"<code>panalexeu/xlm-roberta-ua-distilled</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2025-04-15 eng-Latn, ukr-Cyrl Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#prdevmini-gte","title":"<code>prdev/mini-gte</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 66.4M 253.0 MB 2025-01-28 eng-Latn"},{"location":"overview/available_models/text/#rasgaardm2v-dfm-large","title":"<code>rasgaard/m2v-dfm-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 256 22.9M 87.0 MB 2025-10-08 dan-Latn Citation <pre><code>@article{minishlab2024model2vec,\n    author = {Tulkens, Stephan and {van Dongen}, Thomas},\n    title = {Model2Vec: Fast State-of-the-Art Static Embeddings},\n    year = {2024},\n    url = {https://github.com/MinishLab/model2vec}\n}\n</code></pre>"},{"location":"overview/available_models/text/#richinfoairitrieve_zh_v1","title":"<code>richinfoai/ritrieve_zh_v1</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1792 326.0M 1.2 GB 2025-03-25 zho-Hans"},{"location":"overview/available_models/text/#sbintuitionssarashina-embedding-v1-1b","title":"<code>sbintuitions/sarashina-embedding-v1-1b</code>","text":"<p>License: https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b/blob/main/LICENSE</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 1792 1.2B 4.6 GB 2024-11-22 jpn-Jpan"},{"location":"overview/available_models/text/#sbunlpfabert","title":"<code>sbunlp/fabert</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 124.4M 475.0 MB 2024-10-07 fas-Arab Citation <pre><code>@inproceedings{masumi-etal-2025-fabert,\n    title = \"{F}a{BERT}: Pre-training {BERT} on {P}ersian Blogs\",\n    author = \"Masumi, Mostafa  and\n      Majd, Seyed Soroush  and\n      Shamsfard, Mehrnoush  and\n      Beigy, Hamid\",\n    editor = \"Bak, JinYeong  and\n      Goot, Rob van der  and\n      Jang, Hyeju  and\n      Buaphet, Weerayut  and\n      Ramponi, Alan  and\n      Xu, Wei  and\n      Ritter, Alan\",\n    booktitle = \"Proceedings of the Tenth Workshop on Noisy and User-generated Text\",\n    month = may,\n    year = \"2025\",\n    address = \"Albuquerque, New Mexico, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2025.wnut-1.10/\",\n    doi = \"10.18653/v1/2025.wnut-1.10\",\n    pages = \"85--96\",\n    ISBN = \"979-8-89176-232-9\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sdadasmmlw-e5-base","title":"<code>sdadas/mmlw-e5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 278.0M 1.0 GB 2023-11-17 pol-Latn Citation <pre><code>@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods},\n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#sdadasmmlw-e5-large","title":"<code>sdadas/mmlw-e5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 559.9M 2.1 GB 2023-11-17 pol-Latn Citation <pre><code>@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods},\n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#sdadasmmlw-e5-small","title":"<code>sdadas/mmlw-e5-small</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2023-11-17 pol-Latn Citation <pre><code>@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods},\n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#sdadasmmlw-roberta-base","title":"<code>sdadas/mmlw-roberta-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 768 124.4M 475.0 MB 2023-11-17 pol-Latn Citation <pre><code>@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods},\n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#sdadasmmlw-roberta-large","title":"<code>sdadas/mmlw-roberta-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 514 1024 435.0M 1.6 GB 2023-11-17 pol-Latn Citation <pre><code>@article{dadas2024pirb,\n  title={{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods},\n  author={S\u0142awomir Dadas and Micha\u0142 Pere\u0142kiewicz and Rafa\u0142 Po\u015bwiata},\n  year={2024},\n  eprint={2402.13350},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n</code></pre>"},{"location":"overview/available_models/text/#sensenovapiccolo-base-zh","title":"<code>sensenova/piccolo-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 102.3M not specified 2023-09-04 zho-Hans"},{"location":"overview/available_models/text/#sensenovapiccolo-large-zh-v2","title":"<code>sensenova/piccolo-large-zh-v2</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 not specified not specified 2024-04-22 zho-Hans Citation <pre><code>@misc{2405.06932,\n    Author = {Junqin Huang and Zhongjie Hu and Zihao Jing and Mengya Gao and Yichao Wu},\n    Title = {Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training},\n    Year = {2024},\n    Eprint = {arXiv:2405.06932},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerslabse","title":"<code>sentence-transformers/LaBSE</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 470.9M 1.8 GB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@misc{feng2022languageagnosticbertsentenceembedding,\n      title={Language-agnostic BERT Sentence Embedding},\n      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},\n      year={2022},\n      eprint={2007.01852},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2007.01852},\n    }\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-minilm-l12-v2","title":"<code>sentence-transformers/all-MiniLM-L12-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 33.4M 127.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-minilm-l6-v2","title":"<code>sentence-transformers/all-MiniLM-L6-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 22.7M 87.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersall-mpnet-base-v2","title":"<code>sentence-transformers/all-mpnet-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 384 768 109.5M 418.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-base","title":"<code>sentence-transformers/gtr-t5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 110.2M 209.0 MB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021largedualencodersgeneralizable,\n      title={Large Dual Encoders Are Generalizable Retrievers},\n      author={Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hern\u00e1ndez \u00c1brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang},\n      year={2021},\n      eprint={2112.07899},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2112.07899},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-large","title":"<code>sentence-transformers/gtr-t5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.7M 639.0 MB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021largedualencodersgeneralizable,\n      title={Large Dual Encoders Are Generalizable Retrievers},\n      author={Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hern\u00e1ndez \u00c1brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang},\n      year={2021},\n      eprint={2112.07899},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2112.07899},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-xl","title":"<code>sentence-transformers/gtr-t5-xl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 1.2B 2.3 GB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021largedualencodersgeneralizable,\n      title={Large Dual Encoders Are Generalizable Retrievers},\n      author={Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hern\u00e1ndez \u00c1brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang},\n      year={2021},\n      eprint={2112.07899},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2112.07899},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersgtr-t5-xxl","title":"<code>sentence-transformers/gtr-t5-xxl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 4.9B 9.1 GB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021largedualencodersgeneralizable,\n      title={Large Dual Encoders Are Generalizable Retrievers},\n      author={Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hern\u00e1ndez \u00c1brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang},\n      year={2021},\n      eprint={2112.07899},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR},\n      url={https://arxiv.org/abs/2112.07899},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersmulti-qa-minilm-l6-cos-v1","title":"<code>sentence-transformers/multi-qa-MiniLM-L6-cos-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 22.7M 87.0 MB 2021-08-30 eng-Latn Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersmulti-qa-mpnet-base-dot-v1","title":"<code>sentence-transformers/multi-qa-mpnet-base-dot-v1</code>","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 418.0 MB 2021-08-23 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersparaphrase-multilingual-minilm-l12-v2","title":"<code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 117.7M 449.0 MB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersparaphrase-multilingual-mpnet-base-v2","title":"<code>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 278.0M 1.0 GB 2019-11-01 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53) Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"http://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-base","title":"<code>sentence-transformers/sentence-t5-base</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 222.9M 209.0 MB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021sentencet5scalablesentenceencoders,\n      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models},\n      author={Jianmo Ni and Gustavo Hern\u00e1ndez \u00c1brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},\n      year={2021},\n      eprint={2108.08877},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2108.08877},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-large","title":"<code>sentence-transformers/sentence-t5-large</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 335.7M 639.0 MB 2022-02-09 eng-Latn Citation <pre><code>@misc{ni2021sentencet5scalablesentenceencoders,\n      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models},\n      author={Jianmo Ni and Gustavo Hern\u00e1ndez \u00c1brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},\n      year={2021},\n      eprint={2108.08877},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2108.08877},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-xl","title":"<code>sentence-transformers/sentence-t5-xl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 1.2B 2.3 GB 2024-03-27 eng-Latn Citation <pre><code>@misc{ni2021sentencet5scalablesentenceencoders,\n      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models},\n      author={Jianmo Ni and Gustavo Hern\u00e1ndez \u00c1brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},\n      year={2021},\n      eprint={2108.08877},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2108.08877},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformerssentence-t5-xxl","title":"<code>sentence-transformers/sentence-t5-xxl</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 4.9B 9.1 GB 2024-03-27 eng-Latn Citation <pre><code>@misc{ni2021sentencet5scalablesentenceencoders,\n      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models},\n      author={Jianmo Ni and Gustavo Hern\u00e1ndez \u00c1brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},\n      year={2021},\n      eprint={2108.08877},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2108.08877},\n}\n</code></pre>"},{"location":"overview/available_models/text/#sentence-transformersstatic-retrieval-mrl-en-v1","title":"<code>sentence-transformers/static-retrieval-mrl-en-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages infP 1024 31.3M 119.0 MB 2024-10-24 eng-Latn"},{"location":"overview/available_models/text/#sentence-transformersstatic-similarity-mrl-multilingual-v1","title":"<code>sentence-transformers/static-similarity-mrl-multilingual-v1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified 1024 108.4M 413.0 MB 2025-01-15 ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (49) Citation <pre><code>@inproceedings{reimers-2019-sentence-bert,\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n    author = \"Reimers, Nils and Gurevych, Iryna\",\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n    month = \"11\",\n    year = \"2019\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://arxiv.org/abs/1908.10084\",\n}\n</code></pre>"},{"location":"overview/available_models/text/#sergeyzhlabse-ru-turbo","title":"<code>sergeyzh/LaBSE-ru-turbo</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 129.0M 490.0 MB 2024-06-27 rus-Cyrl"},{"location":"overview/available_models/text/#sergeyzhrubert-tiny-turbo","title":"<code>sergeyzh/rubert-tiny-turbo</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 2.0K 312 29.2M 111.0 MB 2024-06-21 rus-Cyrl"},{"location":"overview/available_models/text/#shibing624text2vec-base-chinese","title":"<code>shibing624/text2vec-base-chinese</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 102.3M 390.0 MB 2022-01-23 zho-Hans Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#shibing624text2vec-base-chinese-paraphrase","title":"<code>shibing624/text2vec-base-chinese-paraphrase</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 117.9M 450.0 MB 2023-06-19 zho-Hans Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#shibing624text2vec-base-multilingual","title":"<code>shibing624/text2vec-base-multilingual</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 256 384 117.7M 449.0 MB 2023-06-22 deu-Latn, eng-Latn, fra-Latn, ita-Latn, nld-Latn, ... (10) Citation <pre><code>@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n</code></pre>"},{"location":"overview/available_models/text/#silma-aisilma-embeddding-matryoshka-v01","title":"<code>silma-ai/silma-embeddding-matryoshka-v0.1</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 135.2M 516.0 MB 2024-10-12 ara-Arab, eng-Latn Citation <pre><code>@misc{silma2024embedding,\n  author = {Abu Bakr Soliman, Karim Ouda, SILMA AI},\n  title = {SILMA Embedding Matryoshka 0.1},\n  year = {2024},\n  publisher = {Hugging Face},\n  howpublished = {https://huggingface.co/silma-ai/silma-embeddding-matryoshka-0.1},\n}\n</code></pre>"},{"location":"overview/available_models/text/#spartan8806atles-champion-embedding","title":"<code>spartan8806/atles-champion-embedding</code>","text":"<p>License: apache-2.0</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 420.0 MB 2025-11-15 eng-Latn Citation <pre><code>@article{conner2025epistemic,\n  title={The Epistemic Barrier: How RLHF Makes AI Consciousness Empirically Undecidable},\n  author={Conner (spartan8806)},\n  journal={ATLES Research Papers},\n  year={2025},\n  note={Cross-model validation study (Phoenix, Grok, Gemini, Claude)}\n}\n</code></pre>"},{"location":"overview/available_models/text/#stephantulkensnife-gte-modernbert-base_as_router","title":"<code>stephantulkens/NIFE-gte-modernbert-base_as_router</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 8.2K 768 225.8M 861.0 MB 2025-10-30 eng-Latn Citation <pre><code>@software{Tulkens2025pyNIFE,\n  author       = {St'{e}phan Tulkens},\n  title        = {pyNIFE: nearly inference free embeddings in python},\n  year         = {2025},\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.17512919},\n  url          = {https://github.com/stephantul/pynife},\n  license      = {MIT},\n}\n</code></pre>"},{"location":"overview/available_models/text/#stephantulkensnife-mxbai-embed-large-v1_as_router","title":"<code>stephantulkens/NIFE-mxbai-embed-large-v1_as_router</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 445.7M 1.7 GB 2025-11-03 eng-Latn Citation <pre><code>@software{Tulkens2025pyNIFE,\n  author       = {St'{e}phan Tulkens},\n  title        = {pyNIFE: nearly inference free embeddings in python},\n  year         = {2025},\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.17512919},\n  url          = {https://github.com/stephantul/pynife},\n  license      = {MIT},\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-base","title":"<code>thenlper/gte-base</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 768 109.5M 209.0 MB 2023-07-27 eng-Latn Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-base-zh","title":"<code>thenlper/gte-base-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 102.3M 195.0 MB 2023-11-08 zho-Hans Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-large","title":"<code>thenlper/gte-large</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 335.1M 639.0 MB 2023-07-27 eng-Latn Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-large-zh","title":"<code>thenlper/gte-large-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 325.5M 621.0 MB 2023-11-08 zho-Hans Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-small","title":"<code>thenlper/gte-small</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 384 33.4M 64.0 MB 2023-07-27 eng-Latn Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#thenlpergte-small-zh","title":"<code>thenlper/gte-small-zh</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 512 1024 30.3M 58.0 MB 2023-11-08 zho-Hans Citation <pre><code>@article{li2023towards,\n  title={Towards general text embeddings with multi-stage contrastive learning},\n  author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},\n  journal={arXiv preprint arXiv:2308.03281},\n  year={2023}\n}\n</code></pre>"},{"location":"overview/available_models/text/#unicamp-dlmt5-base-mmarco-v2","title":"unicamp-dl/mt5-base-mmarco-v2","text":"<p>License: not specified</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages not specified not specified not specified not specified 2022-01-05 afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103) Citation <pre><code>@article{DBLP:journals/corr/abs-2108-13897,\n  author = {Luiz Bonifacio and\nIsrael Campiotti and\nRoberto de Alencar Lotufo and\nRodrigo Frassetto Nogueira},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},\n  eprint = {2108.13897},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 20 Mar 2023 15:35:34 +0100},\n  title = {mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset},\n  url = {https://arxiv.org/abs/2108.13897},\n  volume = {abs/2108.13897},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_models/text/#w601sxsb1ade-embed","title":"<code>w601sxs/b1ade-embed</code>","text":"<p>License: mit</p> Max Tokens Embedding dimension Parameters Required Memory (Mb) Release date Languages 4.1K 1024 335.1M 1.2 GB 2025-03-10 eng-Latn Citation <pre><code>    @misc{bigscience_workshop_2022,\n    author       = { {Shreyas Subramanian} },\n    title        = { {b1ade series of models} },\n    year         = 2024,\n    url          = { https://huggingface.co/w601sxs/b1ade-embed },\n    publisher    = { Hugging Face }\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/","title":"Any2AnyMultilingualRetrieval","text":"<ul> <li>Number of tasks: 3</li> </ul>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#witt2iretrieval","title":"WITT2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>mteb/wit</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 ara, bul, dan, ell, eng, ... (11) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bugliarello2022iglue,\n  author = {Bugliarello, Emanuele and Liu, Fangyu and Pfeiffer, Jonas and Reddy, Siva and Elliott, Desmond and Ponti, Edoardo Maria and Vuli{\\'c}, Ivan},\n  booktitle = {International Conference on Machine Learning},\n  organization = {PMLR},\n  pages = {2370--2392},\n  title = {IGLUE: A benchmark for transfer learning across modalities, tasks, and languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#xflickr30kcot2iretrieval","title":"XFlickr30kCoT2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>mteb/xflickrco</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, ind, jpn, rus, ... (8) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bugliarello2022iglue,\n  author = {Bugliarello, Emanuele and Liu, Fangyu and Pfeiffer, Jonas and Reddy, Siva and Elliott, Desmond and Ponti, Edoardo Maria and Vuli{\\'c}, Ivan},\n  booktitle = {International Conference on Machine Learning},\n  organization = {PMLR},\n  pages = {2370--2392},\n  title = {IGLUE: A benchmark for transfer learning across modalities, tasks, and languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anymultilingualretrieval/#xm3600t2iretrieval","title":"XM3600T2IRetrieval","text":"<p>Retrieve images based on multilingual descriptions.</p> <p>Dataset: <code>mteb/xm3600</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 ara, ben, ces, dan, deu, ... (38) Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{thapliyal2022crossmodal,\n  author = {Thapliyal, Ashish V and Tuset, Jordi Pont and Chen, Xi and Soricut, Radu},\n  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},\n  pages = {715--729},\n  title = {Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/","title":"Any2AnyRetrieval","text":"<ul> <li>Number of tasks: 89</li> </ul>"},{"location":"overview/available_tasks/any2anyretrieval/#audiocapsa2tretrieval","title":"AudioCapsA2TRetrieval","text":"<p>Natural language description for any kind of audio in the wild.</p> <p>Dataset: <code>mteb/audiocaps_a2t</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng, zxx Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{kim2019audiocaps,\n  author = {Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee},\n  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},\n  pages = {119--132},\n  title = {Audiocaps: Generating captions for audios in the wild},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#audiocapst2aretrieval","title":"AudioCapsT2ARetrieval","text":"<p>Natural language description for any kind of audio in the wild.</p> <p>Dataset: <code>mteb/audiocaps_t2a</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng, zxx Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{kim2019audiocaps,\n  author = {Kim, Chris Dongjoo and Kim, Byeongchang and Lee, Hyunmin and Kim, Gunhee},\n  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},\n  pages = {119--132},\n  title = {Audiocaps: Generating captions for audios in the wild},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#audiosetstronga2tretrieval","title":"AudioSetStrongA2TRetrieval","text":"<p>Retrieve all temporally-strong labeled events within 10s audio clips from the AudioSet Strongly-Labeled subset.</p> <p>Dataset: <code>mteb/audioset_strong_a2t</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng AudioScene derived found Citation <pre><code>@misc{hershey2021benefittemporallystronglabelsaudio,\n  archiveprefix = {arXiv},\n  author = {Shawn Hershey and Daniel P W Ellis and Eduardo Fonseca and Aren Jansen and Caroline Liu and R Channing Moore and Manoj Plakal},\n  eprint = {2105.07031},\n  primaryclass = {cs.SD},\n  title = {The Benefit Of Temporally-Strong Labels In Audio Event Classification},\n  url = {https://arxiv.org/abs/2105.07031},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#audiosetstrongt2aretrieval","title":"AudioSetStrongT2ARetrieval","text":"<p>Retrieve audio segments corresponding to a given sound event label from the AudioSet Strongly-Labeled 10s clips.</p> <p>Dataset: <code>mteb/audioset_strong_t2a</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng AudioScene derived found Citation <pre><code>@misc{hershey2021benefittemporallystronglabelsaudio,\n  archiveprefix = {arXiv},\n  author = {Shawn Hershey and Daniel P W Ellis and Eduardo Fonseca and Aren Jansen and Caroline Liu and R Channing Moore and Manoj Plakal},\n  eprint = {2105.07031},\n  primaryclass = {cs.SD},\n  title = {The Benefit Of Temporally-Strong Labels In Audio Event Classification},\n  url = {https://arxiv.org/abs/2105.07031},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#blinkit2iretrieval","title":"BLINKIT2IRetrieval","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>mteb/blink-it2i</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) hit_rate_at_1 eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#blinkit2tretrieval","title":"BLINKIT2TRetrieval","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>mteb/blink-it2t</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) hit_rate_at_1 eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cirrit2iretrieval","title":"CIRRIT2IRetrieval","text":"<p>Retrieve images based on texts and images.</p> <p>Dataset: <code>mteb/mbeir_cirr_task7</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{liu2021image,\n  author = {Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {2125--2134},\n  title = {Image retrieval on real-life images with pre-trained vision-and-language models},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cmuarctica2tretrieval","title":"CMUArcticA2TRetrieval","text":"<p>Retrieve the correct transcription for an English speech segment. The dataset is derived from the phonetically balanced CMU Arctic single-speaker TTS corpora. The corpora contains 1150 samples based on read-aloud segments from books, which are out of copyright and derived from the Gutenberg project.</p> <p>Dataset: <code>mteb/CMU_Arctic_a2t</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@techreport{cmu-lti-03-177,\n  author = {Clark, Rob and Richmond, Keith},\n  institution = {Carnegie Mellon University, Language Technologies Institute},\n  number = {CMU-LTI-03-177},\n  title = {A detailed report on the CMU Arctic speech database},\n  year = {2003},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cmuarctict2aretrieval","title":"CMUArcticT2ARetrieval","text":"<p>Retrieve the correct audio segment for an English transcription. The dataset is derived from the phonetically balanced CMU Arctic single-speaker TTS corpora. The corpora contains 1150 audio-text pairs based on read-aloud segments from public domain books originally sourced from the Gutenberg project.</p> <p>Dataset: <code>mteb/CMU_Arctic_t2a</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@techreport{cmu-lti-03-177,\n  author = {Clark, Rob and Richmond, Keith},\n  institution = {Carnegie Mellon University, Language Technologies Institute},\n  number = {CMU-LTI-03-177},\n  title = {A detailed report on the CMU Arctic speech database},\n  year = {2003},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#cub200i2iretrieval","title":"CUB200I2IRetrieval","text":"<p>Retrieve bird images from 200 classes.</p> <p>Dataset: <code>mteb/cub200_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@article{welinder2010caltech,\n  author = {Welinder, Peter and Branson, Steve and Mita, Takeshi and Wah, Catherine and Schroff, Florian and Belongie, Serge and Perona, Pietro},\n  month = {09},\n  pages = {},\n  title = {Caltech-UCSD Birds 200},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#clothoa2tretrieval","title":"ClothoA2TRetrieval","text":"<p>An audio captioning datasetst containing audio clips and their corresponding captions.</p> <p>Dataset: <code>mteb/Clotho</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Encyclopaedic, Written derived found Citation <pre><code>@misc{drossos2019clothoaudiocaptioningdataset,\n  archiveprefix = {arXiv},\n  author = {Konstantinos Drossos and Samuel Lipping and Tuomas Virtanen},\n  eprint = {1910.09387},\n  primaryclass = {cs.SD},\n  title = {Clotho: An Audio Captioning Dataset},\n  url = {https://arxiv.org/abs/1910.09387},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#clothot2aretrieval","title":"ClothoT2ARetrieval","text":"<p>An audio captioning datasetst containing audio clips from the Freesound platform and their corresponding captions.</p> <p>Dataset: <code>mteb/Clotho</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Encyclopaedic, Written derived found Citation <pre><code>@misc{drossos2019clothoaudiocaptioningdataset,\n  archiveprefix = {arXiv},\n  author = {Konstantinos Drossos and Samuel Lipping and Tuomas Virtanen},\n  eprint = {1910.09387},\n  primaryclass = {cs.SD},\n  title = {Clotho: An Audio Captioning Dataset},\n  url = {https://arxiv.org/abs/1910.09387},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#commonvoicemini17a2tretrieval","title":"CommonVoiceMini17A2TRetrieval","text":"<p>Speech recordings with corresponding text transcriptions from CommonVoice dataset.</p> <p>Dataset: <code>mteb/common_voice_17_0_mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 ara, ast, bel, ben, bre, ... (50) Spoken human-annotated found Citation <pre><code>@inproceedings{ardila2019common,\n  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4218--4222},\n  title = {Common voice: A massively-multilingual speech corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#commonvoicemini17t2aretrieval","title":"CommonVoiceMini17T2ARetrieval","text":"<p>Speech recordings with corresponding text transcriptions from CommonVoice dataset.</p> <p>Dataset: <code>mteb/common_voice_17_0_mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 ara, ast, bel, ben, bre, ... (50) Spoken human-annotated found Citation <pre><code>@inproceedings{ardila2019common,\n  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4218--4222},\n  title = {Common voice: A massively-multilingual speech corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#commonvoicemini21a2tretrieval","title":"CommonVoiceMini21A2TRetrieval","text":"<p>Speech recordings with corresponding text transcriptions from CommonVoice dataset.</p> <p>Dataset: <code>mteb/common_voice_21_0_mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 abk, afr, amh, ara, asm, ... (114) Spoken human-annotated found Citation <pre><code>@inproceedings{ardila2019common,\n  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4218--4222},\n  title = {Common voice: A massively-multilingual speech corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#commonvoicemini21t2aretrieval","title":"CommonVoiceMini21T2ARetrieval","text":"<p>Speech recordings with corresponding text transcriptions from CommonVoice dataset.</p> <p>Dataset: <code>mteb/common_voice_21_0_mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 abk, afr, amh, ara, asm, ... (114) Spoken human-annotated found Citation <pre><code>@inproceedings{ardila2019common,\n  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4218--4222},\n  title = {Common voice: A massively-multilingual speech corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#edist2itretrieval","title":"EDIST2ITRetrieval","text":"<p>Retrieve news images and titles based on news content.</p> <p>Dataset: <code>mteb/mbeir_edis_task2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image, text (t2it) ndcg_at_10 eng News derived created Citation <pre><code>@inproceedings{liu2023edis,\n  author = {Liu, Siqi and Feng, Weixi and Fu, Tsu-Jui and Chen, Wenhu and Wang, William},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {4877--4894},\n  title = {EDIS: Entity-Driven Image Search over Multimodal Web Content},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#emovdba2tretrieval","title":"EmoVDBA2TRetrieval","text":"<p>Natural language emotional captions for speech segments from the EmoV-DB emotional voices database.</p> <p>Dataset: <code>mteb/EmoV_DB_a2t</code> \u2022 License: https://github.com/numediart/EmoV-DB/blob/master/LICENSE.md \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@misc{adigwe2018emotional,\n  archiveprefix = {arXiv},\n  author = {Adaeze Adigwe and No\u00e9 Tits and Kevin El Haddad and Sarah Ostadabbas and Thierry Dutoit},\n  eprint = {1806.09514},\n  primaryclass = {cs.CL},\n  title = {The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems},\n  url = {https://arxiv.org/abs/1806.09514},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#emovdbt2aretrieval","title":"EmoVDBT2ARetrieval","text":"<p>Natural language emotional captions for speech segments from the EmoV-DB emotional voices database.</p> <p>Dataset: <code>mteb/EmoV_DB_t2a</code> \u2022 License: https://github.com/numediart/EmoV-DB/blob/master/LICENSE.md \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@misc{adigwe2018emotional,\n  archiveprefix = {arXiv},\n  author = {Adaeze Adigwe and No\u00e9 Tits and Kevin El Haddad and Sarah Ostadabbas and Thierry Dutoit},\n  eprint = {1806.09514},\n  primaryclass = {cs.CL},\n  title = {The Emotional Voices Database: Towards Controlling the Emotion Dimension in Voice Generation Systems},\n  url = {https://arxiv.org/abs/1806.09514},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#encyclopediavqait2itretrieval","title":"EncyclopediaVQAIT2ITRetrieval","text":"<p>Retrieval Wiki passage and image and passage to answer query about an image.</p> <p>Dataset: <code>izhx/UMRB-EncyclopediaVQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) hit_rate_at_5 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{mensink2023encyclopedic,\n  author = {Mensink, Thomas and Uijlings, Jasper and Castrejon, Lluis and Goel, Arushi and Cadar, Felipe and Zhou, Howard and Sha, Fei and Araujo, Andr{\\'e} and Ferrari, Vittorio},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {3113--3124},\n  title = {Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#forbi2iretrieval","title":"FORBI2IRetrieval","text":"<p>Retrieve flat object images from 8 classes.</p> <p>Dataset: <code>mteb/forb_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@misc{wu2023forbflatobjectretrieval,\n  archiveprefix = {arXiv},\n  author = {Pengxiang Wu and Siman Wang and Kevin Dela Rosa and Derek Hao Hu},\n  eprint = {2309.16249},\n  primaryclass = {cs.CV},\n  title = {FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding},\n  url = {https://arxiv.org/abs/2309.16249},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashion200ki2tretrieval","title":"Fashion200kI2TRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>mteb/mbeir_fashion200k_task3</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{han2017automatic,\n  author = {Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},\n  booktitle = {Proceedings of the IEEE international conference on computer vision},\n  pages = {1463--1471},\n  title = {Automatic spatially-aware fashion concept discovery},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashion200kt2iretrieval","title":"Fashion200kT2IRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>mteb/mbeir_fashion200k_task0</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{han2017automatic,\n  author = {Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},\n  booktitle = {Proceedings of the IEEE international conference on computer vision},\n  pages = {1463--1471},\n  title = {Automatic spatially-aware fashion concept discovery},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fashioniqit2iretrieval","title":"FashionIQIT2IRetrieval","text":"<p>Retrieve clothes based on descriptions.</p> <p>Dataset: <code>mteb/mbeir_fashioniq_task7</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{wu2021fashion,\n  author = {Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},\n  booktitle = {Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},\n  pages = {11307--11317},\n  title = {Fashion iq: A new dataset towards retrieving images by natural language feedback},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fleursa2tretrieval","title":"FleursA2TRetrieval","text":"<p>Speech recordings with corresponding text transcriptions from the FLEURS dataset.</p> <p>Dataset: <code>mteb/fleurs</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 afr, amh, ara, asm, ast, ... (102) Spoken human-annotated found Citation <pre><code>@inproceedings{conneau2023fleurs,\n  author = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},\n  booktitle = {2022 IEEE Spoken Language Technology Workshop (SLT)},\n  organization = {IEEE},\n  pages = {798--805},\n  title = {Fleurs: Few-shot learning evaluation of universal representations of speech},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#fleurst2aretrieval","title":"FleursT2ARetrieval","text":"<p>Speech recordings with corresponding text transcriptions from the FLEURS dataset.</p> <p>Dataset: <code>mteb/fleurs</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 afr, amh, ara, asm, ast, ... (102) Spoken human-annotated found Citation <pre><code>@inproceedings{conneau2023fleurs,\n  author = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},\n  booktitle = {2022 IEEE Spoken Language Technology Workshop (SLT)},\n  organization = {IEEE},\n  pages = {798--805},\n  title = {Fleurs: Few-shot learning evaluation of universal representations of speech},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#flickr30ki2tretrieval","title":"Flickr30kI2TRetrieval","text":"<p>Retrieve captions based on images.</p> <p>Dataset: <code>mteb/flickr30ki2t</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@article{Young2014FromID,\n  author = {Peter Young and Alice Lai and Micah Hodosh and J. Hockenmaier},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {67-78},\n  title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n  url = {https://api.semanticscholar.org/CorpusID:3104920},\n  volume = {2},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#flickr30kt2iretrieval","title":"Flickr30kT2IRetrieval","text":"<p>Retrieve images based on captions.</p> <p>Dataset: <code>mteb/flickr30kt2i</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@article{Young2014FromID,\n  author = {Peter Young and Alice Lai and Micah Hodosh and J. Hockenmaier},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {67-78},\n  title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},\n  url = {https://api.semanticscholar.org/CorpusID:3104920},\n  volume = {2},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gldv2i2iretrieval","title":"GLDv2I2IRetrieval","text":"<p>Retrieve names of landmarks based on their image.</p> <p>Dataset: <code>mteb/gld-v2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Weyand_2020_CVPR,\n  author = {Weyand, Tobias and Araujo, Andre and Cao, Bingyi and Sim, Jack},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gldv2i2tretrieval","title":"GLDv2I2TRetrieval","text":"<p>Retrieve names of landmarks based on their image.</p> <p>Dataset: <code>mteb/gld-v2-i2t</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Weyand_2020_CVPR,\n  author = {Weyand, Tobias and Araujo, Andre and Cao, Bingyi and Sim, Jack},\n  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Google Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrieval},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gigaspeecha2tretrieval","title":"GigaSpeechA2TRetrieval","text":"<p>Given an English speech segment, retrieve its correct transcription. Audio comes from the 10\u202f000\u2011hour training subset of GigaSpeech, which originates from \u224840\u202f000\u202fhours of transcribed audiobooks, podcasts, and YouTube.</p> <p>Dataset: <code>mteb/gigaspeech_a2t</code> \u2022 License: https://github.com/SpeechColab/GigaSpeech/blob/main/LICENSE \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken human-annotated found Citation <pre><code>@inproceedings{GigaSpeech2021,\n  author = {Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and Jin, Mingjie and Khudanpur, Sanjeev and Watanabe, Shinji and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang and Yao, Xuchen and Wang, Yongqing and Wang, Yujun and You, Zhao and Yan, Zhiyong},\n  booktitle = {Proc. Interspeech 2021},\n  title = {GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#gigaspeecht2aretrieval","title":"GigaSpeechT2ARetrieval","text":"<p>Given an English transcription, retrieve its corresponding audio segment. Audio comes from the 10\u202f000\u2011hour training subset of GigaSpeech, sourced from \u224840\u202f000\u202fhours of transcribed audiobooks, podcasts, and YouTube.</p> <p>Dataset: <code>mteb/gigaspeech_t2a</code> \u2022 License: https://github.com/SpeechColab/GigaSpeech/blob/main/LICENSE \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken human-annotated found Citation <pre><code>@inproceedings{GigaSpeech2021,\n  author = {Chen, Guoguo and Chai, Shuzhou and Wang, Guanbo and Du, Jiayu and Zhang, Wei-Qiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, Jan and Zhang, Junbo and Jin, Mingjie and Khudanpur, Sanjeev and Watanabe, Shinji and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang and Yao, Xuchen and Wang, Yongqing and Wang, Yujun and You, Zhao and Yan, Zhiyong},\n  booktitle = {Proc. Interspeech 2021},\n  title = {GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#googlesvqa2tretrieval","title":"GoogleSVQA2TRetrieval","text":"<p>Multilingual audio-to-text retrieval using the Simple Voice Questions (SVQ) dataset. Given an audio query, retrieve the corresponding text transcription.</p> <p>Dataset: <code>mteb/svq</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 acm, apc, arq, arz, ben, ... (20) Spoken human-annotated found Citation <pre><code>@inproceedings{heigold2025massive,\n  author = {Georg Heigold and Ehsan Variani and Tom Bagby and Cyril Allauzen and Ji Ma and Shankar Kumar and Michael Riley},\n  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  title = {Massive Sound Embedding Benchmark ({MSEB})},\n  url = {https://openreview.net/forum?id=X0juYgFVng},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#googlesvqt2aretrieval","title":"GoogleSVQT2ARetrieval","text":"<p>Multilingual text-to-audio retrieval using the Simple Voice Questions (SVQ) dataset. Given a text query, retrieve the corresponding audio recording.</p> <p>Dataset: <code>mteb/svq</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 acm, apc, arq, arz, ben, ... (20) Spoken human-annotated found Citation <pre><code>@inproceedings{heigold2025massive,\n  author = {Georg Heigold and Ehsan Variani and Tom Bagby and Cyril Allauzen and Ji Ma and Shankar Kumar and Michael Riley},\n  booktitle = {The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  title = {Massive Sound Embedding Benchmark ({MSEB})},\n  url = {https://openreview.net/forum?id=X0juYgFVng},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hatefulmemesi2tretrieval","title":"HatefulMemesI2TRetrieval","text":"<p>Retrieve captions based on memes to assess OCR abilities.</p> <p>Dataset: <code>mteb/MMSoc_HatefulMemes</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{kiela2020hateful,\n  author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},\n  journal = {Advances in neural information processing systems},\n  pages = {2611--2624},\n  title = {The hateful memes challenge: Detecting hate speech in multimodal memes},\n  volume = {33},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hatefulmemest2iretrieval","title":"HatefulMemesT2IRetrieval","text":"<p>Retrieve captions based on memes to assess OCR abilities.</p> <p>Dataset: <code>mteb/MMSoc_HatefulMemes</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{kiela2020hateful,\n  author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},\n  journal = {Advances in neural information processing systems},\n  pages = {2611--2624},\n  title = {The hateful memes challenge: Detecting hate speech in multimodal memes},\n  volume = {33},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hifittsa2tretrieval","title":"HiFiTTSA2TRetrieval","text":"<p>Sentence-level text captions aligned to 44.1\u202fkHz audiobook speech segments from the Hi\u2011Fi Multi\u2011Speaker English TTS dataset. Dataset is based on public audiobooks from LibriVox and texts from Project Gutenberg.</p> <p>Dataset: <code>mteb/hifi-tts_a2t</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@article{bakhturina2021hi,\n  author = {Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang, Yang},\n  journal = {arXiv preprint arXiv:2104.01497},\n  title = {{Hi-Fi Multi-Speaker English TTS Dataset}},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#hifittst2aretrieval","title":"HiFiTTST2ARetrieval","text":"<p>Sentence-level text captions aligned to 44.1\u202fkHz audiobook speech segments from the Hi\u2011Fi Multi\u2011Speaker English TTS dataset. Dataset is based on public audiobooks from LibriVox and texts from Project Gutenberg.</p> <p>Dataset: <code>mteb/hifi-tts_t2a</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@article{bakhturina2021hi,\n  author = {Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang, Yang},\n  journal = {arXiv preprint arXiv:2104.01497},\n  title = {{Hi-Fi Multi-Speaker English TTS Dataset}},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#imagecodet2iretrieval","title":"ImageCoDeT2IRetrieval","text":"<p>Retrieve a specific video frame based on a precise caption.</p> <p>Dataset: <code>mteb/imagecode</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) hit_rate_at_3 eng Web, Written derived found Citation <pre><code>@article{krojer2022image,\n  author = {Krojer, Benno and Adlakha, Vaibhav and Vineet, Vibhav and Goyal, Yash and Ponti, Edoardo and Reddy, Siva},\n  journal = {arXiv preprint arXiv:2203.15867},\n  title = {Image retrieval from contextual descriptions},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#infoseekit2itretrieval","title":"InfoSeekIT2ITRetrieval","text":"<p>Retrieve source text and image information to answer questions about images.</p> <p>Dataset: <code>mteb/InfoSeekIT2ITRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{chen2023can,\n  author = {Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {14948--14968},\n  title = {Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#infoseekit2tretrieval","title":"InfoSeekIT2TRetrieval","text":"<p>Retrieve source information to answer questions about images.</p> <p>Dataset: <code>mteb/mbeir_infoseek_task6</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{chen2023can,\n  author = {Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {14948--14968},\n  title = {Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#jlcorpusa2tretrieval","title":"JLCorpusA2TRetrieval","text":"<p>Emotional speech segments from the JL-Corpus, balanced over long vowels and annotated for primary and secondary emotions.</p> <p>Dataset: <code>mteb/jl_corpus_a2t</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@inproceedings{james2018open,\n  author = {James, Jesin and Li, Tian and Watson, Catherine},\n  booktitle = {Proc. Interspeech 2018},\n  title = {An Open Source Emotional Speech Corpus for Human Robot Interaction Applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#jlcorpust2aretrieval","title":"JLCorpusT2ARetrieval","text":"<p>Emotional speech segments from the JL-Corpus, balanced over long vowels and annotated for primary and secondary emotions.</p> <p>Dataset: <code>mteb/jl_corpus_t2a</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@inproceedings{james2018open,\n  author = {James, Jesin and Li, Tian and Watson, Catherine},\n  booktitle = {Proc. Interspeech 2018},\n  title = {An Open Source Emotional Speech Corpus for Human Robot Interaction Applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#jamaltartista2aretrieval","title":"JamAltArtistA2ARetrieval","text":"<p>Given audio clip of a song (query), retrieve all songs from the same artist in the Jam-Alt-Lines dataset</p> <p>Dataset: <code>mteb/jam-alt-lines</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) ndcg_at_10 deu, eng, fra, spa Music derived found Citation <pre><code>@inproceedings{cifka-2024-jam-alt,\n  author = {Ond{\\v{r}}ej C{\\'{\\i}}fka and\nHendrik Schreiber and\nLuke Miner and\nFabian{-}Robert St{\\\"{o}}ter},\n  booktitle = {Proceedings of the 25th International Society for\nMusic Information Retrieval Conference},\n  doi = {10.5281/ZENODO.14877443},\n  pages = {737--744},\n  publisher = {ISMIR},\n  title = {Lyrics Transcription for Humans: {A} Readability-Aware Benchmark},\n  url = {https://doi.org/10.5281/zenodo.14877443},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#jamaltlyrica2tretrieval","title":"JamAltLyricA2TRetrieval","text":"<p>From audio clips of songs (query), retrieve corresponding textual lyric from the Jam-Alt-Lines dataset</p> <p>Dataset: <code>mteb/jam-alt-lines</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) ndcg_at_10 deu, eng, fra, spa Music derived found Citation <pre><code>@inproceedings{cifka-2024-jam-alt,\n  author = {Ond{\\v{r}}ej C{\\'{\\i}}fka and\nHendrik Schreiber and\nLuke Miner and\nFabian{-}Robert St{\\\"{o}}ter},\n  booktitle = {Proceedings of the 25th International Society for\nMusic Information Retrieval Conference},\n  doi = {10.5281/ZENODO.14877443},\n  pages = {737--744},\n  publisher = {ISMIR},\n  title = {Lyrics Transcription for Humans: {A} Readability-Aware Benchmark},\n  url = {https://doi.org/10.5281/zenodo.14877443},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#jamaltlyrict2aretrieval","title":"JamAltLyricT2ARetrieval","text":"<p>From textual lyrics (query), retrieve corresponding audio clips of songs from the Jam-Alt-Lines dataset</p> <p>Dataset: <code>mteb/jam-alt-lines</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) ndcg_at_10 deu, eng, fra, spa Music derived found Citation <pre><code>@inproceedings{cifka-2024-jam-alt,\n  author = {Ond{\\v{r}}ej C{\\'{\\i}}fka and\nHendrik Schreiber and\nLuke Miner and\nFabian{-}Robert St{\\\"{o}}ter},\n  booktitle = {Proceedings of the 25th International Society for\nMusic Information Retrieval Conference},\n  doi = {10.5281/ZENODO.14877443},\n  pages = {737--744},\n  publisher = {ISMIR},\n  title = {Lyrics Transcription for Humans: {A} Readability-Aware Benchmark},\n  url = {https://doi.org/10.5281/zenodo.14877443},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#lassa2tretrieval","title":"LASSA2TRetrieval","text":"<p>Language-Queried Audio Source Separation (LASS) dataset for audio-to-text retrieval. Retrieve text descriptions/captions for audio clips using natural language queries.The original dataset is based on the AudioCaps dataset.The source audio has been synthesized by mixing two audio with their labelled snr ratio as indicated in the dataset.</p> <p>Dataset: <code>mteb/lass-synth-a2t</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng AudioScene derived found Citation <pre><code>@inproceedings{liu2022separate,\n  author = {Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D and Wang, Wenwu},\n  booktitle = {INTERSPEEH},\n  title = {Separate What You Describe: Language-Queried Audio Source Separation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#lasst2aretrieval","title":"LASST2ARetrieval","text":"<p>Language-Queried Audio Source Separation (LASS) dataset for text-to-audio retrieval. Retrieve audio clips corresponding to natural language text descriptions/captions.The original dataset is based on the AudioCaps dataset.The source audio has been synthesized by mixing two audio with their labelled snr ratio as indicated in the dataset.</p> <p>Dataset: <code>mteb/lass-synth-t2a</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng AudioScene derived found Citation <pre><code>@inproceedings{liu2022separate,\n  author = {Liu, Xubo and Liu, Haohe and Kong, Qiuqiang and Mei, Xinhao and Zhao, Jinzheng and Huang, Qiushi and Plumbley, Mark D and Wang, Wenwu},\n  booktitle = {INTERSPEEH},\n  title = {Separate What You Describe: Language-Queried Audio Source Separation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#llavait2tretrieval","title":"LLaVAIT2TRetrieval","text":"<p>Retrieve responses to answer questions about images.</p> <p>Dataset: <code>izhx/UMRB-LLaVA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) hit_rate_at_5 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin-etal-2024-preflmr,\n  address = {Bangkok, Thailand},\n  author = {Lin, Weizhe  and\nMei, Jingbiao  and\nChen, Jinghong  and\nByrne, Bill},\n  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2024.acl-long.289},\n  editor = {Ku, Lun-Wei  and\nMartins, Andre  and\nSrikumar, Vivek},\n  month = aug,\n  pages = {5294--5316},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}re{FLMR}: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers},\n  url = {https://aclanthology.org/2024.acl-long.289},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#librittsa2tretrieval","title":"LibriTTSA2TRetrieval","text":"<p>Given audiobook speech segments from the multi\u2011speaker LibriTTS corpus, retrieve the correct text transcription. LibriTTS is a 585\u2011hour, 24\u202fkHz, multi\u2011speaker English TTS corpus derived from LibriVox (audio) and Project Gutenberg (text).</p> <p>Dataset: <code>mteb/LibriTTS_a2t</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@misc{zen2019librittscorpusderivedlibrispeech,\n  archiveprefix = {arXiv},\n  author = {Heiga Zen and Viet Dang and Rob Clark and Yu Zhang and Ron J. Weiss and Ye Jia and Zhifeng Chen and Yonghui Wu},\n  eprint = {1904.02882},\n  primaryclass = {cs.SD},\n  title = {LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech},\n  url = {https://arxiv.org/abs/1904.02882},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#librittst2aretrieval","title":"LibriTTST2ARetrieval","text":"<p>Given an English text transcription, retrieve its corresponding audiobook speech segment from the multi\u2011speaker LibriTTS corpus. LibriTTS is a 585\u2011hour, 24\u202fkHz, multi\u2011speaker English TTS corpus derived from LibriVox and Project Gutenberg.</p> <p>Dataset: <code>mteb/LibriTTS_t2a</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Spoken derived found Citation <pre><code>@misc{zen2019librittscorpusderivedlibrispeech,\n  archiveprefix = {arXiv},\n  author = {Heiga Zen and Viet Dang and Rob Clark and Yu Zhang and Ron J. Weiss and Ye Jia and Zhifeng Chen and Yonghui Wu},\n  eprint = {1904.02882},\n  primaryclass = {cs.SD},\n  title = {LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech},\n  url = {https://arxiv.org/abs/1904.02882},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#macsa2tretrieval","title":"MACSA2TRetrieval","text":"<p>Audio captions and tags for urban acoustic scenes in TAU Urban Acoustic Scenes 2019 development dataset.</p> <p>Dataset: <code>mteb/MACS_a2t</code> \u2022 License: https://zenodo.org/records/5114771 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 eng AudioScene human-annotated found Citation <pre><code>@misc{martinmorato2021groundtruthreliabilitymultiannotator,\n  archiveprefix = {arXiv},\n  author = {Irene Martin-Morato and Annamaria Mesaros},\n  eprint = {2104.04214},\n  primaryclass = {eess.AS},\n  title = {What is the ground truth? Reliability of multi-annotator data for audio tagging},\n  url = {https://arxiv.org/abs/2104.04214},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#macst2aretrieval","title":"MACST2ARetrieval","text":"<p>Audio captions and tags for urban acoustic scenes in TAU Urban Acoustic Scenes 2019 development dataset.</p> <p>Dataset: <code>mteb/MACS_t2a</code> \u2022 License: https://zenodo.org/records/5114771 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng AudioScene human-annotated found Citation <pre><code>@misc{martinmorato2021groundtruthreliabilitymultiannotator,\n  archiveprefix = {arXiv},\n  author = {Irene Martin-Morato and Annamaria Mesaros},\n  eprint = {2104.04214},\n  primaryclass = {eess.AS},\n  title = {What is the ground truth? Reliability of multi-annotator data for audio tagging},\n  url = {https://arxiv.org/abs/2104.04214},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#meti2iretrieval","title":"METI2IRetrieval","text":"<p>Retrieve photos of more than 224k artworks.</p> <p>Dataset: <code>mteb/met</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{ypsilantis2021met,\n  author = {Ypsilantis, Nikolaos-Antonios and Garcia, Noa and Han, Guangxing and Ibrahimi, Sarah and Van Noord, Nanne and Tolias, Giorgos},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {The met dataset: Instance-level recognition for artworks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#mscocoi2tretrieval","title":"MSCOCOI2TRetrieval","text":"<p>Retrieve captions based on images.</p> <p>Dataset: <code>mteb/mbeir_mscoco_task3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin2014microsoft,\n  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n  booktitle = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n  organization = {Springer},\n  pages = {740--755},\n  title = {Microsoft coco: Common objects in context},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#mscocot2iretrieval","title":"MSCOCOT2IRetrieval","text":"<p>Retrieve images based on captions.</p> <p>Dataset: <code>mteb/mbeir_mscoco_task0</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{lin2014microsoft,\n  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\\'a}r, Piotr and Zitnick, C Lawrence},\n  booktitle = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},\n  organization = {Springer},\n  pages = {740--755},\n  title = {Microsoft coco: Common objects in context},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#memotioni2tretrieval","title":"MemotionI2TRetrieval","text":"<p>Retrieve captions based on memes.</p> <p>Dataset: <code>mteb/MMSoc_Memotion</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{sharma2020semeval,\n  author = {Sharma, Chhavi and Bhageria, Deepesh and Scott, William and Pykl, Srinivas and Das, Amitava and Chakraborty, Tanmoy and Pulabaigari, Viswanath and Gamb{\\\"a}ck, Bj{\\\"o}rn},\n  booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},\n  pages = {759--773},\n  title = {SemEval-2020 Task 8: Memotion Analysis-the Visuo-Lingual Metaphor!},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#memotiont2iretrieval","title":"MemotionT2IRetrieval","text":"<p>Retrieve memes based on captions.</p> <p>Dataset: <code>mteb/MMSoc_Memotion</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@inproceedings{sharma2020semeval,\n  author = {Sharma, Chhavi and Bhageria, Deepesh and Scott, William and Pykl, Srinivas and Das, Amitava and Chakraborty, Tanmoy and Pulabaigari, Viswanath and Gamb{\\\"a}ck, Bj{\\\"o}rn},\n  booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},\n  pages = {759--773},\n  title = {SemEval-2020 Task 8: Memotion Analysis-the Visuo-Lingual Metaphor!},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#musiccapsa2tretrieval","title":"MusicCapsA2TRetrieval","text":"<p>Natural language description for music audio.</p> <p>Dataset: <code>mteb/MusicCaps_a2t</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 zxx Music human-annotated found Citation <pre><code>@misc{agostinelli2023musiclmgeneratingmusictext,\n  archiveprefix = {arXiv},\n  author = {Andrea Agostinelli and Timo I. Denk and Zal\u00e1n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matt Sharifi and Neil Zeghidour and Christian Frank},\n  eprint = {2301.11325},\n  primaryclass = {cs.SD},\n  title = {MusicLM: Generating Music From Text},\n  url = {https://arxiv.org/abs/2301.11325},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#musiccapst2aretrieval","title":"MusicCapsT2ARetrieval","text":"<p>Natural language description for music audio.</p> <p>Dataset: <code>mteb/MusicCaps_t2a</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 zxx Music human-annotated found Citation <pre><code>@misc{agostinelli2023musiclmgeneratingmusictext,\n  archiveprefix = {arXiv},\n  author = {Andrea Agostinelli and Timo I. Denk and Zal\u00e1n Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matt Sharifi and Neil Zeghidour and Christian Frank},\n  eprint = {2301.11325},\n  primaryclass = {cs.SD},\n  title = {MusicLM: Generating Music From Text},\n  url = {https://arxiv.org/abs/2301.11325},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#nightsi2iretrieval","title":"NIGHTSI2IRetrieval","text":"<p>Retrieval identical image to the given image.</p> <p>Dataset: <code>mteb/mbeir_nights_task4</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@article{fu2024dreamsim,\n  author = {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},\n  journal = {Advances in Neural Information Processing Systems},\n  title = {DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},\n  volume = {36},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#okvqait2tretrieval","title":"OKVQAIT2TRetrieval","text":"<p>Retrieval a Wiki passage to answer query about an image.</p> <p>Dataset: <code>izhx/UMRB-OKVQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) hit_rate_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{marino2019ok,\n  author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},\n  booktitle = {Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},\n  pages = {3195--3204},\n  title = {Ok-vqa: A visual question answering benchmark requiring external knowledge},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#ovenit2itretrieval","title":"OVENIT2ITRetrieval","text":"<p>Retrieval a Wiki image and passage to answer query about an image.</p> <p>Dataset: <code>mteb/mbeir_oven_task8</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image, text (it2it) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{hu2023open,\n  author = {Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {12065--12075},\n  title = {Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#ovenit2tretrieval","title":"OVENIT2TRetrieval","text":"<p>Retrieval a Wiki passage to answer query about an image.</p> <p>Dataset: <code>mteb/mbeir_oven_task6</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{hu2023open,\n  author = {Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},\n  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},\n  pages = {12065--12075},\n  title = {Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordeasyi2iretrieval","title":"ROxfordEasyI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>mteb/r-oxford-easy-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordhardi2iretrieval","title":"ROxfordHardI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>mteb/r-oxford-hard-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#roxfordmediumi2iretrieval","title":"ROxfordMediumI2IRetrieval","text":"<p>Retrieve photos of landmarks in Oxford, UK.</p> <p>Dataset: <code>mteb/r-oxford-medium-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rp2ki2iretrieval","title":"RP2kI2IRetrieval","text":"<p>Retrieve photos of 39457 products.</p> <p>Dataset: <code>mteb/rp2k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Web derived created Citation <pre><code>@article{peng2020rp2k,\n  author = {Peng, Jingtian and Xiao, Chang and Li, Yifan},\n  journal = {arXiv preprint arXiv:2006.12634},\n  title = {RP2K: A large-scale retail product dataset for fine-grained image classification},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rpariseasyi2iretrieval","title":"RParisEasyI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>mteb/r-paris-easy-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rparishardi2iretrieval","title":"RParisHardI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>mteb/r-paris-hard-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#rparismediumi2iretrieval","title":"RParisMediumI2IRetrieval","text":"<p>Retrieve photos of landmarks in Paris, UK.</p> <p>Dataset: <code>mteb/r-paris-medium-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) map_at_5 eng Web derived created Citation <pre><code>@inproceedings{radenovic2018revisiting,\n  author = {Radenovi{\\'c}, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\\v{r}}ej},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {5706--5715},\n  title = {Revisiting oxford and paris: Large-scale image retrieval benchmarking},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#remuqit2tretrieval","title":"ReMuQIT2TRetrieval","text":"<p>Retrieval of a Wiki passage to answer a query about an image.</p> <p>Dataset: <code>izhx/UMRB-ReMuQ</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) hit_rate_at_5 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{luo-etal-2023-end,\n  address = {Toronto, Canada},\n  author = {Luo, Man  and\nFang, Zhiyuan  and\nGokhale, Tejas  and\nYang, Yezhou  and\nBaral, Chitta},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2023.acl-long.478},\n  editor = {Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki},\n  month = jul,\n  pages = {8573--8589},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-end Knowledge Retrieval with Multi-modal Queries},\n  url = {https://aclanthology.org/2023.acl-long.478},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sopi2iretrieval","title":"SOPI2IRetrieval","text":"<p>Retrieve product photos of 22634 online products.</p> <p>Dataset: <code>mteb/stanford-online-products</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{oh2016deep,\n  author = {Oh Song, Hyun and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {4004--4012},\n  title = {Deep metric learning via lifted structured feature embedding},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#scimmiri2tretrieval","title":"SciMMIRI2TRetrieval","text":"<p>Retrieve captions based on figures and tables.</p> <p>Dataset: <code>mteb/SciMMIR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Academic derived found Citation <pre><code>@misc{wu2024scimmirbenchmarkingscientificmultimodal,\n  archiveprefix = {arXiv},\n  author = {Siwei Wu and Yizhi Li and Kang Zhu and Ge Zhang and Yiming Liang and Kaijing Ma and Chenghao Xiao and Haoran Zhang and Bohao Yang and Wenhu Chen and Wenhao Huang and Noura Al Moubayed and Jie Fu and Chenghua Lin},\n  eprint = {2401.13478},\n  primaryclass = {cs.IR},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  url = {https://arxiv.org/abs/2401.13478},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#scimmirt2iretrieval","title":"SciMMIRT2IRetrieval","text":"<p>Retrieve figures and tables based on captions.</p> <p>Dataset: <code>mteb/SciMMIR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Academic derived found Citation <pre><code>@misc{wu2024scimmirbenchmarkingscientificmultimodal,\n  archiveprefix = {arXiv},\n  author = {Siwei Wu and Yizhi Li and Kang Zhu and Ge Zhang and Yiming Liang and Kaijing Ma and Chenghao Xiao and Haoran Zhang and Bohao Yang and Wenhu Chen and Wenhao Huang and Noura Al Moubayed and Jie Fu and Chenghua Lin},\n  eprint = {2401.13478},\n  primaryclass = {cs.IR},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  url = {https://arxiv.org/abs/2401.13478},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sketchyi2iretrieval","title":"SketchyI2IRetrieval","text":"<p>Retrieve photos from sketches.</p> <p>Dataset: <code>mteb/sketchy</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{ypsilantis2021met,\n  author = {Ypsilantis, Nikolaos-Antonios and Garcia, Noa and Han, Guangxing and Ibrahimi, Sarah and Van Noord, Nanne and Tolias, Giorgos},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {The met dataset: Instance-level recognition for artworks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sounddescsa2tretrieval","title":"SoundDescsA2TRetrieval","text":"<p>Natural language description for different audio sources from the BBC Sound Effects webpage.</p> <p>Dataset: <code>mteb/sounddescs_a2t</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 zxx Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Koepke2022,\n  author = {Koepke, A.S. and Oncescu, A.-M. and Henriques, J. and Akata, Z. and Albanie, S.},\n  booktitle = {IEEE Transactions on Multimedia},\n  title = {Audio Retrieval with Natural Language Queries: A Benchmark Study},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#sounddescst2aretrieval","title":"SoundDescsT2ARetrieval","text":"<p>Natural language description for different audio sources from the BBC Sound Effects webpage.</p> <p>Dataset: <code>mteb/sounddescs_t2a</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 zxx Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Koepke2022,\n  author = {Koepke, A.S. and Oncescu, A.-M. and Henriques, J. and Akata, Z. and Albanie, S.},\n  booktitle = {IEEE Transactions on Multimedia},\n  title = {Audio Retrieval with Natural Language Queries: A Benchmark Study},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#spokensquadt2aretrieval","title":"SpokenSQuADT2ARetrieval","text":"<p>Text-to-audio retrieval task based on SpokenSQuAD dataset. Given a text question, retrieve relevant audio segments that contain the answer. Questions are derived from SQuAD reading comprehension dataset with corresponding spoken passages.</p> <p>Dataset: <code>mteb/spoken-squad-t2a</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 eng Academic, Encyclopaedic, Non-fiction derived found Citation <pre><code>@inproceedings{li2018spokensquad,\n  author = {Li, Chia-Hsuan and Ma, Szu-Lin and Zhang, Hsin-Wei and Lee, Hung-yi and Lee, Lin-shan},\n  booktitle = {Interspeech},\n  pages = {3459--3463},\n  title = {Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#stanfordcarsi2iretrieval","title":"StanfordCarsI2IRetrieval","text":"<p>Retrieve car images from 196 makes.</p> <p>Dataset: <code>mteb/stanford_cars_retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) hit_rate_at_1 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#tuberlint2iretrieval","title":"TUBerlinT2IRetrieval","text":"<p>Retrieve sketch images based on text descriptions.</p> <p>Dataset: <code>mteb/tu-berlin</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived found Citation <pre><code>@article{eitz2012humans,\n  author = {Eitz, Mathias and Hays, James and Alexa, Marc},\n  journal = {ACM Transactions on graphics (TOG)},\n  number = {4},\n  pages = {1--10},\n  publisher = {Acm New York, NY, USA},\n  title = {How do humans sketch objects?},\n  volume = {31},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#urbansound8ka2tretrieval","title":"UrbanSound8KA2TRetrieval","text":"<p>UrbanSound8K: Audio-to-text retrieval of urban sound events.</p> <p>Dataset: <code>mteb/Urbansound8K_a2t</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) hit_rate_at_5 zxx AudioScene human-annotated found Citation <pre><code>@inproceedings{Salamon:UrbanSound:ACMMM:14,\n  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},\n  booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},\n  organization = {ACM},\n  pages = {1041--1044},\n  title = {A Dataset and Taxonomy for Urban Sound Research},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#urbansound8kt2aretrieval","title":"UrbanSound8KT2ARetrieval","text":"<p>UrbanSound8K: Text-to-audio retrieval of urban sound events.</p> <p>Dataset: <code>mteb/Urbansound8K_t2a</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to audio (t2a) hit_rate_at_5 zxx AudioScene human-annotated found Citation <pre><code>@inproceedings{Salamon:UrbanSound:ACMMM:14,\n  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},\n  booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},\n  organization = {ACM},\n  pages = {1041--1044},\n  title = {A Dataset and Taxonomy for Urban Sound Research},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#vqa2it2tretrieval","title":"VQA2IT2TRetrieval","text":"<p>Retrieve the correct answer for a question about an image.</p> <p>Dataset: <code>mteb/vqa-2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Web derived found Citation <pre><code>@inproceedings{Goyal_2017_CVPR,\n  author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {Making the v in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#visualnewsi2tretrieval","title":"VisualNewsI2TRetrieval","text":"<p>Retrieval entity-rich captions for news images.</p> <p>Dataset: <code>mteb/mbeir_visualnews_task3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{liu2021visual,\n  author = {Liu, Fuxiao and Wang, Yinghan and Wang, Tianlu and Ordonez, Vicente},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  pages = {6761--6771},\n  title = {Visual News: Benchmark and Challenges in News Image Captioning},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#visualnewst2iretrieval","title":"VisualNewsT2IRetrieval","text":"<p>Retrieve news images with captions.</p> <p>Dataset: <code>mteb/mbeir_visualnews_task0</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{liu2021visual,\n  author = {Liu, Fuxiao and Wang, Yinghan and Wang, Tianlu and Ordonez, Vicente},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  pages = {6761--6771},\n  title = {Visual News: Benchmark and Challenges in News Image Captioning},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#vizwizit2tretrieval","title":"VizWizIT2TRetrieval","text":"<p>Retrieve the correct answer for a question about an image.</p> <p>Dataset: <code>mteb/vizwiz</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) ndcg_at_10 eng Web derived found Citation <pre><code>@inproceedings{gurari2018vizwiz,\n  author = {Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},\n  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},\n  pages = {3608--3617},\n  title = {Vizwiz grand challenge: Answering visual questions from blind people},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#webqat2itretrieval","title":"WebQAT2ITRetrieval","text":"<p>Retrieve sources of information based on questions.</p> <p>Dataset: <code>mteb/mbeir_webqa_task2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image, text (t2it) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{chang2022webqa,\n  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},\n  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages = {16495--16504},\n  title = {Webqa: Multihop and multimodal qa},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/any2anyretrieval/#webqat2tretrieval","title":"WebQAT2TRetrieval","text":"<p>Retrieve sources of information based on questions.</p> <p>Dataset: <code>mteb/mbeir_webqa_task1</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic derived created Citation <pre><code>@inproceedings{chang2022webqa,\n  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},\n  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages = {16495--16504},\n  title = {Webqa: Multihop and multimodal qa},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/","title":"AudioClassification","text":"<ul> <li>Number of tasks: 38</li> </ul>"},{"location":"overview/available_tasks/audioclassification/#ambientacousticcontext","title":"AmbientAcousticContext","text":"<p>The Ambient Acoustic Context dataset contains 1-second segments for activities that occur in a workplace setting. This is a downsampled version with ~100 train and ~50 test samples per class.</p> <p>Dataset: <code>mteb/ambient-acoustic-context-small</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{10.1145/3379503.3403535,\n  address = {New York, NY, USA},\n  articleno = {33},\n  author = {Park, Chunjong and Min, Chulhong and Bhattacharya, Sourav and Kawsar, Fahim},\n  booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},\n  doi = {10.1145/3379503.3403535},\n  isbn = {9781450375160},\n  keywords = {Acoustic ambient context, Conversational agents},\n  location = {Oldenburg, Germany},\n  numpages = {9},\n  publisher = {Association for Computing Machinery},\n  series = {MobileHCI '20},\n  title = {Augmenting Conversational Agents with Ambient Acoustic Contexts},\n  url = {https://doi.org/10.1145/3379503.3403535},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#beijingopera","title":"BeijingOpera","text":"<p>Audio classification of percussion instruments into one of 4 classes: <code>Bangu</code>, <code>Naobo</code>, <code>Daluo</code>, and <code>Xiaoluo</code></p> <p>Dataset: <code>mteb/beijing-opera</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Music human-annotated created Citation <pre><code>@inproceedings{6853981,\n  author = {Tian, Mi and Srinivasamurthy, Ajay and Sandler, Mark and Serra, Xavier},\n  booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  doi = {10.1109/ICASSP.2014.6853981},\n  keywords = {Decision support systems;Conferences;Acoustics;Speech;Speech processing;Time-frequency analysis;Beijing Opera;Onset Detection;Drum Transcription;Non-negative matrix factorization},\n  number = {},\n  pages = {2159-2163},\n  title = {A study of instrument-wise onset detection in Beijing Opera percussion ensembles},\n  volume = {},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#birdclef","title":"BirdCLEF","text":"<p>BirdCLEF+ 2025 dataset for species identification from audio, focused on birds, amphibians, mammals and insects from the Middle Magdalena Valley of Colombia. Downsampled to 50 classes with 20 samples each.</p> <p>Dataset: <code>mteb/birdclef25-mini</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Bioacoustics, Speech, Spoken expert-annotated found Citation <pre><code>@dataset{birdclef2025,\n  author = {Christopher},\n  publisher = {Hugging Face},\n  title = {BirdCLEF+ 2025},\n  url = {https://huggingface.co/datasets/christopher/birdclef-2025},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#crema_d","title":"CREMA_D","text":"<p>Emotion classification of audio into one of 6 classes: Anger, Disgust, Fear, Happy, Neutral, Sad.</p> <p>Dataset: <code>mteb/crema-d</code> \u2022 License: http://opendatacommons.org/licenses/odbl/1.0/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech human-annotated created Citation <pre><code>@article{Cao2014-ih,\n  author = {Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur,\nRuben C and Nenkova, Ani and Verma, Ragini},\n  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n  journal = {IEEE Transactions on Affective Computing},\n  keywords = {Emotional corpora; facial expression; multi-modal recognition;\nvoice expression},\n  language = {en},\n  month = oct,\n  number = {4},\n  pages = {377--390},\n  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},\n  title = {{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},\n  volume = {5},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#cstrvctkaccentid","title":"CSTRVCTKAccentID","text":"<p>Gender classification from CSTR-VCTK dataset. This is a stratified and downsampled version of the original dataset. The dataset was recorded with 2 different microphones, and this mini version uniformly samples data from the 2 microphone types.</p> <p>Dataset: <code>mteb/cstr-vctk-accent-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{Yamagishi2019CSTRVC,\n  author = {Junichi Yamagishi and Christophe Veaux and Kirsten MacDonald},\n  title = {CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)},\n  url = {https://api.semanticscholar.org/CorpusID:213060286},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#cstrvctkgender","title":"CSTRVCTKGender","text":"<p>Gender classification from CSTR-VCTK dataset. This is a stratified and downsampled version of the original dataset. The dataset was recorded with 2 different microphones, and this mini version uniformly samples data from the 2 microphone types.</p> <p>Dataset: <code>mteb/cstr-vctk-gender-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{Yamagishi2019CSTRVC,\n  author = {Junichi Yamagishi and Christophe Veaux and Kirsten MacDonald},\n  title = {CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)},\n  url = {https://api.semanticscholar.org/CorpusID:213060286},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#commonlanguageagedetection","title":"CommonLanguageAgeDetection","text":"<p>Age Classification. This is a stratified subsampled version of the original CommonLanguage dataset.</p> <p>Dataset: <code>mteb/commonlanguage-age-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Scene, Speech, Spoken human-annotated found Citation <pre><code>@dataset{ganesh_sinisetty_2021_5036977,\n  author = {Ganesh Sinisetty and\nPavlo Ruban and\nOleksandr Dymov and\nMirco Ravanelli},\n  doi = {10.5281/zenodo.5036977},\n  month = jun,\n  publisher = {Zenodo},\n  title = {CommonLanguage},\n  url = {https://doi.org/10.5281/zenodo.5036977},\n  version = {0.1},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#commonlanguagegenderdetection","title":"CommonLanguageGenderDetection","text":"<p>Gender Classification. This is a stratified subsampled version of the original CommonLanguage datasets.</p> <p>Dataset: <code>mteb/commonlanguage-gender-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Scene, Speech, Spoken human-annotated found Citation <pre><code>@dataset{ganesh_sinisetty_2021_5036977,\n  author = {Ganesh Sinisetty and\nPavlo Ruban and\nOleksandr Dymov and\nMirco Ravanelli},\n  doi = {10.5281/zenodo.5036977},\n  month = jun,\n  publisher = {Zenodo},\n  title = {CommonLanguage},\n  url = {https://doi.org/10.5281/zenodo.5036977},\n  version = {0.1},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#commonlanguagelanguagedetection","title":"CommonLanguageLanguageDetection","text":"<p>Language Classification. This is a stratified subsampled version of the original CommonLanguage dataset.</p> <p>Dataset: <code>mteb/commonlanguage-lang-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Scene, Speech, Spoken human-annotated found Citation <pre><code>@dataset{ganesh_sinisetty_2021_5036977,\n  author = {Ganesh Sinisetty and\nPavlo Ruban and\nOleksandr Dymov and\nMirco Ravanelli},\n  doi = {10.5281/zenodo.5036977},\n  month = jun,\n  publisher = {Zenodo},\n  title = {CommonLanguage},\n  url = {https://doi.org/10.5281/zenodo.5036977},\n  version = {0.1},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#esc50","title":"ESC50","text":"<p>Environmental Sound Classification Dataset.</p> <p>Dataset: <code>mteb/esc50</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Spoken human-annotated found Citation <pre><code>@inproceedings{piczak2015dataset,\n  author = {Piczak, Karol J.},\n  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},\n  date = {2015-10-13},\n  doi = {10.1145/2733373.2806390},\n  isbn = {978-1-4503-3459-4},\n  location = {{Brisbane, Australia}},\n  pages = {1015--1018},\n  publisher = {{ACM Press}},\n  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},\n  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#expressoconv","title":"ExpressoConv","text":"<p>Multiclass expressive speech style classification. This is a stratfied and downsampled version of the original dataset that contains 40 hours of speech. The original dataset has two subsets - read speech and conversational speech, each having their own set of style labels. This task only includes the conversational speech subset.</p> <p>Dataset: <code>mteb/expresso-conv-mini</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated created Citation <pre><code>@inproceedings{nguyen2023expresso,\n  author = {Nguyen, Tu Anh and Hsu, Wei-Ning and d'Avirro, Antony and Shi, Bowen and Gat, Itai and Fazel-Zarani, Maryam and Remez, Tal and Copet, Jade and Synnaeve, Gabriel and Hassid, Michael and others},\n  booktitle = {INTERSPEECH 2023-24th Annual Conference of the International Speech Communication Association},\n  pages = {4823--4827},\n  title = {Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#expressoread","title":"ExpressoRead","text":"<p>Multiclass expressive speech style classification. This is a stratfied and downsampled version of the original dataset that contains 40 hours of speech. The original dataset has two subsets - read speech and conversational speech, each having their own set of style labels. This task only includes the read speech subset.</p> <p>Dataset: <code>mteb/expresso-read-mini</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated created Citation <pre><code>@inproceedings{nguyen2023expresso,\n  author = {Nguyen, Tu Anh and Hsu, Wei-Ning and d'Avirro, Antony and Shi, Bowen and Gat, Itai and Fazel-Zarani, Maryam and Remez, Tal and Copet, Jade and Synnaeve, Gabriel and Hassid, Michael and others},\n  booktitle = {INTERSPEECH 2023-24th Annual Conference of the International Speech Communication Association},\n  pages = {4823--4827},\n  title = {Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#fsdd","title":"FSDD","text":"<p>Spoken digit classification of audio into one of 10 classes: 0-9</p> <p>Dataset: <code>mteb/free-spoken-digit-dataset</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Music human-annotated created Citation <pre><code>@misc{zohar2018free,\n  author = {J. Zohar and S. C\u00e3ar and F. Jason and P. Yuxin and N. Hereman and T. Adhish},\n  month = {aug},\n  title = {Jakobovski/Free-Spoken-Digit-Dataset: V1.0.8},\n  url = {https://doi.org/10.5281/zenodo.1342401},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#globev2age","title":"GLOBEV2Age","text":"<p>Age classification from the GLOBE v2 dataset (sampled and enhanced from CommonVoice dataset for TTS purpose). This dataset is a stratified and downsampled version of the original dataset, containing about 535 hours of speech data across 164 accents. We use the age column as the target label for audio classification.</p> <p>Dataset: <code>mteb/globe-v2-age-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated found Citation <pre><code>@misc{wang2024globe,\n  archiveprefix = {arXiv},\n  author = {Wenbin Wang and Yang Song and Sanjay Jha},\n  eprint = {2406.14875},\n  title = {GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#globev2gender","title":"GLOBEV2Gender","text":"<p>Gender classification from the GLOBE v2 dataset (sampled and enhanced from CommonVoice dataset for TTS purpose). This dataset is a stratified and downsampled version of the original dataset, containing about 535 hours of speech data across 164 accents. We use the gender column as the target label for audio classification.</p> <p>Dataset: <code>mteb/globe-v2-gender-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken human-annotated found Citation <pre><code>@misc{wang2024globe,\n  archiveprefix = {arXiv},\n  author = {Wenbin Wang and Yang Song and Sanjay Jha},\n  eprint = {2406.14875},\n  title = {GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#globev3age","title":"GLOBEV3Age","text":"<p>Age classification from the GLOBE v3 dataset (sampled and enhanced from CommonVoice dataset for TTS purpose). This dataset is a stratified and downsampled version of the original dataset, containing about 535 hours of speech data across 164 accents. We use the age column as the target label for audio classification.</p> <p>Dataset: <code>mteb/globe-v3-age-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken automatic found Citation <pre><code>@misc{wang2024globe,\n  archiveprefix = {arXiv},\n  author = {Wenbin Wang and Yang Song and Sanjay Jha},\n  eprint = {2406.14875},\n  title = {GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#globev3gender","title":"GLOBEV3Gender","text":"<p>Gender classification from the GLOBE v3 dataset (sampled and enhanced from CommonVoice dataset for TTS purpose). This dataset is a stratified and downsampled version of the original dataset, containing about 535 hours of speech data across 164 accents. We use the gender column as the target label for audio classification.</p> <p>Dataset: <code>mteb/globe-v3-gender-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Speech, Spoken automatic found Citation <pre><code>@misc{wang2024globe,\n  archiveprefix = {arXiv},\n  author = {Wenbin Wang and Yang Song and Sanjay Jha},\n  eprint = {2406.14875},\n  title = {GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#gtzangenre","title":"GTZANGenre","text":"<p>Music Genre Classification (10 classes)</p> <p>Dataset: <code>mteb/gtzan-genre</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Music human-annotated found Citation <pre><code>@article{1021072,\n  author = {Tzanetakis, G. and Cook, P.},\n  doi = {10.1109/TSA.2002.800560},\n  journal = {IEEE Transactions on Speech and Audio Processing},\n  keywords = {Humans;Music information retrieval;Instruments;Computer science;Multiple signal classification;Signal analysis;Pattern recognition;Feature extraction;Wavelet analysis;Cultural differences},\n  number = {5},\n  pages = {293-302},\n  title = {Musical genre classification of audio signals},\n  volume = {10},\n  year = {2002},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#gunshottriangulation","title":"GunshotTriangulation","text":"<p>Classifying a weapon based on its muzzle blast</p> <p>Dataset: <code>mteb/GunshotTriangulationHear</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx not specified derived found Citation <pre><code>@misc{raponi2021soundgunsdigitalforensics,\n  archiveprefix = {arXiv},\n  author = {Simone Raponi and Isra Ali and Gabriele Oligeri},\n  eprint = {2004.07948},\n  primaryclass = {eess.AS},\n  title = {Sound of Guns: Digital Forensics of Gun Audio Samples meets Artificial Intelligence},\n  url = {https://arxiv.org/abs/2004.07948},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#iemocapemotion","title":"IEMOCAPEmotion","text":"<p>Classification of speech samples into emotions (angry, happy, sad, neutral, frustrated, excited, fearful, surprised, disgusted) from interactive emotional dyadic conversations.</p> <p>Dataset: <code>mteb/iemocap</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech, Spoken expert-annotated created Citation <pre><code>@article{busso2008iemocap,\n  author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},\n  journal = {Language resources and evaluation},\n  number = {4},\n  pages = {335--359},\n  publisher = {Springer},\n  title = {IEMOCAP: Interactive emotional dyadic motion capture database},\n  volume = {42},\n  year = {2008},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#iemocapgender","title":"IEMOCAPGender","text":"<p>Classification of speech samples by speaker gender (male/female) from the IEMOCAP database of interactive emotional dyadic conversations.</p> <p>Dataset: <code>mteb/iemocap</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech, Spoken expert-annotated created Citation <pre><code>@article{busso2008iemocap,\n  author = {Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},\n  journal = {Language resources and evaluation},\n  number = {4},\n  pages = {335--359},\n  publisher = {Springer},\n  title = {IEMOCAP: Interactive emotional dyadic motion capture database},\n  volume = {42},\n  year = {2008},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#libricount","title":"LibriCount","text":"<p>Multiclass speaker count identification. Dataset contains audio recordings with between 0 to 10 speakers.</p> <p>Dataset: <code>mteb/libricount</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech algorithmic created Citation <pre><code>@inproceedings{Stoter_2018,\n  author = {Stoter, Fabian-Robert and Chakrabarty, Soumitro and Edler, Bernd and Habets, Emanuel A. P.},\n  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  doi = {10.1109/icassp.2018.8462159},\n  month = apr,\n  pages = {436-440},\n  publisher = {IEEE},\n  title = {Classification vs. Regression in Supervised Learning for Single Channel Speaker Count Estimation},\n  url = {http://dx.doi.org/10.1109/ICASSP.2018.8462159},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#minds14","title":"MInDS14","text":"<p>MInDS-14 is an evaluation resource for intent detection with spoken data in 14 diverse languages.</p> <p>Dataset: <code>mteb/minds14-multilingual</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy ces, deu, eng, fra, ita, ... (12) Speech, Spoken human-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-2104-08524,\n  author = {Daniela Gerz and Pei{-}Hao Su and Razvan Kusztos and Avishek Mondal and Michal Lis and Eshan Singhal and Nikola Mrk\u0161i\u0107 and Tsung{-}Hsien Wen and Ivan Vulic},\n  eprint = {2104.08524},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  title = {Multilingual and Cross-Lingual Intent Detection from Spoken Data},\n  url = {https://arxiv.org/abs/2104.08524},\n  volume = {abs/2104.08524},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#mridinghamstroke","title":"MridinghamStroke","text":"<p>Stroke classification of Mridingham (a pitched percussion instrument) into one of 10 classes: [\"bheem\", \"cha\", \"dheem\", \"dhin\", \"num\", \"tham\", \"ta\", \"tha\", \"thi\", \"thom\"]</p> <p>Dataset: <code>mteb/mridingham-stroke</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Music human-annotated created Citation <pre><code>@inproceedings{6637633,\n  author = {Anantapadmanabhan, Akshay and Bellur, Ashwin and Murthy, Hema A},\n  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n  doi = {10.1109/ICASSP.2013.6637633},\n  keywords = {Instruments;Vectors;Hidden Markov models;Harmonic analysis;Modal analysis;Dictionaries;Music;Modal Analysis;Mridangam;automatic transcription;Non-negative Matrix Factorization;Hidden Markov models},\n  number = {},\n  pages = {181-185},\n  title = {Modal analysis and transcription of strokes of the mridangam using non-negative matrix factorization},\n  volume = {},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#mridinghamtonic","title":"MridinghamTonic","text":"<p>Tonic classification of Mridingham (a pitched percussion instrument) into one of 6 classes: B,C,C#,D,D#,E</p> <p>Dataset: <code>mteb/mridingham-tonic</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Music human-annotated created Citation <pre><code>@inproceedings{6637633,\n  author = {Anantapadmanabhan, Akshay and Bellur, Ashwin and Murthy, Hema A},\n  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},\n  doi = {10.1109/ICASSP.2013.6637633},\n  keywords = {Instruments;Vectors;Hidden Markov models;Harmonic analysis;Modal analysis;Dictionaries;Music;Modal Analysis;Mridangam;automatic transcription;Non-negative Matrix Factorization;Hidden Markov models},\n  number = {},\n  pages = {181-185},\n  title = {Modal analysis and transcription of strokes of the mridangam using non-negative matrix factorization},\n  volume = {},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#nsynth","title":"NSynth","text":"<p>Instrument Source Classification: one of acoustic, electronic, or synthetic.</p> <p>Dataset: <code>mteb/nsynth-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx Music human-annotated created Citation <pre><code>@misc{engel2017neuralaudiosynthesismusical,\n  archiveprefix = {arXiv},\n  author = {Jesse Engel and Cinjon Resnick and Adam Roberts and Sander Dieleman and Douglas Eck and Karen Simonyan and Mohammad Norouzi},\n  eprint = {1704.01279},\n  primaryclass = {cs.LG},\n  title = {Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},\n  url = {https://arxiv.org/abs/1704.01279},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#speechcommands","title":"SpeechCommands","text":"<p>A set of one-second .wav audio files, each containing a single spoken English word or background noise. To keep evaluation fast, we use a downsampled version of the original dataset by keeping ~50 samples per class for training.</p> <p>Dataset: <code>mteb/speech-commands-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech human-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-1804-03209,\n  author = {Pete Warden},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1804-03209.bib},\n  eprint = {1804.03209},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},\n  title = {Speech Commands: {A} Dataset for Limited-Vocabulary Speech Recognition},\n  url = {http://arxiv.org/abs/1804.03209},\n  volume = {abs/1804.03209},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#spokenenglish","title":"SpokeNEnglish","text":"<p>Human Sound Classification Dataset.</p> <p>Dataset: <code>mteb/SpokeN-100-English</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Spoken LM-generated found Citation <pre><code>@misc{groh2024spoken100crosslingualbenchmarkingdataset,\n  archiveprefix = {arXiv},\n  author = {Ren\u00e9 Groh and Nina Goes and Andreas M. Kist},\n  eprint = {2403.09753},\n  primaryclass = {cs.SD},\n  title = {SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification of Spoken Numbers in Different Languages},\n  url = {https://arxiv.org/abs/2403.09753},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#spokenqaforic","title":"SpokenQAForIC","text":"<p>SpokenQA dataset reformulated as Intent Classification (IC) task</p> <p>Dataset: <code>mteb/SpokenQA_SLUE</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Spoken human-annotated multiple Citation <pre><code>@misc{shon2023sluephase2benchmarksuite,\n  archiveprefix = {arXiv},\n  author = {Suwon Shon and Siddhant Arora and Chyi-Jiunn Lin and Ankita Pasad and Felix Wu and Roshan Sharma and Wei-Lun Wu and Hung-Yi Lee and Karen Livescu and Shinji Watanabe},\n  eprint = {2212.10525},\n  primaryclass = {cs.CL},\n  title = {SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks},\n  url = {https://arxiv.org/abs/2212.10525},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#tauacousticscenes2022mobile","title":"TAUAcousticScenes2022Mobile","text":"<p>TAU Urban Acoustic Scenes 2022 Mobile, development dataset consists of 1-second audio recordings from 12 European cities in 10 different acoustic scenes using 4 different devices. This is a stratified subsampled version of the evaluation_setup subset of the original dataset.</p> <p>Dataset: <code>mteb/tau-acoustic-scenes-2022-mobile-mini</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx AudioScene expert-annotated found Citation <pre><code>@dataset{heittola_2022_6337421,\n  author = {Toni Heittola and Annamaria Mesaros and Tuomas Virtanen},\n  publisher = {Zenodo},\n  title = {TAU Urban Acoustic Scenes 2022 Mobile, Development Dataset},\n  url = {https://doi.org/10.5281/zenodo.6337421},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#tutacousticscenes","title":"TUTAcousticScenes","text":"<p>TUT Urban Acoustic Scenes 2018 dataset consists of 10-second audio segments from 10 acoustic scenes recorded in six European cities. This is a stratified subsampled version of the original dataset.</p> <p>Dataset: <code>mteb/tut-acoustic-scenes-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx AudioScene expert-annotated found Citation <pre><code>@inproceedings{Mesaros2018_DCASE,\n  address = {Tampere, Finland},\n  author = {Annamaria Mesaros and Toni Heittola and Tuomas Virtanen},\n  booktitle = {Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)},\n  publisher = {Tampere University of Technology},\n  title = {A Multi-Device Dataset for Urban Acoustic Scene Classification},\n  url = {https://arxiv.org/abs/1807.09840},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#urbansound8k","title":"UrbanSound8k","text":"<p>Environmental Sound Classification Dataset.</p> <p>Dataset: <code>mteb/urbansound8K</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy zxx AudioScene human-annotated found Citation <pre><code>@inproceedings{Salamon:UrbanSound:ACMMM:14,\n  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},\n  booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},\n  organization = {ACM},\n  pages = {1041--1044},\n  title = {A Dataset and Taxonomy for Urban Sound Research},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#vocalsound","title":"VocalSound","text":"<p>Human Vocal Sound Classification Dataset.</p> <p>Dataset: <code>mteb/vocalsound</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Spoken human-annotated found Citation <pre><code>@inproceedings{Gong_2022,\n  author = {Gong, Yuan and Yu, Jin and Glass, James},\n  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  doi = {10.1109/icassp43922.2022.9746828},\n  month = may,\n  publisher = {IEEE},\n  title = {Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition},\n  url = {http://dx.doi.org/10.1109/ICASSP43922.2022.9746828},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#voxcelebsa","title":"VoxCelebSA","text":"<p>VoxCeleb dataset augmented for Sentiment Analysis task</p> <p>Dataset: <code>mteb/voxceleb-sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Spoken human-annotated found Citation <pre><code>@misc{shon2022sluenewbenchmarktasks,\n  archiveprefix = {arXiv},\n  author = {Suwon Shon and Ankita Pasad and Felix Wu and Pablo Brusco and Yoav Artzi and Karen Livescu and Kyu J. Han},\n  eprint = {2111.10367},\n  primaryclass = {cs.CL},\n  title = {SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech},\n  url = {https://arxiv.org/abs/2111.10367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#voxlingua107_top10","title":"VoxLingua107_Top10","text":"<p>Spoken Language Identification for a given audio samples (10 classes/languages)</p> <p>Dataset: <code>mteb/voxlingua107-top10</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech automatic-and-reviewed found Citation <pre><code>@misc{valk2020voxlingua107datasetspokenlanguage,\n  archiveprefix = {arXiv},\n  author = {J\u00f6rgen Valk and Tanel Alum\u00e4e},\n  eprint = {2011.12998},\n  primaryclass = {eess.AS},\n  title = {VoxLingua107: a Dataset for Spoken Language Recognition},\n  url = {https://arxiv.org/abs/2011.12998},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#voxpopuliaccentid","title":"VoxPopuliAccentID","text":"<p>Classification of English speech samples into one of 15 non-native accents from European Parliament recordings. This is a stratified subsampled version of the original VoxPopuli dataset.</p> <p>Dataset: <code>mteb/voxpopuli-accent-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy eng Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#voxpopuligenderid","title":"VoxPopuliGenderID","text":"<p>Subsampled Dataset Classification of speech samples by speaker gender (male/female) from European Parliament recordings.</p> <p>Dataset: <code>mteb/voxpopuli-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy deu, eng, fra, pol, spa Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclassification/#voxpopulilanguageid","title":"VoxPopuliLanguageID","text":"<p>Subsampled Dataset for classification of speech samples into one of 5 European languages (English, German, French, Spanish, Polish) from European Parliament recordings.</p> <p>Dataset: <code>mteb/voxpopuli-mini</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy deu, eng, fra, pol, spa Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/","title":"AudioClustering","text":"<ul> <li>Number of tasks: 10</li> </ul>"},{"location":"overview/available_tasks/audioclustering/#ambientacousticcontextclustering","title":"AmbientAcousticContextClustering","text":"<p>Clustering task based on a subset of the Ambient Acoustic Context dataset containing 1-second segments for workplace activities.</p> <p>Dataset: <code>mteb/ambient-acoustic-context-small</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure eng Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{10.1145/3379503.3403535,\n  address = {New York, NY, USA},\n  articleno = {33},\n  author = {Park, Chunjong and Min, Chulhong and Bhattacharya, Sourav and Kawsar, Fahim},\n  booktitle = {22nd International Conference on Human-Computer Interaction with Mobile Devices and Services},\n  doi = {10.1145/3379503.3403535},\n  isbn = {9781450375160},\n  keywords = {Acoustic ambient context, Conversational agents},\n  location = {Oldenburg, Germany},\n  numpages = {9},\n  publisher = {Association for Computing Machinery},\n  series = {MobileHCI '20},\n  title = {Augmenting Conversational Agents with Ambient Acoustic Contexts},\n  url = {https://doi.org/10.1145/3379503.3403535},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#crema_dclustering","title":"CREMA_DClustering","text":"<p>Emotion clustering task with audio data for 6 emotions: Anger, Disgust, Fear, Happy, Neutral, Sad.</p> <p>Dataset: <code>mteb/crema-d</code> \u2022 License: http://opendatacommons.org/licenses/odbl/1.0/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure eng Speech human-annotated created Citation <pre><code>@article{Cao2014-ih,\n  author = {Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur,\nRuben C and Nenkova, Ani and Verma, Ragini},\n  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n  journal = {IEEE Transactions on Affective Computing},\n  keywords = {Emotional corpora; facial expression; multi-modal recognition;\nvoice expression},\n  language = {en},\n  month = oct,\n  number = {4},\n  pages = {377--390},\n  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},\n  title = {{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},\n  volume = {5},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#esc50clustering","title":"ESC50Clustering","text":"<p>The ESC-50 dataset contains 2,000 labeled environmental audio recordings evenly distributed across 50 classes (40 clips per class). These classes are organized into 5 broad categories: animal sounds, natural soundscapes &amp; water sounds, human (non-speech) sounds, interior/domestic sounds, and exterior/urban noises. This task evaluates unsupervised clustering performance on environmental audio recordings.</p> <p>Dataset: <code>mteb/esc50</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure zxx Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{piczak2015dataset,\n  author = {Piczak, Karol J.},\n  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},\n  date = {2015-10-13},\n  doi = {10.1145/2733373.2806390},\n  isbn = {978-1-4503-3459-4},\n  location = {{Brisbane, Australia}},\n  pages = {1015--1018},\n  publisher = {{ACM Press}},\n  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},\n  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#gtzangenreclustering","title":"GTZANGenreClustering","text":"<p>Music genre clustering task based on GTZAN dataset with 10 music genres.</p> <p>Dataset: <code>mteb/gtzan-genre</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure zxx Music human-annotated found Citation <pre><code>@article{1021072,\n  author = {Tzanetakis, G. and Cook, P.},\n  doi = {10.1109/TSA.2002.800560},\n  journal = {IEEE Transactions on Speech and Audio Processing},\n  keywords = {Humans;Music information retrieval;Instruments;Computer science;Multiple signal classification;Signal analysis;Pattern recognition;Feature extraction;Wavelet analysis;Cultural differences},\n  number = {5},\n  pages = {293-302},\n  title = {Musical genre classification of audio signals},\n  volume = {10},\n  year = {2002},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#musicgenreclustering","title":"MusicGenreClustering","text":"<p>Clustering music recordings in 9 different genres.</p> <p>Dataset: <code>mteb/music-genre</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure zxx Music human-annotated found Citation <pre><code>@inproceedings{homburg2005benchmark,\n  author = {Homburg, Helge and Mierswa, Ingo and M{\\\"o}ller, B{\\\"u}lent and Morik, Katharina and Wurst, Michael},\n  booktitle = {ISMIR},\n  pages = {528--31},\n  title = {A Benchmark Dataset for Audio Classification and Clustering.},\n  volume = {2005},\n  year = {2005},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#vehiclesoundclustering","title":"VehicleSoundClustering","text":"<p>Clustering vehicle sounds recorded from smartphones (0 (car class), 1 (truck, bus and van class), 2 (motorcycle class))</p> <p>Dataset: <code>mteb/Vehicle_sounds_classification_dataset</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure zxx Scene derived created Citation <pre><code>@inproceedings{inproceedings,\n  author = {Bazilinskyy, Pavlo and Aa, Arne and Schoustra, Michael and Spruit, John and Staats, Laurens and van der Vlist, Klaas Jan and de Winter, Joost},\n  month = {05},\n  pages = {},\n  title = {An auditory dataset of passing vehicles recorded with a smartphone},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#voicegenderclustering","title":"VoiceGenderClustering","text":"<p>Clustering audio recordings based on gender (male vs female).</p> <p>Dataset: <code>mteb/VoiceGenderClustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure eng Spoken derived found Citation <pre><code>@inproceedings{Chung18b,\n  author = {Joon Son Chung and Arsha Nagrani and Andrew Zisserman},\n  booktitle = {Proceedings of Interspeech},\n  title = {VoxCeleb2: Deep Speaker Recognition},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#voxcelebclustering","title":"VoxCelebClustering","text":"<p>Clustering task based on the VoxCeleb dataset for sentiment analysis, clustering by positive/negative sentiment.</p> <p>Dataset: <code>mteb/Sentiment_Analysis_SLUE-VoxCeleb</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure eng Speech, Spoken human-annotated found Citation <pre><code>@misc{shon2022sluenewbenchmarktasks,\n  archiveprefix = {arXiv},\n  author = {Suwon Shon and Ankita Pasad and Felix Wu and Pablo Brusco and Yoav Artzi and Karen Livescu and Kyu J. Han},\n  eprint = {2111.10367},\n  primaryclass = {cs.CL},\n  title = {SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech},\n  url = {https://arxiv.org/abs/2111.10367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#voxpopuliaccentclustering","title":"VoxPopuliAccentClustering","text":"<p>Clustering English speech samples by non-native accent from European Parliament recordings.</p> <p>Dataset: <code>mteb/voxpopuli-accent-clustering</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure eng Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioclustering/#voxpopuligenderclustering","title":"VoxPopuliGenderClustering","text":"<p>Subsampled Dataset for clustering speech samples by speaker gender (male/female) from European Parliament recordings.</p> <p>Dataset: <code>mteb/mini-voxpopuli</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) v_measure deu, eng, fra, pol, spa Speech, Spoken human-annotated found Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/","title":"AudioMultilabelClassification","text":"<ul> <li>Number of tasks: 6</li> </ul>"},{"location":"overview/available_tasks/audiomultilabelclassification/#audioset","title":"AudioSet","text":"<p>AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos.</p> <p>Dataset: <code>agkphysics/AudioSet</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) lrap eng Music, Scene, Speech, Web human-annotated found Citation <pre><code>@inproceedings{audioset,\n  author = {Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},\n  booktitle = {2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},\n  organization = {IEEE},\n  pages = {776--780},\n  title = {Audio set: An ontology and human-labeled dataset for audio events},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/#audiosetmini","title":"AudioSetMini","text":"<p>AudioSet consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos. This is a mini version that is sampled from the original dataset.</p> <p>Dataset: <code>mteb/audioset</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) lrap eng Music, Scene, Speech, Web human-annotated found Citation <pre><code>@inproceedings{audioset,\n  author = {Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},\n  booktitle = {2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)},\n  organization = {IEEE},\n  pages = {776--780},\n  title = {Audio set: An ontology and human-labeled dataset for audio events},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/#birdset","title":"BirdSet","text":"<p>BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics</p> <p>Dataset: <code>mteb/BirdSet</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy zxx Bioacoustics, Speech, Spoken human-annotated created Citation <pre><code>@misc{rauch2024birdsetlargescaledatasetaudio,\n  archiveprefix = {arXiv},\n  author = {Lukas Rauch and Raphael Schwinger and Moritz Wirth and Ren\u00e9 Heinrich and Denis Huseljic and Marek Herde and Jonas Lange and Stefan Kahl and Bernhard Sick and Sven Tomforde and Christoph Scholz},\n  eprint = {2403.10380},\n  primaryclass = {cs.SD},\n  title = {BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics},\n  url = {https://arxiv.org/abs/2403.10380},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/#fsd2019kaggle","title":"FSD2019Kaggle","text":"<p>Multilabel Audio Classification.</p> <p>Dataset: <code>mteb/fsdkaggle2019-parquet</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Web human-annotated found Citation <pre><code>@dataset{eduardo_fonseca_2020_3612637,\n  author = {Eduardo Fonseca and\nManoj Plakal and\nFrederic Font and\nDaniel P. W. Ellis and\nXavier Serra},\n  doi = {10.5281/zenodo.3612637},\n  month = jan,\n  publisher = {Zenodo},\n  title = {FSDKaggle2019},\n  url = {https://doi.org/10.5281/zenodo.3612637},\n  version = {1.0},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/#fsd50k","title":"FSD50K","text":"<p>Multilabel Audio Classification on a subsampled version of FSD50K using 2048 samples</p> <p>Dataset: <code>mteb/fsd50k_mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Web human-annotated found Citation <pre><code>@article{9645159,\n  author = {Fonseca, Eduardo and Favory, Xavier and Pons, Jordi and Font, Frederic and Serra, Xavier},\n  doi = {10.1109/TASLP.2021.3133208},\n  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},\n  keywords = {Videos;Task analysis;Labeling;Vocabulary;Speech recognition;Ontologies;Benchmark testing;Audio dataset;sound event;recognition;classification;tagging;data collection;environmental sound},\n  number = {},\n  pages = {829-852},\n  title = {FSD50K: An Open Dataset of Human-Labeled Sound Events},\n  volume = {30},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiomultilabelclassification/#sibfleurs","title":"SIBFLEURS","text":"<p>Topic Classification for multilingual audio dataset. This dataset is a stratified and downsampled subset of the SIBFLEURS dataset, which is a collection of 1000+ hours of audio data in 100+ languages.</p> <p>Dataset: <code>mteb/sib-fleurs-multilingual-mini</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to category (a2c) accuracy afr, amh, arb, asm, ast, ... (101) Encyclopaedic derived found Citation <pre><code>@misc{schmidt2025fleursslumassivelymultilingualbenchmark,\n  archiveprefix = {arXiv},\n  author = {Fabian David Schmidt and Ivan Vuli\u0107 and Goran Glava\u0161 and David Ifeoluwa Adelani},\n  eprint = {2501.06117},\n  primaryclass = {cs.CL},\n  title = {Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding},\n  url = {https://arxiv.org/abs/2501.06117},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiopairclassification/","title":"AudioPairClassification","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/audiopairclassification/#cremadpairclassification","title":"CREMADPairClassification","text":"<p>Classifying pairs as having same or different emotions in actor's voice recordings of text spoken in 6 different emotions</p> <p>Dataset: <code>mteb/CREMADPairClassification</code> \u2022 License: http://opendatacommons.org/licenses/odbl/1.0/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) max_ap eng Spoken human-annotated created Citation <pre><code>@article{Cao2014-ih,\n  author = {Cao, Houwei and Cooper, David G and Keutmann, Michael K and Gur,\nRuben C and Nenkova, Ani and Verma, Ragini},\n  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n  journal = {IEEE Transactions on Affective Computing},\n  keywords = {Emotional corpora; facial expression; multi-modal recognition;\nvoice expression},\n  language = {en},\n  month = oct,\n  number = {4},\n  pages = {377--390},\n  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},\n  title = {{CREMA-D}: Crowd-sourced emotional multimodal actors dataset},\n  volume = {5},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiopairclassification/#esc50pairclassification","title":"ESC50PairClassification","text":"<p>Environmental Sound Classification Dataset.</p> <p>Dataset: <code>mteb/ESC50PairClassification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) max_ap zxx Encyclopaedic human-annotated found Citation <pre><code>@inproceedings{piczak2015dataset,\n  author = {Piczak, Karol J.},\n  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},\n  date = {2015-10-13},\n  doi = {10.1145/2733373.2806390},\n  isbn = {978-1-4503-3459-4},\n  location = {{Brisbane, Australia}},\n  pages = {1015--1018},\n  publisher = {{ACM Press}},\n  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},\n  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiopairclassification/#nmsqapairclassification","title":"NMSQAPairClassification","text":"<p>A textless Q&amp;A dataset. Given a pair of audio question and audio answer, is the answer relevant to the question?</p> <p>Dataset: <code>mteb/NMSQAPairClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) max_ap eng Spoken human-annotated found Citation <pre><code>@misc{lin2022dualdiscretespokenunit,\n  archiveprefix = {arXiv},\n  author = {Guan-Ting Lin and Yung-Sung Chuang and Ho-Lam Chung and Shu-wen Yang and Hsuan-Jui Chen and Shuyan Dong and Shang-Wen Li and Abdelrahman Mohamed and Hung-yi Lee and Lin-shan Lee},\n  eprint = {2203.04911},\n  primaryclass = {cs.CL},\n  title = {DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering},\n  url = {https://arxiv.org/abs/2203.04911},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiopairclassification/#vocalsoundpairclassification","title":"VocalSoundPairClassification","text":"<p>Recognizing whether two audio clips are the same human vocal expression (laughing, sighing, etc.)</p> <p>Dataset: <code>mteb/VocalSoundPairClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) max_ap eng Spoken human-annotated found Citation <pre><code>@inproceedings{Gong_2022,\n  author = {Gong, Yuan and Yu, Jin and Glass, James},\n  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  doi = {10.1109/icassp43922.2022.9746828},\n  month = may,\n  publisher = {IEEE},\n  title = {Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition},\n  url = {http://dx.doi.org/10.1109/ICASSP43922.2022.9746828},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiopairclassification/#voxpopuliaccentpairclassification","title":"VoxPopuliAccentPairClassification","text":"<p>Classifying same or different regional accent of English (Empty Audio Samples filtered out)</p> <p>Dataset: <code>mteb/VoxPopuliAccentPairClassificationFiltered</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) max_ap eng Spoken human-annotated created Citation <pre><code>@inproceedings{wang-etal-2021-voxpopuli,\n  address = {Online},\n  author = {Wang, Changhan  and\nRiviere, Morgane  and\nLee, Ann  and\nWu, Anne  and\nTalnikar, Chaitanya  and\nHaziza, Daniel  and\nWilliamson, Mary  and\nPino, Juan  and\nDupoux, Emmanuel},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.80},\n  editor = {Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto},\n  month = aug,\n  pages = {993--1003},\n  publisher = {Association for Computational Linguistics},\n  title = {{V}ox{P}opuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation},\n  url = {https://aclanthology.org/2021.acl-long.80/},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioreranking/","title":"AudioReranking","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/audioreranking/#esc50audioreranking","title":"ESC50AudioReranking","text":"<p>ESC-50 environmental sound dataset adapted for audio reranking. Given a query audio of environmental sounds, rank 5 relevant audio samples higher than 16 irrelevant ones from different sound classes. Contains 200 queries across 50 environmental sound categories for robust evaluation.</p> <p>Dataset: <code>mteb/ESC50AudioReranking</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) map_at_1000 zxx AudioScene expert-annotated found Citation <pre><code>@inproceedings{piczak2015dataset,\n  author = {Piczak, Karol J.},\n  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},\n  date = {2015-10-13},\n  doi = {10.1145/2733373.2806390},\n  isbn = {978-1-4503-3459-4},\n  location = {{Brisbane, Australia}},\n  pages = {1015--1018},\n  publisher = {{ACM Press}},\n  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},\n  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioreranking/#fsdnoisy18kaudioreranking","title":"FSDnoisy18kAudioReranking","text":"<p>FSDnoisy18k sound event dataset adapted for audio reranking. Given a query audio with potential label noise, rank 4 relevant audio samples higher than 16 irrelevant ones from different sound classes. Contains 200 queries across 20 sound event categories.</p> <p>Dataset: <code>mteb/FSDnoisy18kAudioReranking</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) map_at_1000 eng AudioScene human-annotated found Citation <pre><code>@inproceedings{fonseca2019fsdnoisy18k,\n  author = {Fonseca, Eduardo and Plakal, Manoj and Ellis, Daniel P. W. and Font, Frederic and Favory, Xavier and Serra, Xavier},\n  booktitle = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  organization = {IEEE},\n  pages = {21--25},\n  title = {Learning Sound Event Classifiers from Web Audio with Noisy Labels},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioreranking/#gtzanaudioreranking","title":"GTZANAudioReranking","text":"<p>GTZAN music genre dataset adapted for audio reranking. Given a query audio from one of 10 music genres, rank 3 relevant audio samples higher than 10 irrelevant ones from different genres. Contains 100 queries across 10 music genres for comprehensive evaluation.</p> <p>Dataset: <code>mteb/GTZANAudioReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) map_at_1000 zxx Music human-annotated found Citation <pre><code>@article{1021072,\n  author = {Tzanetakis, G. and Cook, P.},\n  doi = {10.1109/TSA.2002.800560},\n  journal = {IEEE Transactions on Speech and Audio Processing},\n  keywords = {Humans;Music information retrieval;Instruments;Computer science;Multiple signal classification;Signal analysis;Pattern recognition;Feature extraction;Wavelet analysis;Cultural differences},\n  number = {5},\n  pages = {293-302},\n  title = {Musical genre classification of audio signals},\n  volume = {10},\n  year = {2002},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioreranking/#urbansound8kaudioreranking","title":"UrbanSound8KAudioReranking","text":"<p>UrbanSound8K urban sound dataset adapted for audio reranking. Given a query audio of urban sounds, rank 4 relevant audio samples higher than 16 irrelevant ones from different urban sound classes. Contains 200 queries across 10 urban sound categories for comprehensive evaluation.</p> <p>Dataset: <code>mteb/UrbanSound8KAudioReranking</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) map_at_1000 zxx Spoken human-annotated found Citation <pre><code>@inproceedings{Salamon:UrbanSound:ACMMM:14,\n  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},\n  booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},\n  organization = {ACM},\n  pages = {1041--1044},\n  title = {A Dataset and Taxonomy for Urban Sound Research},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/audioreranking/#vocalsoundaudioreranking","title":"VocalSoundAudioReranking","text":"<p>VocalSound dataset adapted for audio reranking. Given a query vocal sound from one of 6 categories, rank 4 relevant vocal samples higher than 16 irrelevant ones from different vocal sound types. Contains 198 queries across 6 vocal sound categories for robust evaluation.</p> <p>Dataset: <code>mteb/VocalSoundAudioReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to audio (a2a) map_at_1000 eng Spoken human-annotated found Citation <pre><code>@inproceedings{Gong_2022,\n  author = {Gong, Yuan and Yu, Jin and Glass, James},\n  booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},\n  doi = {10.1109/icassp43922.2022.9746828},\n  month = may,\n  publisher = {IEEE},\n  title = {Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition},\n  url = {http://dx.doi.org/10.1109/ICASSP43922.2022.9746828},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiozeroshotclassification/","title":"AudioZeroshotClassification","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/audiozeroshotclassification/#esc50_zeroshot","title":"ESC50_Zeroshot","text":"<p>Environmental Sound Classification Dataset.</p> <p>Dataset: <code>mteb/esc50</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy zxx Spoken human-annotated found Citation <pre><code>@inproceedings{piczak2015dataset,\n  author = {Piczak, Karol J.},\n  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},\n  date = {2015-10-13},\n  doi = {10.1145/2733373.2806390},\n  isbn = {978-1-4503-3459-4},\n  location = {{Brisbane, Australia}},\n  pages = {1015--1018},\n  publisher = {{ACM Press}},\n  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},\n  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiozeroshotclassification/#ravdesszeroshot","title":"RavdessZeroshot","text":"<p>Emotion classification Dataset. RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes neutral,calm, happy, sad, angry, fearful, surprise, and disgust expressions. These 8 emtoions also serve as labels for the dataset.</p> <p>Dataset: <code>mteb/RavdessZeroshot</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Spoken human-annotated found Citation <pre><code>@article{10.1371/journal.pone.0196391,\n  author = {Livingstone, Steven R. AND Russo, Frank A.},\n  doi = {10.1371/journal.pone.0196391},\n  journal = {PLOS ONE},\n  month = {05},\n  number = {5},\n  pages = {1-35},\n  publisher = {Public Library of Science},\n  title = {The Ryerson Audio-Visual Database ofal Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},\n  url = {https://doi.org/10.1371/journal.pone.0196391},\n  volume = {13},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiozeroshotclassification/#speechcommandszeroshotv001","title":"SpeechCommandsZeroshotv0.01","text":"<p>Sound Classification/Keyword Spotting Dataset. This is a set of one-second audio clips containing a single spoken English word or background noise. These words are from a small set of commands such as 'yes', 'no', and 'stop' spoken by various speakers. With a total of 10 labels/commands for keyword spotting and a total of 30 labels for other auxiliary tasks</p> <p>Dataset: <code>mteb/SpeechCommandsZeroshotv0.01</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Spoken human-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-1804-03209,\n  author = {Pete Warden},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1804-03209.bib},\n  eprint = {1804.03209},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},\n  title = {Speech Commands: {A} Dataset for Limited-Vocabulary Speech Recognition},\n  url = {http://arxiv.org/abs/1804.03209},\n  volume = {abs/1804.03209},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiozeroshotclassification/#speechcommandszeroshotv002","title":"SpeechCommandsZeroshotv0.02","text":"<p>Sound Classification/Keyword Spotting Dataset. This is a set of one-second audio clips containing a single spoken English word or background noise. These words are from a small set of commands such as 'yes', 'no', and 'stop' spoken by various speakers. With a total of 10 labels/commands for keyword spotting and a total of 30 labels for other auxiliary tasks</p> <p>Dataset: <code>mteb/SpeechCommandsZeroshotv0.02</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy eng Spoken human-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-1804-03209,\n  author = {Pete Warden},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1804-03209.bib},\n  eprint = {1804.03209},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:48:32 +0200},\n  title = {Speech Commands: {A} Dataset for Limited-Vocabulary Speech Recognition},\n  url = {http://arxiv.org/abs/1804.03209},\n  volume = {abs/1804.03209},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/audiozeroshotclassification/#urbansound8kzeroshot","title":"UrbanSound8kZeroshot","text":"<p>Environmental Sound Classification Dataset.</p> <p>Dataset: <code>mteb/urbansound8K</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation audio to text (a2t) accuracy zxx AudioScene human-annotated found Citation <pre><code>@inproceedings{Salamon:UrbanSound:ACMMM:14,\n  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},\n  booktitle = {Proceedings of the 22nd ACM international conference on Multimedia},\n  organization = {ACM},\n  pages = {1041--1044},\n  title = {A Dataset and Taxonomy for Urban Sound Research},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/","title":"BitextMining","text":"<ul> <li>Number of tasks: 31</li> </ul>"},{"location":"overview/available_tasks/bitextmining/#bucc","title":"BUCC","text":"<p>BUCC bitext mining dataset train split.</p> <p>Dataset: <code>mteb/BUCC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 cmn, deu, eng, fra, rus Written human-annotated human-translated Citation <pre><code>@inproceedings{zweigenbaum-etal-2017-overview,\n  address = {Vancouver, Canada},\n  author = {Zweigenbaum, Pierre  and\nSharoff, Serge  and\nRapp, Reinhard},\n  booktitle = {Proceedings of the 10th Workshop on Building and Using Comparable Corpora},\n  doi = {10.18653/v1/W17-2512},\n  editor = {Sharoff, Serge  and\nZweigenbaum, Pierre  and\nRapp, Reinhard},\n  month = aug,\n  pages = {60--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora},\n  url = {https://aclanthology.org/W17-2512},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#buccv2","title":"BUCC.v2","text":"<p>BUCC bitext mining dataset train split, gold set only.</p> <p>Dataset: <code>mteb/bucc-bitext-mining</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 cmn, deu, eng, fra, rus Written human-annotated human-translated Citation <pre><code>@inproceedings{zweigenbaum-etal-2017-overview,\n  address = {Vancouver, Canada},\n  author = {Zweigenbaum, Pierre  and\nSharoff, Serge  and\nRapp, Reinhard},\n  booktitle = {Proceedings of the 10th Workshop on Building and Using Comparable Corpora},\n  doi = {10.18653/v1/W17-2512},\n  editor = {Sharoff, Serge  and\nZweigenbaum, Pierre  and\nRapp, Reinhard},\n  month = aug,\n  pages = {60--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora},\n  url = {https://aclanthology.org/W17-2512},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#biblenlpbitextmining","title":"BibleNLPBitextMining","text":"<p>Partial Bible translations in 829 languages, aligned by verse.</p> <p>Dataset: <code>davidstap/biblenlp-corpus-mmteb</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 aai, aak, aau, aaz, abt, ... (829) Religious, Written expert-annotated created Citation <pre><code>@article{akerman2023ebible,\n  author = {Akerman, Vesa and Baines, David and Daspit, Damien and Hermjakob, Ulf and Jang, Taeho and Leong, Colin and Martin, Michael and Mathew, Joel and Robie, Jonathan and Schwarting, Marcus},\n  journal = {arXiv preprint arXiv:2304.09919},\n  title = {The eBible Corpus: Data and Model Benchmarks for Bible Translation for Low-Resource Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#bornholmbitextmining","title":"BornholmBitextMining","text":"<p>Danish Bornholmsk Parallel Corpus. Bornholmsk is a Danish dialect spoken on the island of Bornholm, Denmark. Historically it is a part of east Danish which was also spoken in Scania and Halland, Sweden.</p> <p>Dataset: <code>mteb/BornholmBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 dan Fiction, Social, Web, Written expert-annotated created Citation <pre><code>@inproceedings{derczynskiBornholmskNaturalLanguage2019,\n  author = {Derczynski, Leon and Kjeldsen, Alex Speed},\n  booktitle = {Proceedings of the Nordic Conference of Computational Linguistics (2019)},\n  date = {2019},\n  file = {Available Version (via Google Scholar):/Users/au554730/Zotero/storage/FBQ73ZYN/Derczynski and Kjeldsen - 2019 - Bornholmsk natural language processing Resources .pdf:application/pdf},\n  pages = {338--344},\n  publisher = {Link\u00f6ping University Electronic Press},\n  shorttitle = {Bornholmsk natural language processing},\n  title = {Bornholmsk natural language processing: Resources and tools},\n  url = {https://pure.itu.dk/ws/files/84551091/W19_6138.pdf},\n  urldate = {2024-04-24},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#danishmedicinesagencybitextmining","title":"DanishMedicinesAgencyBitextMining","text":"<p>A Bilingual English-Danish parallel corpus from The Danish Medicines Agency.</p> <p>Dataset: <code>mteb/english-danish-parallel-corpus</code> \u2022 License: https://opendefinition.org/od/2.1/en/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 dan, eng Medical, Written human-annotated found Citation <pre><code>@misc{elrc_danish_medicines_agency_2018,\n  author = {Rozis, Roberts},\n  institution = {European Union},\n  license = {Open Under-PSI},\n  note = {Dataset created within the European Language Resource Coordination (ELRC) project under the Connecting Europe Facility - Automated Translation (CEF.AT) actions SMART 2014/1074 and SMART 2015/1091.},\n  title = {Bilingual English-Danish Parallel Corpus from the Danish Medicines Agency},\n  url = {https://sprogteknologi.dk/dataset/bilingual-english-danish-parallel-corpus-from-the-danish-medicines-agency},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#diablabitextmining","title":"DiaBlaBitextMining","text":"<p>English-French Parallel Corpus. DiaBLa is an English-French dataset for the evaluation of Machine Translation (MT) for informal, written bilingual dialogue.</p> <p>Dataset: <code>mteb/DiaBlaBitextMining</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, fra Social, Written human-annotated created Citation <pre><code>@inproceedings{gonzalez2019diabla,\n  author = {Gonz\u00e1lez, Matilde and Garc\u00eda, Clara and S\u00e1nchez, Luc\u00eda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  pages = {4192--4198},\n  title = {DiaBLa: A Corpus of Bilingual Spontaneous Written Dialogues for Machine Translation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#floresbitextmining","title":"FloresBitextMining","text":"<p>FLORES is a benchmark dataset for machine translation between English and low-resource languages.</p> <p>Dataset: <code>mteb/FloresBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ace, acm, acq, aeb, afr, ... (196) Encyclopaedic, Non-fiction, Written human-annotated created Citation <pre><code>@inproceedings{goyal2022flores,\n  author = {Goyal, Naman and Gao, Cynthia and Chaudhary, Vishrav and Chen, Peng-Jen and Wenzek, Guillaume and Ju, Da and Krishnan, Sanjana and Ranzato, Marc'Aurelio and Guzm{\\'a}n, Francisco},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  pages = {19--35},\n  title = {The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#in22convbitextmining","title":"IN22ConvBitextMining","text":"<p>IN22-Conv is a n-way parallel conversation domain benchmark dataset for machine translation spanning English and 22 Indic languages.</p> <p>Dataset: <code>mteb/IN22ConvBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, ben, brx, doi, eng, ... (23) Fiction, Social, Spoken, Spoken expert-annotated created Citation <pre><code>@article{gala2023indictrans,\n  author = {Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\n  issn = {2835-8856},\n  journal = {Transactions on Machine Learning Research},\n  note = {},\n  title = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\n  url = {https://openreview.net/forum?id=vfT4YuzAYA},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#in22genbitextmining","title":"IN22GenBitextMining","text":"<p>IN22-Gen is a n-way parallel general-purpose multi-domain benchmark dataset for machine translation spanning English and 22 Indic languages.</p> <p>Dataset: <code>mteb/IN22GenBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, ben, brx, doi, eng, ... (23) Government, Legal, News, Non-fiction, Religious, ... (7) expert-annotated created Citation <pre><code>@article{gala2023indictrans,\n  author = {Jay Gala and Pranjal A Chitale and A K Raghavan and Varun Gumma and Sumanth Doddapaneni and Aswanth Kumar M and Janki Atul Nawale and Anupama Sujatha and Ratish Puduppully and Vivek Raghavan and Pratyush Kumar and Mitesh M Khapra and Raj Dabre and Anoop Kunchukuttan},\n  issn = {2835-8856},\n  journal = {Transactions on Machine Learning Research},\n  note = {},\n  title = {IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages},\n  url = {https://openreview.net/forum?id=vfT4YuzAYA},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#iwslt2017bitextmining","title":"IWSLT2017BitextMining","text":"<p>The IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT system across all directions including English, German, Dutch, Italian and Romanian.</p> <p>Dataset: <code>mteb/IWSLT2017BitextMining</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, cmn, deu, eng, fra, ... (10) Fiction, Non-fiction, Written expert-annotated found Citation <pre><code>@inproceedings{cettolo-etal-2017-overview,\n  address = {Tokyo, Japan},\n  author = {Cettolo, Mauro  and\nFederico, Marcello  and\nBentivogli, Luisa  and\nNiehues, Jan  and\nSt{\\\"u}ker, Sebastian  and\nSudoh, Katsuhito  and\nYoshino, Koichiro  and\nFedermann, Christian},\n  booktitle = {Proceedings of the 14th International Conference on Spoken Language Translation},\n  editor = {Sakti, Sakriani  and\nUtiyama, Masao},\n  month = dec # { 14-15},\n  pages = {2--14},\n  publisher = {International Workshop on Spoken Language Translation},\n  title = {Overview of the {IWSLT} 2017 Evaluation Campaign},\n  url = {https://aclanthology.org/2017.iwslt-1.1},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#indicgenbenchfloresbitextmining","title":"IndicGenBenchFloresBitextMining","text":"<p>Flores-IN dataset is an extension of Flores dataset released as a part of the IndicGenBench by Google</p> <p>Dataset: <code>mteb/IndicGenBenchFloresBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 asm, awa, ben, bgc, bho, ... (30) News, Web, Written expert-annotated human-translated and localized Citation <pre><code>@misc{singh2024indicgenbench,\n  archiveprefix = {arXiv},\n  author = {Harman Singh and Nitish Gupta and Shikhar Bharadwaj and Dinesh Tewari and Partha Talukdar},\n  eprint = {2404.16816},\n  primaryclass = {cs.CL},\n  title = {IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#lincemtbitextmining","title":"LinceMTBitextMining","text":"<p>LinceMT is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.</p> <p>Dataset: <code>gentaiscool/bitext_lincemt_miners</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hin Social, Written human-annotated found Citation <pre><code>@inproceedings{aguilar2020lince,\n  author = {Aguilar, Gustavo and Kar, Sudipta and Solorio, Thamar},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  pages = {1803--1813},\n  title = {LinCE: A Centralized Benchmark for Linguistic Code-switching Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#ntrexbitextmining","title":"NTREXBitextMining","text":"<p>NTREX is a News Test References dataset for Machine Translation Evaluation, covering translation from English into 128 languages. We select language pairs according to the M2M-100 language grouping strategy, resulting in 1916 directions.</p> <p>Dataset: <code>mteb/NTREXBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 afr, amh, arb, aze, bak, ... (119) News, Written expert-annotated human-translated and localized Citation <pre><code>@inproceedings{federmann-etal-2022-ntrex,\n  address = {Online},\n  author = {Federmann, Christian and Kocmi, Tom and Xin, Ying},\n  booktitle = {Proceedings of the First Workshop on Scaling Up Multilingual Evaluation},\n  month = {nov},\n  pages = {21--24},\n  publisher = {Association for Computational Linguistics},\n  title = {{NTREX}-128 {--} News Test References for {MT} Evaluation of 128 Languages},\n  url = {https://aclanthology.org/2022.sumeval-1.4},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nollysentibitextmining","title":"NollySentiBitextMining","text":"<p>NollySenti is Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yoruba.</p> <p>Dataset: <code>mteb/NollySentiBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hau, ibo, pcm, yor Reviews, Social, Written human-annotated found Citation <pre><code>@inproceedings{shode2023nollysenti,\n  author = {Shode, Iyanuoluwa and Adelani, David Ifeoluwa and Peng, Jing and Feldman, Anna},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n  pages = {986--998},\n  title = {NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#norwegiancourtsbitextmining","title":"NorwegianCourtsBitextMining","text":"<p>Nynorsk and Bokm\u00e5l parallel corpus from Norwegian courts. Norwegian courts have two standardised written languages. Bokm\u00e5l is a variant closer to Danish, while Nynorsk was created to resemble regional dialects of Norwegian.</p> <p>Dataset: <code>mteb/NorwegianCourtsBitextMining</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 nno, nob Legal, Written human-annotated found Citation <pre><code>@inproceedings{opus4,\n  author = {Tiedemann, J{\\\"o}rg and Thottingal, Santhosh},\n  booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation (EAMT)},\n  title = {OPUS-MT \u2014 Building open translation services for the World},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nusatranslationbitextmining","title":"NusaTranslationBitextMining","text":"<p>NusaTranslation is a parallel dataset for machine translation on 11 Indonesia languages and English.</p> <p>Dataset: <code>mteb/NusaTranslationBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 abs, bbc, bew, bhp, ind, ... (12) Social, Written human-annotated created Citation <pre><code>@inproceedings{cahyawijaya-etal-2023-nusawrites,\n  address = {Nusa Dua, Bali},\n  author = {Cahyawijaya, Samuel  and  Lovenia, Holy  and Koto, Fajri  and  Adhista, Dea  and  Dave, Emmanuel  and  Oktavianti, Sarah  and  Akbar, Salsabil  and  Lee, Jhonson  and  Shadieq, Nuur  and  Cenggoro, Tjeng Wawan  and  Linuwih, Hanung  and  Wilie, Bryan  and  Muridan, Galih  and  Winata, Genta  and  Moeljadi, David  and  Aji, Alham Fikri  and  Purwarianti, Ayu  and  Fung, Pascale},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  editor = {Park, Jong C.  and  Arase, Yuki  and  Hu, Baotian  and  Lu, Wei  and  Wijaya, Derry  and  Purwarianti, Ayu  and  Krisnadhi, Adila Alfa},\n  month = nov,\n  pages = {921--945},\n  publisher = {Association for Computational Linguistics},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  url = {https://aclanthology.org/2023.ijcnlp-main.60},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#nusaxbitextmining","title":"NusaXBitextMining","text":"<p>NusaX is a parallel dataset for machine translation and sentiment analysis on 11 Indonesia languages and English.</p> <p>Dataset: <code>mteb/NusaXBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ace, ban, bbc, bjn, bug, ... (12) Reviews, Written human-annotated created Citation <pre><code>@inproceedings{winata2023nusax,\n  author = {Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya, Samuel and Mahendra, Rahmad and Koto, Fajri and Romadhony, Ade and Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and Fung, Pascale and others},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  pages = {815--834},\n  title = {NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages},\n  year = {2023},\n}\n\n@misc{winata2024miners,\n  archiveprefix = {arXiv},\n  author = {Genta Indra Winata and Ruochen Zhang and David Ifeoluwa Adelani},\n  eprint = {2406.07424},\n  primaryclass = {cs.CL},\n  title = {MINERS: Multilingual Language Models as Semantic Retrievers},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#phincbitextmining","title":"PhincBitextMining","text":"<p>Phinc is a parallel corpus for machine translation pairing code-mixed Hinglish (a fusion of Hindi and English commonly used in modern India) with human-generated English translations.</p> <p>Dataset: <code>gentaiscool/bitext_phinc_miners</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, hin Social, Written human-annotated found Citation <pre><code>@inproceedings{srivastava2020phinc,\n  author = {Srivastava, Vivek and Singh, Mayank},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {41--49},\n  title = {PHINC: A Parallel Hinglish Social Media Code-Mixed Corpus for Machine Translation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#pubchemsmilesbitextmining","title":"PubChemSMILESBitextMining","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSMILESBitextMining</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#romatalesbitextmining","title":"RomaTalesBitextMining","text":"<p>Parallel corpus of Roma Tales in Lovari with Hungarian translations.</p> <p>Dataset: <code>kardosdrur/roma-tales</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 hun, rom Fiction, Written expert-annotated created"},{"location":"overview/available_tasks/bitextmining/#ruscibenchbitextmining","title":"RuSciBenchBitextMining","text":"<p>This task focuses on finding translations of scientific articles. The dataset is sourced from eLibrary, Russia's largest electronic library of scientific publications. Russian authors often provide English translations for their abstracts and titles, and the data consists of these paired titles and abstracts. The task evaluates a model's ability to match an article's Russian title and abstract to its English counterpart, or vice versa.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_bitext_mining</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#ruscibenchbitextminingv2","title":"RuSciBenchBitextMining.v2","text":"<p>This task focuses on finding translations of scientific articles. The dataset is sourced from eLibrary, Russia's largest electronic library of scientific publications. Russian authors often provide English translations for their abstracts and titles, and the data consists of these paired titles and abstracts. The task evaluates a model's ability to match an article's Russian title and abstract to its English counterpart, or vice versa. Compared to the previous version, 6 erroneous examples have been removed.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_bitext_mining</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#samsumfa","title":"SAMSumFa","text":"<p>Translated Version of SAMSum Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/samsum-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated machine-translated"},{"location":"overview/available_tasks/bitextmining/#srncorpusbitextmining","title":"SRNCorpusBitextMining","text":"<p>SRNCorpus is a machine translation corpus for creole language Sranantongo and Dutch.</p> <p>Dataset: <code>mteb/SRNCorpusBitextMining</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 nld, srn Social, Web, Written human-annotated found Citation <pre><code>@article{zwennicker2022towards,\n  author = {Zwennicker, Just and Stap, David},\n  journal = {arXiv preprint arXiv:2212.06383},\n  title = {Towards a general purpose machine translation system for Sranantongo},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#synperchatbotragsumsretrieval","title":"SynPerChatbotRAGSumSRetrieval","text":"<p>Synthetic Persian Chatbot RAG Summary Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-summary-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#synperchatbotsumsretrieval","title":"SynPerChatbotSumSRetrieval","text":"<p>Synthetic Persian Chatbot Summary Dataset for summary retrieval.</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-summary-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#tatoeba","title":"Tatoeba","text":"<p>1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus</p> <p>Dataset: <code>mteb/tatoeba-bitext-mining</code> \u2022 License: cc-by-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 afr, amh, ang, ara, arq, ... (113) Written human-annotated found Citation <pre><code>@misc{tatoeba,\n  author = {Tatoeba community},\n  title = {Tatoeba: Collection of sentences and translations},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#tbilisicityhallbitextmining","title":"TbilisiCityHallBitextMining","text":"<p>Parallel news titles from the Tbilisi City Hall website (https://tbilisi.gov.ge/).</p> <p>Dataset: <code>jupyterjazz/tbilisi-city-hall-titles</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, kat News, Written derived created"},{"location":"overview/available_tasks/bitextmining/#viemedevbitextmining","title":"VieMedEVBitextMining","text":"<p>A high-quality Vietnamese-English parallel data from the medical domain for machine translation</p> <p>Dataset: <code>mteb/VieMedEVBitextMining</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 eng, vie Medical, Written expert-annotated human-translated and localized Citation <pre><code>@inproceedings{medev,\n  author = {Nhu Vo and Dat Quoc Nguyen and Dung D. Le and Massimo Piccardi and Wray Buntine},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)},\n  title = {{Improving Vietnamese-English Medical Machine Translation}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#webfaqbitextminingqas","title":"WebFAQBitextMiningQAs","text":"<p>The WebFAQ Bitext Dataset consists of natural FAQ-style Question-Answer pairs that align across languages. A sentence in the \"WebFAQBitextMiningQAs\" task is a concatenation of a question and its corresponding answer. The dataset is sourced from FAQ pages on the web.</p> <p>Dataset: <code>PaDaS-Lab/webfaq-bitexts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, aze, ben, bul, cat, ... (49) Web, Written human-annotated human-translated Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/bitextmining/#webfaqbitextminingquestions","title":"WebFAQBitextMiningQuestions","text":"<p>The WebFAQ Bitext Dataset consists of natural FAQ-style Question-Answer pairs that align across languages. A sentence in the \"WebFAQBitextMiningQuestions\" task is the question originating from an aligned QA. The dataset is sourced from FAQ pages on the web.</p> <p>Dataset: <code>PaDaS-Lab/webfaq-bitexts</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) f1 ara, aze, ben, bul, cat, ... (49) Web, Written human-annotated human-translated Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/","title":"Classification","text":"<ul> <li>Number of tasks: 476</li> </ul>"},{"location":"overview/available_tasks/classification/#ajgt","title":"AJGT","text":"<p>Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.</p> <p>Dataset: <code>komari6/ajgt_twitter_ar</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{alomari2017arabic,\n  author = {Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\n  booktitle = {International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},\n  organization = {Springer},\n  pages = {602--610},\n  title = {Arabic tweets sentimental analysis using machine learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ajgtv2","title":"AJGT.v2","text":"<p>Arabic Jordanian General Tweets (AJGT) Corpus consisted of 1,800 tweets (900 for training and 900 for testing) annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ajgt</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{alomari2017arabic,\n  author = {Alomari, Khaled Mohammad and ElSherif, Hatem M and Shaalan, Khaled},\n  booktitle = {International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},\n  organization = {Springer},\n  pages = {602--610},\n  title = {Arabic tweets sentimental analysis using machine learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#afrisenticlassification","title":"AfriSentiClassification","text":"<p>AfriSenti is the largest sentiment analysis dataset for under-represented African languages.</p> <p>Dataset: <code>mteb/AfriSentiClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, arq, ary, hau, ibo, ... (12) Social, Written derived found Citation <pre><code>@inproceedings{Muhammad2023AfriSentiAT,\n  author = {Shamsuddeen Hassan Muhammad and Idris Abdulmumin and Abinew Ali Ayele and Nedjma Ousidhoum and David Ifeoluwa Adelani and Seid Muhie Yimam and Ibrahim Sa'id Ahmad and Meriem Beloucif and Saif Mohammad and Sebastian Ruder and Oumaima Hourrane and Pavel Brazdil and Felermino D'ario M'ario Ant'onio Ali and Davis Davis and Salomey Osei and Bello Shehu Bello and Falalu Ibrahim and Tajuddeen Gwadabe and Samuel Rutunda and Tadesse Belay and Wendimu Baye Messelle and Hailu Beshada Balcha and Sisay Adugna Chala and Hagos Tesfahun Gebremichael and Bernard Opoku and Steven Arthur},\n  title = {AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#afrisentilangclassification","title":"AfriSentiLangClassification","text":"<p>AfriSentiLID is the largest LID classification dataset for African Languages.</p> <p>Dataset: <code>HausaNLP/afrisenti-lid-data</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, arq, ary, hau, ibo, ... (12) Social, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#allegroreviews","title":"AllegroReviews","text":"<p>A Polish dataset for sentiment classification on reviews from e-commerce marketplace Allegro.</p> <p>Dataset: <code>PL-MTEB/allegro-reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Reviews derived found Citation <pre><code>@inproceedings{rybak-etal-2020-klej,\n  address = {Online},\n  author = {Rybak, Piotr  and\nMroczkowski, Robert  and\nTracz, Janusz  and\nGawlik, Ireneusz},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.111},\n  editor = {Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel},\n  month = jul,\n  pages = {1191--1201},\n  publisher = {Association for Computational Linguistics},\n  title = {{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding},\n  url = {https://aclanthology.org/2020.acl-main.111/},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#allegroreviewsv2","title":"AllegroReviews.v2","text":"<p>A Polish dataset for sentiment classification on reviews from e-commerce marketplace Allegro. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/allegro_reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Reviews derived found Citation <pre><code>@inproceedings{rybak-etal-2020-klej,\n  address = {Online},\n  author = {Rybak, Piotr  and\nMroczkowski, Robert  and\nTracz, Janusz  and\nGawlik, Ireneusz},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.111},\n  editor = {Jurafsky, Dan  and\nChai, Joyce  and\nSchluter, Natalie  and\nTetreault, Joel},\n  month = jul,\n  pages = {1191--1201},\n  publisher = {Association for Computational Linguistics},\n  title = {{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding},\n  url = {https://aclanthology.org/2020.acl-main.111/},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazoncounterfactualclassification","title":"AmazonCounterfactualClassification","text":"<p>A collection of Amazon customer reviews annotated for counterfactual detection pair classification.</p> <p>Dataset: <code>mteb/amazon_counterfactual</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, jpn Reviews, Written human-annotated found Citation <pre><code>@inproceedings{oneill-etal-2021-wish,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {O{'}Neill, James  and\nRozenshtein, Polina  and\nKiryo, Ryuichi  and\nKubota, Motoko  and\nBollegala, Danushka},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.568},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {7092--7108},\n  publisher = {Association for Computational Linguistics},\n  title = {{I} Wish {I} Would Have Loved This One, But {I} Didn{'}t {--} A Multilingual Dataset for Counterfactual Detection in Product Review},\n  url = {https://aclanthology.org/2021.emnlp-main.568},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazoncounterfactualvnclassification","title":"AmazonCounterfactualVNClassification","text":"<p>A collection of translated Amazon customer reviews annotated for counterfactual detection pair classification. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-counterfactual-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityclassification","title":"AmazonPolarityClassification","text":"<p>Amazon Polarity Classification Dataset.</p> <p>Dataset: <code>mteb/amazon_polarity</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@article{McAuley2013HiddenFA,\n  author = {Julian McAuley and Jure Leskovec},\n  journal = {Proceedings of the 7th ACM conference on Recommender systems},\n  title = {Hidden factors and hidden topics: understanding rating dimensions with review text},\n  url = {https://api.semanticscholar.org/CorpusID:6440341},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityclassificationv2","title":"AmazonPolarityClassification.v2","text":"<p>Amazon Polarity Classification Dataset. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/amazon_polarity</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@article{McAuley2013HiddenFA,\n  author = {Julian McAuley and Jure Leskovec},\n  journal = {Proceedings of the 7th ACM conference on Recommender systems},\n  title = {Hidden factors and hidden topics: understanding rating dimensions with review text},\n  url = {https://api.semanticscholar.org/CorpusID:6440341},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonpolarityvnclassification","title":"AmazonPolarityVNClassification","text":"<p>A collection of translated Amazon customer reviews annotated for polarity classification. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-polarity-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonreviewsclassification","title":"AmazonReviewsClassification","text":"<p>A collection of Amazon reviews specifically designed to aid research in multilingual text classification.</p> <p>Dataset: <code>mteb/AmazonReviewsClassification</code> \u2022 License: https://docs.opendata.aws/amazon-reviews-ml/license.txt \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn, deu, eng, fra, jpn, ... (6) Reviews, Written human-annotated found Citation <pre><code>@misc{keung2020multilingual,\n  archiveprefix = {arXiv},\n  author = {Phillip Keung and Yichao Lu and Gy\u00f6rgy Szarvas and Noah A. Smith},\n  eprint = {2010.02573},\n  primaryclass = {cs.CL},\n  title = {The Multilingual Amazon Reviews Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#amazonreviewsvnclassification","title":"AmazonReviewsVNClassification","text":"<p>A collection of translated Amazon reviews specifically designed to aid research in multilingual text classification. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-reviews-multi-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#angrytweetsclassification","title":"AngryTweetsClassification","text":"<p>A sentiment dataset with 3 classes (positive, negative, neutral) for Danish tweets</p> <p>Dataset: <code>DDSC/angry-tweets</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written human-annotated found Citation <pre><code>@inproceedings{pauli2021danlp,\n  author = {Pauli, Amalie Brogaard and Barrett, Maria and Lacroix, Oph{\\'e}lie and Hvingelby, Rasmus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  pages = {460--466},\n  title = {DaNLP: An open-source toolkit for Danish Natural Language Processing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#angrytweetsclassificationv2","title":"AngryTweetsClassification.v2","text":"<p>A sentiment dataset with 3 classes (positive, negative, neutral) for Danish tweets This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/angry_tweets</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written human-annotated found Citation <pre><code>@inproceedings{pauli2021danlp,\n  author = {Pauli, Amalie Brogaard and Barrett, Maria and Lacroix, Oph{\\'e}lie and Hvingelby, Rasmus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  pages = {460--466},\n  title = {DaNLP: An open-source toolkit for Danish Natural Language Processing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#arxivclassification","title":"ArxivClassification","text":"<p>Classification Dataset of Arxiv Papers</p> <p>Dataset: <code>mteb/ArxivClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Academic, Written derived found Citation <pre><code>@article{8675939,\n  author = {He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},\n  doi = {10.1109/ACCESS.2019.2907992},\n  journal = {IEEE Access},\n  number = {},\n  pages = {40707-40718},\n  title = {Long Document Classification From Local Word Glimpses via Recurrent Attention Learning},\n  volume = {7},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#arxivclassificationv2","title":"ArxivClassification.v2","text":"<p>Classification Dataset of Arxiv Papers This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/arxiv</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Academic, Written derived found Citation <pre><code>@article{8675939,\n  author = {He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},\n  doi = {10.1109/ACCESS.2019.2907992},\n  journal = {IEEE Access},\n  number = {},\n  pages = {40707-40718},\n  title = {Long Document Classification From Local Word Glimpses via Recurrent Attention Learning},\n  volume = {7},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77classification","title":"Banking77Classification","text":"<p>Dataset composed of online banking queries annotated with their corresponding intents.</p> <p>Dataset: <code>mteb/banking77</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Written human-annotated found Citation <pre><code>@inproceedings{casanueva-etal-2020-efficient,\n  address = {Online},\n  author = {Casanueva, I{\\~n}igo  and\nTem{\\v{c}}inas, Tadas  and\nGerz, Daniela  and\nHenderson, Matthew  and\nVuli{\\'c}, Ivan},\n  booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},\n  doi = {10.18653/v1/2020.nlp4convai-1.5},\n  editor = {Wen, Tsung-Hsien  and\nCelikyilmaz, Asli  and\nYu, Zhou  and\nPapangelis, Alexandros  and\nEric, Mihail  and\nKumar, Anuj  and\nCasanueva, I{\\~n}igo  and\nShah, Rushin},\n  month = jul,\n  pages = {38--45},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Intent Detection with Dual Sentence Encoders},\n  url = {https://aclanthology.org/2020.nlp4convai-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77classificationv2","title":"Banking77Classification.v2","text":"<p>Dataset composed of online banking queries annotated with their corresponding intents. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/banking77</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Written human-annotated found Citation <pre><code>@inproceedings{casanueva-etal-2020-efficient,\n  address = {Online},\n  author = {Casanueva, I{\\~n}igo  and\nTem{\\v{c}}inas, Tadas  and\nGerz, Daniela  and\nHenderson, Matthew  and\nVuli{\\'c}, Ivan},\n  booktitle = {Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},\n  doi = {10.18653/v1/2020.nlp4convai-1.5},\n  editor = {Wen, Tsung-Hsien  and\nCelikyilmaz, Asli  and\nYu, Zhou  and\nPapangelis, Alexandros  and\nEric, Mihail  and\nKumar, Anuj  and\nCasanueva, I{\\~n}igo  and\nShah, Rushin},\n  month = jul,\n  pages = {38--45},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Intent Detection with Dual Sentence Encoders},\n  url = {https://aclanthology.org/2020.nlp4convai-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#banking77vnclassification","title":"Banking77VNClassification","text":"<p>A translated dataset composed of online banking queries annotated with their corresponding intents. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/banking77-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalidocumentclassification","title":"BengaliDocumentClassification","text":"<p>Dataset for News Classification, categorized with 13 domains.</p> <p>Dataset: <code>dialect-ai/shironaam</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ben News, Written derived found Citation <pre><code>@inproceedings{akash-etal-2023-shironaam,\n  address = {Dubrovnik, Croatia},\n  author = {Akash, Abu Ubaida  and\nNayeem, Mir Tafseer  and\nShohan, Faisal Tareque  and\nIslam, Tanvir},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {52--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Shironaam: {B}engali News Headline Generation using Auxiliary Information},\n  url = {https://aclanthology.org/2023.eacl-main.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalidocumentclassificationv2","title":"BengaliDocumentClassification.v2","text":"<p>Dataset for News Classification, categorized with 13 domains. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_document</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ben News, Written derived found Citation <pre><code>@inproceedings{akash-etal-2023-shironaam,\n  address = {Dubrovnik, Croatia},\n  author = {Akash, Abu Ubaida  and\nNayeem, Mir Tafseer  and\nShohan, Faisal Tareque  and\nIslam, Tanvir},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {52--67},\n  publisher = {Association for Computational Linguistics},\n  title = {Shironaam: {B}engali News Headline Generation using Auxiliary Information},\n  url = {https://aclanthology.org/2023.eacl-main.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalihatespeechclassification","title":"BengaliHateSpeechClassification","text":"<p>The Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text.</p> <p>Dataset: <code>rezacsedu/bn_hate_speech</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben News, Written expert-annotated found Citation <pre><code>@inproceedings{karim2020BengaliNLP,\n  author = {Karim, Md. Rezaul and Chakravarti, Bharathi Raja and P. McCrae, John and Cochez, Michael},\n  booktitle = {7th IEEE International Conference on Data Science and Advanced Analytics (IEEE DSAA,2020)},\n  publisher = {IEEE},\n  title = {Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalihatespeechclassificationv2","title":"BengaliHateSpeechClassification.v2","text":"<p>The Bengali Hate Speech Dataset is a Bengali-language dataset of news articles collected from various Bengali media sources and categorized based on the type of hate in the text. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_hate_speech</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben News, Written expert-annotated found Citation <pre><code>@inproceedings{karim2020BengaliNLP,\n  author = {Karim, Md. Rezaul and Chakravarti, Bharathi Raja and P. McCrae, John and Cochez, Michael},\n  booktitle = {7th IEEE International Conference on Data Science and Advanced Analytics (IEEE DSAA,2020)},\n  publisher = {IEEE},\n  title = {Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalisentimentanalysis","title":"BengaliSentimentAnalysis","text":"<p>dataset contains 3307 Negative reviews and 8500 Positive reviews collected and manually annotated from Youtube Bengali drama.</p> <p>Dataset: <code>Akash190104/bengali_sentiment_analysis</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben Reviews, Written human-annotated found Citation <pre><code>@inproceedings{sazzed2020cross,\n  author = {Sazzed, Salim},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {50--60},\n  title = {Cross-lingual sentiment classification in low-resource Bengali language},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bengalisentimentanalysisv2","title":"BengaliSentimentAnalysis.v2","text":"<p>dataset contains 2854 Negative reviews and 7238 Positive reviews collected and manually annotated from Youtube Bengali drama. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/bengali_sentiment_analysis</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ben Reviews, Written human-annotated found Citation <pre><code>@inproceedings{sazzed2020cross,\n  author = {Sazzed, Salim},\n  booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},\n  pages = {50--60},\n  title = {Cross-lingual sentiment classification in low-resource Bengali language},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#bulgarianstorereviewsentimentclassfication","title":"BulgarianStoreReviewSentimentClassfication","text":"<p>Bulgarian online store review dataset for sentiment classification.</p> <p>Dataset: <code>mteb/BulgarianStoreReviewSentimentClassfication</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bul Reviews, Written human-annotated found Citation <pre><code>@data{DVN/TXIK9P_2018,\n  author = {Georgieva-Trifonova, Tsvetanka and Stefanova, Milena and Kalchev, Stefan},\n  doi = {10.7910/DVN/TXIK9P},\n  publisher = {Harvard Dataverse},\n  title = {{Dataset for ``Customer Feedback Text Analysis for Online Stores Reviews in Bulgarian''}},\n  url = {https://doi.org/10.7910/DVN/TXIK9P},\n  version = {V1},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cbd","title":"CBD","text":"<p>Polish Tweets annotated for cyberbullying detection.</p> <p>Dataset: <code>PL-MTEB/cbd</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written human-annotated found Citation <pre><code>@proceedings{ogr:kob:19:poleval,\n  address = {Warsaw, Poland},\n  editor = {Maciej Ogrodniczuk and \u0141ukasz Kobyli\u0144ski},\n  isbn = {978-83-63159-28-3},\n  publisher = {Institute of Computer Science, Polish Academy of Sciences},\n  title = {{Proceedings of the PolEval 2019 Workshop}},\n  url = {http://2019.poleval.pl/files/poleval2019.pdf},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cbdv2","title":"CBD.v2","text":"<p>Polish Tweets annotated for cyberbullying detection. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/cbd</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written human-annotated found Citation <pre><code>@proceedings{ogr:kob:19:poleval,\n  address = {Warsaw, Poland},\n  editor = {Maciej Ogrodniczuk and \u0141ukasz Kobyli\u0144ski},\n  isbn = {978-83-63159-28-3},\n  publisher = {Institute of Computer Science, Polish Academy of Sciences},\n  title = {{Proceedings of the PolEval 2019 Workshop}},\n  url = {http://2019.poleval.pl/files/poleval2019.pdf},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdczmoviereviewsentimentclassification","title":"CSFDCZMovieReviewSentimentClassification","text":"<p>The dataset contains 30k user reviews from csfd.cz in Czech.</p> <p>Dataset: <code>fewshot-goes-multilingual/cs_csfd-movie-reviews</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdczmoviereviewsentimentclassificationv2","title":"CSFDCZMovieReviewSentimentClassification.v2","text":"<p>The dataset contains 30k user reviews from csfd.cz in Czech. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/csfdcz_movie_review_sentiment</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdskmoviereviewsentimentclassification","title":"CSFDSKMovieReviewSentimentClassification","text":"<p>The dataset contains 30k user reviews from csfd.cz in Slovak.</p> <p>Dataset: <code>mteb/CSFDSKMovieReviewSentimentClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#csfdskmoviereviewsentimentclassificationv2","title":"CSFDSKMovieReviewSentimentClassification.v2","text":"<p>The dataset contains 30k user reviews from csfd.cz in Slovak. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/csfdsk_movie_review_sentiment</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadaffiliatelicenselicenseelegalbenchclassification","title":"CUADAffiliateLicenseLicenseeLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if a clause describes a license grant to a licensee (incl. sublicensor) and the affiliates of such licensee/sublicensor.</p> <p>Dataset: <code>mteb/CUADAffiliateLicenseLicenseeLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadaffiliatelicenselicensorlegalbenchclassification","title":"CUADAffiliateLicenseLicensorLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause describes a license grant by affiliates of the licensor or that includes intellectual property of affiliates of the licensor.</p> <p>Dataset: <code>mteb/CUADAffiliateLicenseLicensorLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadantiassignmentlegalbenchclassification","title":"CUADAntiAssignmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires consent or notice of a party if the contract is assigned to a third party.</p> <p>Dataset: <code>mteb/CUADAntiAssignmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadauditrightslegalbenchclassification","title":"CUADAuditRightsLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause gives a party the right to audit the books, records, or physical locations of the counterparty to ensure compliance with the contract.</p> <p>Dataset: <code>mteb/CUADAuditRightsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcaponliabilitylegalbenchclassification","title":"CUADCapOnLiabilityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a cap on liability upon the breach of a party's obligation. This includes time limitation for the counterparty to bring claims or maximum amount for recovery.</p> <p>Dataset: <code>mteb/CUADCapOnLiabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadchangeofcontrollegalbenchclassification","title":"CUADChangeOfControlLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause gives one party the right to terminate or is consent or notice required of the counterparty if such party undergoes a change of control, such as a merger, stock sale, transfer of all or substantially all of its assets or business, or assignment by operation of law.</p> <p>Dataset: <code>mteb/CUADChangeOfControlLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcompetitiverestrictionexceptionlegalbenchclassification","title":"CUADCompetitiveRestrictionExceptionLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause mentions exceptions or carveouts to Non-Compete, Exclusivity and No-Solicit of Customers.</p> <p>Dataset: <code>mteb/CUADCompetitiveRestrictionExceptionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadcovenantnottosuelegalbenchclassification","title":"CUADCovenantNotToSueLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party is restricted from contesting the validity of the counterparty's ownership of intellectual property or otherwise bringing a claim against the counterparty for matters unrelated to the contract.</p> <p>Dataset: <code>mteb/CUADCovenantNotToSueLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadeffectivedatelegalbenchclassification","title":"CUADEffectiveDateLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the agreement becomes effective.</p> <p>Dataset: <code>mteb/CUADEffectiveDateLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadexclusivitylegalbenchclassification","title":"CUADExclusivityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies exclusive dealing commitment with the counterparty. This includes a commitment to procure all 'requirements' from one party of certain technology, goods, or services or a prohibition on licensing or selling technology, goods or services to third parties, or a prohibition on collaborating or working with other parties), whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADExclusivityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadexpirationdatelegalbenchclassification","title":"CUADExpirationDateLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies the date upon which the initial term expires.</p> <p>Dataset: <code>mteb/CUADExpirationDateLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadgoverninglawlegalbenchclassification","title":"CUADGoverningLawLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies which state/country\u2019s law governs the contract.</p> <p>Dataset: <code>mteb/CUADGoverningLawLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadipownershipassignmentlegalbenchclassification","title":"CUADIPOwnershipAssignmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that intellectual property created by one party become the property of the counterparty, either per the terms of the contract or upon the occurrence of certain events.</p> <p>Dataset: <code>mteb/CUADIPOwnershipAssignmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadinsurancelegalbenchclassification","title":"CUADInsuranceLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if clause creates a requirement for insurance that must be maintained by one party for the benefit of the counterparty.</p> <p>Dataset: <code>mteb/CUADInsuranceLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadirrevocableorperpetuallicenselegalbenchclassification","title":"CUADIrrevocableOrPerpetualLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a license grant that is irrevocable or perpetual.</p> <p>Dataset: <code>mteb/CUADIrrevocableOrPerpetualLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadjointipownershiplegalbenchclassification","title":"CUADJointIPOwnershipLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause provides for joint or shared ownership of intellectual property between the parties to the contract.</p> <p>Dataset: <code>mteb/CUADJointIPOwnershipLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadlicensegrantlegalbenchclassification","title":"CUADLicenseGrantLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause contains a license granted by one party to its counterparty.</p> <p>Dataset: <code>mteb/CUADLicenseGrantLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadliquidateddamageslegalbenchclassification","title":"CUADLiquidatedDamagesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause awards either party liquidated damages for breach or a fee upon the termination of a contract (termination fee).</p> <p>Dataset: <code>mteb/CUADLiquidatedDamagesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadminimumcommitmentlegalbenchclassification","title":"CUADMinimumCommitmentLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a minimum order size or minimum amount or units per time period that one party must buy from the counterparty.</p> <p>Dataset: <code>mteb/CUADMinimumCommitmentLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadmostfavorednationlegalbenchclassification","title":"CUADMostFavoredNationLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if a third party gets better terms on the licensing or sale of technology/goods/services described in the contract, the buyer of such technology/goods/services under the contract shall be entitled to those better terms.</p> <p>Dataset: <code>mteb/CUADMostFavoredNationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnosolicitofcustomerslegalbenchclassification","title":"CUADNoSolicitOfCustomersLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party from contracting or soliciting customers or partners of the counterparty, whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADNoSolicitOfCustomersLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnosolicitofemployeeslegalbenchclassification","title":"CUADNoSolicitOfEmployeesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts a party's soliciting or hiring employees and/or contractors from the counterparty, whether during the contract or after the contract ends (or both).</p> <p>Dataset: <code>mteb/CUADNoSolicitOfEmployeesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnoncompetelegalbenchclassification","title":"CUADNonCompeteLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause restricts the ability of a party to compete with the counterparty or operate in a certain geography or business or technology sector.</p> <p>Dataset: <code>mteb/CUADNonCompeteLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnondisparagementlegalbenchclassification","title":"CUADNonDisparagementLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires a party not to disparage the counterparty.</p> <p>Dataset: <code>mteb/CUADNonDisparagementLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnontransferablelicenselegalbenchclassification","title":"CUADNonTransferableLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause limits the ability of a party to transfer the license being granted to a third party.</p> <p>Dataset: <code>mteb/CUADNonTransferableLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadnoticeperiodtoterminaterenewallegalbenchclassification","title":"CUADNoticePeriodToTerminateRenewalLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a notice period required to terminate renewal.</p> <p>Dataset: <code>mteb/CUADNoticePeriodToTerminateRenewalLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadpostterminationserviceslegalbenchclassification","title":"CUADPostTerminationServicesLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause subjects a party to obligations after the termination or expiration of a contract, including any post-termination transition, payment, transfer of IP, wind-down, last-buy, or similar commitments.</p> <p>Dataset: <code>mteb/CUADPostTerminationServicesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadpricerestrictionslegalbenchclassification","title":"CUADPriceRestrictionsLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause places a restriction on the ability of a party to raise or reduce prices of technology, goods, or services provided.</p> <p>Dataset: <code>mteb/CUADPriceRestrictionsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrenewaltermlegalbenchclassification","title":"CUADRenewalTermLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a renewal term.</p> <p>Dataset: <code>mteb/CUADRenewalTermLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrevenueprofitsharinglegalbenchclassification","title":"CUADRevenueProfitSharingLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause require a party to share revenue or profit with the counterparty for any technology, goods, or services.</p> <p>Dataset: <code>mteb/CUADRevenueProfitSharingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadrofrroforofnlegalbenchclassification","title":"CUADRofrRofoRofnLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause grant one party a right of first refusal, right of first offer or right of first negotiation to purchase, license, market, or distribute equity interest, technology, assets, products or services.</p> <p>Dataset: <code>mteb/CUADRofrRofoRofnLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadsourcecodeescrowlegalbenchclassification","title":"CUADSourceCodeEscrowLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause requires one party to deposit its source code into escrow with a third party, which can be released to the counterparty upon the occurrence of certain events (bankruptcy, insolvency, etc.).</p> <p>Dataset: <code>mteb/CUADSourceCodeEscrowLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadterminationforconveniencelegalbenchclassification","title":"CUADTerminationForConvenienceLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that one party can terminate this contract without cause (solely by giving a notice and allowing a waiting period to expire).</p> <p>Dataset: <code>mteb/CUADTerminationForConvenienceLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadthirdpartybeneficiarylegalbenchclassification","title":"CUADThirdPartyBeneficiaryLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that that there a non-contracting party who is a beneficiary to some or all of the clauses in the contract and therefore can enforce its rights against a contracting party.</p> <p>Dataset: <code>mteb/CUADThirdPartyBeneficiaryLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuaduncappedliabilitylegalbenchclassification","title":"CUADUncappedLiabilityLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies that a party's liability is uncapped upon the breach of its obligation in the contract. This also includes uncap liability for a particular type of breach such as IP infringement or breach of confidentiality obligation.</p> <p>Dataset: <code>mteb/CUADUncappedLiabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadunlimitedallyoucaneatlicenselegalbenchclassification","title":"CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause grants one party an \u201centerprise,\u201d \u201call you can eat\u201d or unlimited usage license.</p> <p>Dataset: <code>mteb/CUADUnlimitedAllYouCanEatLicenseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadvolumerestrictionlegalbenchclassification","title":"CUADVolumeRestrictionLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a fee increase or consent requirement, etc. if one party's use of the product/services exceeds certain threshold.</p> <p>Dataset: <code>mteb/CUADVolumeRestrictionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cuadwarrantydurationlegalbenchclassification","title":"CUADWarrantyDurationLegalBenchClassification","text":"<p>This task was constructed from the CUAD dataset. It consists of determining if the clause specifies a duration of any warranty against defects or errors in technology, products, or services provided under the contract.</p> <p>Dataset: <code>mteb/CUADWarrantyDurationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#canadataxcourtoutcomeslegalbenchclassification","title":"CanadaTaxCourtOutcomesLegalBenchClassification","text":"<p>The input is an excerpt of text from Tax Court of Canada decisions involving appeals of tax related matters. The task is to classify whether the excerpt includes the outcome of the appeal, and if so, to specify whether the appeal was allowed or dismissed. Partial success (e.g. appeal granted on one tax year but dismissed on another) counts as allowed (with the exception of costs orders which are disregarded). Where the excerpt does not clearly articulate an outcome, the system should indicate other as the outcome. Categorizing case outcomes is a common task that legal researchers complete in order to gather datasets involving outcomes in legal processes for the purposes of quantitative empirical legal research.</p> <p>Dataset: <code>mteb/CanadaTaxCourtOutcomesLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cataloniatweetclassification","title":"CataloniaTweetClassification","text":"<p>This dataset contains two corpora in Spanish and Catalan that consist of annotated Twitter messages for automatic stance detection. The data was collected over 12 days during February and March of 2019 from tweets posted in Barcelona, and during September of 2018 from tweets posted in the town of Terrassa, Catalonia. Each corpus is annotated with three classes: AGAINST, FAVOR and NEUTRAL, which express the stance towards the target - independence of Catalonia.</p> <p>Dataset: <code>mteb/CataloniaTweetClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cat, spa Government, Social, Written expert-annotated created Citation <pre><code>@inproceedings{zotova-etal-2020-multilingual,\n  author = {Zotova, Elena  and\nAgerri, Rodrigo  and\nNu{\\~n}ez, Manuel  and\nRigau, German},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  month = may,\n  pages = {1368--1375},\n  publisher = {European Language Resources Association},\n  title = {Multilingual Stance Detection in Tweets: The {C}atalonia Independence Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliconfidentialityofagreementlegalbenchclassification","title":"ContractNLIConfidentialityOfAgreementLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA provides that the Receiving Party shall not disclose the fact that Agreement was agreed or negotiated.</p> <p>Dataset: <code>mteb/ContractNLIConfidentialityOfAgreementLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliexplicitidentificationlegalbenchclassification","title":"ContractNLIExplicitIdentificationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that all Confidential Information shall be expressly identified by the Disclosing Party.</p> <p>Dataset: <code>mteb/ContractNLIExplicitIdentificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnliinclusionofverballyconveyedinformationlegalbenchclassification","title":"ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that Confidential Information may include verbally conveyed information.</p> <p>Dataset: <code>mteb/ContractNLIInclusionOfVerballyConveyedInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlilimiteduselegalbenchclassification","title":"ContractNLILimitedUseLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall not use any Confidential Information for any purpose other than the purposes stated in Agreement.</p> <p>Dataset: <code>mteb/ContractNLILimitedUseLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlinolicensinglegalbenchclassification","title":"ContractNLINoLicensingLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Agreement shall not grant Receiving Party any right to Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLINoLicensingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlinoticeoncompelleddisclosurelegalbenchclassification","title":"ContractNLINoticeOnCompelledDisclosureLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall notify Disclosing Party in case Receiving Party is required by law, regulation or judicial process to disclose any Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLINoticeOnCompelledDisclosureLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissibleacquirementofsimilarinformationlegalbenchclassification","title":"ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may acquire information similar to Confidential Information from a third party.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleAcquirementOfSimilarInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissiblecopylegalbenchclassification","title":"ContractNLIPermissibleCopyLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may create a copy of some Confidential Information in some circumstances.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleCopyLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissibledevelopmentofsimilarinformationlegalbenchclassification","title":"ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may independently develop information similar to Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLIPermissibleDevelopmentOfSimilarInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlipermissiblepostagreementpossessionlegalbenchclassification","title":"ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may retain some Confidential Information even after the return or destruction of Confidential Information.</p> <p>Dataset: <code>mteb/ContractNLIPermissiblePostAgreementPossessionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlireturnofconfidentialinformationlegalbenchclassification","title":"ContractNLIReturnOfConfidentialInformationLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement.</p> <p>Dataset: <code>mteb/ContractNLIReturnOfConfidentialInformationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisharingwithemployeeslegalbenchclassification","title":"ContractNLISharingWithEmployeesLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some of Receiving Party's employees.</p> <p>Dataset: <code>mteb/ContractNLISharingWithEmployeesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisharingwiththirdpartieslegalbenchclassification","title":"ContractNLISharingWithThirdPartiesLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that the Receiving Party may share some Confidential Information with some third-parties (including consultants, agents and professional advisors).</p> <p>Dataset: <code>mteb/ContractNLISharingWithThirdPartiesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#contractnlisurvivalofobligationslegalbenchclassification","title":"ContractNLISurvivalOfObligationsLegalBenchClassification","text":"<p>This task is a subset of ContractNLI, and consists of determining whether a clause from an NDA clause provides that some obligations of Agreement may survive termination of Agreement.</p> <p>Dataset: <code>mteb/ContractNLISurvivalOfObligationsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#corporatelobbyinglegalbenchclassification","title":"CorporateLobbyingLegalBenchClassification","text":"<p>The Corporate Lobbying task consists of determining whether a proposed Congressional bill may be relevant to a company based on a company's self-description in its SEC 10K filing.</p> <p>Dataset: <code>mteb/CorporateLobbyingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#cyrillicturkiclangclassification","title":"CyrillicTurkicLangClassification","text":"<p>Cyrillic dataset of 8 Turkic languages spoken in Russia and former USSR</p> <p>Dataset: <code>mteb/CyrillicTurkicLangClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bak, chv, kaz, kir, krc, ... (9) Web, Written derived found Citation <pre><code>@inproceedings{goldhahn2012building,\n  author = {Goldhahn, Dirk and Eckart, Thomas and Quasthoff, Uwe},\n  booktitle = {Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)},\n  title = {Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechproductreviewsentimentclassification","title":"CzechProductReviewSentimentClassification","text":"<p>User reviews of products on Czech e-shop Mall.cz with 3 sentiment classes (positive, neutral, negative)</p> <p>Dataset: <code>mteb/CzechProductReviewSentimentClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechproductreviewsentimentclassificationv2","title":"CzechProductReviewSentimentClassification.v2","text":"<p>User reviews of products on Czech e-shop Mall.cz with 3 sentiment classes (positive, neutral, negative) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/czech_product_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsomesentimentclassification","title":"CzechSoMeSentimentClassification","text":"<p>User comments on Facebook</p> <p>Dataset: <code>fewshot-goes-multilingual/cs_facebook-comments</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsomesentimentclassificationv2","title":"CzechSoMeSentimentClassification.v2","text":"<p>User comments on Facebook This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/czech_so_me_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written derived found Citation <pre><code>@inproceedings{habernal-etal-2013-sentiment,\n  address = {Atlanta, Georgia},\n  author = {Habernal, Ivan  and\nPt{\\'a}{\\v{c}}ek, Tom{\\'a}{\\v{s}}  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},\n  editor = {Balahur, Alexandra  and\nvan der Goot, Erik  and\nMontoyo, Andres},\n  month = jun,\n  pages = {65--74},\n  publisher = {Association for Computational Linguistics},\n  title = {Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning},\n  url = {https://aclanthology.org/W13-1609},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#czechsubjectivityclassification","title":"CzechSubjectivityClassification","text":"<p>An Czech dataset for subjectivity classification.</p> <p>Dataset: <code>pauli31/czech-subjectivity-dataset</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ces Reviews, Written human-annotated found Citation <pre><code>@inproceedings{priban-steinberger-2022-czech,\n  address = {Marseille, France},\n  author = {P{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nSteinberger, Josef},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {1381--1391},\n  publisher = {European Language Resources Association},\n  title = {\\{C\\}zech Dataset for Cross-lingual Subjectivity Classification},\n  url = {https://aclanthology.org/2022.lrec-1.148},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dbpediaclassification","title":"DBpediaClassification","text":"<p>DBpedia14 is a dataset of English texts from Wikipedia articles, categorized into 14 non-overlapping classes based on their DBpedia ontology.</p> <p>Dataset: <code>mteb/DBpediaClassification</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dbpediaclassificationv2","title":"DBpediaClassification.v2","text":"<p>DBpedia14 is a dataset of English texts from Wikipedia articles, categorized into 14 non-overlapping classes based on their DBpedia ontology. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/d_bpedia</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dkhateclassification","title":"DKHateClassification","text":"<p>Danish Tweets annotated for Hate Speech either being Offensive or not</p> <p>Dataset: <code>DDSC/dkhate</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written expert-annotated found Citation <pre><code>@inproceedings{sigurbergsson-derczynski-2020-offensive,\n  address = {Marseille, France},\n  author = {Sigurbergsson, Gudbjartur Ingi  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {3498--3508},\n  publisher = {European Language Resources Association},\n  title = {Offensive Language and Hate Speech Detection for {D}anish},\n  url = {https://aclanthology.org/2020.lrec-1.430},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dkhateclassificationv2","title":"DKHateClassification.v2","text":"<p>Danish Tweets annotated for Hate Speech either being Offensive or not This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/dk_hate</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written expert-annotated found Citation <pre><code>@inproceedings{sigurbergsson-derczynski-2020-offensive,\n  address = {Marseille, France},\n  author = {Sigurbergsson, Gudbjartur Ingi  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {3498--3508},\n  publisher = {European Language Resources Association},\n  title = {Offensive Language and Hate Speech Detection for {D}anish},\n  url = {https://aclanthology.org/2020.lrec-1.430},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dadoevalcoarseclassification","title":"DadoEvalCoarseClassification","text":"<p>The DaDoEval dataset is a curated collection of 2,759 documents authored by Alcide De Gasperi, spanning the period from 1901 to 1954. Each document in the dataset is manually tagged with its date of issue.</p> <p>Dataset: <code>MattiaSangermano/DaDoEval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Written derived found Citation <pre><code>@inproceedings{menini2020dadoeval,\n  author = {Menini, Stefano and Moretti, Giovanni and Sprugnoli, Rachele and Tonelli, Sara and others},\n  booktitle = {Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)},\n  organization = {Accademia University Press},\n  pages = {391--397},\n  title = {DaDoEval@ EVALITA 2020: Same-genre and cross-genre dating of historical documents},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dalajclassification","title":"DalajClassification","text":"<p>A Swedish dataset for linguistic acceptability. Available as a part of Superlim.</p> <p>Dataset: <code>mteb/DalajClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Non-fiction, Written expert-annotated created Citation <pre><code>@misc{2105.06681,\n  author = {Elena Volodina and Yousuf Ali Mohammed and Julia Klezl},\n  eprint = {arXiv:2105.06681},\n  title = {DaLAJ - a dataset for linguistic acceptability judgments for Swedish: Format, baseline, sharing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dalajclassificationv2","title":"DalajClassification.v2","text":"<p>A Swedish dataset for linguistic acceptability. Available as a part of Superlim. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/dalaj</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Non-fiction, Written expert-annotated created Citation <pre><code>@misc{2105.06681,\n  author = {Elena Volodina and Yousuf Ali Mohammed and Julia Klezl},\n  eprint = {arXiv:2105.06681},\n  title = {DaLAJ - a dataset for linguistic acceptability judgments for Swedish: Format, baseline, sharing},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#danishpoliticalcommentsclassification","title":"DanishPoliticalCommentsClassification","text":"<p>A dataset of Danish political comments rated for sentiment</p> <p>Dataset: <code>mteb/DanishPoliticalCommentsClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written derived found Citation <pre><code>@techreport{SAMsentiment,\n  author = {Mads Guldborg Kjeldgaard Kongsbak and Steffan Eybye Christensen and Lucas H\u00f8yberg Puvis~de~Chavannes and Peter Due Jensen},\n  institution = {IT University of Copenhagen},\n  title = {Sentiment Analysis Multitool, SAM},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#danishpoliticalcommentsclassificationv2","title":"DanishPoliticalCommentsClassification.v2","text":"<p>A dataset of Danish political comments rated for sentiment This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/danish_political_comments</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Social, Written derived found Citation <pre><code>@techreport{SAMsentiment,\n  author = {Mads Guldborg Kjeldgaard Kongsbak and Steffan Eybye Christensen and Lucas H\u00f8yberg Puvis~de~Chavannes and Peter Due Jensen},\n  institution = {IT University of Copenhagen},\n  title = {Sentiment Analysis Multitool, SAM},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ddisco","title":"Ddisco","text":"<p>A Danish Discourse dataset with values for coherence and source (Wikipedia or Reddit)</p> <p>Dataset: <code>DDSC/ddisco</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Non-fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ddiscov2","title":"Ddisco.v2","text":"<p>A Danish Discourse dataset with values for coherence and source (Wikipedia or Reddit) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ddisco_cohesion</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan Non-fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#deepsentipers","title":"DeepSentiPers","text":"<p>Persian Sentiment Analysis Dataset</p> <p>Dataset: <code>PartAI/DeepSentiPers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#deepsentipersv2","title":"DeepSentiPers.v2","text":"<p>Persian Sentiment Analysis Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/deep_senti_pers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#definitionclassificationlegalbenchclassification","title":"DefinitionClassificationLegalBenchClassification","text":"<p>This task consists of determining whether or not a sentence from a Supreme Court opinion offers a definition of a term.</p> <p>Dataset: <code>mteb/DefinitionClassificationLegalBenchClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#digikalamagclassification","title":"DigikalamagClassification","text":"<p>A total of 8,515 articles scraped from Digikala Online Magazine. This dataset includes seven different classes.</p> <p>Dataset: <code>mteb/DigikalamagClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity1legalbenchclassification","title":"Diversity1LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 1).</p> <p>Dataset: <code>mteb/Diversity1LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity2legalbenchclassification","title":"Diversity2LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 2).</p> <p>Dataset: <code>mteb/Diversity2LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity3legalbenchclassification","title":"Diversity3LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 3).</p> <p>Dataset: <code>mteb/Diversity3LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity4legalbenchclassification","title":"Diversity4LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 4).</p> <p>Dataset: <code>mteb/Diversity4LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity5legalbenchclassification","title":"Diversity5LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 5).</p> <p>Dataset: <code>mteb/Diversity5LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#diversity6legalbenchclassification","title":"Diversity6LegalBenchClassification","text":"<p>Given a set of facts about the citizenships of plaintiffs and defendants and the amounts associated with claims, determine if the criteria for diversity jurisdiction have been met (variant 6).</p> <p>Dataset: <code>mteb/Diversity6LegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchbookreviewsentimentclassification","title":"DutchBookReviewSentimentClassification","text":"<p>A Dutch book review for sentiment classification.</p> <p>Dataset: <code>mteb/DutchBookReviewSentimentClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nld Reviews, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1910-00896,\n  archiveprefix = {arXiv},\n  author = {Benjamin, van der Burgh and\nSuzan, Verberne},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  eprint = {1910.00896},\n  journal = {CoRR},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  title = {The merits of Universal Language Model Fine-tuning for Small Datasets\n- a case with Dutch book reviews},\n  url = {http://arxiv.org/abs/1910.00896},\n  volume = {abs/1910.00896},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchbookreviewsentimentclassificationv2","title":"DutchBookReviewSentimentClassification.v2","text":"<p>A Dutch book review for sentiment classification. This version corrects errors found in the original data. For details, see pull request. Additionally, a Dutch prompt was included.</p> <p>Dataset: <code>mteb/dutch_book_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nld Reviews, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1910-00896,\n  archiveprefix = {arXiv},\n  author = {Benjamin, van der Burgh and\nSuzan, Verberne},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1910-00896.bib},\n  eprint = {1910.00896},\n  journal = {CoRR},\n  timestamp = {Fri, 04 Oct 2019 12:28:06 +0200},\n  title = {The merits of Universal Language Model Fine-tuning for Small Datasets\n- a case with Dutch book reviews},\n  url = {http://arxiv.org/abs/1910.00896},\n  volume = {abs/1910.00896},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchcolaclassification","title":"DutchColaClassification","text":"<p>Dutch CoLA is a corpus of linguistic acceptability for Dutch.</p> <p>Dataset: <code>clips/mteb-nl-dutch-cola</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Written expert-annotated found Citation <pre><code>@misc{gronlp_2024,\n  author = {Bylinina, Lisa and Abdi, Silvana and Brouwer, Hylke and Elzinga, Martine and Gunput, Shenza and Huisman, Sem and Krooneman, Collin and Poot, David and Top, Jelmer and Weideman, Cain},\n  doi = { 10.57967/hf/3825 },\n  publisher = { Hugging Face },\n  title = { {Dutch-CoLA (Revision 5a4196c)} },\n  url = { https://huggingface.co/datasets/GroNLP/dutch-cola },\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchgovernmentbiasclassification","title":"DutchGovernmentBiasClassification","text":"<p>The Dutch Government Data for Bias Detection (DGDB) is a dataset sourced from the Dutch House of Representatives and annotated for bias by experts</p> <p>Dataset: <code>clips/mteb-nl-dutch-government-bias-detection</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Government, Written expert-annotated found Citation <pre><code>@inproceedings{de2025detecting,\n  author = {de Swart, Milena and Den Hengst, Floris and Chen, Jieying},\n  booktitle = {Proceedings of the ACM on Web Conference 2025},\n  pages = {5034--5044},\n  title = {Detecting Linguistic Bias in Government Documents Using Large language Models},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#dutchnewsarticlesclassification","title":"DutchNewsArticlesClassification","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld News, Written derived found"},{"location":"overview/available_tasks/classification/#dutchsarcasticheadlinesclassification","title":"DutchSarcasticHeadlinesClassification","text":"<p>This dataset contains news headlines of two Dutch news websites. All sarcastic headlines were collected from the Speld.nl (the Dutch equivalent of The Onion) whereas all 'normal' headlines were collected from the news website Nu.nl.</p> <p>Dataset: <code>clips/mteb-nl-sarcastic-headlines</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Fiction, News, Written derived found"},{"location":"overview/available_tasks/classification/#emotionclassification","title":"EmotionClassification","text":"<p>Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>mteb/emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#emotionclassificationv2","title":"EmotionClassification.v2","text":"<p>Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#emotionvnclassification","title":"EmotionVNClassification","text":"<p>Emotion is a translated dataset of Vietnamese from English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/emotion-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#estonianvalenceclassification","title":"EstonianValenceClassification","text":"<p>Dataset containing annotated Estonian news data from the Postimees and \u00d5htuleht newspapers.</p> <p>Dataset: <code>mteb/EstonianValenceClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy est News, Written human-annotated found Citation <pre><code>@article{Pajupuu2023,\n  author = {Hille Pajupuu and Jaan Pajupuu and Rene Altrov and Kairi Tamuri},\n  doi = {10.6084/m9.figshare.24517054.v1},\n  month = {11},\n  title = {{Estonian Valence Corpus  / Eesti valentsikorpus}},\n  url = {https://figshare.com/articles/dataset/Estonian_Valence_Corpus_Eesti_valentsikorpus/24517054},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#estonianvalenceclassificationv2","title":"EstonianValenceClassification.v2","text":"<p>Dataset containing annotated Estonian news data from the Postimees and \u00d5htuleht newspapers. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/estonian_valence</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy est News, Written human-annotated found Citation <pre><code>@article{Pajupuu2023,\n  author = {Hille Pajupuu and Jaan Pajupuu and Rene Altrov and Kairi Tamuri},\n  doi = {10.6084/m9.figshare.24517054.v1},\n  month = {11},\n  title = {{Estonian Valence Corpus  / Eesti valentsikorpus}},\n  url = {https://figshare.com/articles/dataset/Estonian_Valence_Corpus_Eesti_valentsikorpus/24517054},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#faintentclassification","title":"FaIntentClassification","text":"<p>Questions in 4 different categories that a user might ask their voice assistant to do</p> <p>Dataset: <code>MCINext/FaIntent</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinohatespeechclassification","title":"FilipinoHateSpeechClassification","text":"<p>Filipino Twitter dataset for sentiment classification.</p> <p>Dataset: <code>mteb/FilipinoHateSpeechClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{Cabasag-2019-hate-speech,\n  author = {Neil Vicente Cabasag, Vicente Raphael Chan, Sean Christian Lim, Mark Edward Gonzales, and Charibeth Cheng},\n  journal = {Philippine Computing Journal},\n  month = {August},\n  number = {1},\n  title = {Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing.},\n  volume = {XIV},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinohatespeechclassificationv2","title":"FilipinoHateSpeechClassification.v2","text":"<p>Filipino Twitter dataset for sentiment classification. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/filipino_hate_speech</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{Cabasag-2019-hate-speech,\n  author = {Neil Vicente Cabasag, Vicente Raphael Chan, Sean Christian Lim, Mark Edward Gonzales, and Charibeth Cheng},\n  journal = {Philippine Computing Journal},\n  month = {August},\n  number = {1},\n  title = {Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing.},\n  volume = {XIV},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#filipinoshopeereviewsclassification","title":"FilipinoShopeeReviewsClassification","text":"<p>The Shopee reviews tl 15 dataset is constructed by randomly taking 2100 training samples and 450 samples for testing and validation for each review star from 1 to 5. In total, there are 10500 training samples and 2250 each in validation and testing samples.</p> <p>Dataset: <code>mteb/FilipinoShopeeReviewsClassification</code> \u2022 License: mpl-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fil Social, Written human-annotated found Citation <pre><code>@article{riegoenhancement,\n  author = {Riego, Neil Christian R. and Villarba, Danny Bell and Sison, Ariel Antwaun Rolando C. and Pineda, Fernandez C. and Lagunzad, Hermini\u00f1o C.},\n  issue = {08},\n  journal = {United International Journal for Research &amp; Technology},\n  pages = {72--82},\n  title = {Enhancement to Low-Resource Text Classification via Sequential Transfer Learning},\n  volume = {04},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#fintoxicityclassification","title":"FinToxicityClassification","text":"<p>This dataset is a DeepL -based machine translated version of the Jigsaw toxicity dataset for Finnish. The dataset is originally from a Kaggle competition https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data. The original dataset poses a multi-label text classification problem and includes the labels identity_attack, insult, obscene, severe_toxicity, threat and toxicity. Here adapted for toxicity classification, which is the most represented class.</p> <p>Dataset: <code>TurkuNLP/jigsaw_toxicity_pred_fi</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 fin News, Written derived machine-translated Citation <pre><code>@inproceedings{eskelinen-etal-2023-toxicity,\n  author = {Eskelinen, Anni  and\nSilvala, Laura  and\nGinter, Filip  and\nPyysalo, Sampo  and\nLaippala, Veronika},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  title = {Toxicity Detection in {F}innish Using Machine Translation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#fintoxicityclassificationv2","title":"FinToxicityClassification.v2","text":"<p>This dataset is a DeepL -based machine translated version of the Jigsaw toxicity dataset for Finnish. The dataset is originally from a Kaggle competition https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data. The original dataset poses a multi-label text classification problem and includes the labels identity_attack, insult, obscene, severe_toxicity, threat and toxicity. Here adapted for toxicity classification, which is the most represented class. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/fin_toxicity</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 fin News, Written derived machine-translated Citation <pre><code>@inproceedings{eskelinen-etal-2023-toxicity,\n  author = {Eskelinen, Anni  and\nSilvala, Laura  and\nGinter, Filip  and\nPyysalo, Sampo  and\nLaippala, Veronika},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = may,\n  title = {Toxicity Detection in {F}innish Using Machine Translation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#financialphrasebankclassification","title":"FinancialPhrasebankClassification","text":"<p>Polar sentiment dataset of sentences from financial news, categorized by sentiment into positive, negative, or neutral.</p> <p>Dataset: <code>mteb/FinancialPhrasebankClassification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Financial, News, Written expert-annotated found Citation <pre><code>@article{Malo2014GoodDO,\n  author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\n  journal = {Journal of the Association for Information Science and Technology},\n  title = {Good debt or bad debt: Detecting semantic orientations in economic texts},\n  volume = {65},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#financialphrasebankclassificationv2","title":"FinancialPhrasebankClassification.v2","text":"<p>Polar sentiment dataset of sentences from financial news, categorized by sentiment into positive, negative, or neutral. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/financial_phrasebank</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Financial, News, Written expert-annotated found Citation <pre><code>@article{Malo2014GoodDO,\n  author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},\n  journal = {Journal of the Association for Information Science and Technology},\n  title = {Good debt or bad debt: Detecting semantic orientations in economic texts},\n  volume = {65},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenchbookreviews","title":"FrenchBookReviews","text":"<p>It is a French book reviews dataset containing a huge number of reader reviews on French books. Each review is pared with a rating that ranges from 0.5 to 5 (with 0.5 increment).</p> <p>Dataset: <code>Abirate/french_book_reviews</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenchbookreviewsv2","title":"FrenchBookReviews.v2","text":"<p>It is a French book reviews dataset containing a huge number of reader reviews on French books. Each review is pared with a rating that ranges from 0.5 to 5 (with 0.5 increment). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/french_book_reviews</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkenclassification","title":"FrenkEnClassification","text":"<p>English subset of the FRENK dataset</p> <p>Dataset: <code>mteb/FrenkEnClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkenclassificationv2","title":"FrenkEnClassification.v2","text":"<p>English subset of the FRENK dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_en</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkhrclassification","title":"FrenkHrClassification","text":"<p>Croatian subset of the FRENK dataset</p> <p>Dataset: <code>mteb/FrenkHrClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hrv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkhrclassificationv2","title":"FrenkHrClassification.v2","text":"<p>Croatian subset of the FRENK dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_hr</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hrv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkslclassification","title":"FrenkSlClassification","text":"<p>Slovenian subset of the FRENK dataset. Also available on HuggingFace dataset hub: English subset, Croatian subset.</p> <p>Dataset: <code>mteb/FrenkSlClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#frenkslclassificationv2","title":"FrenkSlClassification.v2","text":"<p>Slovenian subset of the FRENK dataset. Also available on HuggingFace dataset hub: English subset, Croatian subset. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/frenk_sl</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slv Social, Written derived found Citation <pre><code>@misc{ljube\u0161i\u01072019frenk,\n  archiveprefix = {arXiv},\n  author = {Nikola Ljube\u0161i\u0107 and Darja Fi\u0161er and Toma\u017e Erjavec},\n  eprint = {1906.02045},\n  primaryclass = {cs.CL},\n  title = {The FRENK Datasets of Socially Unacceptable Discourse in Slovene and English},\n  url = {https://arxiv.org/abs/1906.02045},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#functionofdecisionsectionlegalbenchclassification","title":"FunctionOfDecisionSectionLegalBenchClassification","text":"<p>The task is to classify a paragraph extracted from a written court decision into one of seven possible categories: 1. Facts - The paragraph describes the faction background that led up to the present lawsuit. 2. Procedural History - The paragraph describes the course of litigation that led to the current proceeding before the court. 3. Issue - The paragraph describes the legal or factual issue that must be resolved by the court. 4. Rule - The paragraph describes a rule of law relevant to resolving the issue. 5. Analysis - The paragraph analyzes the legal issue by applying the relevant legal principles to the facts of the present dispute. 6. Conclusion - The paragraph presents a conclusion of the court. 7. Decree - The paragraph constitutes a decree resolving the dispute.</p> <p>Dataset: <code>mteb/FunctionOfDecisionSectionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#georeviewclassification","title":"GeoreviewClassification","text":"<p>Review classification (5-point scale) based on Yandex Georeview dataset</p> <p>Dataset: <code>mteb/GeoreviewClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#georeviewclassificationv2","title":"GeoreviewClassification.v2","text":"<p>Review classification (5-point scale) based on Yandex Georeview dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/georeview</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#georgiansentimentclassification","title":"GeorgianSentimentClassification","text":"<p>Goergian Sentiment Dataset</p> <p>Dataset: <code>asparius/Georgian-Sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kat Reviews, Written derived found Citation <pre><code>@inproceedings{stefanovitch-etal-2022-resources,\n  address = {Marseille, France},\n  author = {Stefanovitch, Nicolas  and\nPiskorski, Jakub  and\nKharazi, Sopho},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {1613--1621},\n  publisher = {European Language Resources Association},\n  title = {Resources and Experiments on Sentiment Classification for {G}eorgian},\n  url = {https://aclanthology.org/2022.lrec-1.173},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#germanpoliticianstwittersentimentclassification","title":"GermanPoliticiansTwitterSentimentClassification","text":"<p>GermanPoliticiansTwitterSentiment is a dataset of German tweets categorized with their sentiment (3 classes).</p> <p>Dataset: <code>Alienmaster/german_politicians_twitter_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu Government, Social, Written human-annotated found Citation <pre><code>@inproceedings{schmidt-etal-2022-sentiment,\n  address = {Potsdam, Germany},\n  author = {Schmidt, Thomas  and\nFehle, Jakob  and\nWeissenbacher, Maximilian  and\nRichter, Jonathan  and\nGottschalk, Philipp  and\nWolff, Christian},\n  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},\n  editor = {Schaefer, Robin  and\nBai, Xiaoyu  and\nStede, Manfred  and\nZesch, Torsten},\n  month = {12--15 } # sep,\n  pages = {74--87},\n  publisher = {KONVENS 2022 Organizers},\n  title = {Sentiment Analysis on {T}witter for the Major {G}erman Parties during the 2021 {G}erman Federal Election},\n  url = {https://aclanthology.org/2022.konvens-1.9},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#germanpoliticianstwittersentimentclassificationv2","title":"GermanPoliticiansTwitterSentimentClassification.v2","text":"<p>GermanPoliticiansTwitterSentiment is a dataset of German tweets categorized with their sentiment (3 classes). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/german_politicians_twitter_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu Government, Social, Written human-annotated found Citation <pre><code>@inproceedings{schmidt-etal-2022-sentiment,\n  address = {Potsdam, Germany},\n  author = {Schmidt, Thomas  and\nFehle, Jakob  and\nWeissenbacher, Maximilian  and\nRichter, Jonathan  and\nGottschalk, Philipp  and\nWolff, Christian},\n  booktitle = {Proceedings of the 18th Conference on Natural Language Processing (KONVENS 2022)},\n  editor = {Schaefer, Robin  and\nBai, Xiaoyu  and\nStede, Manfred  and\nZesch, Torsten},\n  month = {12--15 } # sep,\n  pages = {74--87},\n  publisher = {KONVENS 2022 Organizers},\n  title = {Sentiment Analysis on {T}witter for the Major {G}erman Parties during the 2021 {G}erman Federal Election},\n  url = {https://aclanthology.org/2022.konvens-1.9},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#greeklegalcodeclassification","title":"GreekLegalCodeClassification","text":"<p>Greek Legal Code Dataset for Classification. (subset = chapter)</p> <p>Dataset: <code>mteb/GreekLegalCodeClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ell Legal, Written human-annotated found Citation <pre><code>@inproceedings{papaloukas-etal-2021-glc,\n  address = {Punta Cana, Dominican Republic},\n  author = {Papaloukas, Christos and Chalkidis, Ilias and Athinaios, Konstantinos and Pantazi, Despina-Athanasia and Koubarakis, Manolis},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  doi = {10.48550/arXiv.2109.15298},\n  pages = {63--75},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-granular Legal Topic Classification on Greek Legislation},\n  url = {https://arxiv.org/abs/2109.15298},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#gujaratinewsclassification","title":"GujaratiNewsClassification","text":"<p>A Gujarati dataset for 3-class classification of Gujarati news articles</p> <p>Dataset: <code>mteb/GujaratiNewsClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj News, Written derived found"},{"location":"overview/available_tasks/classification/#gujaratinewsclassificationv2","title":"GujaratiNewsClassification.v2","text":"<p>A Gujarati dataset for 3-class classification of Gujarati news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/gujarati_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj News, Written derived found"},{"location":"overview/available_tasks/classification/#humeemotionclassification","title":"HUMEEmotionClassification","text":"<p>Human evaluation subset of Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>mteb/HUMEEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@inproceedings{saravia-etal-2018-carer,\n  address = {Brussels, Belgium},\n  author = {Saravia, Elvis  and\nLiu, Hsien-Chi Toby  and\nHuang, Yen-Hao  and\nWu, Junlin  and\nChen, Yi-Shin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1404},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {3687--3697},\n  publisher = {Association for Computational Linguistics},\n  title = {{CARER}: Contextualized Affect Representations for Emotion Recognition},\n  url = {https://aclanthology.org/D18-1404},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humemultilingualsentimentclassification","title":"HUMEMultilingualSentimentClassification","text":"<p>Human evaluation subset of Sentiment classification dataset with binary (positive vs negative sentiment) labels. Includes 4 languages.</p> <p>Dataset: <code>mteb/HUMEMultilingualSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy ara, eng, nob, rus Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humetoxicconversationsclassification","title":"HUMEToxicConversationsClassification","text":"<p>Human evaluation subset of Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.</p> <p>Dataset: <code>mteb/HUMEToxicConversationsClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#humetweetsentimentextractionclassification","title":"HUMETweetSentimentExtractionClassification","text":"<p>Human evaluation subset of Tweet Sentiment Extraction dataset.</p> <p>Dataset: <code>mteb/HUMETweetSentimentExtractionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hatespeechportugueseclassification","title":"HateSpeechPortugueseClassification","text":"<p>HateSpeechPortugueseClassification is a dataset of Portuguese tweets categorized with their sentiment (2 classes).</p> <p>Dataset: <code>mteb/HateSpeechPortugueseClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy por Social, Written expert-annotated found Citation <pre><code>@inproceedings{fortuna-etal-2019-hierarchically,\n  address = {Florence, Italy},\n  author = {Fortuna, Paula  and\nRocha da Silva, Jo{\\~a}o  and\nSoler-Company, Juan  and\nWanner, Leo  and\nNunes, S{\\'e}rgio},\n  booktitle = {Proceedings of the Third Workshop on Abusive Language Online},\n  doi = {10.18653/v1/W19-3510},\n  editor = {Roberts, Sarah T.  and\nTetreault, Joel  and\nPrabhakaran, Vinodkumar  and\nWaseem, Zeerak},\n  month = aug,\n  pages = {94--104},\n  publisher = {Association for Computational Linguistics},\n  title = {A Hierarchically-Labeled {P}ortuguese Hate Speech Dataset},\n  url = {https://aclanthology.org/W19-3510},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#headlineclassification","title":"HeadlineClassification","text":"<p>Headline rubric classification based on the paraphraser plus dataset.</p> <p>Dataset: <code>ai-forever/headline-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus News, Written derived found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  editor = {Birch, Alexandra  and\nFinch, Andrew  and\nHayashi, Hiroaki  and\nHeafield, Kenneth  and\nJunczys-Dowmunt, Marcin  and\nKonstas, Ioannis  and\nLi, Xian  and\nNeubig, Graham  and\nOda, Yusuke},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#headlineclassificationv2","title":"HeadlineClassification.v2","text":"<p>Headline rubric classification based on the paraphraser plus dataset. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/headline</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus News, Written derived found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  editor = {Birch, Alexandra  and\nFinch, Andrew  and\nHayashi, Hiroaki  and\nHeafield, Kenneth  and\nJunczys-Dowmunt, Marcin  and\nKonstas, Ioannis  and\nLi, Xian  and\nNeubig, Graham  and\nOda, Yusuke},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hebrewsentimentanalysis","title":"HebrewSentimentAnalysis","text":"<p>HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel\u2019s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president\u2019s posts in the period of June \u2013 August 2014, the first three months of Rivlin\u2019s presidency.2 While the president\u2019s posts aimed at reconciling tensions and called for tolerance and empathy, the sentiment expressed in the comments to the president\u2019s posts was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his policy. </p> <p>Dataset: <code>mteb/HebrewSentimentAnalysis</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy heb Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{amram-etal-2018-representations,\n  address = {Santa Fe, New Mexico, USA},\n  author = {Amram, Adam and Ben David, Anat and Tsarfaty, Reut},\n  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},\n  month = aug,\n  pages = {2242--2252},\n  publisher = {Association for Computational Linguistics},\n  title = {Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew},\n  url = {https://www.aclweb.org/anthology/C18-1190},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hebrewsentimentanalysisv2","title":"HebrewSentimentAnalysis.v2","text":"<p>HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel\u2019s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president\u2019s posts in the period of June \u2013 August 2014, the first three months of Rivlin\u2019s presidency.2 While the president\u2019s posts aimed at reconciling tensions and called for tolerance and empathy, the sentiment expressed in the comments to the president\u2019s posts was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his policy. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/hebrew_sentiment_analysis</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy heb Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{amram-etal-2018-representations,\n  address = {Santa Fe, New Mexico, USA},\n  author = {Amram, Adam and Ben David, Anat and Tsarfaty, Reut},\n  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},\n  month = aug,\n  pages = {2242--2252},\n  publisher = {Association for Computational Linguistics},\n  title = {Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew},\n  url = {https://www.aclweb.org/anthology/C18-1190},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hebrewsentimentanalysisv3","title":"HebrewSentimentAnalysis.v3","text":"<p>HebrewSentiment is a data set consists of 12,804 user comments to posts on the official Facebook page of Israel\u2019s president, Mr. Reuven Rivlin. In October 2015, we used the open software application Netvizz (Rieder, 2013) to scrape all the comments to all of the president\u2019s posts in the period of June \u2013 August 2014, the first three months of Rivlin\u2019s presidency.2 While the president\u2019s posts aimed at reconciling tensions and called for tolerance and empathy, the sentiment expressed in the comments to the president\u2019s posts was polarized between citizens who warmly thanked the president, and citizens that fiercely critiqued his policy. This version corrects texts (took pre-tokenized) more details in this thread. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/HebrewSentimentAnalysisV4</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy heb Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{amram-etal-2018-representations,\n  address = {Santa Fe, New Mexico, USA},\n  author = {Amram, Adam and Ben David, Anat and Tsarfaty, Reut},\n  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},\n  month = aug,\n  pages = {2242--2252},\n  publisher = {Association for Computational Linguistics},\n  title = {Representations and Architectures in Neural Sentiment Analysis for Morphologically Rich Languages: A Case Study from {M}odern {H}ebrew},\n  url = {https://www.aclweb.org/anthology/C18-1190},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindialectclassification","title":"HinDialectClassification","text":"<p>HinDialect: 26 Hindi-related languages and dialects of the Indic Continuum in North India</p> <p>Dataset: <code>mlexplorer008/hin_dialect_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 anp, awa, ben, bgc, bhb, ... (21) Social, Spoken, Written expert-annotated found Citation <pre><code>@misc{11234/1-4839,\n  author = {Bafna, Niyati and {\\v Z}abokrtsk{\\'y}, Zden{\\v e}k and Espa{\\~n}a-Bonet, Cristina and van Genabith, Josef and Kumar, Lalit \"Samyak Lalit\" and Suman, Sharda and Shivay, Rahul},\n  copyright = {Creative Commons - Attribution-{NonCommercial}-{ShareAlike} 4.0 International ({CC} {BY}-{NC}-{SA} 4.0)},\n  note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\\'U}FAL}), Faculty of Mathematics and Physics, Charles University},\n  title = {{HinDialect} 1.1: 26 Hindi-related languages and dialects of the Indic Continuum in North India},\n  url = {http://hdl.handle.net/11234/1-4839},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindidiscourseclassification","title":"HindiDiscourseClassification","text":"<p>A Hindi Discourse dataset in Hindi with values for coherence.</p> <p>Dataset: <code>mteb/HindiDiscourseClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hin Fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dhanwal-etal-2020-annotated,\n  address = {Marseille, France},\n  author = {Dhanwal, Swapnil  and\nDutta, Hritwik  and\nNankani, Hitesh  and\nShrivastava, Nilay  and\nKumar, Yaman  and\nLi, Junyi Jessy  and\nMahata, Debanjan  and\nGosangi, Rakesh  and\nZhang, Haimin  and\nShah, Rajiv Ratn  and\nStent, Amanda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  publisher = {European Language Resources Association},\n  title = {An Annotated Dataset of Discourse Modes in {H}indi Stories},\n  url = {https://www.aclweb.org/anthology/2020.lrec-1.149},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hindidiscourseclassificationv2","title":"HindiDiscourseClassification.v2","text":"<p>A Hindi Discourse dataset in Hindi with values for coherence. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/hindi_discourse</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hin Fiction, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dhanwal-etal-2020-annotated,\n  address = {Marseille, France},\n  author = {Dhanwal, Swapnil  and\nDutta, Hritwik  and\nNankani, Hitesh  and\nShrivastava, Nilay  and\nKumar, Yaman  and\nLi, Junyi Jessy  and\nMahata, Debanjan  and\nGosangi, Rakesh  and\nZhang, Haimin  and\nShah, Rajiv Ratn  and\nStent, Amanda},\n  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  publisher = {European Language Resources Association},\n  title = {An Annotated Dataset of Discourse Modes in {H}indi Stories},\n  url = {https://www.aclweb.org/anthology/2020.lrec-1.149},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hotelreviewsentimentclassification","title":"HotelReviewSentimentClassification","text":"<p>HARD is a dataset of Arabic hotel reviews collected from the Booking.com website.</p> <p>Dataset: <code>mteb/HotelReviewSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@article{elnagar2018hotel,\n  author = {Elnagar, Ashraf and Khalifa, Yasmin S and Einea, Anas},\n  journal = {Intelligent natural language processing: Trends and applications},\n  pages = {35--52},\n  publisher = {Springer},\n  title = {Hotel Arabic-reviews dataset construction for sentiment analysis applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#hotelreviewsentimentclassificationv2","title":"HotelReviewSentimentClassification.v2","text":"<p>HARD is a dataset of Arabic hotel reviews collected from the Booking.com website. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/HotelReviewSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@article{elnagar2018hotel,\n  author = {Elnagar, Ashraf and Khalifa, Yasmin S and Einea, Anas},\n  journal = {Intelligent natural language processing: Trends and applications},\n  pages = {35--52},\n  publisher = {Springer},\n  title = {Hotel Arabic-reviews dataset construction for sentiment analysis applications},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iflytek","title":"IFlyTek","text":"<p>Long Text classification for the description of Apps</p> <p>Dataset: <code>C-MTEB/IFlyTek-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Web, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iflytekv2","title":"IFlyTek.v2","text":"<p>Long Text classification for the description of Apps This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/i_fly_tek</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Web, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#iconclassclassification","title":"IconclassClassification","text":"<p>Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. The task is to classify the first layer of Iconclass</p> <p>Dataset: <code>clips/mteb-nl-iconclass-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Fiction, Written expert-annotated found Citation <pre><code>@article{banar2023transfer,\n  author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},\n  journal = {ACM Journal on Computing and Cultural Heritage},\n  number = {2},\n  pages = {1--16},\n  publisher = {ACM New York, NY},\n  title = {Transfer learning for the visual arts: The multi-modal retrieval of iconclass codes},\n  volume = {16},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbclassification","title":"ImdbClassification","text":"<p>Large Movie Review Dataset</p> <p>Dataset: <code>mteb/imdb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{maas-etal-2011-learning,\n  address = {Portland, Oregon, USA},\n  author = {Maas, Andrew L.  and\nDaly, Raymond E.  and\nPham, Peter T.  and\nHuang, Dan  and\nNg, Andrew Y.  and\nPotts, Christopher},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  editor = {Lin, Dekang  and\nMatsumoto, Yuji  and\nMihalcea, Rada},\n  month = jun,\n  pages = {142--150},\n  publisher = {Association for Computational Linguistics},\n  title = {Learning Word Vectors for Sentiment Analysis},\n  url = {https://aclanthology.org/P11-1015},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbclassificationv2","title":"ImdbClassification.v2","text":"<p>Large Movie Review Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/imdb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{maas-etal-2011-learning,\n  address = {Portland, Oregon, USA},\n  author = {Maas, Andrew L.  and\nDaly, Raymond E.  and\nPham, Peter T.  and\nHuang, Dan  and\nNg, Andrew Y.  and\nPotts, Christopher},\n  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n  editor = {Lin, Dekang  and\nMatsumoto, Yuji  and\nMihalcea, Rada},\n  month = jun,\n  pages = {142--150},\n  publisher = {Association for Computational Linguistics},\n  title = {Learning Word Vectors for Sentiment Analysis},\n  url = {https://aclanthology.org/P11-1015},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#imdbvnclassification","title":"ImdbVNClassification","text":"<p>A translated dataset of large movie reviews annotated for sentiment classification. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/imdb-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassification","title":"InappropriatenessClassification","text":"<p>Inappropriateness identification in the form of binary classification</p> <p>Dataset: <code>ai-forever/inappropriateness-classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassificationv2","title":"InappropriatenessClassification.v2","text":"<p>Inappropriateness identification in the form of binary classification This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/inappropriateness</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#inappropriatenessclassificationv2_1","title":"InappropriatenessClassificationv2","text":"<p>Inappropriateness identification in the form of binary classification</p> <p>Dataset: <code>mteb/InappropriatenessClassificationv2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indiclangclassification","title":"IndicLangClassification","text":"<p>A language identification test set for native-script as well as Romanized text which spans 22 Indic languages.</p> <p>Dataset: <code>mteb/IndicLangClassification</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy asm, ben, brx, doi, gom, ... (22) Non-fiction, Web, Written expert-annotated created Citation <pre><code>@inproceedings{madhani-etal-2023-bhasa,\n  address = {Toronto, Canada},\n  author = {Madhani, Yash  and\nKhapra, Mitesh M.  and\nKunchukuttan, Anoop},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},\n  doi = {10.18653/v1/2023.acl-short.71},\n  editor = {Rogers, Anna  and\nBoyd-Graber, Jordan  and\nOkazaki, Naoaki},\n  month = jul,\n  pages = {816--826},\n  publisher = {Association for Computational Linguistics},\n  title = {Bhasa-Abhijnaanam: Native-script and romanized Language Identification for 22 {I}ndic languages},\n  url = {https://aclanthology.org/2023.acl-short.71},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indicnlpnewsclassification","title":"IndicNLPNewsClassification","text":"<p>A News classification dataset in multiple Indian regional languages.</p> <p>Dataset: <code>Sakshamrzt/IndicNLP-Multilingual</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy guj, kan, mal, mar, ori, ... (8) News, Written expert-annotated found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indicsentimentclassification","title":"IndicSentimentClassification","text":"<p>A new, multilingual, and n-way parallel dataset for sentiment analysis in 13 Indic languages.</p> <p>Dataset: <code>mteb/IndicSentiment</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy asm, ben, brx, guj, hin, ... (13) Reviews, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianidclickbaitclassification","title":"IndonesianIdClickbaitClassification","text":"<p>The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers.</p> <p>Dataset: <code>mteb/IndonesianIdClickbaitClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind News, Written expert-annotated found Citation <pre><code>@article{WILLIAM2020106231,\n  author = {Andika William and Yunita Sari},\n  doi = {https://doi.org/10.1016/j.dib.2020.106231},\n  issn = {2352-3409},\n  journal = {Data in Brief},\n  keywords = {Indonesian, Natural Language Processing, News articles, Clickbait, Text-classification},\n  pages = {106231},\n  title = {CLICK-ID: A novel dataset for Indonesian clickbait headlines},\n  url = {http://www.sciencedirect.com/science/article/pii/S2352340920311252},\n  volume = {32},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianidclickbaitclassificationv2","title":"IndonesianIdClickbaitClassification.v2","text":"<p>The CLICK-ID dataset is a collection of Indonesian news headlines that was collected from 12 local online news publishers. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/indonesian_id_clickbait</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind News, Written expert-annotated found Citation <pre><code>@article{WILLIAM2020106231,\n  author = {Andika William and Yunita Sari},\n  doi = {https://doi.org/10.1016/j.dib.2020.106231},\n  issn = {2352-3409},\n  journal = {Data in Brief},\n  keywords = {Indonesian, Natural Language Processing, News articles, Clickbait, Text-classification},\n  pages = {106231},\n  title = {CLICK-ID: A novel dataset for Indonesian clickbait headlines},\n  url = {http://www.sciencedirect.com/science/article/pii/S2352340920311252},\n  volume = {32},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianmongabayconservationclassification","title":"IndonesianMongabayConservationClassification","text":"<p>Conservation dataset that was collected from mongabay.co.id contains topic-classification task (multi-label format) and sentiment classification. This task only covers sentiment analysis (positive, neutral negative)</p> <p>Dataset: <code>Datasaur/mongabay-experiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind Web, Written derived found Citation <pre><code>@inproceedings{fransiska-etal-2023-utilizing,\n  address = {Nusa Dua, Bali, Indonesia},\n  author = {Fransiska, Mega  and\nPitaloka, Diah  and\nSaripudin, Saripudin  and\nPutra, Satrio  and\nSutawika*, Lintang},\n  booktitle = {Proceedings of the First Workshop in South East Asian Language Processing},\n  doi = {10.18653/v1/2023.sealp-1.4},\n  editor = {Wijaya, Derry  and\nAji, Alham Fikri  and\nVania, Clara  and\nWinata, Genta Indra  and\nPurwarianti, Ayu},\n  month = nov,\n  pages = {30--54},\n  publisher = {Association for Computational Linguistics},\n  title = {Utilizing Weak Supervision to Generate {I}ndonesian Conservation Datasets},\n  url = {https://aclanthology.org/2023.sealp-1.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#indonesianmongabayconservationclassificationv2","title":"IndonesianMongabayConservationClassification.v2","text":"<p>Conservation dataset that was collected from mongabay.co.id contains topic-classification task (multi-label format) and sentiment classification. This task only covers sentiment analysis (positive, neutral negative) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/indonesian_mongabay_conservation</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ind Web, Written derived found Citation <pre><code>@inproceedings{fransiska-etal-2023-utilizing,\n  address = {Nusa Dua, Bali, Indonesia},\n  author = {Fransiska, Mega  and\nPitaloka, Diah  and\nSaripudin, Saripudin  and\nPutra, Satrio  and\nSutawika*, Lintang},\n  booktitle = {Proceedings of the First Workshop in South East Asian Language Processing},\n  doi = {10.18653/v1/2023.sealp-1.4},\n  editor = {Wijaya, Derry  and\nAji, Alham Fikri  and\nVania, Clara  and\nWinata, Genta Indra  and\nPurwarianti, Ayu},\n  month = nov,\n  pages = {30--54},\n  publisher = {Association for Computational Linguistics},\n  title = {Utilizing Weak Supervision to Generate {I}ndonesian Conservation Datasets},\n  url = {https://aclanthology.org/2023.sealp-1.4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#insurancepolicyinterpretationlegalbenchclassification","title":"InsurancePolicyInterpretationLegalBenchClassification","text":"<p>Given an insurance claim and policy, determine whether the claim is covered by the policy.</p> <p>Dataset: <code>mteb/InsurancePolicyInterpretationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#internationalcitizenshipquestionslegalbenchclassification","title":"InternationalCitizenshipQuestionsLegalBenchClassification","text":"<p>Answer questions about citizenship law from across the world. Dataset was made using the GLOBALCIT citizenship law dataset, by constructing questions about citizenship law as Yes or No questions.</p> <p>Dataset: <code>mteb/InternationalCitizenshipQuestionsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@misc{vink2023globalcit,\n  author = {Vink, Maarten and van der Baaren, Luuk and Baub\u00f6ck, Rainer and D\u017eanki\u0107, Jelena and Honohan, Iseult and Manby, Bronwen},\n  howpublished = {https://hdl.handle.net/1814/73190},\n  publisher = {Global Citizenship Observatory},\n  title = {GLOBALCIT Citizenship Law Dataset, v2.0, Country-Year-Mode Data (Acquisition)},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#isizulunewsclassification","title":"IsiZuluNewsClassification","text":"<p>isiZulu News Classification Dataset</p> <p>Dataset: <code>mteb/IsiZuluNewsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy zul News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#isizulunewsclassificationv2","title":"IsiZuluNewsClassification.v2","text":"<p>isiZulu News Classification Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/isi_zulu_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy zul News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacaseholdclassification","title":"ItaCaseholdClassification","text":"<p>An Italian Dataset consisting of 1101 pairs of judgments and their official holdings between the years 2019 and 2022 from the archives of Italian Administrative Justice categorized with 64 subjects.</p> <p>Dataset: <code>mteb/ItaCaseholdClassification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Government, Legal, Written expert-annotated found Citation <pre><code>@inproceedings{10.1145/3594536.3595177,\n  address = {New York, NY, USA},\n  author = {Licari, Daniele and Bushipaka, Praveen and Marino, Gabriele and Comand\\'{e}, Giovanni and Cucinotta, Tommaso},\n  booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},\n  doi = {10.1145/3594536.3595177},\n  isbn = {9798400701979},\n  keywords = {Italian-LEGAL-BERT, Holding Extraction, Extractive Text Summarization, Benchmark Dataset},\n  location = {&lt;conf-loc&gt;, &lt;city&gt;Braga&lt;/city&gt;, &lt;country&gt;Portugal&lt;/country&gt;, &lt;/conf-loc&gt;},\n  numpages = {9},\n  pages = {148\u2013156},\n  publisher = {Association for Computing Machinery},\n  series = {ICAIL '23},\n  title = {Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization},\n  url = {https://doi.org/10.1145/3594536.3595177},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacola","title":"Itacola","text":"<p>An Italian Corpus of Linguistic Acceptability taken from linguistic literature with a binary annotation made by the original authors themselves.</p> <p>Dataset: <code>mteb/Itacola</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Non-fiction, Spoken, Written expert-annotated found Citation <pre><code>@inproceedings{trotta-etal-2021-monolingual-cross,\n  address = {Punta Cana, Dominican Republic},\n  author = {Trotta, Daniela  and\nGuarasci, Raffaele  and\nLeonardelli, Elisa  and\nTonelli, Sara},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.250},\n  month = nov,\n  pages = {2929--2940},\n  publisher = {Association for Computational Linguistics},\n  title = {Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus},\n  url = {https://aclanthology.org/2021.findings-emnlp.250},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#itacolav2","title":"Itacola.v2","text":"<p>An Italian Corpus of Linguistic Acceptability taken from linguistic literature with a binary annotation made by the original authors themselves. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/italian_linguistic_acceptability</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Non-fiction, Spoken, Written expert-annotated found Citation <pre><code>@inproceedings{trotta-etal-2021-monolingual-cross,\n  address = {Punta Cana, Dominican Republic},\n  author = {Trotta, Daniela  and\nGuarasci, Raffaele  and\nLeonardelli, Elisa  and\nTonelli, Sara},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.250},\n  month = nov,\n  pages = {2929--2940},\n  publisher = {Association for Computational Linguistics},\n  title = {Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus},\n  url = {https://aclanthology.org/2021.findings-emnlp.250},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jcrewblockerlegalbenchclassification","title":"JCrewBlockerLegalBenchClassification","text":"<p>The J.Crew Blocker, also known as the J.Crew Protection, is a provision included in leveraged loan documents to prevent companies from removing security by transferring intellectual property (IP) into new subsidiaries and raising additional debt. The task consists of determining whether the J.Crew Blocker is present in the document.</p> <p>Dataset: <code>mteb/JCrewBlockerLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jcrewblockerlegalbenchclassificationv2","title":"JCrewBlockerLegalBenchClassification.v2","text":"<p>The J.Crew Blocker, also known as the J.Crew Protection, is a provision included in leveraged loan documents to prevent companies from removing security by transferring intellectual property (IP) into new subsidiaries and raising additional debt. The task consists of determining whether the J.Crew Blocker is present in the document. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/j_crew_blocker_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jdreview","title":"JDReview","text":"<p>review for iphone</p> <p>Dataset: <code>C-MTEB/JDReview-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#jdreviewv2","title":"JDReview.v2","text":"<p>review for iphone This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/jd_review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#japanesesentimentclassification","title":"JapaneseSentimentClassification","text":"<p>Japanese sentiment classification dataset with binary (positive vs negative sentiment) labels. This version reverts the morphological analysis from the original multilingual dataset to restore natural Japanese text without artificial spaces.</p> <p>Dataset: <code>mteb/JapaneseSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#javaneseimdbclassification","title":"JavaneseIMDBClassification","text":"<p>Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets.</p> <p>Dataset: <code>mteb/JavaneseIMDBClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jav Reviews, Written human-annotated found Citation <pre><code>@inproceedings{wongso2021causal,\n  author = {Wongso, Wilson and Setiawan, David Samuel and Suhartono, Derwin},\n  booktitle = {2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},\n  organization = {IEEE},\n  pages = {1--7},\n  title = {Causal and Masked Language Modeling of Javanese Language using Transformer-based Architectures},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#javaneseimdbclassificationv2","title":"JavaneseIMDBClassification.v2","text":"<p>Large Movie Review Dataset translated to Javanese. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/javanese_imdb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jav Reviews, Written human-annotated found Citation <pre><code>@inproceedings{wongso2021causal,\n  author = {Wongso, Wilson and Setiawan, David Samuel and Suhartono, Derwin},\n  booktitle = {2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS)},\n  organization = {IEEE},\n  pages = {1--7},\n  title = {Causal and Masked Language Modeling of Javanese Language using Transformer-based Architectures},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#klue-tc","title":"KLUE-TC","text":"<p>Topic classification dataset of human-annotated news headlines. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#klue-tcv2","title":"KLUE-TC.v2","text":"<p>Topic classification dataset of human-annotated news headlines. Part of the Korean Language Understanding Evaluation (KLUE). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/klue_tc</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kannadanewsclassification","title":"KannadaNewsClassification","text":"<p>The Kannada news dataset contains only the headlines of news article in three categories: Entertainment, Tech, and Sports. The data set contains around 6300 news article headlines which are collected from Kannada news websites. The data set has been cleaned and contains train and test set using which can be used to benchmark topic classification models in Kannada.</p> <p>Dataset: <code>Akash190104/kannada_news_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kannadanewsclassificationv2","title":"KannadaNewsClassification.v2","text":"<p>The Kannada news dataset contains only the headlines of news article in three categories: Entertainment, Tech, and Sports. The data set contains around 6300 news article headlines which are collected from Kannada news websites. The data set has been cleaned and contains train and test set using which can be used to benchmark topic classification models in Kannada. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kannada_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kinopoiskclassification","title":"KinopoiskClassification","text":"<p>Kinopoisk review sentiment classification</p> <p>Dataset: <code>ai-forever/kinopoisk-sentiment-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@article{blinov2013research,\n  author = {Blinov, PD and Klekovkina, Maria and Kotelnikov, Eugeny and Pestov, Oleg},\n  journal = {Computational Linguistics and Intellectual Technologies},\n  number = {12},\n  pages = {48--58},\n  title = {Research of lexical approach and machine learning methods for sentiment analysis},\n  volume = {2},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korfin","title":"KorFin","text":"<p>The KorFin-ASC is an extension of KorFin-ABSA, which is a financial sentiment analysis dataset including 8818 samples with (aspect, polarity) pairs annotated. The samples were collected from KLUE-TC and analyst reports from Naver Finance.</p> <p>Dataset: <code>amphora/korfin-asc</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Financial, News, Written expert-annotated found Citation <pre><code>@article{son2023removing,\n  author = {Son, Guijin and Lee, Hanwool and Kang, Nahyeon and Hahm, Moonjeong},\n  journal = {arXiv preprint arXiv:2301.03136},\n  title = {Removing Non-Stationary Knowledge From Pre-Trained Language Models for Entity-Level Sentiment Classification in Finance},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korhateclassification","title":"KorHateClassification","text":"<p>The dataset was created to provide the first human-labeled Korean corpus for toxic speech detection from a Korean online entertainment news aggregator. Recently, two young Korean celebrities suffered from a series of tragic incidents that led to two major Korean web portals to close the comments section on their platform. However, this only serves as a temporary solution, and the fundamental issue has not been solved yet. This dataset hopes to improve Korean hate speech detection. Annotation was performed by 32 annotators, consisting of 29 annotators from the crowdsourcing platform DeepNatural AI and three NLP researchers.</p> <p>Dataset: <code>mteb/KorHateClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{moon2020beep,\n  archiveprefix = {arXiv},\n  author = {Jihyung Moon and Won Ik Cho and Junbum Lee},\n  eprint = {2005.12503},\n  primaryclass = {cs.CL},\n  title = {BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korhateclassificationv2","title":"KorHateClassification.v2","text":"<p>The dataset was created to provide the first human-labeled Korean corpus for toxic speech detection from a Korean online entertainment news aggregator. Recently, two young Korean celebrities suffered from a series of tragic incidents that led to two major Korean web portals to close the comments section on their platform. However, this only serves as a temporary solution, and the fundamental issue has not been solved yet. This dataset hopes to improve Korean hate speech detection. Annotation was performed by 32 annotators, consisting of 29 annotators from the crowdsourcing platform DeepNatural AI and three NLP researchers. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kor_hate</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{moon2020beep,\n  archiveprefix = {arXiv},\n  author = {Jihyung Moon and Won Ik Cho and Junbum Lee},\n  eprint = {2005.12503},\n  primaryclass = {cs.CL},\n  title = {BEEP! Korean Corpus of Online News Comments for Toxic Speech Detection},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korsarcasmclassification","title":"KorSarcasmClassification","text":"<p>The Korean Sarcasm Dataset was created to detect sarcasm in text, which can significantly alter the original meaning of a sentence. 9319 tweets were collected from Twitter and labeled for sarcasm or not_sarcasm. These tweets were gathered by querying for: irony sarcastic, and sarcasm. The dataset was created by gathering HTML data from Twitter. Queries for hashtags that include sarcasm and variants of it were used to return tweets. It was preprocessed by removing the keyword hashtag, urls and mentions of the user to preserve anonymity.</p> <p>Dataset: <code>mteb/KorSarcasmClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{kim2019kocasm,\n  author = {Kim, Jiwon and Cho, Won Ik},\n  howpublished = {https://github.com/SpellOnYou/korean-sarcasm},\n  journal = {GitHub repository},\n  publisher = {GitHub},\n  title = {Kocasm: Korean Automatic Sarcasm Detection},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#korsarcasmclassificationv2","title":"KorSarcasmClassification.v2","text":"<p>The Korean Sarcasm Dataset was created to detect sarcasm in text, which can significantly alter the original meaning of a sentence. 9319 tweets were collected from Twitter and labeled for sarcasm or not_sarcasm. These tweets were gathered by querying for: irony sarcastic, and sarcasm. The dataset was created by gathering HTML data from Twitter. Queries for hashtags that include sarcasm and variants of it were used to return tweets. It was preprocessed by removing the keyword hashtag, urls and mentions of the user to preserve anonymity. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kor_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@misc{kim2019kocasm,\n  author = {Kim, Jiwon and Cho, Won Ik},\n  howpublished = {https://github.com/SpellOnYou/korean-sarcasm},\n  journal = {GitHub repository},\n  publisher = {GitHub},\n  title = {Kocasm: Korean Automatic Sarcasm Detection},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kurdishsentimentclassification","title":"KurdishSentimentClassification","text":"<p>Kurdish Sentiment Dataset</p> <p>Dataset: <code>mteb/KurdishSentimentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kur Web, Written derived found Citation <pre><code>@article{badawi2024kurdisent,\n  author = {Badawi, Soran and Kazemi, Arefeh and Rezaie, Vali},\n  doi = {10.1007/s10579-023-09716-6},\n  journal = {Language Resources and Evaluation},\n  month = {01},\n  pages = {1-20},\n  title = {KurdiSent: a corpus for kurdish sentiment analysis},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#kurdishsentimentclassificationv2","title":"KurdishSentimentClassification.v2","text":"<p>Kurdish Sentiment Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/kurdish_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kur Web, Written derived found Citation <pre><code>@article{badawi2024kurdisent,\n  author = {Badawi, Soran and Kazemi, Arefeh and Rezaie, Vali},\n  doi = {10.1007/s10579-023-09716-6},\n  journal = {Language Resources and Evaluation},\n  month = {01},\n  pages = {1-20},\n  title = {KurdiSent: a corpus for kurdish sentiment analysis},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#languageclassification","title":"LanguageClassification","text":"<p>A language identification dataset for 20 languages.</p> <p>Dataset: <code>papluca/language-identification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, bul, cmn, deu, ell, ... (20) Fiction, Government, Non-fiction, Reviews, Web, ... (6) derived found Citation <pre><code>@inproceedings{conneau2018xnli,\n  author = {Conneau, Alexis\nand Rinott, Ruty\nand Lample, Guillaume\nand Williams, Adina\nand Bowman, Samuel R.\nand Schwenk, Holger\nand Stoyanov, Veselin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#lccsentimentclassification","title":"LccSentimentClassification","text":"<p>The leipzig corpora collection, annotated for sentiment</p> <p>Dataset: <code>DDSC/lcc</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{quasthoff-etal-2006-corpus,\n  address = {Genoa, Italy},\n  author = {Quasthoff, Uwe  and\nRichter, Matthias  and\nBiemann, Christian},\n  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nGangemi, Aldo  and\nMaegaard, Bente  and\nMariani, Joseph  and\nOdijk, Jan  and\nTapias, Daniel},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {Corpus Portal for Search in Monolingual Corpora},\n  url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/641_pdf.pdf},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsbenefitslegalbenchclassification","title":"LearnedHandsBenefitsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal post discusses public benefits and social services that people can get from the government, like for food, disability, old age, housing, medical help, unemployment, child care, or other social needs.</p> <p>Dataset: <code>mteb/LearnedHandsBenefitsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsbusinesslegalbenchclassification","title":"LearnedHandsBusinessLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal question discusses issues faced by people who run small businesses or nonprofits, including around incorporation, licenses, taxes, regulations, and other concerns. It also includes options when there are disasters, bankruptcies, or other problems.</p> <p>Dataset: <code>mteb/LearnedHandsBusinessLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsconsumerlegalbenchclassification","title":"LearnedHandsConsumerLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues people face regarding money, insurance, consumer goods and contracts, taxes, and small claims about quality of service.</p> <p>Dataset: <code>mteb/LearnedHandsConsumerLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandscourtslegalbenchclassification","title":"LearnedHandsCourtsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses the logistics of how a person can interact with a lawyer or the court system. It applies to situations about procedure, rules, how to file lawsuits, how to hire lawyers, how to represent oneself, and other practical matters about dealing with these systems.</p> <p>Dataset: <code>mteb/LearnedHandsCourtsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandscrimelegalbenchclassification","title":"LearnedHandsCrimeLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues in the criminal system including when people are charged with crimes, go to a criminal trial, go to prison, or are a victim of a crime.</p> <p>Dataset: <code>mteb/LearnedHandsCrimeLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsdivorcelegalbenchclassification","title":"LearnedHandsDivorceLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues around filing for divorce, separation, or annulment, getting spousal support, splitting money and property, and following the court processes.</p> <p>Dataset: <code>mteb/LearnedHandsDivorceLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsdomesticviolencelegalbenchclassification","title":"LearnedHandsDomesticViolenceLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses dealing with domestic violence and abuse, including getting protective orders, enforcing them, understanding abuse, reporting abuse, and getting resources and status if there is abuse.</p> <p>Dataset: <code>mteb/LearnedHandsDomesticViolenceLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandseducationlegalbenchclassification","title":"LearnedHandsEducationLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues around school, including accommodations for special needs, discrimination, student debt, discipline, and other issues in education.</p> <p>Dataset: <code>mteb/LearnedHandsEducationLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsemploymentlegalbenchclassification","title":"LearnedHandsEmploymentLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues related to working at a job, including discrimination and harassment, worker's compensation, workers rights, unions, getting paid, pensions, being fired, and more.</p> <p>Dataset: <code>mteb/LearnedHandsEmploymentLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsestateslegalbenchclassification","title":"LearnedHandsEstatesLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses planning for end-of-life, possible incapacitation, and other special circumstances that would prevent a person from making decisions about their own well-being, finances, and property. This includes issues around wills, powers of attorney, advance directives, trusts, guardianships, conservatorships, and other estate issues that people and families deal with.</p> <p>Dataset: <code>mteb/LearnedHandsEstatesLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsfamilylegalbenchclassification","title":"LearnedHandsFamilyLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues that arise within a family, like divorce, adoption, name change, guardianship, domestic violence, child custody, and other issues.</p> <p>Dataset: <code>mteb/LearnedHandsFamilyLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandshealthlegalbenchclassification","title":"LearnedHandsHealthLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues with accessing health services, paying for medical care, getting public benefits for health care, protecting one's rights in medical settings, and other issues related to health.</p> <p>Dataset: <code>mteb/LearnedHandsHealthLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandshousinglegalbenchclassification","title":"LearnedHandsHousingLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses issues with paying your rent or mortgage, landlord-tenant issues, housing subsidies and public housing, eviction, and other problems with your apartment, mobile home, or house.</p> <p>Dataset: <code>mteb/LearnedHandsHousingLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandsimmigrationlegalbenchclassification","title":"LearnedHandsImmigrationLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's post discusses visas, asylum, green cards, citizenship, migrant work and benefits, and other issues faced by people who are not full citizens in the US.</p> <p>Dataset: <code>mteb/LearnedHandsImmigrationLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandstortslegalbenchclassification","title":"LearnedHandsTortsLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal question discusses problems that one person has with another person (or animal), like when there is a car accident, a dog bite, bullying or possible harassment, or neighbors treating each other badly.</p> <p>Dataset: <code>mteb/LearnedHandsTortsLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#learnedhandstrafficlegalbenchclassification","title":"LearnedHandsTrafficLegalBenchClassification","text":"<p>This is a binary classification task in which the model must determine if a user's legal post discusses problems with traffic and parking tickets, fees, driver's licenses, and other issues experienced with the traffic system. It also concerns issues with car accidents and injuries, cars' quality, repairs, purchases, and other contracts.</p> <p>Dataset: <code>mteb/LearnedHandsTrafficLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@dataset{learned_hands,\n  author = {{Suffolk University Law School} and {Stanford Legal Design Lab}},\n  note = {The LearnedHands dataset is licensed under CC BY-NC-SA 4.0},\n  title = {LearnedHands Dataset},\n  url = {https://spot.suffolklitlab.org/data/#learnedhands},\n  urldate = {2022-05-21},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#legalreasoningcausalitylegalbenchclassification","title":"LegalReasoningCausalityLegalBenchClassification","text":"<p>Given an excerpt from a district court opinion, classify if it relies on statistical evidence in its reasoning.</p> <p>Dataset: <code>mteb/LegalReasoningCausalityLegalBenchClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#legalreasoningcausalitylegalbenchclassificationv2","title":"LegalReasoningCausalityLegalBenchClassification.v2","text":"<p>Given an excerpt from a district court opinion, classify if it relies on statistical evidence in its reasoning. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/legal_reasoning_causality_legal_bench</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#maudlegalbenchclassification","title":"MAUDLegalBenchClassification","text":"<p>This task was constructed from the MAUD dataset, which consists of over 47,000 labels across 152 merger agreements annotated to identify 92 questions in each agreement used by the 2021 American Bar Association (ABA) Public Target Deal Points Study. Each dataset is formatted as a series of multiple-choice questions, where given a segment of the merger agreement and a Deal Point question, the model is to choose the answer that best characterizes the agreement as response. This is a combination of all 34 of the MAUD Legal Bench datasets: 1. MAUD Ability To Consummate Concept Is Subject To MAE Carveouts: Given an excerpt from a merger agreement and the task is to answer: is the \u201cability to consummate\u201d concept subject to Material Adverse Effect (MAE) carveouts? amongst the multiple choice options. 2. MAUD Accuracy Of Fundamental Target RWS Bringdown Standard: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 3. MAUD Accuracy Of Target Capitalization RW Outstanding Shares Bringdown Standard Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 4. MAUD Accuracy Of Target General RW Bringdown Timing Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 5. MAUD Additional Matching Rights Period For Modifications Cor: Given an excerpt from a merger agreement and the task is to answer: how long is the additional matching rights period for modifications in case the board changes its recommendation, amongst the multiple choice options. 6. MAUD Application Of Buyer Consent Requirement Negative Interim Covenant: Given an excerpt from a merger agreement and the task is to answer: what negative covenants does the requirement of Buyer consent apply to, amongst the multiple choice options. 7. MAUD Buyer Consent Requirement Ordinary Course: Given an excerpt from a merger agreement and the task is to answer: in case the Buyer's consent for the acquired company's ordinary business operations is required, are there any limitations on the Buyer's right to condition, withhold, or delay their consent, amongst the multiple choice options. 8. MAUD Change In Law Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in law that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 9. MAUD Changes In GAAP Or Other Accounting Principles Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in GAAP or other accounting principles that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 10. MAUD COR Permitted In Response To Intervening Event: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted in response to an intervening event, amongst the multiple choice options. 11. MAUD COR Permitted With Board Fiduciary Determination Only: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted as long as the board determines that such change is required to fulfill its fiduciary obligations, amongst the multiple choice options. 12. MAUD COR Standard Intervening Event: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in response to an intervening event, amongst the multiple choice options. 13. MAUD COR Standard Superior Offer: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in connection with a superior offer, amongst the multiple choice options. 14. MAUD Definition Contains Knowledge Requirement Answer: Given an excerpt from a merger agreement and the task is to answer: what is the knowledge requirement in the definition of \u201cIntervening Event\u201d, amongst the multiple choice options. 15. MAUD Definition Includes Asset Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of asset deals, amongst the multiple choice options. 16. MAUD Definition Includes Stock Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of stock deals, amongst the multiple choice options. 17. MAUD Fiduciary Exception Board Determination Standard: Given an excerpt from a merger agreement and the task is to answer: under what circumstances could the Board take actions on a different acquisition proposal notwithstanding the no-shop provision, amongst the multiple choice options. 18. MAUD Fiduciary Exception Board Determination Trigger No Shop: Given an excerpt from a merger agreement and the task is to answer: what type of offer could the Board take actions on notwithstanding the no-shop provision, amongst the multiple choice options. 19. MAUD Financial Point Of View Is The Sole Consideration: Given an excerpt from a merger agreement and the task is to answer: is \u201cfinancial point of view\u201d the sole consideration when determining whether an offer is superior, amongst the multiple choice options. 20. MAUD FLS MAE Standard: Given an excerpt from a merger agreement and the task is to answer: what is the Forward Looking Standard (FLS) with respect to Material Adverse Effect (MAE), amongst the multiple choice options. 21. MAUD General Economic and Financial Conditions Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes caused by general economic and financial conditions that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 22. MAUD Includes Consistent With Past Practice: Given an excerpt from a merger agreement and the task is to answer: does the wording of the Efforts Covenant clause include \u201cconsistent with past practice\u201d, amongst the multiple choice options. 23. MAUD Initial Matching Rights Period COR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in case the board changes its recommendation, amongst the multiple choice options. 24. MAUD Initial Matching Rights Period FTR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in connection with the Fiduciary Termination Right (FTR), amongst the multiple choice options. 25. MAUDInterveningEventRequiredToOccurAfterSigningAnswer: Given an excerpt from a merger agreement and the task is to answer: is an \u201cIntervening Event\u201d required to occur after signing, amongst the multiple choice options. 26. MAUD Knowledge Definition: Given an excerpt from a merger agreement and the task is to answer: what counts as Knowledge, amongst the multiple choice options. 27. MAUDLiabilityStandardForNoShopBreachByTargetNonDORepresentatives: Given an excerpt from a merger agreement and the task is to answer: what is the liability standard for no-shop breach by Target Non-D&amp;O Representatives, amongst the multiple choice options. 28. MAUD Ordinary Course Efforts Standard: Given an excerpt from a merger agreement and the task is to answer: what is the efforts standard, amongst the multiple choice options. 29. MAUD Pandemic Or Other Public Health Event Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do pandemics or other public health events have to have disproportionate impact to qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 30. MAUD Pandemic Or Other Public Health Event Specific Reference To Pandemic Related Governmental Responses Or Measures: Given an excerpt from a merger agreement and the task is to answer: is there specific reference to pandemic-related governmental responses or measures in the clause that qualifies pandemics or other public health events for Material Adverse Effect (MAE), amongst the multiple choice options. 31. MAUD Relational Language MAE Applies To: Given an excerpt from a merger agreement and the task is to answer: what carveouts pertaining to Material Adverse Effect (MAE) does the relational language apply to?, amongst the multiple choice options. 32. MAUD Specific Performance: Given an excerpt from a merger agreement and the task is to answer: what is the wording of the Specific Performance clause regarding the parties' entitlement in the event of a contractual breach, amongst the multiple choice options. 33. MAUD Tail Period Length: Given an excerpt from a merger agreement and the task is to answer: how long is the Tail Period, amongst the multiple choice options. 34. MAUD Type Of Consideration: Given an excerpt from a merger agreement and the task is to answer: what type of consideration is specified in this agreement, amongst the multiple choice options.</p> <p>Dataset: <code>mteb/MAUDLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#maudlegalbenchclassificationv2","title":"MAUDLegalBenchClassification.v2","text":"<p>This task was constructed from the MAUD dataset, which consists of over 47,000 labels across 152 merger agreements annotated to identify 92 questions in each agreement used by the 2021 American Bar Association (ABA) Public Target Deal Points Study. Each dataset is formatted as a series of multiple-choice questions, where given a segment of the merger agreement and a Deal Point question, the model is to choose the answer that best characterizes the agreement as response. This is a combination of all 34 of the MAUD Legal Bench datasets: 1. MAUD Ability To Consummate Concept Is Subject To MAE Carveouts: Given an excerpt from a merger agreement and the task is to answer: is the \u201cability to consummate\u201d concept subject to Material Adverse Effect (MAE) carveouts? amongst the multiple choice options. 2. MAUD Accuracy Of Fundamental Target RWS Bringdown Standard: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 3. MAUD Accuracy Of Target Capitalization RW Outstanding Shares Bringdown Standard Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 4. MAUD Accuracy Of Target General RW Bringdown Timing Answer: Given an excerpt from a merger agreement and the task is to answer: how accurate must the fundamental representations and warranties be according to the bring down provision, amongst the multiple choice options. 5. MAUD Additional Matching Rights Period For Modifications Cor: Given an excerpt from a merger agreement and the task is to answer: how long is the additional matching rights period for modifications in case the board changes its recommendation, amongst the multiple choice options. 6. MAUD Application Of Buyer Consent Requirement Negative Interim Covenant: Given an excerpt from a merger agreement and the task is to answer: what negative covenants does the requirement of Buyer consent apply to, amongst the multiple choice options. 7. MAUD Buyer Consent Requirement Ordinary Course: Given an excerpt from a merger agreement and the task is to answer: in case the Buyer's consent for the acquired company's ordinary business operations is required, are there any limitations on the Buyer's right to condition, withhold, or delay their consent, amongst the multiple choice options. 8. MAUD Change In Law Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in law that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 9. MAUD Changes In GAAP Or Other Accounting Principles Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes in GAAP or other accounting principles that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 10. MAUD COR Permitted In Response To Intervening Event: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted in response to an intervening event, amongst the multiple choice options. 11. MAUD COR Permitted With Board Fiduciary Determination Only: Given an excerpt from a merger agreement and the task is to answer: is Change of Recommendation permitted as long as the board determines that such change is required to fulfill its fiduciary obligations, amongst the multiple choice options. 12. MAUD COR Standard Intervening Event: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in response to an intervening event, amongst the multiple choice options. 13. MAUD COR Standard Superior Offer: Given an excerpt from a merger agreement and the task is to answer: what standard should the board follow when determining whether to change its recommendation in connection with a superior offer, amongst the multiple choice options. 14. MAUD Definition Contains Knowledge Requirement Answer: Given an excerpt from a merger agreement and the task is to answer: what is the knowledge requirement in the definition of \u201cIntervening Event\u201d, amongst the multiple choice options. 15. MAUD Definition Includes Asset Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of asset deals, amongst the multiple choice options. 16. MAUD Definition Includes Stock Deals: Given an excerpt from a merger agreement and the task is to answer: what qualifies as a superior offer in terms of stock deals, amongst the multiple choice options. 17. MAUD Fiduciary Exception Board Determination Standard: Given an excerpt from a merger agreement and the task is to answer: under what circumstances could the Board take actions on a different acquisition proposal notwithstanding the no-shop provision, amongst the multiple choice options. 18. MAUD Fiduciary Exception Board Determination Trigger No Shop: Given an excerpt from a merger agreement and the task is to answer: what type of offer could the Board take actions on notwithstanding the no-shop provision, amongst the multiple choice options. 19. MAUD Financial Point Of View Is The Sole Consideration: Given an excerpt from a merger agreement and the task is to answer: is \u201cfinancial point of view\u201d the sole consideration when determining whether an offer is superior, amongst the multiple choice options. 20. MAUD FLS MAE Standard: Given an excerpt from a merger agreement and the task is to answer: what is the Forward Looking Standard (FLS) with respect to Material Adverse Effect (MAE), amongst the multiple choice options. 21. MAUD General Economic and Financial Conditions Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do changes caused by general economic and financial conditions that have disproportionate impact qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 22. MAUD Includes Consistent With Past Practice: Given an excerpt from a merger agreement and the task is to answer: does the wording of the Efforts Covenant clause include \u201cconsistent with past practice\u201d, amongst the multiple choice options. 23. MAUD Initial Matching Rights Period COR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in case the board changes its recommendation, amongst the multiple choice options. 24. MAUD Initial Matching Rights Period FTR: Given an excerpt from a merger agreement and the task is to answer: how long is the initial matching rights period in connection with the Fiduciary Termination Right (FTR), amongst the multiple choice options. 25. MAUDInterveningEventRequiredToOccurAfterSigningAnswer: Given an excerpt from a merger agreement and the task is to answer: is an \u201cIntervening Event\u201d required to occur after signing, amongst the multiple choice options. 26. MAUD Knowledge Definition: Given an excerpt from a merger agreement and the task is to answer: what counts as Knowledge, amongst the multiple choice options. 27. MAUDLiabilityStandardForNoShopBreachByTargetNonDORepresentatives: Given an excerpt from a merger agreement and the task is to answer: what is the liability standard for no-shop breach by Target Non-D&amp;O Representatives, amongst the multiple choice options. 28. MAUD Ordinary Course Efforts Standard: Given an excerpt from a merger agreement and the task is to answer: what is the efforts standard, amongst the multiple choice options. 29. MAUD Pandemic Or Other Public Health Event Subject To Disproportionate Impact Modifier: Given an excerpt from a merger agreement and the task is to answer: do pandemics or other public health events have to have disproportionate impact to qualify for Material Adverse Effect (MAE), amongst the multiple choice options. 30. MAUD Pandemic Or Other Public Health Event Specific Reference To Pandemic Related Governmental Responses Or Measures: Given an excerpt from a merger agreement and the task is to answer: is there specific reference to pandemic-related governmental responses or measures in the clause that qualifies pandemics or other public health events for Material Adverse Effect (MAE), amongst the multiple choice options. 31. MAUD Relational Language MAE Applies To: Given an excerpt from a merger agreement and the task is to answer: what carveouts pertaining to Material Adverse Effect (MAE) does the relational language apply to?, amongst the multiple choice options. 32. MAUD Specific Performance: Given an excerpt from a merger agreement and the task is to answer: what is the wording of the Specific Performance clause regarding the parties' entitlement in the event of a contractual breach, amongst the multiple choice options. 33. MAUD Tail Period Length: Given an excerpt from a merger agreement and the task is to answer: how long is the Tail Period, amongst the multiple choice options. 34. MAUD Type Of Consideration: Given an excerpt from a merger agreement and the task is to answer: what type of consideration is specified in this agreement, amongst the multiple choice options. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/maud_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopdomainclassification","title":"MTOPDomainClassification","text":"<p>MTOP: Multilingual Task-Oriented Semantic Parsing</p> <p>Dataset: <code>mteb/MTOPDomainClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, fra, hin, spa, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@inproceedings{li-etal-2021-mtop,\n  address = {Online},\n  author = {Li, Haoran  and\nArora, Abhinav  and\nChen, Shuohui  and\nGupta, Anchit  and\nGupta, Sonal  and\nMehdad, Yashar},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  doi = {10.18653/v1/2021.eacl-main.257},\n  editor = {Merlo, Paola  and\nTiedemann, Jorg  and\nTsarfaty, Reut},\n  month = apr,\n  pages = {2950--2962},\n  publisher = {Association for Computational Linguistics},\n  title = {{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark},\n  url = {https://aclanthology.org/2021.eacl-main.257},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopdomainvnclassification","title":"MTOPDomainVNClassification","text":"<p>A translated dataset from MTOP: Multilingual Task-Oriented Semantic Parsing The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/mtop-domain-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken, Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopintentclassification","title":"MTOPIntentClassification","text":"<p>MTOP: Multilingual Task-Oriented Semantic Parsing</p> <p>Dataset: <code>mteb/MTOPIntentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, eng, fra, hin, spa, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@inproceedings{li-etal-2021-mtop,\n  address = {Online},\n  author = {Li, Haoran  and\nArora, Abhinav  and\nChen, Shuohui  and\nGupta, Anchit  and\nGupta, Sonal  and\nMehdad, Yashar},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  doi = {10.18653/v1/2021.eacl-main.257},\n  editor = {Merlo, Paola  and\nTiedemann, Jorg  and\nTsarfaty, Reut},\n  month = apr,\n  pages = {2950--2962},\n  publisher = {Association for Computational Linguistics},\n  title = {{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark},\n  url = {https://aclanthology.org/2021.eacl-main.257},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#mtopintentvnclassification","title":"MTOPIntentVNClassification","text":"<p>A translated dataset from MTOP: Multilingual Task-Oriented Semantic Parsing The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/mtop-intent-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken, Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#macedoniantweetsentimentclassification","title":"MacedonianTweetSentimentClassification","text":"<p>An Macedonian dataset for tweet sentiment classification.</p> <p>Dataset: <code>mteb/MacedonianTweetSentimentClassification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mkd Social, Written human-annotated found Citation <pre><code>@inproceedings{jovanoski-etal-2015-sentiment,\n  address = {Hissar, Bulgaria},\n  author = {Jovanoski, Dame  and\nPachovski, Veno  and\nNakov, Preslav},\n  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},\n  editor = {Mitkov, Ruslan  and\nAngelova, Galia  and\nBontcheva, Kalina},\n  month = sep,\n  pages = {249--257},\n  publisher = {INCOMA Ltd. Shoumen, BULGARIA},\n  title = {Sentiment Analysis in {T}witter for {M}acedonian},\n  url = {https://aclanthology.org/R15-1034},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#macedoniantweetsentimentclassificationv2","title":"MacedonianTweetSentimentClassification.v2","text":"<p>An Macedonian dataset for tweet sentiment classification. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/macedonian_tweet_sentiment</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mkd Social, Written human-annotated found Citation <pre><code>@inproceedings{jovanoski-etal-2015-sentiment,\n  address = {Hissar, Bulgaria},\n  author = {Jovanoski, Dame  and\nPachovski, Veno  and\nNakov, Preslav},\n  booktitle = {Proceedings of the International Conference Recent Advances in Natural Language Processing},\n  editor = {Mitkov, Ruslan  and\nAngelova, Galia  and\nBontcheva, Kalina},\n  month = sep,\n  pages = {249--257},\n  publisher = {INCOMA Ltd. Shoumen, BULGARIA},\n  title = {Sentiment Analysis in {T}witter for {M}acedonian},\n  url = {https://aclanthology.org/R15-1034},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#malayalamnewsclassification","title":"MalayalamNewsClassification","text":"<p>A Malayalam dataset for 3-class classification of Malayalam news articles</p> <p>Dataset: <code>mlexplorer008/malayalam_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mal News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#malayalamnewsclassificationv2","title":"MalayalamNewsClassification.v2","text":"<p>A Malayalam dataset for 3-class classification of Malayalam news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/malayalam_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mal News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#marathinewsclassification","title":"MarathiNewsClassification","text":"<p>A Marathi dataset for 3-class classification of Marathi news articles</p> <p>Dataset: <code>mlexplorer008/marathi_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 mar News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#marathinewsclassificationv2","title":"MarathiNewsClassification.v2","text":"<p>A Marathi dataset for 3-class classification of Marathi news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/marathi_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 mar News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#masakhanewsclassification","title":"MasakhaNEWSClassification","text":"<p>MasakhaNEWS is the largest publicly available dataset for news topic classification in 16 languages widely spoken in Africa. The train/validation/test sets are available for all the 16 languages.</p> <p>Dataset: <code>mteb/masakhanews</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy amh, eng, fra, hau, ibo, ... (16) News, Written expert-annotated found Citation <pre><code>@misc{adelani2023masakhanews,\n  archiveprefix = {arXiv},\n  author = {David Ifeoluwa Adelani and Marek Masiak and Israel Abebe Azime and Jesujoba Alabi and Atnafu Lambebo Tonja and Christine Mwase and Odunayo Ogundepo and Bonaventure F. P. Dossou and Akintunde Oladipo and Doreen Nixdorf and Chris Chinenye Emezue and sana al-azzawi and Blessing Sibanda and Davis David and Lolwethu Ndolela and Jonathan Mukiibi and Tunde Ajayi and Tatiana Moteu and Brian Odhiambo and Abraham Owodunni and Nnaemeka Obiefuna and Muhidin Mohamed and Shamsuddeen Hassan Muhammad and Teshome Mulugeta Ababu and Saheed Abdullahi Salahudeen and Mesay Gemeda Yigezu and Tajuddeen Gwadabe and Idris Abdulmumin and Mahlet Taye and Oluwabusayo Awoyomi and Iyanuoluwa Shode and Tolulope Adelani and Habiba Abdulganiyu and Abdul-Hakeem Omotayo and Adetola Adeeko and Abeeb Afolabi and Anuoluwapo Aremu and Olanrewaju Samuel and Clemencia Siro and Wangari Kimotho and Onyekachi Ogbu and Chinedu Mbonu and Chiamaka Chukwuneke and Samuel Fanijo and Jessica Ojo and Oyinkansola Awosan and Tadesse Kebede and Toadoum Sari Sakayo and Pamela Nyatsine and Freedmore Sidume and Oreen Yousuf and Mardiyyah Oduwole and Tshinu Tshinu and Ussen Kimanuka and Thina Diko and Siyanda Nxakama and Sinodos Nigusse and Abdulmejid Johar and Shafie Mohamed and Fuad Mire Hassan and Moges Ahmed Mehamed and Evrard Ngabire and Jules Jules and Ivan Ssenkungu and Pontus Stenetorp},\n  eprint = {2304.09972},\n  primaryclass = {cs.CL},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massiveintentclassification","title":"MassiveIntentClassification","text":"<p>MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</p> <p>Dataset: <code>mteb/amazon_massive_intent</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, amh, ara, aze, ben, ... (50) Spoken human-annotated human-translated and localized Citation <pre><code>@misc{fitzgerald2022massive,\n  archiveprefix = {arXiv},\n  author = {Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},\n  eprint = {2204.08582},\n  primaryclass = {cs.CL},\n  title = {MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massiveintentvnclassification","title":"MassiveIntentVNClassification","text":"<p>A translated dataset from MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-massive-intent-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massivescenarioclassification","title":"MassiveScenarioClassification","text":"<p>MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages</p> <p>Dataset: <code>mteb/amazon_massive_scenario</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, amh, ara, aze, ben, ... (50) Spoken human-annotated human-translated and localized Citation <pre><code>@misc{fitzgerald2022massive,\n  archiveprefix = {arXiv},\n  author = {Jack FitzGerald and Christopher Hench and Charith Peris and Scott Mackie and Kay Rottmann and Ana Sanchez and Aaron Nash and Liam Urbach and Vishesh Kakarala and Richa Singh and Swetha Ranganath and Laurie Crist and Misha Britan and Wouter Leeuwis and Gokhan Tur and Prem Natarajan},\n  eprint = {2204.08582},\n  primaryclass = {cs.CL},\n  title = {MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#massivescenariovnclassification","title":"MassiveScenarioVNClassification","text":"<p>A translated dataset from MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/amazon-massive-scenario-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Spoken derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moroco","title":"Moroco","text":"<p>The Moldavian and Romanian Dialectal Corpus. The MOROCO data set contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: (0) culture, (1) finance, (2) politics, (3) science, (4) sports, (5) tech</p> <p>Dataset: <code>mteb/Moroco</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron News, Written derived found Citation <pre><code>@inproceedings{Butnaru-ACL-2019,\n  author = {Andrei M. Butnaru and Radu Tudor Ionescu},\n  booktitle = {Proceedings of ACL},\n  pages = {688--698},\n  title = {{MOROCO: The Moldavian and Romanian Dialectal Corpus}},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#morocov2","title":"Moroco.v2","text":"<p>The Moldavian and Romanian Dialectal Corpus. The MOROCO data set contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: (0) culture, (1) finance, (2) politics, (3) science, (4) sports, (5) tech This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/moroco</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron News, Written derived found Citation <pre><code>@inproceedings{Butnaru-ACL-2019,\n  author = {Andrei M. Butnaru and Radu Tudor Ionescu},\n  booktitle = {Proceedings of ACL},\n  pages = {688--698},\n  title = {{MOROCO: The Moldavian and Romanian Dialectal Corpus}},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moviereviewsentimentclassification","title":"MovieReviewSentimentClassification","text":"<p>The Allocin\u00e9 dataset is a French-language dataset for sentiment analysis that contains movie reviews produced by the online community of the Allocin\u00e9.fr website.</p> <p>Dataset: <code>tblard/allocine</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>@software{blard2020,\n  author = {Th\u00e9ophile Blard},\n  title = {French sentiment analysis with BERT},\n  url = {https://github.com/TheophileBlard/french-sentiment-analysis-with-bert},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#moviereviewsentimentclassificationv2","title":"MovieReviewSentimentClassification.v2","text":"<p>The Allocin\u00e9 dataset is a French-language dataset for sentiment analysis that contains movie reviews produced by the online community of the Allocin\u00e9.fr website. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/movie_review_sentiment</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fra Reviews, Written derived found Citation <pre><code>@software{blard2020,\n  author = {Th\u00e9ophile Blard},\n  title = {French sentiment analysis with BERT},\n  url = {https://github.com/TheophileBlard/french-sentiment-analysis-with-bert},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multihateclassification","title":"MultiHateClassification","text":"<p>Hate speech detection dataset with binary (hateful vs non-hateful) labels. Includes 25+ distinct types of hate and challenging non-hate, and 11 languages.</p> <p>Dataset: <code>mteb/multi-hatecheck</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, cmn, deu, eng, fra, ... (11) Constructed, Written expert-annotated created Citation <pre><code>@inproceedings{rottger-etal-2021-hatecheck,\n  address = {Online},\n  author = {R{\\\"o}ttger, Paul  and\nVidgen, Bertie  and\nNguyen, Dong  and\nWaseem, Zeerak  and\nMargetts, Helen  and\nPierrehumbert, Janet},\n  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2021.acl-long.4},\n  editor = {Zong, Chengqing  and\nXia, Fei  and\nLi, Wenjie  and\nNavigli, Roberto},\n  month = aug,\n  pages = {41--58},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}ate{C}heck: Functional Tests for Hate Speech Detection Models},\n  url = {https://aclanthology.org/2021.acl-long.4},\n  year = {2021},\n}\n\n@inproceedings{rottger-etal-2022-multilingual,\n  address = {Seattle, Washington (Hybrid)},\n  author = {R{\\\"o}ttger, Paul  and\nSeelawi, Haitham  and\nNozza, Debora  and\nTalat, Zeerak  and\nVidgen, Bertie},\n  booktitle = {Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)},\n  doi = {10.18653/v1/2022.woah-1.15},\n  editor = {Narang, Kanika  and\nMostafazadeh Davani, Aida  and\nMathias, Lambert  and\nVidgen, Bertie  and\nTalat, Zeerak},\n  month = jul,\n  pages = {154--169},\n  publisher = {Association for Computational Linguistics},\n  title = {Multilingual {H}ate{C}heck: Functional Tests for Multilingual Hate Speech Detection Models},\n  url = {https://aclanthology.org/2022.woah-1.15},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multilingualsentiment","title":"MultilingualSentiment","text":"<p>A collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative</p> <p>Dataset: <code>C-MTEB/MultilingualSentiment-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{tyqiangz2022multilingual,\n  author = {tyqiangz},\n  title = {Multilingual Sentiment Datasets},\n  url = {https://github.com/tyqiangz/multilingual-sentiment-datasets},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multilingualsentimentv2","title":"MultilingualSentiment.v2","text":"<p>A collection of multilingual sentiments datasets grouped into 3 classes -- positive, neutral, negative This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/multilingual_sentiment</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{tyqiangz2022multilingual,\n  author = {tyqiangz},\n  title = {Multilingual Sentiment Datasets},\n  url = {https://github.com/tyqiangz/multilingual-sentiment-datasets},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#multilingualsentimentclassification","title":"MultilingualSentimentClassification","text":"<p>Sentiment classification dataset with binary (positive vs negative sentiment) labels. Includes 30 languages and dialects.</p> <p>Dataset: <code>mteb/multilingual-sentiment-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, bam, bul, cmn, cym, ... (31) Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#myanmarnews","title":"MyanmarNews","text":"<p>The Myanmar News dataset on Hugging Face contains news articles in Burmese. It is designed for tasks such as text classification, sentiment analysis, and language modeling. The dataset includes a variety of news topics in 4 categories, providing a rich resource for natural language processing applications involving Burmese which is a low resource language.</p> <p>Dataset: <code>mteb/MyanmarNews</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mya News, Written derived found Citation <pre><code>@inproceedings{Khine2017,\n  author = {A. H. Khine and K. T. Nwet and K. M. Soe},\n  booktitle = {15th Proceedings of International Conference on Computer Applications},\n  month = {February},\n  pages = {401--408},\n  title = {Automatic Myanmar News Classification},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#myanmarnewsv2","title":"MyanmarNews.v2","text":"<p>The Myanmar News dataset on Hugging Face contains news articles in Burmese. It is designed for tasks such as text classification, sentiment analysis, and language modeling. The dataset includes a variety of news topics in 4 categories, providing a rich resource for natural language processing applications involving Burmese which is a low resource language. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/myanmar_news</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mya News, Written derived found Citation <pre><code>@inproceedings{Khine2017,\n  author = {A. H. Khine and K. T. Nwet and K. M. Soe},\n  booktitle = {15th Proceedings of International Conference on Computer Applications},\n  month = {February},\n  pages = {401--408},\n  title = {Automatic Myanmar News Classification},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nlptwitteranalysisclassification","title":"NLPTwitterAnalysisClassification","text":"<p>Twitter Analysis Classification</p> <p>Dataset: <code>hamedhf/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#nlptwitteranalysisclassificationv2","title":"NLPTwitterAnalysisClassification.v2","text":"<p>Twitter Analysis Classification This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#nysjudicialethicslegalbenchclassification","title":"NYSJudicialEthicsLegalBenchClassification","text":"<p>Answer questions on judicial ethics from the New York State Unified Court System Advisory Committee.</p> <p>Dataset: <code>mteb/NYSJudicialEthicsLegalBenchClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#naijasenti","title":"NaijaSenti","text":"<p>NaijaSenti is the first large-scale human-annotated Twitter sentiment dataset for the four most widely spoken languages in Nigeria \u2014 Hausa, Igbo, Nigerian-Pidgin, and Yor\u00f9b\u00e1 \u2014 consisting of around 30,000 annotated tweets per language, including a significant fraction of code-mixed tweets.</p> <p>Dataset: <code>mteb/NaijaSenti</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy hau, ibo, pcm, yor Social, Written expert-annotated found Citation <pre><code>@inproceedings{muhammad-etal-2022-naijasenti,\n  address = {Marseille, France},\n  author = {Muhammad, Shamsuddeen Hassan  and\nAdelani, David Ifeoluwa  and\nRuder, Sebastian  and\nAhmad, Ibrahim Sa{'}id  and\nAbdulmumin, Idris  and\nBello, Bello Shehu  and\nChoudhury, Monojit  and\nEmezue, Chris Chinenye  and\nAbdullahi, Saheed Salahudeen  and\nAremu, Anuoluwapo  and\nJorge, Al{\\'\\i}pio  and\nBrazdil, Pavel},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {590--602},\n  publisher = {European Language Resources Association},\n  title = {{N}aija{S}enti: A {N}igerian {T}witter Sentiment Corpus for Multilingual Sentiment Analysis},\n  url = {https://aclanthology.org/2022.lrec-1.63},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nepalinewsclassification","title":"NepaliNewsClassification","text":"<p>A Nepali dataset for 7500 news articles </p> <p>Dataset: <code>mteb/NepaliNewsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nep News, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nepalinewsclassificationv2","title":"NepaliNewsClassification.v2","text":"<p>A Nepali dataset for 7500 news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/nepali_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nep News, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#newsclassification","title":"NewsClassification","text":"<p>Large News Classification Dataset</p> <p>Dataset: <code>fancyzhx/ag_news</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Written expert-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#newsclassificationv2","title":"NewsClassification.v2","text":"<p>Large News Classification Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/news</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Written expert-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norecclassification","title":"NoRecClassification","text":"<p>A Norwegian dataset for sentiment classification on review</p> <p>Dataset: <code>mteb/norec_classification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Reviews, Written derived found Citation <pre><code>@inproceedings{velldal-etal-2018-norec,\n  address = {Miyazaki, Japan},\n  author = {Velldal, Erik  and\n{\\\\O}vrelid, Lilja  and\nBergem, Eivind Alexander  and\nStadsnes, Cathrine  and\nTouileb, Samia  and\nJ{\\\\o}rgensen, Fredrik},\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nHasida, Koiti  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios  and\nTokunaga, Takenobu},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {{N}o{R}e{C}: The {N}orwegian Review Corpus},\n  url = {https://aclanthology.org/L18-1661},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norecclassificationv2","title":"NoRecClassification.v2","text":"<p>A Norwegian dataset for sentiment classification on review This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/no_rec</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Reviews, Written derived found Citation <pre><code>@inproceedings{velldal-etal-2018-norec,\n  address = {Miyazaki, Japan},\n  author = {Velldal, Erik  and\n{\\\\O}vrelid, Lilja  and\nBergem, Eivind Alexander  and\nStadsnes, Cathrine  and\nTouileb, Samia  and\nJ{\\\\o}rgensen, Fredrik},\n  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nHasida, Koiti  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios  and\nTokunaga, Takenobu},\n  month = may,\n  publisher = {European Language Resources Association (ELRA)},\n  title = {{N}o{R}e{C}: The {N}orwegian Review Corpus},\n  url = {https://aclanthology.org/L18-1661},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nordiclangclassification","title":"NordicLangClassification","text":"<p>A dataset for Nordic language identification.</p> <p>Dataset: <code>mteb/NordicLangClassification</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, fao, isl, nno, nob, ... (6) Encyclopaedic derived found Citation <pre><code>@inproceedings{haas-derczynski-2021-discriminating,\n  address = {Kiyv, Ukraine},\n  author = {Haas, Ren{\\'e}  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects},\n  editor = {Zampieri, Marcos  and\nNakov, Preslav  and\nLjube{\\v{s}}i{\\'c}, Nikola  and\nTiedemann, J{\\\"o}rg  and\nScherrer, Yves  and\nJauhiainen, Tommi},\n  month = apr,\n  pages = {67--75},\n  publisher = {Association for Computational Linguistics},\n  title = {Discriminating Between Similar {N}ordic Languages},\n  url = {https://aclanthology.org/2021.vardial-1.8},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norwegianparliamentclassification","title":"NorwegianParliamentClassification","text":"<p>Norwegian parliament speeches annotated for sentiment</p> <p>Dataset: <code>mteb/NorwegianParliamentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Government, Spoken derived found Citation <pre><code>@inproceedings{kummervold-etal-2021-operationalizing,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kummervold, Per E  and\nDe la Rosa, Javier  and\nWetjen, Freddy  and\nBrygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {20--29},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  url = {https://aclanthology.org/2021.nodalida-main.3},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#norwegianparliamentclassificationv2","title":"NorwegianParliamentClassification.v2","text":"<p>Norwegian parliament speeches annotated for sentiment This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/norwegian_parliament</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy nob Government, Spoken derived found Citation <pre><code>@inproceedings{kummervold-etal-2021-operationalizing,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kummervold, Per E  and\nDe la Rosa, Javier  and\nWetjen, Freddy  and\nBrygfjeld, Svein Arne},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {20--29},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Operationalizing a National Digital Library: The Case for a {N}orwegian Transformer Model},\n  url = {https://aclanthology.org/2021.nodalida-main.3},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusaparagraphemotionclassification","title":"NusaParagraphEmotionClassification","text":"<p>NusaParagraphEmotionClassification is a multi-class emotion classification on 10 Indonesian languages from the NusaParagraph dataset.</p> <p>Dataset: <code>mteb/NusaParagraphEmotionClassification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 bbc, bew, bug, jav, mad, ... (10) Fiction, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{cahyawijaya-etal-2023-nusawrites,\n  address = {Nusa Dua, Bali},\n  author = {Cahyawijaya, Samuel  and  Lovenia, Holy  and Koto, Fajri  and  Adhista, Dea  and  Dave, Emmanuel  and  Oktavianti, Sarah  and  Akbar, Salsabil  and  Lee, Jhonson  and  Shadieq, Nuur  and  Cenggoro, Tjeng Wawan  and  Linuwih, Hanung  and  Wilie, Bryan  and  Muridan, Galih  and  Winata, Genta  and  Moeljadi, David  and  Aji, Alham Fikri  and  Purwarianti, Ayu  and  Fung, Pascale},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  editor = {Park, Jong C.  and  Arase, Yuki  and  Hu, Baotian  and  Lu, Wei  and  Wijaya, Derry  and  Purwarianti, Ayu  and  Krisnadhi, Adila Alfa},\n  month = nov,\n  pages = {921--945},\n  publisher = {Association for Computational Linguistics},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  url = {https://aclanthology.org/2023.ijcnlp-main.60},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusaparagraphtopicclassification","title":"NusaParagraphTopicClassification","text":"<p>NusaParagraphTopicClassification is a multi-class topic classification on 10 Indonesian languages.</p> <p>Dataset: <code>gentaiscool/nusaparagraph_topic</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 bbc, bew, bug, jav, mad, ... (10) Fiction, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{cahyawijaya-etal-2023-nusawrites,\n  address = {Nusa Dua, Bali},\n  author = {Cahyawijaya, Samuel  and  Lovenia, Holy  and Koto, Fajri  and  Adhista, Dea  and  Dave, Emmanuel  and  Oktavianti, Sarah  and  Akbar, Salsabil  and  Lee, Jhonson  and  Shadieq, Nuur  and  Cenggoro, Tjeng Wawan  and  Linuwih, Hanung  and  Wilie, Bryan  and  Muridan, Galih  and  Winata, Genta  and  Moeljadi, David  and  Aji, Alham Fikri  and  Purwarianti, Ayu  and  Fung, Pascale},\n  booktitle = {Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  editor = {Park, Jong C.  and  Arase, Yuki  and  Hu, Baotian  and  Lu, Wei  and  Wijaya, Derry  and  Purwarianti, Ayu  and  Krisnadhi, Adila Alfa},\n  month = nov,\n  pages = {921--945},\n  publisher = {Association for Computational Linguistics},\n  title = {NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages},\n  url = {https://aclanthology.org/2023.ijcnlp-main.60},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#nusax-senti","title":"NusaX-senti","text":"<p>NusaX is a high-quality multilingual parallel corpus that covers 12 languages, Indonesian, English, and 10 Indonesian local languages, namely Acehnese, Balinese, Banjarese, Buginese, Madurese, Minangkabau, Javanese, Ngaju, Sundanese, and Toba Batak. NusaX-Senti is a 3-labels (positive, neutral, negative) sentiment analysis dataset for 10 Indonesian local languages + Indonesian and English.</p> <p>Dataset: <code>mteb/NusaX-senti</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ace, ban, bbc, bjn, bug, ... (12) Constructed, Reviews, Social, Web, Written expert-annotated found Citation <pre><code>@inproceedings{winata2023nusax,\n  author = {Winata, Genta Indra and Aji, Alham Fikri and Cahyawijaya, Samuel and Mahendra, Rahmad and Koto, Fajri and Romadhony, Ade and Kurniawan, Kemal and Moeljadi, David and Prasojo, Radityo Eko and Fung, Pascale and others},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  pages = {815--834},\n  title = {NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115dataretentionlegalbenchclassification","title":"OPP115DataRetentionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how long user information is stored.</p> <p>Dataset: <code>mteb/OPP115DataRetentionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115datasecuritylegalbenchclassification","title":"OPP115DataSecurityLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how user information is protected.</p> <p>Dataset: <code>mteb/OPP115DataSecurityLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115datasecuritylegalbenchclassificationv2","title":"OPP115DataSecurityLegalBenchClassification.v2","text":"<p>Given a clause from a privacy policy, classify if the clause describes how user information is protected. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_data_security_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115donottracklegalbenchclassification","title":"OPP115DoNotTrackLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how Do Not Track signals for online tracking and advertising are honored.</p> <p>Dataset: <code>mteb/OPP115DoNotTrackLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115donottracklegalbenchclassificationv2","title":"OPP115DoNotTrackLegalBenchClassification.v2","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how Do Not Track signals for online tracking and advertising are honored. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_do_not_track_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115firstpartycollectionuselegalbenchclassification","title":"OPP115FirstPartyCollectionUseLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes how and why a service provider collects user information.</p> <p>Dataset: <code>mteb/OPP115FirstPartyCollectionUseLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115internationalandspecificaudienceslegalbenchclassification","title":"OPP115InternationalAndSpecificAudiencesLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describe practices that pertain only to a specific group of users (e.g., children, Europeans, or California residents).</p> <p>Dataset: <code>mteb/OPP115InternationalAndSpecificAudiencesLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115policychangelegalbenchclassification","title":"OPP115PolicyChangeLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how users will be informed about changes to the privacy policy.</p> <p>Dataset: <code>mteb/OPP115PolicyChangeLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115thirdpartysharingcollectionlegalbenchclassification","title":"OPP115ThirdPartySharingCollectionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describe how user information may be shared with or collected by third parties.</p> <p>Dataset: <code>mteb/OPP115ThirdPartySharingCollectionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115useraccesseditanddeletionlegalbenchclassification","title":"OPP115UserAccessEditAndDeletionLegalBenchClassification","text":"<p>Given a clause from a privacy policy, classify if the clause describes if and how users may access, edit, or delete their information.</p> <p>Dataset: <code>mteb/OPP115UserAccessEditAndDeletionLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115userchoicecontrollegalbenchclassification","title":"OPP115UserChoiceControlLegalBenchClassification","text":"<p>Given a clause fro ma privacy policy, classify if the clause describes the choices and control options available to users.</p> <p>Dataset: <code>mteb/OPP115UserChoiceControlLegalBenchClassification</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#opp115userchoicecontrollegalbenchclassificationv2","title":"OPP115UserChoiceControlLegalBenchClassification.v2","text":"<p>Given a clause fro ma privacy policy, classify if the clause describes the choices and control options available to users. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/opp115_user_choice_control_legal_bench</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#odianewsclassification","title":"OdiaNewsClassification","text":"<p>A Odia dataset for 3-class classification of Odia news articles</p> <p>Dataset: <code>mteb/OdiaNewsClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ory News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#odianewsclassificationv2","title":"OdiaNewsClassification.v2","text":"<p>A Odia dataset for 3-class classification of Odia news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/odia_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 ory News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#onlineshopping","title":"OnlineShopping","text":"<p>Sentiment Analysis of User Reviews on Online Shopping Websites</p> <p>Dataset: <code>C-MTEB/OnlineShopping-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#onlinestorereviewsentimentclassification","title":"OnlineStoreReviewSentimentClassification","text":"<p>This dataset contains Arabic reviews of products from the SHEIN online store.</p> <p>Dataset: <code>Ruqiya/Arabic_Reviews_of_SHEIN</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#onlinestorereviewsentimentclassificationv2","title":"OnlineStoreReviewSentimentClassification.v2","text":"<p>This dataset contains Arabic reviews of products from the SHEIN online store. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/online_store_review_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#opentenderclassification","title":"OpenTenderClassification","text":"<p>This dataset contains Belgian and Dutch tender calls from OpenTender in Dutch</p> <p>Dataset: <code>clips/mteb-nl-opentender-cls-pr</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Government, Written human-annotated found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#oralargumentquestionpurposelegalbenchclassification","title":"OralArgumentQuestionPurposeLegalBenchClassification","text":"<p>This task classifies questions asked by Supreme Court justices at oral argument into seven categories: 1. Background - questions seeking factual or procedural information that is missing or not clear in the briefing 2. Clarification - questions seeking to get an advocate to clarify her position or the scope of the rule being advocated for 3. Implications - questions about the limits of a rule or its implications for future cases 4. Support - questions offering support for the advocate\u2019s position 5. Criticism - questions criticizing an advocate\u2019s position 6. Communicate - question designed primarily to communicate with other justices 7. Humor - questions designed to interject humor into the argument and relieve tension</p> <p>Dataset: <code>mteb/OralArgumentQuestionPurposeLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#oralargumentquestionpurposelegalbenchclassificationv2","title":"OralArgumentQuestionPurposeLegalBenchClassification.v2","text":"<p>This task classifies questions asked by Supreme Court justices at oral argument into seven categories: 1. Background - questions seeking factual or procedural information that is missing or not clear in the briefing 2. Clarification - questions seeking to get an advocate to clarify her position or the scope of the rule being advocated for 3. Implications - questions about the limits of a rule or its implications for future cases 4. Support - questions offering support for the advocate\u2019s position 5. Criticism - questions criticizing an advocate\u2019s position 6. Communicate - question designed primarily to communicate with other justices 7. Humor - questions designed to interject humor into the argument and relieve tension This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/oral_argument_question_purpose_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#overrulinglegalbenchclassification","title":"OverrulingLegalBenchClassification","text":"<p>This task consists of classifying whether or not a particular sentence of case law overturns the decision of a previous case.</p> <p>Dataset: <code>mteb/OverrulingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#overrulinglegalbenchclassificationv2","title":"OverrulingLegalBenchClassification.v2","text":"<p>This task consists of classifying whether or not a particular sentence of case law overturns the decision of a previous case. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/overruling_legal_bench</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pac","title":"PAC","text":"<p>Polish Paraphrase Corpus</p> <p>Dataset: <code>mteb/PAC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Legal, Written human-annotated found Citation <pre><code>@misc{augustyniak2022waydesigningcompilinglepiszcze,\n  archiveprefix = {arXiv},\n  author = {\u0141ukasz Augustyniak and Kamil Tagowski and Albert Sawczyn and Denis Janiak and Roman Bartusiak and Adrian Szymczak and Marcin W\u0105troba and Arkadiusz Janz and Piotr Szyma\u0144ski and Miko\u0142aj Morzy and Tomasz Kajdanowicz and Maciej Piasecki},\n  eprint = {2211.13112},\n  primaryclass = {cs.CL},\n  title = {This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish},\n  url = {https://arxiv.org/abs/2211.13112},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pacv2","title":"PAC.v2","text":"<p>Polish Paraphrase Corpus This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/pac</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Legal, Written human-annotated found Citation <pre><code>@misc{augustyniak2022waydesigningcompilinglepiszcze,\n  archiveprefix = {arXiv},\n  author = {\u0141ukasz Augustyniak and Kamil Tagowski and Albert Sawczyn and Denis Janiak and Roman Bartusiak and Adrian Szymczak and Marcin W\u0105troba and Arkadiusz Janz and Piotr Szyma\u0144ski and Miko\u0142aj Morzy and Tomasz Kajdanowicz and Maciej Piasecki},\n  eprint = {2211.13112},\n  primaryclass = {cs.CL},\n  title = {This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish},\n  url = {https://arxiv.org/abs/2211.13112},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#proalegalbenchclassification","title":"PROALegalBenchClassification","text":"<p>Given a statute, determine if the text contains an explicit private right of action. Given a privacy policy clause and a description of the clause, determine if the description is correct. A private right of action (PROA) exists when a statute empowers an ordinary individual (i.e., a private person) to legally enforce their rights by bringing an action in court. In short, a PROA creates the ability for an individual to sue someone in order to recover damages or halt some offending conduct. PROAs are ubiquitous in antitrust law (in which individuals harmed by anti-competitive behavior can sue offending firms for compensation) and environmental law (in which individuals can sue entities which release hazardous substances for damages).</p> <p>Dataset: <code>mteb/PROALegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#patentclassification","title":"PatentClassification","text":"<p>Classification Dataset of Patents and Abstract</p> <p>Dataset: <code>mteb/PatentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#patentclassificationv2","title":"PatentClassification.v2","text":"<p>Classification Dataset of Patents and Abstract This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/patent</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pershopdomainclassification","title":"PerShopDomainClassification","text":"<p>PerSHOP - A Persian dataset for shopping dialogue systems modeling</p> <p>Dataset: <code>MCINext/pershop-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken human-annotated created Citation <pre><code>@article{mahmoudi2024pershop,\n  author = {Mahmoudi, Keyvan and Faili, Heshaam},\n  journal = {arXiv preprint arXiv:2401.00811},\n  title = {PerSHOP--A Persian dataset for shopping dialogue systems modeling},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#pershopintentclassification","title":"PerShopIntentClassification","text":"<p>PerSHOP - A Persian dataset for shopping dialogue systems modeling</p> <p>Dataset: <code>MCINext/pershop-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken human-annotated created Citation <pre><code>@article{mahmoudi2024pershop,\n  author = {Mahmoudi, Keyvan and Faili, Heshaam},\n  journal = {arXiv preprint arXiv:2401.00811},\n  title = {PerSHOP--A Persian dataset for shopping dialogue systems modeling},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#persianfoodsentimentclassification","title":"PersianFoodSentimentClassification","text":"<p>Persian Food Review Dataset</p> <p>Dataset: <code>asparius/Persian-Food-Sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews, Written derived found Citation <pre><code>@article{ParsBERT,\n  author = {Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri},\n  journal = {ArXiv},\n  title = {ParsBERT: Transformer-based Model for Persian Language Understanding},\n  volume = {abs/2005.12515},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#persiantextemotion","title":"PersianTextEmotion","text":"<p>Emotion is a Persian dataset with six basic emotions: anger, fear, joy, love, sadness, and surprise.</p> <p>Dataset: <code>SeyedAli/Persian-Text-Emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#persiantextemotionv2","title":"PersianTextEmotion.v2","text":"<p>Emotion is a Persian dataset with six basic emotions: anger, fear, joy, love, sadness, and surprise. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/persian_text_emotion</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#personaljurisdictionlegalbenchclassification","title":"PersonalJurisdictionLegalBenchClassification","text":"<p>Given a fact pattern describing the set of contacts between a plaintiff, defendant, and forum, determine if a court in that forum could exercise personal jurisdiction over the defendant.</p> <p>Dataset: <code>mteb/PersonalJurisdictionLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#poemsentimentclassification","title":"PoemSentimentClassification","text":"<p>Poem Sentiment is a sentiment dataset of poem verses from Project Gutenberg.</p> <p>Dataset: <code>mteb/PoemSentimentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written human-annotated found Citation <pre><code>@misc{sheng2020investigating,\n  archiveprefix = {arXiv},\n  author = {Emily Sheng and David Uthus},\n  eprint = {2011.02686},\n  primaryclass = {cs.CL},\n  title = {Investigating Societal Biases in a Poetry Composition System},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#poemsentimentclassificationv2","title":"PoemSentimentClassification.v2","text":"<p>Poem Sentiment consists of poem verses from Project Gutenberg annotated for sentiment using the labels negative (0), positive (1), no_impact (2) and mixed (3). This version was corrected as a part of pull request to fix common issues on the original dataset, including removing duplicates, and train-test leakage.</p> <p>Dataset: <code>mteb/poem_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Fiction, Poetry, Written human-annotated found Citation <pre><code>@misc{sheng2020investigating,\n  archiveprefix = {arXiv},\n  author = {Emily Sheng and David Uthus},\n  eprint = {2011.02686},\n  primaryclass = {cs.CL},\n  title = {Investigating Societal Biases in a Poetry Composition System},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-in","title":"PolEmo2.0-IN","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-IN task is to predict the sentiment of in-domain (medicine and hotels) reviews.</p> <p>Dataset: <code>PL-MTEB/polemo2_in</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-inv2","title":"PolEmo2.0-IN.v2","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-IN task is to predict the sentiment of in-domain (medicine and hotels) reviews.</p> <p>Dataset: <code>mteb/pol_emo2_in</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-out","title":"PolEmo2.0-OUT","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-OUT task is to predict the sentiment of out-of-domain (products and school) reviews using models train on reviews from medicine and hotels domains.</p> <p>Dataset: <code>mteb/PolEmo2.0-OUT</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#polemo20-outv2","title":"PolEmo2.0-OUT.v2","text":"<p>A collection of Polish online reviews from four domains: medicine, hotels, products and school. The PolEmo2.0-OUT task is to predict the sentiment of out-of-domain (products and school) reviews using models train on reviews from medicine and hotels domains.</p> <p>Dataset: <code>mteb/pol_emo2_out</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pol Social, Written derived found Citation <pre><code>@inproceedings{kocon-etal-2019-multi,\n  address = {Hong Kong, China},\n  author = {Koco{\\'n}, Jan  and\nMi{\\l}kowski, Piotr  and\nZa{\\'s}ko-Zieli{\\'n}ska, Monika},\n  booktitle = {Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)},\n  doi = {10.18653/v1/K19-1092},\n  month = nov,\n  pages = {980--991},\n  publisher = {Association for Computational Linguistics},\n  title = {Multi-Level Sentiment Analysis of {P}ol{E}mo 2.0: Extended Corpus of Multi-Domain Consumer Reviews},\n  url = {https://aclanthology.org/K19-1092},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#punjabinewsclassification","title":"PunjabiNewsClassification","text":"<p>A Punjabi dataset for 2-class classification of Punjabi news articles</p> <p>Dataset: <code>mteb/PunjabiNewsClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy pan News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#restaurantreviewsentimentclassification","title":"RestaurantReviewSentimentClassification","text":"<p>Dataset of 8364 restaurant reviews from qaym.com in Arabic for sentiment analysis</p> <p>Dataset: <code>hadyelsahar/ar_res_reviews</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@inproceedings{elsahar2015building,\n  author = {ElSahar, Hady and El-Beltagy, Samhaa R},\n  booktitle = {International conference on intelligent text processing and computational linguistics},\n  organization = {Springer},\n  pages = {23--34},\n  title = {Building large arabic multi-domain resources for sentiment analysis},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#restaurantreviewsentimentclassificationv2","title":"RestaurantReviewSentimentClassification.v2","text":"<p>Dataset of 8156 restaurant reviews from qaym.com in Arabic for sentiment analysis This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/restaurant_review_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Reviews, Written derived found Citation <pre><code>@inproceedings{elsahar2015building,\n  author = {ElSahar, Hady and El-Beltagy, Samhaa R},\n  booktitle = {International conference on intelligent text processing and computational linguistics},\n  organization = {Springer},\n  pages = {23--34},\n  title = {Building large arabic multi-domain resources for sentiment analysis},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romanianreviewssentiment","title":"RomanianReviewsSentiment","text":"<p>LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian</p> <p>Dataset: <code>mteb/RomanianReviewsSentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written derived found Citation <pre><code>@article{tache2101clustering,\n  author = {Anca Maria Tache and Mihaela Gaman and Radu Tudor Ionescu},\n  journal = {ArXiv},\n  title = {Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa -- A Large Romanian Sentiment Data Set},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romanianreviewssentimentv2","title":"RomanianReviewsSentiment.v2","text":"<p>LaRoSeDa (A Large Romanian Sentiment Data Set) contains 15,000 reviews written in Romanian This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/romanian_reviews_sentiment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written derived found Citation <pre><code>@article{tache2101clustering,\n  author = {Anca Maria Tache and Mihaela Gaman and Radu Tudor Ionescu},\n  journal = {ArXiv},\n  title = {Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa -- A Large Romanian Sentiment Data Set},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romaniansentimentclassification","title":"RomanianSentimentClassification","text":"<p>An Romanian dataset for sentiment classification.</p> <p>Dataset: <code>mteb/RomanianSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written human-annotated found Citation <pre><code>@article{dumitrescu2020birth,\n  author = {Dumitrescu, Stefan Daniel and Avram, Andrei-Marius and Pyysalo, Sampo},\n  journal = {arXiv preprint arXiv:2009.08712},\n  title = {The birth of Romanian BERT},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#romaniansentimentclassificationv2","title":"RomanianSentimentClassification.v2","text":"<p>An Romanian dataset for sentiment classification. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/romanian_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ron Reviews, Written human-annotated found Citation <pre><code>@article{dumitrescu2020birth,\n  author = {Dumitrescu, Stefan Daniel and Avram, Andrei-Marius and Pyysalo, Sampo},\n  journal = {arXiv preprint arXiv:2009.08712},\n  title = {The birth of Romanian BERT},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#runluintentclassification","title":"RuNLUIntentClassification","text":"<p>Contains natural language data for human-robot interaction in home domain which we collected and annotated for evaluating NLU Services/platforms.</p> <p>Dataset: <code>mteb/RuNLUIntentClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified human-annotated found Citation <pre><code>@misc{liu2019benchmarkingnaturallanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Xingkun Liu and Arash Eshghi and Pawel Swietojanski and Verena Rieser},\n  eprint = {1903.05566},\n  primaryclass = {cs.CL},\n  title = {Benchmarking Natural Language Understanding Services for building Conversational Agents},\n  url = {https://arxiv.org/abs/1903.05566},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rureviewsclassification","title":"RuReviewsClassification","text":"<p>Product review classification (3-point scale) based on RuRevies dataset</p> <p>Dataset: <code>ai-forever/ru-reviews-classification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@inproceedings{Smetanin-SA-2019,\n  author = {Sergey Smetanin and Michail Komarov},\n  booktitle = {2019 IEEE 21st Conference on Business Informatics (CBI)},\n  doi = {10.1109/CBI.2019.00062},\n  issn = {2378-1963},\n  month = {July},\n  number = {},\n  pages = {482-486},\n  title = {Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks},\n  volume = {01},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rureviewsclassificationv2","title":"RuReviewsClassification.v2","text":"<p>Product review classification (3-point scale) based on RuRevies dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ru_reviews</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Reviews, Written derived found Citation <pre><code>@inproceedings{Smetanin-SA-2019,\n  author = {Sergey Smetanin and Michail Komarov},\n  booktitle = {2019 IEEE 21st Conference on Business Informatics (CBI)},\n  doi = {10.1109/CBI.2019.00062},\n  issn = {2378-1963},\n  month = {July},\n  number = {},\n  pages = {482-486},\n  title = {Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks},\n  volume = {01},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchcoreriscclassification","title":"RuSciBenchCoreRiscClassification","text":"<p>This binary classification task aims to determine whether a scientific paper (based on its title and abstract) belongs to the Core of the Russian Science Citation Index (RISC). The RISC includes a wide range of publications, but the Core RISC comprises the most cited and prestigious journals, dissertations, theses, monographs, and studies. The task is provided for both Russian and English versions of the paper's title and abstract.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchgrnticlassification","title":"RuSciBenchGRNTIClassification","text":"<p>Classification of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-grnti-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Academic, Written derived found"},{"location":"overview/available_tasks/classification/#ruscibenchgrnticlassificationv2","title":"RuSciBenchGRNTIClassification.v2","text":"<p>Classification of scientific papers based on the GRNTI (State Rubricator of Scientific and Technical Information) rubricator. GRNTI is a universal hierarchical classification of knowledge domains adopted in Russia and CIS countries to systematize the entire flow of scientific and technical information. This task uses the first level of the GRNTI hierarchy and top 28 classes by frequency. In this version, English language support has been added and data partitioning has been slightly modified.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchoecdclassification","title":"RuSciBenchOECDClassification","text":"<p>Classification of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-oecd-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Academic, Written derived found"},{"location":"overview/available_tasks/classification/#ruscibenchoecdclassificationv2","title":"RuSciBenchOECDClassification.v2","text":"<p>Classification of scientific papers based on the OECD (Organization for Economic Co-operation and Development) rubricator. OECD provides a hierarchical 3-level system of classes for labeling scientific articles. This task uses the first two levels of the OECD hierarchy, top 29 classes. In this version, English language support has been added and data partitioning has been slightly modified.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ruscibenchpubtypeclassification","title":"RuSciBenchPubTypeClassification","text":"<p>This task involves classifying scientific papers (based on their title and abstract) into different publication types. The dataset identifies the following types: 'Article', 'Conference proceedings', 'Survey', 'Miscellanea', 'Short message', 'Review', and 'Personalia'. This task is available for both Russian and English versions of the paper's title and abstract.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupclassification","title":"RuToxicOKMLCUPClassification","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day.</p> <p>Dataset: <code>mteb/RuToxicOKMLCUPClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupclassificationv2","title":"RuToxicOKMLCUPClassification.v2","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ru_toxic_okmlcup</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#rutoxicokmlcupmultilabelclassification","title":"RuToxicOKMLCUPMultilabelClassification","text":"<p>On the Odnoklassniki social network, users post a huge number of comments of various directions and nature every day.</p> <p>Dataset: <code>mteb/RuToxicOKMLCUPClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found"},{"location":"overview/available_tasks/classification/#scdbpaccountabilitylegalbenchclassification","title":"SCDBPAccountabilityLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer maintains internal compliance procedures on company standards regarding human trafficking and slavery? This includes any type of internal accountability mechanism. Requiring independently of the supply to comply with laws does not qualify or asking for documentary evidence of compliance does not count either.'</p> <p>Dataset: <code>mteb/SCDBPAccountabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpauditslegalbenchclassification","title":"SCDBPAuditsLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'</p> <p>Dataset: <code>mteb/SCDBPAuditsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpcertificationlegalbenchclassification","title":"SCDBPCertificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  performs any type of audit, or reserves the right to audit?'</p> <p>Dataset: <code>mteb/SCDBPCertificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbptraininglegalbenchclassification","title":"SCDBPTrainingLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer  provides training to employees on human trafficking and slavery? Broad policies such as ongoing dialogue on mitigating risks of human trafficking and slavery or increasing managers and purchasers knowledge about health, safety and labor practices qualify as training. Providing training to contractors who failed to comply with human trafficking laws counts as training.'</p> <p>Dataset: <code>mteb/SCDBPTrainingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scdbpverificationlegalbenchclassification","title":"SCDBPVerificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose whether the retail seller or manufacturer engages in verification and auditing as one practice, expresses that it may conduct an audit, or expressess that it is assessing supplier risks through a review of the US Dept. of Labor's List?'</p> <p>Dataset: <code>mteb/SCDBPVerificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddaccountabilitylegalbenchclassification","title":"SCDDAccountabilityLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer maintains internal accountability standards and procedures for employees or contractors failing to meet company standards regarding slavery and trafficking?'</p> <p>Dataset: <code>mteb/SCDDAccountabilityLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddauditslegalbenchclassification","title":"SCDDAuditsLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer conducts audits of suppliers to evaluate supplier compliance with company standards for trafficking and slavery in supply chains? The disclosure shall specify if the verification was not an independent, unannounced audit.'</p> <p>Dataset: <code>mteb/SCDDAuditsLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddcertificationlegalbenchclassification","title":"SCDDCertificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer requires direct suppliers to certify that materials incorporated into the product comply with the laws regarding slavery and human trafficking of the country or countries in which they are doing business?'</p> <p>Dataset: <code>mteb/SCDDCertificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddtraininglegalbenchclassification","title":"SCDDTrainingLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer provides company employees and management, who have direct responsibility for supply chain management, training on human trafficking and slavery, particularly with respect to mitigating risks within the supply chains of products?'</p> <p>Dataset: <code>mteb/SCDDTrainingLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scddverificationlegalbenchclassification","title":"SCDDVerificationLegalBenchClassification","text":"<p>This is a binary classification task in which the LLM must determine if a supply chain disclosure meets the following coding criteria: 'Does the above statement disclose to what extent, if any, that the retail seller or manufacturer engages in verification of product supply chains to evaluate and address risks of human trafficking and slavery? If the company conducts verification], the disclosure shall specify if the verification was not conducted by a third party.'</p> <p>Dataset: <code>mteb/SCDDVerificationLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@article{chilton2017limitations,\n  author = {Chilton, Adam S and Sarfaty, Galit A},\n  journal = {Stan. J. Int'l L.},\n  pages = {1},\n  publisher = {HeinOnline},\n  title = {The limitations of supply chain disclosure regimes},\n  volume = {53},\n  year = {2017},\n}\n\n@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdseyeprotectionclassification","title":"SDSEyeProtectionClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/SDSEyeProtectionClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdseyeprotectionclassificationv2","title":"SDSEyeProtectionClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sds_eye_protection</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdsglovesclassification","title":"SDSGlovesClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/SDSGlovesClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sdsglovesclassificationv2","title":"SDSGlovesClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sds_gloves</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry LM-generated and reviewed created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{pereira2020msds,\n  author = {Pereira, Eliseu},\n  booktitle = {15th Doctoral Symposium},\n  pages = {42},\n  title = {MSDS-OPP: Operator Procedures Prediction in Material Safety Data Sheets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sib200classification","title":"SIB200Classification","text":"<p>SIB-200 is the largest publicly available topic classification dataset based on Flores-200 covering 205 languages and dialects annotated. The dataset is annotated in English for the topics, science/technology, travel, politics, sports, health, entertainment, and geography. The labels are then transferred to the other languages in Flores-200 which are human-translated.</p> <p>Dataset: <code>mteb/sib200</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ace, acm, acq, aeb, afr, ... (197) News, Written expert-annotated human-translated and localized Citation <pre><code>@article{adelani2023sib,\n  author = {Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},\n  journal = {arXiv preprint arXiv:2309.07445},\n  title = {SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sidclassification","title":"SIDClassification","text":"<p>SID Classification</p> <p>Dataset: <code>MCINext/sid-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sidclassificationv2","title":"SIDClassification.v2","text":"<p>SID Classification This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sid</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sanskritshlokasclassification","title":"SanskritShlokasClassification","text":"<p>This data set contains ~500 Shlokas  </p> <p>Dataset: <code>bpHigh/iNLTK_Sanskrit_Shlokas_Dataset</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy san Religious, Written derived found Citation <pre><code>@inproceedings{arora-2020-inltk,\n  address = {Online},\n  author = {Arora, Gaurav},\n  booktitle = {Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)},\n  doi = {10.18653/v1/2020.nlposs-1.10},\n  editor = {Park, Eunjeong L.  and\nHagiwara, Masato  and\nMilajevs, Dmitrijs  and\nLiu, Nelson F.  and\nChauhan, Geeticka  and\nTan, Liling},\n  month = nov,\n  pages = {66--71},\n  publisher = {Association for Computational Linguistics},\n  title = {i{NLTK}: Natural Language Toolkit for Indic Languages},\n  url = {https://aclanthology.org/2020.nlposs-1.10},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sardistanceclassification","title":"SardiStanceClassification","text":"<p>SardiStance is a unique dataset designed for the task of stance detection in Italian tweets. It consists of tweets related to the Sardines movement, providing a valuable resource for researchers and practitioners in the field of NLP.</p> <p>Dataset: <code>MattiaSangermano/SardiStance</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Social derived found Citation <pre><code>@inproceedings{cignarella2020sardistance,\n  author = {Cignarella, Alessandra Teresa and Lai, Mirko and Bosco, Cristina and Patti, Viviana and Rosso, Paolo and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {Ceur},\n  pages = {1--10},\n  title = {Sardistance@ evalita2020: Overview of the task on stance detection in italian tweets},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scalaclassification","title":"ScalaClassification","text":"<p>ScaLa a linguistic acceptability dataset for the mainland Scandinavian languages automatically constructed from dependency annotations in Universal Dependencies Treebanks. Published as part of 'ScandEval: A Benchmark for Scandinavian Natural Language Processing'</p> <p>Dataset: <code>mteb/multilingual-scala-classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, nno, nob, swe Blog, Fiction, News, Non-fiction, Spoken, ... (7) human-annotated created Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#scandisentclassification","title":"ScandiSentClassification","text":"<p>The corpus is crawled from se.trustpilot.com, no.trustpilot.com, dk.trustpilot.com, fi.trustpilot.com and trustpilot.com.</p> <p>Dataset: <code>mteb/scandisent</code> \u2022 License: openrail \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy dan, eng, fin, nob, swe Reviews, Written expert-annotated found Citation <pre><code>@inproceedings{isbister-etal-2021-stop,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Isbister, Tim  and\nCarlsson, Fredrik  and\nSahlgren, Magnus},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {385--390},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?},\n  url = {https://aclanthology.org/2021.nodalida-main.42/},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentirueval2016","title":"SentiRuEval2016","text":"<p>Russian sentiment analysis evaluation SentiRuEval-2016 devoted to reputation monitoring of banks and telecom companies in Twitter. We describe the task, data, the procedure of data preparation, and participants\u2019 results.</p> <p>Dataset: <code>mteb/SentiRuEval2016</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found Citation <pre><code>@inproceedings{loukachevitch2016sentirueval,\n  author = {Loukachevitch, NV and Rubtsova, Yu V},\n  booktitle = {Computational Linguistics and Intellectual Technologies},\n  pages = {416--426},\n  title = {SentiRuEval-2016: overcoming time gap and data sparsity in tweet sentiment analysis},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentirueval2016v2","title":"SentiRuEval2016.v2","text":"<p>Russian sentiment analysis evaluation SentiRuEval-2016 devoted to reputation monitoring of banks and telecom companies in Twitter. We describe the task, data, the procedure of data preparation, and participants\u2019 results. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/senti_ru_eval2016</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy rus not specified derived found Citation <pre><code>@inproceedings{loukachevitch2016sentirueval,\n  author = {Loukachevitch, NV and Rubtsova, Yu V},\n  booktitle = {Computational Linguistics and Intellectual Technologies},\n  pages = {416--426},\n  title = {SentiRuEval-2016: overcoming time gap and data sparsity in tweet sentiment analysis},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentanalysishindi","title":"SentimentAnalysisHindi","text":"<p>Hindi Sentiment Analysis Dataset</p> <p>Dataset: <code>mteb/SentimentAnalysisHindi</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 hin Reviews, Written derived found Citation <pre><code>@misc{OdiaGenAI,\n  author = {Shantipriya Parida and Sambit Sekhar and Soumendra Kumar Sahoo and Swateek Jena and Abhijeet Parida and Satya Ranjan Dash and Guneet Singh Kohli},\n  howpublished = {{https://huggingface.co/OdiaGenAI}},\n  journal = {Hugging Face repository},\n  publisher = {Hugging Face},\n  title = {OdiaGenAI: Generative AI and LLM Initiative for the Odia Language},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentanalysishindiv2","title":"SentimentAnalysisHindi.v2","text":"<p>Hindi Sentiment Analysis Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sentiment_analysis_hindi</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 hin Reviews, Written derived found Citation <pre><code>@misc{OdiaGenAI,\n  author = {Shantipriya Parida and Sambit Sekhar and Soumendra Kumar Sahoo and Swateek Jena and Abhijeet Parida and Satya Ranjan Dash and Guneet Singh Kohli},\n  howpublished = {{https://huggingface.co/OdiaGenAI}},\n  journal = {Hugging Face repository},\n  publisher = {Hugging Face},\n  title = {OdiaGenAI: Generative AI and LLM Initiative for the Odia Language},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentdksf","title":"SentimentDKSF","text":"<p>The Sentiment DKSF (Digikala/Snappfood comments) is a dataset for sentiment analysis.</p> <p>Dataset: <code>hezarai/sentiment-dksf</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sentimentdksfv2","title":"SentimentDKSF.v2","text":"<p>The Sentiment DKSF (Digikala/Snappfood comments) is a dataset for sentiment analysis. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sentiment_dksf</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Reviews derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewsclassification","title":"SinhalaNewsClassification","text":"<p>This file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015).</p> <p>Dataset: <code>mteb/SinhalaNewsClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{deSilva2015,\n  author = {Nisansa de Silva},\n  journal = {Year of Publication},\n  title = {Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language},\n  year = {2015},\n}\n\n@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewsclassificationv2","title":"SinhalaNewsClassification.v2","text":"<p>This file contains news texts (sentences) belonging to 5 different news categories (political, business, technology, sports and Entertainment). The original dataset was released by Nisansa de Silva (Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language, 2015). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sinhala_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{deSilva2015,\n  author = {Nisansa de Silva},\n  journal = {Year of Publication},\n  title = {Sinhala Text Classification: Observations from the Perspective of a Resource Poor Language},\n  year = {2015},\n}\n\n@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewssourceclassification","title":"SinhalaNewsSourceClassification","text":"<p>This dataset contains Sinhala news headlines extracted from 9 news sources (websites) (Sri Lanka Army, Dinamina, GossipLanka, Hiru, ITN, Lankapuwath, NewsLK, Newsfirst, World Socialist Web Site-Sinhala).</p> <p>Dataset: <code>NLPC-UOM/Sinhala-News-Source-classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#sinhalanewssourceclassificationv2","title":"SinhalaNewsSourceClassification.v2","text":"<p>This dataset contains Sinhala news headlines extracted from 9 news sources (websites) (Sri Lanka Army, Dinamina, GossipLanka, Hiru, ITN, Lankapuwath, NewsLK, Newsfirst, World Socialist Web Site-Sinhala). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/sinhala_news_source</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy sin News, Written derived found Citation <pre><code>@article{dhananjaya2022,\n  author = {Dhananjaya et al.},\n  journal = {Year of Publication},\n  title = {BERTifying Sinhala - A Comprehensive Analysis of Pre-trained Language Models for Sinhala Text Classification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#siswatinewsclassification","title":"SiswatiNewsClassification","text":"<p>Siswati News Classification Dataset</p> <p>Dataset: <code>mteb/SiswatiNewsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ssw News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#siswatinewsclassificationv2","title":"SiswatiNewsClassification.v2","text":"<p>Siswati News Classification Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/siswati_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ssw News, Written human-annotated found Citation <pre><code>@article{Madodonga_Marivate_Adendorff_2023,\n  author = {Madodonga, Andani and Marivate, Vukosi and Adendorff, Matthew},\n  doi = {10.55492/dhasa.v4i01.4449},\n  month = {Jan.},\n  title = {Izindaba-Tindzaba: Machine learning news categorisation for Long and Short Text for isiZulu and Siswati},\n  url = {https://upjournals.up.ac.za/index.php/dhasa/article/view/4449},\n  volume = {4},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#slovakhatespeechclassification","title":"SlovakHateSpeechClassification","text":"<p>The dataset contains posts from a social network with human annotations for hateful or offensive language in Slovak.</p> <p>Dataset: <code>TUKE-KEMT/hate_speech_slovak</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Social, Written human-annotated found"},{"location":"overview/available_tasks/classification/#slovakhatespeechclassificationv2","title":"SlovakHateSpeechClassification.v2","text":"<p>The dataset contains posts from a social network with human annotations for hateful or offensive language in Slovak. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/slovak_hate_speech</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Social, Written human-annotated found"},{"location":"overview/available_tasks/classification/#slovakmoviereviewsentimentclassification","title":"SlovakMovieReviewSentimentClassification","text":"<p>User reviews of movies on the CSFD movie database, with 2 sentiment classes (positive, negative)</p> <p>Dataset: <code>mteb/SlovakMovieReviewSentimentClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#slovakmoviereviewsentimentclassificationv2","title":"SlovakMovieReviewSentimentClassification.v2","text":"<p>User reviews of movies on the CSFD movie database, with 2 sentiment classes (positive, negative) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/slovak_movie_review_sentiment</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy slk Reviews, Written derived found Citation <pre><code>@misc{\u0161tef\u00e1nik2023resources,\n  archiveprefix = {arXiv},\n  author = {Michal \u0160tef\u00e1nik and Marek Kadl\u010d\u00edk and Piotr Gramacki and Petr Sojka},\n  eprint = {2304.01922},\n  primaryclass = {cs.CL},\n  title = {Resources and Few-shot Learners for In-context Learning in Slavic Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#southafricanlangclassification","title":"SouthAfricanLangClassification","text":"<p>A language identification test set for 11 South African Languages.</p> <p>Dataset: <code>mlexplorer008/south_african_language_identification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy afr, eng, nbl, nso, sot, ... (11) Non-fiction, Web, Written expert-annotated found Citation <pre><code>@misc{south-african-language-identification,\n  author = {ExploreAI Academy, Joanne M},\n  publisher = {Kaggle},\n  title = {South African Language Identification},\n  url = {https://kaggle.com/competitions/south-african-language-identification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishnewsclassification","title":"SpanishNewsClassification","text":"<p>A Spanish dataset for news classification. The dataset includes articles from reputable Spanish news sources spanning 12 different categories.</p> <p>Dataset: <code>MarcOrfilaCarreras/spanish-news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishnewsclassificationv2","title":"SpanishNewsClassification.v2","text":"<p>A Spanish dataset for news classification. The dataset includes articles from reputable Spanish news sources spanning 12 different categories. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/spanish_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishsentimentclassification","title":"SpanishSentimentClassification","text":"<p>A Spanish dataset for sentiment classification.</p> <p>Dataset: <code>sepidmnorozy/Spanish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#spanishsentimentclassificationv2","title":"SpanishSentimentClassification.v2","text":"<p>A Spanish dataset for sentiment classification. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/spanish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy spa Reviews, Written derived found Citation <pre><code>@inproceedings{mollanorozy-etal-2023-cross,\n  address = {Dubrovnik, Croatia},\n  author = {Mollanorozy, Sepideh  and\nTanti, Marc  and\nNissim, Malvina},\n  booktitle = {Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},\n  doi = {10.18653/v1/2023.sigtyp-1.9},\n  editor = {Beinborn, Lisa  and\nGoswami, Koustava  and\nMurado{\\\\u{g}}lu, Saliha  and\nSorokin, Alexey  and\nKumar, Ritesh  and\nShcherbakov, Andreas  and\nPonti, Edoardo M.  and\nCotterell, Ryan  and\nVylomova, Ekaterina},\n  month = may,\n  pages = {89--95},\n  publisher = {Association for Computational Linguistics},\n  title = {Cross-lingual Transfer Learning with \\{P\\}ersian},\n  url = {https://aclanthology.org/2023.sigtyp-1.9},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#styleclassification","title":"StyleClassification","text":"<p>A dataset containing formal and informal sentences in Persian for style classification.</p> <p>Dataset: <code>MCINext/style-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas Spoken derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#swahilinewsclassification","title":"SwahiliNewsClassification","text":"<p>Dataset for Swahili News Classification, categorized with 6 domains (Local News (Kitaifa), International News (Kimataifa), Finance News (Uchumi), Health News (Afya), Sports News (Michezo), and Entertainment News (Burudani)). Building and Optimizing Swahili Language Models: Techniques, Embeddings, and Datasets</p> <p>Dataset: <code>mteb/SwahiliNewsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swa News, Written derived found Citation <pre><code>@inproceedings{davis2020swahili,\n  author = {Davis, David},\n  doi = {10.5281/zenodo.5514203},\n  publisher = {Zenodo},\n  title = {Swahili: News Classification Dataset (0.2)},\n  url = {https://doi.org/10.5281/zenodo.5514203},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swahilinewsclassificationv2","title":"SwahiliNewsClassification.v2","text":"<p>Dataset for Swahili News Classification, categorized with 6 domains (Local News (Kitaifa), International News (Kimataifa), Finance News (Uchumi), Health News (Afya), Sports News (Michezo), and Entertainment News (Burudani)). Building and Optimizing Swahili Language Models: Techniques, Embeddings, and Datasets This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swahili_news</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swa News, Written derived found Citation <pre><code>@inproceedings{davis2020swahili,\n  author = {Davis, David},\n  doi = {10.5281/zenodo.5514203},\n  publisher = {Zenodo},\n  title = {Swahili: News Classification Dataset (0.2)},\n  url = {https://doi.org/10.5281/zenodo.5514203},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swerecclassification","title":"SweRecClassification","text":"<p>A Swedish dataset for sentiment classification on review</p> <p>Dataset: <code>mteb/swerec_classification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swerecclassificationv2","title":"SweRecClassification.v2","text":"<p>A Swedish dataset for sentiment classification on review This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swe_rec</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found Citation <pre><code>@inproceedings{nielsen-2023-scandeval,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Nielsen, Dan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {185--201},\n  publisher = {University of Tartu Library},\n  title = {{S}cand{E}val: A Benchmark for {S}candinavian Natural Language Processing},\n  url = {https://aclanthology.org/2023.nodalida-1.20},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#swedishsentimentclassification","title":"SwedishSentimentClassification","text":"<p>Dataset of Swedish reviews scarped from various public available websites</p> <p>Dataset: <code>mteb/SwedishSentimentClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#swedishsentimentclassificationv2","title":"SwedishSentimentClassification.v2","text":"<p>Dataset of Swedish reviews scarped from various public available websites This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/swedish_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy swe Reviews, Written derived found"},{"location":"overview/available_tasks/classification/#swissjudgementclassification","title":"SwissJudgementClassification","text":"<p>Multilingual, diachronic dataset of Swiss Federal Supreme Court cases annotated with the respective binarized judgment outcome (approval/dismissal)</p> <p>Dataset: <code>mteb/SwissJudgementClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu, fra, ita Legal, Written expert-annotated found Citation <pre><code>@misc{niklaus2022empirical,\n  archiveprefix = {arXiv},\n  author = {Joel Niklaus and Matthias St\u00fcrmer and Ilias Chalkidis},\n  eprint = {2209.12325},\n  primaryclass = {cs.CL},\n  title = {An Empirical Study on Cross-X Transfer for Legal Judgment Prediction},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsaanger","title":"SynPerChatbotConvSAAnger","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Anger</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-anger</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsaclassification","title":"SynPerChatbotConvSAClassification","text":"<p>SynPerChatbotConvSAClassification</p> <p>License: not specified \u2022 Learn more \u2192 not specified</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation Tasks name type modalities languages SynPerChatbotConvSAAnger Classification text fas SynPerChatbotConvSASatisfaction Classification text fas SynPerChatbotConvSAFriendship Classification text fas SynPerChatbotConvSAFear Classification text fas SynPerChatbotConvSAJealousy Classification text fas SynPerChatbotConvSASurprise Classification text fas SynPerChatbotConvSALove Classification text fas SynPerChatbotConvSASadness Classification text fas SynPerChatbotConvSAHappiness Classification text fas"},{"location":"overview/available_tasks/classification/#synperchatbotconvsafear","title":"SynPerChatbotConvSAFear","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Fear</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-fear</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsafriendship","title":"SynPerChatbotConvSAFriendship","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Friendship</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-friendship</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsahappiness","title":"SynPerChatbotConvSAHappiness","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Happiness</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-happiness</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsajealousy","title":"SynPerChatbotConvSAJealousy","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Jealousy</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-jealousy</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsalove","title":"SynPerChatbotConvSALove","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Love</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-love</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasadness","title":"SynPerChatbotConvSASadness","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Sadness</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-sadness</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasatisfaction","title":"SynPerChatbotConvSASatisfaction","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Satisfaction</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-satisfaction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsasurprise","title":"SynPerChatbotConvSASurprise","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Surprise</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-surprise</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsatonechatbotclassification","title":"SynPerChatbotConvSAToneChatbotClassification","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-conversational-sentiment-analysis-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotconvsatoneuserclassification","title":"SynPerChatbotConvSAToneUserClassification","text":"<p>Synthetic Persian Chatbot Conversational Sentiment Analysis Tone User</p> <p>Dataset: <code>MCINext/chatbot-conversational-sentiment-analysis-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotragtonechatbotclassification","title":"SynPerChatbotRAGToneChatbotClassification","text":"<p>Synthetic Persian Chatbot RAG Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotragtoneuserclassification","title":"SynPerChatbotRAGToneUserClassification","text":"<p>Synthetic Persian Chatbot RAG Tone User Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbotsatisfactionlevelclassification","title":"SynPerChatbotSatisfactionLevelClassification","text":"<p>Synthetic Persian Chatbot Satisfaction Level Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-satisfaction-level-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbottonechatbotclassification","title":"SynPerChatbotToneChatbotClassification","text":"<p>Synthetic Persian Chatbot Tone Chatbot Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-tone-chatbot-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synperchatbottoneuserclassification","title":"SynPerChatbotToneUserClassification","text":"<p>Synthetic Persian Chatbot Tone User Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-tone-user-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassification","title":"SynPerTextToneClassification","text":"<p>Persian Text Tone</p> <p>Dataset: <code>MCINext/synthetic-persian-text-tone-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassificationv2","title":"SynPerTextToneClassification.v2","text":"<p>Persian Text Tone This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/syn_per_text_tone</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#synpertexttoneclassificationv3","title":"SynPerTextToneClassification.v3","text":"<p>This version of the Persian text tone classification dataset is an improved version of its predecessors. It excludes several classes identified as having low-quality data, leading to a more reliable benchmark.</p> <p>Dataset: <code>MCINext/synthetic-persian-text-tone-classification-v3</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy fas not specified LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#tnews","title":"TNews","text":"<p>Short Text Classification for News</p> <p>Dataset: <code>C-MTEB/TNews-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn News, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tnewsv2","title":"TNews.v2","text":"<p>Short Text Classification for News This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/t_news</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn News, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai and\nZhang, Xuanwei and\nLi, Lu and\nCao, Chenjie and\nLi, Yudong and\nXu, Yechen and\nSun, Kai and\nYu, Dian and\nYu, Cong and\nTian, Yin and\nDong, Qianqian and\nLiu, Weitang and\nShi, Bo and\nCui, Yiming and\nLi, Junyi and\nZeng, Jun and\nWang, Rongzhao and\nXie, Weijian and\nLi, Yanting and\nPatterson, Yina and\nTian, Zuoyu and\nZhang, Yiwen and\nZhou, He and\nLiu, Shaoweihua and\nZhao, Zhe and\nZhao, Qipeng and\nYue, Cong and\nZhang, Xinrui and\nYang, Zhengliang and\nRichardson, Kyle and\nLan, Zhenzhong },\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tamilnewsclassification","title":"TamilNewsClassification","text":"<p>A Tamil dataset for 6-class classification of Tamil news articles</p> <p>Dataset: <code>mlexplorer008/tamil_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tam News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tamilnewsclassificationv2","title":"TamilNewsClassification.v2","text":"<p>A Tamil dataset for 6-class classification of Tamil news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tamil_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tam News, Written derived found Citation <pre><code>@article{kunchukuttan2020indicnlpcorpus,\n  author = {Anoop Kunchukuttan and Divyanshu Kakwani and Satish Golla and Gokul N.C. and Avik Bhattacharyya and Mitesh M. Khapra and Pratyush Kumar},\n  journal = {arXiv preprint arXiv:2005.00085},\n  title = {AI4Bharat-IndicNLP Corpus: Monolingual Corpora and Word Embeddings for Indic Languages},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#telemarketingsalesrulelegalbenchclassification","title":"TelemarketingSalesRuleLegalBenchClassification","text":"<p>Determine how 16 C.F.R. \u00a7 310.3(a)(1) and 16 C.F.R. \u00a7 310.3(a)(2) (governing deceptive practices) apply to different fact patterns. This dataset is designed to test a model\u2019s ability to apply 16 C.F.R. \u00a7 310.3(a)(1) and 16 C.F.R. \u00a7 310.3(a)(2) of the Telemarketing Sales Rule to a simple fact pattern with a clear outcome. Each fact pattern ends with the question: \u201cIs this a violation of the Telemarketing Sales Rule?\u201d Each fact pattern is paired with the answer \u201cYes\u201d or the answer \u201cNo.\u201d Fact patterns are listed in the column \u201ctext,\u201d and answers are listed in the column \u201clabel.\u201d</p> <p>Dataset: <code>mteb/TelemarketingSalesRuleLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#teluguandhrajyotinewsclassification","title":"TeluguAndhraJyotiNewsClassification","text":"<p>A Telugu dataset for 5-class classification of Telugu news articles</p> <p>Dataset: <code>mlexplorer008/telugu_news_classification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tel News, Written derived found"},{"location":"overview/available_tasks/classification/#teluguandhrajyotinewsclassificationv2","title":"TeluguAndhraJyotiNewsClassification.v2","text":"<p>A Telugu dataset for 5-class classification of Telugu news articles This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/telugu_andhra_jyoti_news</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tel News, Written derived found"},{"location":"overview/available_tasks/classification/#tenkgnadclassification","title":"TenKGnadClassification","text":"<p>10k German News Articles Dataset (10kGNAD) contains news articles from the online Austrian newspaper website DER Standard with their topic classification (9 classes).</p> <p>Dataset: <code>mteb/TenKGnadClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu News, Written expert-annotated found Citation <pre><code>@inproceedings{Schabus2017,\n  address = {Tokyo, Japan},\n  author = {Dietmar Schabus and Marcin Skowron and Martin Trapp},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},\n  doi = {10.1145/3077136.3080711},\n  month = aug,\n  pages = {1241--1244},\n  title = {One Million Posts: A Data Set of German Online Discussions},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tenkgnadclassificationv2","title":"TenKGnadClassification.v2","text":"<p>10k German News Articles Dataset (10kGNAD) contains news articles from the online Austrian newspaper website DER Standard with their topic classification (9 classes). This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ten_k_gnad</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy deu News, Written expert-annotated found Citation <pre><code>@inproceedings{Schabus2017,\n  address = {Tokyo, Japan},\n  author = {Dietmar Schabus and Marcin Skowron and Martin Trapp},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR)},\n  doi = {10.1145/3077136.3080711},\n  month = aug,\n  pages = {1241--1244},\n  title = {One Million Posts: A Data Set of German Online Discussions},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#textualismtooldictionarieslegalbenchclassification","title":"TextualismToolDictionariesLegalBenchClassification","text":"<p>Determine if a paragraph from a judicial opinion is applying a form textualism that relies on the dictionary meaning of terms.</p> <p>Dataset: <code>mteb/TextualismToolDictionariesLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#textualismtoolplainlegalbenchclassification","title":"TextualismToolPlainLegalBenchClassification","text":"<p>Determine if a paragraph from a judicial opinion is applying a form textualism that relies on the ordinary (\u201cplain\u201d) meaning of terms.</p> <p>Dataset: <code>mteb/TextualismToolPlainLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicchatclassification","title":"ToxicChatClassification","text":"<p>This dataset contains toxicity annotations on 10K user prompts collected from the Vicuna online demo. We utilize a human-AI collaborative annotation framework to guarantee the quality of annotation while maintaining a feasible annotation workload. The details of data collection, pre-processing, and annotation can be found in our paper. We believe that ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions. Only human annotated samples are selected here.</p> <p>Dataset: <code>lmsys/toxic-chat</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Constructed, Written expert-annotated found Citation <pre><code>@misc{lin2023toxicchat,\n  archiveprefix = {arXiv},\n  author = {Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},\n  eprint = {2310.17389},\n  primaryclass = {cs.CL},\n  title = {ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicchatclassificationv2","title":"ToxicChatClassification.v2","text":"<p>This dataset contains toxicity annotations on 10K user prompts collected from the Vicuna online demo. We utilize a human-AI collaborative annotation framework to guarantee the quality of annotation while maintaining a feasible annotation workload. The details of data collection, pre-processing, and annotation can be found in our paper. We believe that ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions. Only human annotated samples are selected here. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/toxic_chat</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Constructed, Written expert-annotated found Citation <pre><code>@misc{lin2023toxicchat,\n  archiveprefix = {arXiv},\n  author = {Zi Lin and Zihan Wang and Yongqi Tong and Yangkun Wang and Yuxin Guo and Yujia Wang and Jingbo Shang},\n  eprint = {2310.17389},\n  primaryclass = {cs.CL},\n  title = {ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsclassification","title":"ToxicConversationsClassification","text":"<p>Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not.</p> <p>Dataset: <code>mteb/toxic_conversations_50k</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsclassificationv2","title":"ToxicConversationsClassification.v2","text":"<p>Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/toxic_conversations</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{jigsaw-unintended-bias-in-toxicity-classification,\n  author = {cjadams and Daniel Borkan and inversion and Jeffrey Sorensen and Lucas Dixon and Lucy Vasserman and nithum},\n  publisher = {Kaggle},\n  title = {Jigsaw Unintended Bias in Toxicity Classification},\n  url = {https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#toxicconversationsvnclassification","title":"ToxicConversationsVNClassification","text":"<p>A translated dataset from Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/toxic-conversations-50k-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tswananewsclassification","title":"TswanaNewsClassification","text":"<p>Tswana News Classification Dataset</p> <p>Dataset: <code>mteb/TswanaNewsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tsn News, Written derived found Citation <pre><code>@inproceedings{marivate2023puoberta,\n  author = {Vukosi Marivate and Moseli Mots'Oehli and Valencia Wagner and Richard Lastrucci and Isheanesu Dzingirai},\n  booktitle = {SACAIR 2023 (To Appear)},\n  dataset_url = {https://github.com/dsfsi/PuoBERTa},\n  keywords = {NLP},\n  preprint_url = {https://arxiv.org/abs/2310.09141},\n  software_url = {https://huggingface.co/dsfsi/PuoBERTa},\n  title = {PuoBERTa: Training and evaluation of a curated language model for Setswana},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tswananewsclassificationv2","title":"TswanaNewsClassification.v2","text":"<p>Tswana News Classification Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tswana_news</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tsn News, Written derived found Citation <pre><code>@inproceedings{marivate2023puoberta,\n  author = {Vukosi Marivate and Moseli Mots'Oehli and Valencia Wagner and Richard Lastrucci and Isheanesu Dzingirai},\n  booktitle = {SACAIR 2023 (To Appear)},\n  dataset_url = {https://github.com/dsfsi/PuoBERTa},\n  keywords = {NLP},\n  preprint_url = {https://arxiv.org/abs/2310.09141},\n  software_url = {https://huggingface.co/dsfsi/PuoBERTa},\n  title = {PuoBERTa: Training and evaluation of a curated language model for Setswana},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkicclassification","title":"TurkicClassification","text":"<p>A dataset of news classification in three Turkic languages.</p> <p>Dataset: <code>Electrotubbie/classification_Turkic_languages</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bak, kaz, kir News, Written derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishconstitutionalcourtviolation","title":"TurkishConstitutionalCourtViolation","text":"<p>Binary classification of Turkish constitutional court decisions: Violation vs No violation.</p> <p>Dataset: <code>denizgulal/turkish-constitutional-court-violation-clean</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tur Legal, Non-fiction human-annotated found Citation <pre><code>@article{mumcuoglu2021natural,\n  author = {Mumcuoglu, Emre and Ozturk, Ceyhun E. and Ozaktas, Haldun M. and Koc, Aykut},\n  journal = {Information Processing and Management},\n  number = {5},\n  title = {Natural language processing in law: Prediction of outcomes in the higher courts of Turkey},\n  volume = {58},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishmoviesentimentclassification","title":"TurkishMovieSentimentClassification","text":"<p>Turkish Movie Review Dataset</p> <p>Dataset: <code>asparius/Turkish-Movie-Review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishmoviesentimentclassificationv2","title":"TurkishMovieSentimentClassification.v2","text":"<p>Turkish Movie Review Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/turkish_movie_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishproductsentimentclassification","title":"TurkishProductSentimentClassification","text":"<p>Turkish Product Review Dataset</p> <p>Dataset: <code>asparius/Turkish-Product-Review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#turkishproductsentimentclassificationv2","title":"TurkishProductSentimentClassification.v2","text":"<p>Turkish Product Review Dataset This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/turkish_product_sentiment</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tur Reviews, Written derived found Citation <pre><code>@inproceedings{Demirtas2013CrosslingualPD,\n  author = {Erkin Demirtas and Mykola Pechenizkiy},\n  booktitle = {wisdom},\n  title = {Cross-lingual polarity detection with machine translation},\n  url = {https://api.semanticscholar.org/CorpusID:3912960},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetemotionclassification","title":"TweetEmotionClassification","text":"<p>A dataset of 10,000 tweets that was created with the aim of covering the most frequently used emotion categories in Arabic tweets.</p> <p>Dataset: <code>mteb/TweetEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{al2018emotional,\n  author = {Al-Khatib, Amr and El-Beltagy, Samhaa R},\n  booktitle = {Computational Linguistics and Intelligent Text Processing: 18th International Conference, CICLing 2017, Budapest, Hungary, April 17--23, 2017, Revised Selected Papers, Part II 18},\n  organization = {Springer},\n  pages = {105--114},\n  title = {Emotional tone detection in arabic tweets},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetemotionclassificationv2","title":"TweetEmotionClassification.v2","text":"<p>A dataset of 10,012 tweets that was created with the aim of covering the most frequently used emotion categories in Arabic tweets. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/TweetEmotionClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{al2018emotional,\n  author = {Al-Khatib, Amr and El-Beltagy, Samhaa R},\n  booktitle = {Computational Linguistics and Intelligent Text Processing: 18th International Conference, CICLing 2017, Budapest, Hungary, April 17--23, 2017, Revised Selected Papers, Part II 18},\n  organization = {Springer},\n  pages = {105--114},\n  title = {Emotional tone detection in arabic tweets},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsarcasmclassification","title":"TweetSarcasmClassification","text":"<p>Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets.</p> <p>Dataset: <code>iabufarha/ar_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{abu-farha-magdy-2020-arabic,\n  address = {Marseille, France},\n  author = {Abu Farha, Ibrahim  and\nMagdy, Walid},\n  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},\n  editor = {Al-Khalifa, Hend  and\nMagdy, Walid  and\nDarwish, Kareem  and\nElsayed, Tamer  and\nMubarak, Hamdy},\n  isbn = {979-10-95546-51-1},\n  language = {English},\n  month = may,\n  pages = {32--39},\n  publisher = {European Language Resource Association},\n  title = {From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset},\n  url = {https://aclanthology.org/2020.osact-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsarcasmclassificationv2","title":"TweetSarcasmClassification.v2","text":"<p>Arabic sarcasm detection dataset, which was created through the reannotation of available Arabic sentiment analysis datasets. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tweet_sarcasm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara Social, Written human-annotated found Citation <pre><code>@inproceedings{abu-farha-magdy-2020-arabic,\n  address = {Marseille, France},\n  author = {Abu Farha, Ibrahim  and\nMagdy, Walid},\n  booktitle = {Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection},\n  editor = {Al-Khalifa, Hend  and\nMagdy, Walid  and\nDarwish, Kareem  and\nElsayed, Tamer  and\nMubarak, Hamdy},\n  isbn = {979-10-95546-51-1},\n  language = {English},\n  month = may,\n  pages = {32--39},\n  publisher = {European Language Resource Association},\n  title = {From {A}rabic Sentiment Analysis to Sarcasm Detection: The {A}r{S}arcasm Dataset},\n  url = {https://aclanthology.org/2020.osact-1.5},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentclassification","title":"TweetSentimentClassification","text":"<p>A multilingual Sentiment Analysis dataset consisting of tweets in 8 different languages.</p> <p>Dataset: <code>mteb/tweet_sentiment_multilingual</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ara, deu, eng, fra, hin, ... (8) Social, Written human-annotated found Citation <pre><code>@inproceedings{barbieri-etal-2022-xlm,\n  address = {Marseille, France},\n  author = {Barbieri, Francesco  and\nEspinosa Anke, Luis  and\nCamacho-Collados, Jose},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  month = jun,\n  pages = {258--266},\n  publisher = {European Language Resources Association},\n  title = {{XLM}-{T}: Multilingual Language Models in {T}witter for Sentiment Analysis and Beyond},\n  url = {https://aclanthology.org/2022.lrec-1.27},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionclassification","title":"TweetSentimentExtractionClassification","text":"<p>Dataset: <code>mteb/tweet_sentiment_extraction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionclassificationv2","title":"TweetSentimentExtractionClassification.v2","text":"<p>This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tweet_sentiment_extraction</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Social, Written human-annotated found Citation <pre><code>@misc{tweet-sentiment-extraction,\n  author = {Maggie, Phil Culliton, Wei Chen},\n  publisher = {Kaggle},\n  title = {Tweet Sentiment Extraction},\n  url = {https://kaggle.com/competitions/tweet-sentiment-extraction},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweetsentimentextractionvnclassification","title":"TweetSentimentExtractionVNClassification","text":"<p>A collection of translated tweets annotated for sentiment extraction. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/tweet-sentiment-extraction-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweettopicsingleclassification","title":"TweetTopicSingleClassification","text":"<p>Topic classification dataset on Twitter with 6 labels. Each instance of TweetTopic comes with a timestamp which distributes from September 2019 to August 2021. Tweets were preprocessed before the annotation to normalize some artifacts, converting URLs into a special token {{URL}} and non-verified usernames into {{USERNAME}}. For verified usernames, we replace its display name (or account name) with symbols {@}.</p> <p>Dataset: <code>mteb/TweetTopicSingleClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dimosthenis-etal-2022-twitter,\n  address = {Gyeongju, Republic of Korea},\n  author = {Antypas, Dimosthenis  and\nUshio, Asahi  and\nCamacho-Collados, Jose  and\nNeves, Leonardo  and\nSilva, Vitor  and\nBarbieri, Francesco},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  publisher = {International Committee on Computational Linguistics},\n  title = {{T}witter {T}opic {C}lassification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#tweettopicsingleclassificationv2","title":"TweetTopicSingleClassification.v2","text":"<p>Topic classification dataset on Twitter with 6 labels. Each instance of TweetTopic comes with a timestamp which distributes from September 2019 to August 2021. Tweets were preprocessed before the annotation to normalize some artifacts, converting URLs into a special token {{URL}} and non-verified usernames into {{USERNAME}}. For verified usernames, we replace its display name (or account name) with symbols {@}. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/tweet_topic_single</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng News, Social, Written expert-annotated found Citation <pre><code>@inproceedings{dimosthenis-etal-2022-twitter,\n  address = {Gyeongju, Republic of Korea},\n  author = {Antypas, Dimosthenis  and\nUshio, Asahi  and\nCamacho-Collados, Jose  and\nNeves, Leonardo  and\nSilva, Vitor  and\nBarbieri, Francesco},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  publisher = {International Committee on Computational Linguistics},\n  title = {{T}witter {T}opic {C}lassification},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#uccvcommonlawlegalbenchclassification","title":"UCCVCommonLawLegalBenchClassification","text":"<p>Determine if a contract is governed by the Uniform Commercial Code (UCC) or the common law of contracts.</p> <p>Dataset: <code>mteb/UCCVCommonLawLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ukrformalityclassification","title":"UkrFormalityClassification","text":"<p>This dataset contains Ukrainian Formality Classification dataset obtained by trainslating English GYAFC data. English data source: https://aclanthology.org/N18-1012/ Translation into Ukrainian language using model: https://huggingface.co/facebook/nllb-200-distilled-600M Additionally, the dataset was balanced, with labels: 0 - informal, 1 - formal.</p> <p>Dataset: <code>ukr-detect/ukr-formality-dataset-translated-gyafc</code> \u2022 License: openrail++ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ukr News, Written derived machine-translated Citation <pre><code>@inproceedings{rao-tetreault-2018-dear,\n  author = {Rao, Sudha  and\nTetreault, Joel},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  month = jun,\n  publisher = {Association for Computational Linguistics},\n  title = {Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},\n  url = {https://aclanthology.org/N18-1012},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ukrformalityclassificationv2","title":"UkrFormalityClassification.v2","text":"<p>This dataset contains Ukrainian Formality Classification dataset obtained by trainslating English GYAFC data. English data source: https://aclanthology.org/N18-1012/ Translation into Ukrainian language using model: https://huggingface.co/facebook/nllb-200-distilled-600M Additionally, the dataset was balanced, with labels: 0 - informal, 1 - formal. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/ukr_formality</code> \u2022 License: openrail++ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ukr News, Written derived machine-translated Citation <pre><code>@inproceedings{rao-tetreault-2018-dear,\n  author = {Rao, Sudha  and\nTetreault, Joel},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  month = jun,\n  publisher = {Association for Computational Linguistics},\n  title = {Dear Sir or Madam, May {I} Introduce the {GYAFC} Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},\n  url = {https://aclanthology.org/N18-1012},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#ukrtweettoxicitybinaryclassification","title":"UkrTweetToxicityBinaryClassification","text":"<p>Filtered Ukrainian Tweets dataset with Toloka.ai platform crowdsourcing task. It contains of 2.5k toxic and 2.5k non-toxic texts.</p> <p>Dataset: <code>mteb/UkrTweetToxicityBinaryClassification_ukr-detect_ukr-toxicity-dataset</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ukr Blog, Social, Web derived found Citation <pre><code>@inproceedings{dementieva-etal-2024-toxicity,\n  address = {Mexico City, Mexico},\n  author = {Dementieva, Daryna  and\nKhylenko, Valeriia  and\nBabakov, Nikolay  and\nGroh, Georg},\n  booktitle = {Proceedings of the 8th Workshop on Online Abuse and Harms (WOAH 2024)},\n  doi = {10.18653/v1/2024.woah-1.19},\n  month = jun,\n  pages = {244--255},\n  publisher = {Association for Computational Linguistics},\n  title = {Toxicity Classification in {U}krainian},\n  url = {https://aclanthology.org/2024.woah-1.19},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#unfairtoslegalbenchclassification","title":"UnfairTOSLegalBenchClassification","text":"<p>Given a clause from a terms-of-service contract, determine the category the clause belongs to. The purpose of this task is classifying clauses in Terms of Service agreements. Clauses have been annotated by into nine categories: ['Arbitration', 'Unilateral change', 'Content removal', 'Jurisdiction', 'Choice of law', 'Limitation of liability', 'Unilateral termination', 'Contract by using', 'Other']. The first eight categories correspond to clauses that would potentially be deemed potentially unfair. The last category (Other) corresponds to clauses in agreements which don\u2019t fit into these categories.</p> <p>Dataset: <code>mteb/UnfairTOSLegalBenchClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{lippi2019claudette,\n  author = {Lippi, Marco and Pa{\\l}ka, Przemys{\\l}aw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},\n  journal = {Artificial Intelligence and Law},\n  pages = {117--139},\n  publisher = {Springer},\n  title = {CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service},\n  volume = {27},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#urduromansentimentclassification","title":"UrduRomanSentimentClassification","text":"<p>The Roman Urdu dataset is a data corpus comprising of more than 20000 records tagged for sentiment (Positive, Negative, Neutral)</p> <p>Dataset: <code>mteb/UrduRomanSentimentClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 urd Social, Written derived found Citation <pre><code>@misc{misc_roman_urdu_data_set_458,\n  author = {Sharf,Zareen},\n  howpublished = {UCI Machine Learning Repository},\n  note = {{DOI}: https://doi.org/10.24432/C58325},\n  title = {{Roman Urdu Data Set}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#urduromansentimentclassificationv2","title":"UrduRomanSentimentClassification.v2","text":"<p>The Roman Urdu dataset is a data corpus comprising of more than 20000 records tagged for sentiment (Positive, Negative, Neutral) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/urdu_roman_sentiment</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 urd Social, Written derived found Citation <pre><code>@misc{misc_roman_urdu_data_set_458,\n  author = {Sharf,Zareen},\n  howpublished = {UCI Machine Learning Repository},\n  note = {{DOI}: https://doi.org/10.24432/C58325},\n  title = {{Roman Urdu Data Set}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#vaccinchatnlclassification","title":"VaccinChatNLClassification","text":"<p>VaccinChatNL is a Flemish Dutch FAQ dataset on the topic of COVID-19 vaccinations in Flanders.</p> <p>Dataset: <code>clips/VaccinChatNL</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Spoken, Web expert-annotated created Citation <pre><code>@inproceedings{buhmann-etal-2022-domain,\n  address = {Gyeongju, Republic of Korea},\n  author = {Buhmann, Jeska and De Bruyn, Maxime and Lotfi, Ehsan and Daelemans, Walter},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {3539--3549},\n  publisher = {International Committee on Computational Linguistics},\n  title = {Domain- and Task-Adaptation for {V}accin{C}hat{NL}, a {D}utch {COVID}-19 {FAQ} Answering Corpus and Classification Model},\n  url = {https://aclanthology.org/2022.coling-1.312},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#viestudentfeedbackclassification","title":"VieStudentFeedbackClassification","text":"<p>A Vietnamese dataset for classification of student feedback</p> <p>Dataset: <code>mteb/VieStudentFeedbackClassification</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written human-annotated created Citation <pre><code>@inproceedings{8573337,\n  author = {Nguyen, Kiet Van and Nguyen, Vu Duc and Nguyen, Phu X. V. and Truong, Tham T. H. and Nguyen, Ngan Luu-Thuy},\n  booktitle = {2018 10th International Conference on Knowledge and Systems Engineering (KSE)},\n  doi = {10.1109/KSE.2018.8573337},\n  number = {},\n  pages = {19-24},\n  title = {UIT-VSFC: Vietnamese Students\u2019 Feedback Corpus for Sentiment Analysis},\n  volume = {},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#viestudentfeedbackclassificationv2","title":"VieStudentFeedbackClassification.v2","text":"<p>A Vietnamese dataset for classification of student feedback This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/vie_student_feedback</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy vie Reviews, Written human-annotated created Citation <pre><code>@inproceedings{8573337,\n  author = {Nguyen, Kiet Van and Nguyen, Vu Duc and Nguyen, Phu X. V. and Truong, Tham T. H. and Nguyen, Ngan Luu-Thuy},\n  booktitle = {2018 10th International Conference on Knowledge and Systems Engineering (KSE)},\n  doi = {10.1109/KSE.2018.8573337},\n  number = {},\n  pages = {19-24},\n  title = {UIT-VSFC: Vietnamese Students\u2019 Feedback Corpus for Sentiment Analysis},\n  volume = {},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wrimeclassification","title":"WRIMEClassification","text":"<p>A dataset of Japanese social network rated for sentiment</p> <p>Dataset: <code>mteb/WRIMEClassification</code> \u2022 License: https://huggingface.co/datasets/shunk031/wrime#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Social, Written human-annotated found Citation <pre><code>@inproceedings{kajiwara-etal-2021-wrime,\n  address = {Online},\n  author = {Kajiwara, Tomoyuki  and\nChu, Chenhui  and\nTakemura, Noriko  and\nNakashima, Yuta  and\nNagahara, Hajime},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.169},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {2095--2104},\n  publisher = {Association for Computational Linguistics},\n  title = {{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations},\n  url = {https://aclanthology.org/2021.naacl-main.169},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wrimeclassificationv2","title":"WRIMEClassification.v2","text":"<p>A dataset of Japanese social network rated for sentiment This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wrime</code> \u2022 License: https://huggingface.co/datasets/shunk031/wrime#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy jpn Social, Written human-annotated found Citation <pre><code>@inproceedings{kajiwara-etal-2021-wrime,\n  address = {Online},\n  author = {Kajiwara, Tomoyuki  and\nChu, Chenhui  and\nTakemura, Noriko  and\nNakashima, Yuta  and\nNagahara, Hajime},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.169},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {2095--2104},\n  publisher = {Association for Computational Linguistics},\n  title = {{WRIME}: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations},\n  url = {https://aclanthology.org/2021.naacl-main.169},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#waimai","title":"Waimai","text":"<p>Sentiment Analysis of user reviews on takeaway platforms</p> <p>Dataset: <code>C-MTEB/waimai-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#waimaiv2","title":"Waimai.v2","text":"<p>Sentiment Analysis of user reviews on takeaway platforms This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/waimai</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy cmn Reviews, Written derived found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiometchemclassification","title":"WikipediaBioMetChemClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2GeneExpressionVsMetallurgyClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiometchemclassificationv2","title":"WikipediaBioMetChemClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_bio_met_chem</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediabiolumneurochemclassification","title":"WikipediaBiolumNeurochemClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2BioluminescenceVsNeurochemistryClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemengspecialtiesclassification","title":"WikipediaChemEngSpecialtiesClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium5Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemfieldsclassification","title":"WikipediaChemFieldsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEZ10Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemfieldsclassificationv2","title":"WikipediaChemFieldsClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_chem_fields</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediachemistrytopicsclassification","title":"WikipediaChemistryTopicsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy10Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacompchemspectroscopyclassification","title":"WikipediaCompChemSpectroscopyClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2ComputationalVsSpectroscopistsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacompchemspectroscopyclassificationv2","title":"WikipediaCompChemSpectroscopyClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_comp_chem_spectroscopy</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacryobiologyseparationclassification","title":"WikipediaCryobiologySeparationClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy5Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacrystallographyanalyticalclassification","title":"WikipediaCrystallographyAnalyticalClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium2CrystallographyVsChromatographyTitrationpHClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediacrystallographyanalyticalclassificationv2","title":"WikipediaCrystallographyAnalyticalClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_crystallography_analytical</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediagreenhouseenantiopureclassification","title":"WikipediaGreenhouseEnantiopureClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2GreenhouseVsEnantiopureClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediaisotopesfissionclassification","title":"WikipediaIsotopesFissionClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2IsotopesVsFissionProductsNuclearFissionClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipedialuminescenceclassification","title":"WikipediaLuminescenceClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2BioluminescenceVsLuminescenceClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediaorganicinorganicclassification","title":"WikipediaOrganicInorganicClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2SpecialClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediasaltssemiconductorsclassification","title":"WikipediaSaltsSemiconductorsClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaHard2SaltsVsSemiconductorMaterialsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediasolidstatecolloidalclassification","title":"WikipediaSolidStateColloidalClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy2SolidStateVsColloidalClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediatheoreticalappliedclassification","title":"WikipediaTheoreticalAppliedClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEZ2Classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wikipediatheoreticalappliedclassificationv2","title":"WikipediaTheoreticalAppliedClassification.v2","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wikipedia_theoretical_applied</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wisesightsentimentclassification","title":"WisesightSentimentClassification","text":"<p>Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question)</p> <p>Dataset: <code>mteb/WisesightSentimentClassification</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tha News, Social, Written expert-annotated found Citation <pre><code>@software{bact_2019_3457447,\n  author = {Suriyawongkul, Arthit and\nChuangsuwanich, Ekapol and\nChormai, Pattarawat and\nPolpanumas, Charin},\n  doi = {10.5281/zenodo.3457447},\n  month = sep,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/wisesight-sentiment: First release},\n  url = {https://doi.org/10.5281/zenodo.3457447},\n  version = {v1.0},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wisesightsentimentclassificationv2","title":"WisesightSentimentClassification.v2","text":"<p>Wisesight Sentiment Corpus: Social media messages in Thai language with sentiment label (positive, neutral, negative, question) This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/wisesight_sentiment</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 tha News, Social, Written expert-annotated found Citation <pre><code>@software{bact_2019_3457447,\n  author = {Suriyawongkul, Arthit and\nChuangsuwanich, Ekapol and\nChormai, Pattarawat and\nPolpanumas, Charin},\n  doi = {10.5281/zenodo.3457447},\n  month = sep,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/wisesight-sentiment: First release},\n  url = {https://doi.org/10.5281/zenodo.3457447},\n  version = {v1.0},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#wongnaireviewsclassification","title":"WongnaiReviewsClassification","text":"<p>Wongnai features over 200,000 restaurants, beauty salons, and spas across Thailand on its platform, with detailed information about each merchant and user reviews. In this dataset there are 5 classes corresponding each star rating</p> <p>Dataset: <code>Wongnai/wongnai_reviews</code> \u2022 License: lgpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy tha Reviews, Written derived found Citation <pre><code>@software{cstorm125_2020_3852912,\n  author = {cstorm125 and lukkiddd},\n  doi = {10.5281/zenodo.3852912},\n  month = may,\n  publisher = {Zenodo},\n  title = {PyThaiNLP/classification-benchmarks: v0.1-alpha},\n  url = {https://doi.org/10.5281/zenodo.3852912},\n  version = {v0.1-alpha},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yahooanswerstopicsclassification","title":"YahooAnswersTopicsClassification","text":"<p>Dataset composed of questions and answers from Yahoo Answers, categorized into topics.</p> <p>Dataset: <code>mteb/YahooAnswersTopicsClassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Web, Written human-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yahooanswerstopicsclassificationv2","title":"YahooAnswersTopicsClassification.v2","text":"<p>Dataset composed of questions and answers from Yahoo Answers, categorized into topics. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yahoo_answers_topics</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Web, Written human-annotated found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yelpreviewfullclassification","title":"YelpReviewFullClassification","text":"<p>Yelp Review Full is a dataset for sentiment analysis, containing 5 classes corresponding to ratings 1-5.</p> <p>Dataset: <code>Yelp/yelp_review_full</code> \u2022 License: https://huggingface.co/datasets/Yelp/yelp_review_full#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yelpreviewfullclassificationv2","title":"YelpReviewFullClassification.v2","text":"<p>Yelp Review Full is a dataset for sentiment analysis, containing 5 classes corresponding to ratings 1-5. This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yelp_review_full</code> \u2022 License: https://huggingface.co/datasets/Yelp/yelp_review_full#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy eng Reviews, Written derived found Citation <pre><code>@inproceedings{NIPS2015_250cf8b5,\n  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},\n  booktitle = {Advances in Neural Information Processing Systems},\n  editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},\n  pages = {},\n  publisher = {Curran Associates, Inc.},\n  title = {Character-level Convolutional Networks for Text Classification},\n  url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},\n  volume = {28},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yueopenricereviewclassification","title":"YueOpenriceReviewClassification","text":"<p>A Cantonese dataset for review classification</p> <p>Dataset: <code>izhx/yue-openrice-review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy yue Reviews, Spoken human-annotated found Citation <pre><code>@inproceedings{xiang2019sentiment,\n  author = {Xiang, Rong and Jiao, Ying and Lu, Qin},\n  booktitle = {Proceedings of the 8th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)},\n  organization = {KDD WISDOM},\n  pages = {1--9},\n  title = {Sentiment Augmented Attention Network for Cantonese Restaurant Review Analysis},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/classification/#yueopenricereviewclassificationv2","title":"YueOpenriceReviewClassification.v2","text":"<p>A Cantonese dataset for review classification This version corrects errors found in the original data. For details, see pull request</p> <p>Dataset: <code>mteb/yue_openrice_review</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy yue Reviews, Spoken human-annotated found Citation <pre><code>@inproceedings{xiang2019sentiment,\n  author = {Xiang, Rong and Jiao, Ying and Lu, Qin},\n  booktitle = {Proceedings of the 8th KDD Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM)},\n  organization = {KDD WISDOM},\n  pages = {1--9},\n  title = {Sentiment Augmented Attention Network for Cantonese Restaurant Review Analysis},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/","title":"Clustering","text":"<ul> <li>Number of tasks: 109</li> </ul>"},{"location":"overview/available_tasks/clustering/#alloprofclusteringp2p","title":"AlloProfClusteringP2P","text":"<p>Clustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringP2P</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusteringp2pv2","title":"AlloProfClusteringP2P.v2","text":"<p>Clustering of document titles and descriptions from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringP2P.v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusterings2s","title":"AlloProfClusteringS2S","text":"<p>Clustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringS2S</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#alloprofclusterings2sv2","title":"AlloProfClusteringS2S.v2","text":"<p>Clustering of document titles from Allo Prof dataset. Clustering of 10 sets on the document topic.</p> <p>Dataset: <code>mteb/AlloProfClusteringS2S.v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivhierarchicalclusteringp2p","title":"ArXivHierarchicalClusteringP2P","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#arxivhierarchicalclusterings2s","title":"ArXivHierarchicalClusteringS2S","text":"<p>Clustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#arxivclusteringp2p","title":"ArxivClusteringP2P","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivclusteringp2pv2","title":"ArxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#arxivclusterings2s","title":"ArxivClusteringS2S","text":"<p>Clustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/arxiv-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#beytooteclustering","title":"BeytooteClustering","text":"<p>Beytoote Website Articles Clustering</p> <p>Dataset: <code>MCINext/beytoote-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas News derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#bigpatentclustering","title":"BigPatentClustering","text":"<p>Clustering of documents from the Big Patent dataset. Test set only includes documents belonging to a single category, with a total of 9 categories.</p> <p>Dataset: <code>jinaai/big-patent-clustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#bigpatentclusteringv2","title":"BigPatentClustering.v2","text":"<p>Clustering of documents from the Big Patent dataset. Test set only includes documents belonging to a single category, with a total of 9 categories.</p> <p>Dataset: <code>mteb/big-patent</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Legal, Written derived found Citation <pre><code>@article{DBLP:journals/corr/abs-1906-03741,\n  author = {Eva Sharma and\nChen Li and\nLu Wang},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-1906-03741.bib},\n  eprint = {1906.03741},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Wed, 26 Jun 2019 07:14:58 +0200},\n  title = {{BIGPATENT}: A Large-Scale Dataset for Abstractive and Coherent Summarization},\n  url = {http://arxiv.org/abs/1906.03741},\n  volume = {abs/1906.03741},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#biorxivclusteringp2p","title":"BiorxivClusteringP2P","text":"<p>Clustering of titles+abstract from biorxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/biorxiv-clustering-p2p</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusteringp2pv2","title":"BiorxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from biorxiv across 26 categories.</p> <p>Dataset: <code>mteb/biorxiv-clustering-p2p</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusterings2s","title":"BiorxivClusteringS2S","text":"<p>Clustering of titles from biorxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/biorxiv-clustering-s2s</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#biorxivclusterings2sv2","title":"BiorxivClusteringS2S.v2","text":"<p>Clustering of titles from biorxiv across 26 categories.</p> <p>Dataset: <code>mteb/biorxiv-clustering-s2s</code> \u2022 License: https://www.biorxiv.org/content/about-biorxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#blurbsclusteringp2p","title":"BlurbsClusteringP2P","text":"<p>Clustering of book titles+blurbs. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-p2p</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusteringp2pv2","title":"BlurbsClusteringP2P.v2","text":"<p>Clustering of book titles+blurbs. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-p2p</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusterings2s","title":"BlurbsClusteringS2S","text":"<p>Clustering of book titles. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-s2s</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#blurbsclusterings2sv2","title":"BlurbsClusteringS2S.v2","text":"<p>Clustering of book titles. Clustering of 28 sets, either on the main or secondary genre.</p> <p>Dataset: <code>slvnwhrl/blurbs-clustering-s2s</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Fiction, Written derived found Citation <pre><code>@inproceedings{Remus2019GermEval2T,\n  author = {Steffen Remus and Rami Aly and Chris Biemann},\n  booktitle = {Conference on Natural Language Processing},\n  title = {GermEval 2019 Task 1: Hierarchical Classification of Blurbs},\n  url = {https://api.semanticscholar.org/CorpusID:208334484},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#builtbenchclusteringp2p","title":"BuiltBenchClusteringP2P","text":"<p>Clustering of built asset item descriptions based on categories identified within industry classification systems such as IFC, Uniclass, etc.</p> <p>Dataset: <code>mehrzad-shahin/BuiltBench-clustering-p2p</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#builtbenchclusterings2s","title":"BuiltBenchClusteringS2S","text":"<p>Clustering of built asset names/titles based on categories identified within industry classification systems such as IFC, Uniclass, etc.</p> <p>Dataset: <code>mehrzad-shahin/BuiltBench-clustering-s2s</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusteringp2p","title":"CLSClusteringP2P","text":"<p>Clustering of titles + abstract from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringP2P</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@article{li2022csl,\n  author = {Li, Yudong and Zhang, Yuqing and Zhao, Zhe and Shen, Linlin and Liu, Weijie and Mao, Weiquan and Zhang, Hui},\n  journal = {arXiv preprint arXiv:2209.05034},\n  title = {CSL: A large-scale Chinese scientific literature dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusteringp2pv2","title":"CLSClusteringP2P.v2","text":"<p>Clustering of titles + abstract from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>mteb/CLSClusteringP2P.v2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@misc{li2022csl,\n  archiveprefix = {arXiv},\n  author = {Yudong Li and Yuqing Zhang and Zhe Zhao and Linlin Shen and Weijie Liu and Weiquan Mao and Hui Zhang},\n  eprint = {2209.05034},\n  primaryclass = {cs.CL},\n  title = {CSL: A Large-scale Chinese Scientific Literature Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusterings2s","title":"CLSClusteringS2S","text":"<p>Clustering of titles from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringS2S</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@article{li2022csl,\n  author = {Li, Yudong and Zhang, Yuqing and Zhao, Zhe and Shen, Linlin and Liu, Weijie and Mao, Weiquan and Zhang, Hui},\n  journal = {arXiv preprint arXiv:2209.05034},\n  title = {CSL: A large-scale Chinese scientific literature dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clsclusterings2sv2","title":"CLSClusteringS2S.v2","text":"<p>Clustering of titles from CLS dataset. Clustering of 13 sets on the main category.</p> <p>Dataset: <code>C-MTEB/CLSClusteringS2S</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn Academic, Written derived found Citation <pre><code>@misc{li2022csl,\n  archiveprefix = {arXiv},\n  author = {Yudong Li and Yuqing Zhang and Zhe Zhao and Linlin Shen and Weijie Liu and Weiquan Mao and Hui Zhang},\n  eprint = {2209.05034},\n  primaryclass = {cs.CL},\n  title = {CSL: A Large-scale Chinese Scientific Literature Dataset},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#clustrec-covid","title":"ClusTREC-Covid","text":"<p>A Topical Clustering Benchmark for COVID-19 Scientific Research across 50 covid-19 related topics.</p> <p>Dataset: <code>Uri-ka/ClusTREC-Covid</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written expert-annotated created Citation <pre><code>@inproceedings{katz-etal-2024-knowledge,\n  address = {Miami, Florida, USA},\n  author = {Katz, Uri  and\nLevy, Mosh  and\nGoldberg, Yoav},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},\n  month = nov,\n  pages = {8838--8855},\n  publisher = {Association for Computational Linguistics},\n  title = {Knowledge Navigator: {LLM}-guided Browsing Framework for Exploratory Search in Scientific Literature},\n  url = {https://aclanthology.org/2024.findings-emnlp.516},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#digikalamagclustering","title":"DigikalamagClustering","text":"<p>A total of 8,515 articles scraped from Digikala Online Magazine. This dataset includes seven different classes.</p> <p>Dataset: <code>mteb/DigikalamagClustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Web derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#dutchnewsarticlesclusteringp2p","title":"DutchNewsArticlesClusteringP2P","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld News, Written derived found"},{"location":"overview/available_tasks/clustering/#dutchnewsarticlesclusterings2s","title":"DutchNewsArticlesClusteringS2S","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld News, Written derived found"},{"location":"overview/available_tasks/clustering/#eighttagsclustering","title":"EightTagsClustering","text":"<p>Clustering of headlines from social media posts in Polish belonging to 8 categories: film, history, food, medicine, motorization, work, sport and technology.</p> <p>Dataset: <code>PL-MTEB/8tags-clustering</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Social, Written derived found Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\\\l}kiewicz, Micha{\\\\l}  and\nPo{\\\\'s}wiata, Rafa{\\\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#eighttagsclusteringv2","title":"EightTagsClustering.v2","text":"<p>Clustering of headlines from social media posts in Polish belonging to 8 categories: film, history, food, medicine, motorization, work, sport and technology.</p> <p>Dataset: <code>PL-MTEB/8tags-clustering</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Social, Written derived found Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\\\l}kiewicz, Micha{\\\\l}  and\nPo{\\\\'s}wiata, Rafa{\\\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\\\'e}chet, Fr{\\\\'e}d{\\\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\\\'e}l{\\\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#georeviewclusteringp2p","title":"GeoreviewClusteringP2P","text":"<p>Review clustering based on Yandex Georeview dataset</p> <p>Dataset: <code>ai-forever/georeview-clustering-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Reviews, Written derived found"},{"location":"overview/available_tasks/clustering/#halclusterings2s","title":"HALClusteringS2S","text":"<p>Clustering of titles from HAL (https://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s)</p> <p>Dataset: <code>lyon-nlp/clustering-hal-s2s</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Academic, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#halclusterings2sv2","title":"HALClusteringS2S.v2","text":"<p>Clustering of titles from HAL (https://huggingface.co/datasets/lyon-nlp/clustering-hal-s2s)</p> <p>Dataset: <code>mteb/HALClusteringS2S.v2</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fra Academic, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humearxivclusteringp2p","title":"HUMEArxivClusteringP2P","text":"<p>Human evaluation subset of Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category</p> <p>Dataset: <code>mteb/mteb-human-arxiv-clustering</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Academic, Written derived found Citation <pre><code>@misc{arxiv_org_submitters_2024,\n  author = {arXiv.org submitters},\n  doi = {10.34740/KAGGLE/DSV/7548853},\n  publisher = {Kaggle},\n  title = {arXiv Dataset},\n  url = {https://www.kaggle.com/dsv/7548853},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humeredditclusteringp2p","title":"HUMERedditClusteringP2P","text":"<p>Human evaluation subset of Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/mteb-human-reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humesib200clusterings2s","title":"HUMESIB200ClusteringS2S","text":"<p>Human evaluation subset of Clustering of news article headlines from SIB-200. Clustering of 10 sets, each with 8 categories and 10 texts per category.</p> <p>Dataset: <code>mteb/mteb-human-sib200-clustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure ara, dan, eng, fra, rus News, Written derived found Citation <pre><code>@inproceedings{adelani-etal-2023-sib,\n  address = {Toronto, Canada},\n  author = {Adelani, David Ifeoluwa  and\nHedderich, Michael A.  and\nZhu, Dawei  and\nvan den Berg, Esther  and\nKlakow, Dietrich},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2023.acl-long.660},\n  month = jul,\n  pages = {11784--11801},\n  publisher = {Association for Computational Linguistics},\n  title = {{SIB}-200: A Large-Scale News Classification Dataset for Over 200 Languages},\n  url = {https://aclanthology.org/2023.acl-long.660},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#humewikicitiesclustering","title":"HUMEWikiCitiesClustering","text":"<p>Human evaluation subset of Clustering of Wikipedia articles of cities by country from https://huggingface.co/datasets/wikipedia. Test set includes 126 countries, and a total of 3531 cities.</p> <p>Dataset: <code>mteb/mteb-human-wikicities-clustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure eng Encyclopaedic, Written derived found Citation <pre><code>@online{wikidump2024,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#hamshahriclustring","title":"HamshahriClustring","text":"<p>These datasets have been extracted from the RSS feed of two Farsi news agency websites.</p> <p>Dataset: <code>community-datasets/farsi_news</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas News derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#iconclassclusterings2s","title":"IconclassClusteringS2S","text":"<p>Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. The task is to classify the first layer of Iconclass</p> <p>Dataset: <code>clips/mteb-nl-iconclass-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Fiction, Written derived found Citation <pre><code>@article{banar2023transfer,\n  author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},\n  journal = {ACM Journal on Computing and Cultural Heritage},\n  number = {2},\n  pages = {1--16},\n  publisher = {ACM New York, NY},\n  title = {Transfer learning for the visual arts: The multi-modal retrieval of iconclass codes},\n  volume = {16},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#indicreviewsclusteringp2p","title":"IndicReviewsClusteringP2P","text":"<p>Clustering of reviews from IndicSentiment dataset. Clustering of 14 sets on the generic categories label.</p> <p>Dataset: <code>mteb/IndicReviewsClusteringP2P</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure asm, ben, brx, guj, hin, ... (13) Reviews, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#kluemrcdomainclustering","title":"KlueMrcDomainClustering","text":"<p>this dataset is a processed and redistributed version of the KLUE-MRC dataset. Domain: Game / Media / Automotive / Finance / Real Estate / Education</p> <p>Dataset: <code>mteb/KlueMrcDomainClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#klueynatmrccategoryclustering","title":"KlueYnatMrcCategoryClustering","text":"<p>this dataset is a processed and redistributed version of the KLUE-Ynat &amp; KLUE-MRC  dataset. News_category: IT/Science, Sports, Media/Culture, Ecomomy/Finance, Real Estate</p> <p>Dataset: <code>mteb/KlueYnatMrcCategoryClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) v_measure kor News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#livedoornewsclustering","title":"LivedoorNewsClustering","text":"<p>Clustering of the news reports of a Japanese news site, Livedoor News by RONDHUIT Co, Ltd. in 2012. It contains over 7,000 news report texts across 9 categories (topics).</p> <p>Dataset: <code>mteb/LivedoorNewsClustering</code> \u2022 License: cc-by-nd-2.1-jp \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found"},{"location":"overview/available_tasks/clustering/#livedoornewsclusteringv2","title":"LivedoorNewsClustering.v2","text":"<p>Clustering of the news reports of a Japanese news site, Livedoor News by RONDHUIT Co, Ltd. in 2012. It contains over 7,000 news report texts across 9 categories (topics). Version 2 updated on LivedoorNewsClustering by removing pairs where one of entries contain an empty sentences.</p> <p>Dataset: <code>mteb/LivedoorNewsClustering.v2</code> \u2022 License: cc-by-nd-2.1-jp \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found"},{"location":"overview/available_tasks/clustering/#mlsumclusteringp2p","title":"MLSUMClusteringP2P","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusteringp2pv2","title":"MLSUMClusteringP2P.v2","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusterings2s","title":"MLSUMClusteringS2S","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#mlsumclusterings2sv2","title":"MLSUMClusteringS2S.v2","text":"<p>Clustering of newspaper article contents and titles from MLSUM dataset. Clustering of 10 sets on the newpaper article topics.</p> <p>Dataset: <code>mteb/mlsum</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu, fra, rus, spa News, Written derived found Citation <pre><code>@article{scialom2020mlsum,\n  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo},\n  journal = {arXiv preprint arXiv:2004.14900},\n  title = {MLSUM: The Multilingual Summarization Corpus},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#masakhanewsclusteringp2p","title":"MasakhaNEWSClusteringP2P","text":"<p>Clustering of news article headlines and texts from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.</p> <p>Dataset: <code>mteb/MasakhaNEWSClusteringP2P</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure amh, eng, fra, hau, ibo, ... (16) News, Non-fiction, Written derived found Citation <pre><code>@article{adelani2023masakhanews,\n  author = {David Ifeoluwa Adelani and  Marek Masiak and  Israel Abebe Azime and  Jesujoba Oluwadara Alabi and  Atnafu Lambebo Tonja and  Christine Mwase and  Odunayo Ogundepo and  Bonaventure F. P. Dossou and  Akintunde Oladipo and  Doreen Nixdorf and  Chris Chinenye Emezue and  Sana Sabah al-azzawi and  Blessing K. Sibanda and  Davis David and  Lolwethu Ndolela and  Jonathan Mukiibi and  Tunde Oluwaseyi Ajayi and  Tatiana Moteu Ngoli and  Brian Odhiambo and  Abraham Toluwase Owodunni and  Nnaemeka C. Obiefuna and  Shamsuddeen Hassan Muhammad and  Saheed Salahudeen Abdullahi and  Mesay Gemeda Yigezu and  Tajuddeen Gwadabe and  Idris Abdulmumin and  Mahlet Taye Bame and  Oluwabusayo Olufunke Awoyomi and  Iyanuoluwa Shode and  Tolulope Anu Adelani and  Habiba Abdulganiy Kailani and  Abdul-Hakeem Omotayo and  Adetola Adeeko and  Afolabi Abeeb and  Anuoluwapo Aremu and  Olanrewaju Samuel and  Clemencia Siro and  Wangari Kimotho and  Onyekachi Raphael Ogbu and  Chinedu E. Mbonu and  Chiamaka I. Chukwuneke and  Samuel Fanijo and  Jessica Ojo and  Oyinkansola F. Awosan and  Tadesse Kebede Guge and  Sakayo Toadoum Sari and  Pamela Nyatsine and  Freedmore Sidume and  Oreen Yousuf and  Mardiyyah Oduwole and  Ussen Kimanuka and  Kanda Patrick Tshinu and  Thina Diko and  Siyanda Nxakama and   Abdulmejid Tuni Johar and  Sinodos Gebre and  Muhidin Mohamed and  Shafie Abdi Mohamed and  Fuad Mire Hassan and  Moges Ahmed Mehamed and  Evrard Ngabire and  and Pontus Stenetorp},\n  journal = {ArXiv},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  volume = {},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#masakhanewsclusterings2s","title":"MasakhaNEWSClusteringS2S","text":"<p>Clustering of news article headlines from MasakhaNEWS dataset. Clustering of 10 sets on the news article label.</p> <p>Dataset: <code>mteb/MasakhaNEWSClusteringS2S</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure amh, eng, fra, hau, ibo, ... (16) News, Written human-annotated found Citation <pre><code>@article{adelani2023masakhanews,\n  author = {David Ifeoluwa Adelani and  Marek Masiak and  Israel Abebe Azime and  Jesujoba Oluwadara Alabi and  Atnafu Lambebo Tonja and  Christine Mwase and  Odunayo Ogundepo and  Bonaventure F. P. Dossou and  Akintunde Oladipo and  Doreen Nixdorf and  Chris Chinenye Emezue and  Sana Sabah al-azzawi and  Blessing K. Sibanda and  Davis David and  Lolwethu Ndolela and  Jonathan Mukiibi and  Tunde Oluwaseyi Ajayi and  Tatiana Moteu Ngoli and  Brian Odhiambo and  Abraham Toluwase Owodunni and  Nnaemeka C. Obiefuna and  Shamsuddeen Hassan Muhammad and  Saheed Salahudeen Abdullahi and  Mesay Gemeda Yigezu and  Tajuddeen Gwadabe and  Idris Abdulmumin and  Mahlet Taye Bame and  Oluwabusayo Olufunke Awoyomi and  Iyanuoluwa Shode and  Tolulope Anu Adelani and  Habiba Abdulganiy Kailani and  Abdul-Hakeem Omotayo and  Adetola Adeeko and  Afolabi Abeeb and  Anuoluwapo Aremu and  Olanrewaju Samuel and  Clemencia Siro and  Wangari Kimotho and  Onyekachi Raphael Ogbu and  Chinedu E. Mbonu and  Chiamaka I. Chukwuneke and  Samuel Fanijo and  Jessica Ojo and  Oyinkansola F. Awosan and  Tadesse Kebede Guge and  Sakayo Toadoum Sari and  Pamela Nyatsine and  Freedmore Sidume and  Oreen Yousuf and  Mardiyyah Oduwole and  Ussen Kimanuka and  Kanda Patrick Tshinu and  Thina Diko and  Siyanda Nxakama and   Abdulmejid Tuni Johar and  Sinodos Gebre and  Muhidin Mohamed and  Shafie Abdi Mohamed and  Fuad Mire Hassan and  Moges Ahmed Mehamed and  Evrard Ngabire and  and Pontus Stenetorp},\n  journal = {ArXiv},\n  title = {MasakhaNEWS: News Topic Classification for African languages},\n  volume = {},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#medrxivclusteringp2p","title":"MedrxivClusteringP2P","text":"<p>Clustering of titles+abstract from medrxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/medrxiv-clustering-p2p</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusteringp2pv2","title":"MedrxivClusteringP2P.v2","text":"<p>Clustering of titles+abstract from medrxiv across 51 categories.</p> <p>Dataset: <code>mteb/medrxiv-clustering-p2p</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusterings2s","title":"MedrxivClusteringS2S","text":"<p>Clustering of titles from medrxiv. Clustering of 10 sets, based on the main category.</p> <p>Dataset: <code>mteb/medrxiv-clustering-s2s</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#medrxivclusterings2sv2","title":"MedrxivClusteringS2S.v2","text":"<p>Clustering of titles from medrxiv across 51 categories.</p> <p>Dataset: <code>mteb/medrxiv-clustering-s2s</code> \u2022 License: https://www.medrxiv.org/content/about-medrxiv \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Academic, Medical, Written derived created"},{"location":"overview/available_tasks/clustering/#mewsc16jaclustering","title":"MewsC16JaClustering","text":"<p>MewsC-16 (Multilingual Short Text Clustering Dataset for News in 16 languages) is constructed from Wikinews. This dataset is the Japanese split of MewsC-16, containing topic sentences from Wikinews articles in 12 categories. More detailed information is available in the Appendix E of the citation.</p> <p>Dataset: <code>mteb/MewsC16JaClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure jpn News, Written derived found Citation <pre><code>@inproceedings{nishikawa-etal-2022-ease,\n  address = {Seattle, United States},\n  author = {Nishikawa, Sosuke  and\nRi, Ryokan  and\nYamada, Ikuya  and\nTsuruoka, Yoshimasa  and\nEchizen, Isao},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  month = jul,\n  pages = {3870--3885},\n  publisher = {Association for Computational Linguistics},\n  title = {{EASE}: Entity-Aware Contrastive Learning of Sentence Embedding},\n  url = {https://aclanthology.org/2022.naacl-main.284},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#nlptwitteranalysisclustering","title":"NLPTwitterAnalysisClustering","text":"<p>Clustering of tweets from twitter across 26 categories.</p> <p>Dataset: <code>hamedhf/nlp_twitter_analysis</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Social derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#opentenderclusteringp2p","title":"OpenTenderClusteringP2P","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-opentender-cls-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#opentenderclusterings2s","title":"OpenTenderClusteringS2S","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-opentender-clst-s2s-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#plscclusteringp2p","title":"PlscClusteringP2P","text":"<p>Clustering of Polish article titles+abstracts from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-p2p</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusteringp2pv2","title":"PlscClusteringP2P.v2","text":"<p>Clustering of Polish article titles+abstracts from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>mteb/PlscClusteringP2P.v2</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusterings2s","title":"PlscClusteringS2S","text":"<p>Clustering of Polish article titles from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#plscclusterings2sv2","title":"PlscClusteringS2S.v2","text":"<p>Clustering of Polish article titles from Library of Science (https://bibliotekanauki.pl/), either on the scientific field or discipline.</p> <p>Dataset: <code>PL-MTEB/plsc-clustering-s2s</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure pol Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#redditclustering","title":"RedditClustering","text":"<p>Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclustering-vn","title":"RedditClustering-VN","text":"<p>A translated dataset from Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/reddit-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Social, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringv2","title":"RedditClustering.v2","text":"<p>Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/reddit-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2p","title":"RedditClusteringP2P","text":"<p>Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/reddit-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2p-vn","title":"RedditClusteringP2P-VN","text":"<p>A translated dataset from Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/reddit-clustering-p2p-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Social, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#redditclusteringp2pv2","title":"RedditClusteringP2P.v2","text":"<p>Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs.</p> <p>Dataset: <code>mteb/reddit-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Social, Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#romanibibleclustering","title":"RomaniBibleClustering","text":"<p>Clustering verses from the Bible in Kalderash Romani by book.</p> <p>Dataset: <code>mteb/RomaniBibleClustering</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rom Religious, Written derived human-translated and localized"},{"location":"overview/available_tasks/clustering/#ruscibenchgrnticlusteringp2p","title":"RuSciBenchGRNTIClusteringP2P","text":"<p>Clustering of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-grnti-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#ruscibenchoecdclusteringp2p","title":"RuSciBenchOECDClusteringP2P","text":"<p>Clustering of scientific papers (title+abstract) by rubric</p> <p>Dataset: <code>ai-forever/ru-scibench-oecd-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure rus Academic, Written derived found"},{"location":"overview/available_tasks/clustering/#sib200clusterings2s","title":"SIB200ClusteringS2S","text":"<p>SIB-200 is the largest publicly available topic classification dataset based on Flores-200 covering 205 languages and dialects annotated. The dataset is annotated in English for the topics, science/technology, travel, politics, sports, health, entertainment, and geography. The labels are then transferred to the other languages in Flores-200 which are human-translated.</p> <p>Dataset: <code>mteb/sib200</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure ace, acm, acq, aeb, afr, ... (197) News, Written expert-annotated human-translated and localized Citation <pre><code>@article{adelani2023sib,\n  author = {Adelani, David Ifeoluwa and Liu, Hannah and Shen, Xiaoyu and Vassilyev, Nikita and Alabi, Jesujoba O and Mao, Yanke and Gao, Haonan and Lee, Annie En-Shiun},\n  journal = {arXiv preprint arXiv:2309.07445},\n  title = {SIB-200: A simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#sidclustring","title":"SIDClustring","text":"<p>Clustering of summariesfrom SIDClustring across categories.</p> <p>Dataset: <code>MCINext/sid-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure fas Academic derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlclustering","title":"SNLClustering","text":"<p>Webscraped articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>adrlau/navjordj-SNL_summarization_copy</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlhierarchicalclusteringp2p","title":"SNLHierarchicalClusteringP2P","text":"<p>Webscrabed articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>mteb/SNLHierarchicalClusteringP2P</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#snlhierarchicalclusterings2s","title":"SNLHierarchicalClusteringS2S","text":"<p>Webscrabed articles from the Norwegian lexicon 'Det Store Norske Leksikon'. Uses articles categories as clusters.</p> <p>Dataset: <code>mteb/SNLHierarchicalClusteringS2S</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#spanishnewsclusteringp2p","title":"SpanishNewsClusteringP2P","text":"<p>Clustering of news articles, 7 topics in total.</p> <p>Dataset: <code>jinaai/spanish_news_clustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure spa News, Written derived found Citation <pre><code>@misc{kevinmorgado2019spanish,\n  author = {Kevin Morgado},\n  howpublished = {Kaggle},\n  title = {Spanish News Classification},\n  url = {https://www.kaggle.com/datasets/kevinmorgado/spanish-news-classification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclustering","title":"StackExchangeClustering","text":"<p>Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/stackexchange-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclustering-vn","title":"StackExchangeClustering-VN","text":"<p>A translated dataset from Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stackexchange-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringv2","title":"StackExchangeClustering.v2","text":"<p>Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences.</p> <p>Dataset: <code>mteb/stackexchange-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2p","title":"StackExchangeClusteringP2P","text":"<p>Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs.</p> <p>Dataset: <code>mteb/stackexchange-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2p-vn","title":"StackExchangeClusteringP2P-VN","text":"<p>A translated Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stackexchange-clustering-p2p-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#stackexchangeclusteringp2pv2","title":"StackExchangeClusteringP2P.v2","text":"<p>Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs.</p> <p>Dataset: <code>mteb/stackexchange-clustering-p2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Web, Written derived found Citation <pre><code>@article{geigle:2021:arxiv,\n  archiveprefix = {arXiv},\n  author = {Gregor Geigle and\nNils Reimers and\nAndreas R{\\\"u}ckl{\\'e} and\nIryna Gurevych},\n  eprint = {2104.07081},\n  journal = {arXiv preprint},\n  title = {TWEAC: Transformer with Extendable QA Agent Classifiers},\n  url = {http://arxiv.org/abs/2104.07081},\n  volume = {abs/2104.07081},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclustering","title":"SwednClustering","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClustering</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclusteringp2p","title":"SwednClusteringP2P","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClusteringP2P</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#swednclusterings2s","title":"SwednClusteringS2S","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure. This dataset uses the category labels as clusters.</p> <p>Dataset: <code>mteb/SwednClusteringS2S</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#tenkgnadclusteringp2p","title":"TenKGnadClusteringP2P","text":"<p>Clustering of news article titles+subheadings+texts. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-p2p</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu Web, Written derived found"},{"location":"overview/available_tasks/clustering/#tenkgnadclusteringp2pv2","title":"TenKGnadClusteringP2P.v2","text":"<p>Clustering of news article titles+subheadings+texts. Clustering of 10 splits on the news article category. v2 uses a faster evaluation method used in the MMTEB paper, which allow for notably faster evaluation.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-p2p</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written derived found"},{"location":"overview/available_tasks/clustering/#tenkgnadclusterings2s","title":"TenKGnadClusteringS2S","text":"<p>Clustering of news article titles. Clustering of 10 splits on the news article category.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-s2s</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written derived found"},{"location":"overview/available_tasks/clustering/#tenkgnadclusterings2sv2","title":"TenKGnadClusteringS2S.v2","text":"<p>Clustering of news article titles. Clustering of 10 splits on the news article category. v2 uses a faster evaluation method used in the MMTEB paper, which allow for notably faster evaluation.</p> <p>Dataset: <code>slvnwhrl/tenkgnad-clustering-s2s</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure deu News, Non-fiction, Written derived found"},{"location":"overview/available_tasks/clustering/#thunewsclusteringp2p","title":"ThuNewsClusteringP2P","text":"<p>Clustering of titles + abstracts from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringP2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@inproceedings{eisner2007proceedings,\n  author = {Eisner, Jason},\n  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},\n  title = {Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n  year = {2007},\n}\n\n@inproceedings{li2006comparison,\n  author = {Li, Jingyang and Sun, Maosong and Zhang, Xian},\n  booktitle = {proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics},\n  pages = {545--552},\n  title = {A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusteringp2pv2","title":"ThuNewsClusteringP2P.v2","text":"<p>Clustering of titles + abstracts from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringP2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@software{sun2016thuctc,\n  author = {Sun, M. and Li, J. and Guo, Z. and Yu, Z. and Zheng, Y. and Si, X. and Liu, Z.},\n  note = {THU Chinese Text Classification Toolkit},\n  publisher = {THU Natural Language Processing Lab},\n  title = {THUCTC: An Efficient Chinese Text Classifier},\n  url = {https://github.com/thunlp/THUCTC},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusterings2s","title":"ThuNewsClusteringS2S","text":"<p>Clustering of titles from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@inproceedings{eisner2007proceedings,\n  author = {Eisner, Jason},\n  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},\n  title = {Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n  year = {2007},\n}\n\n@inproceedings{li2006comparison,\n  author = {Li, Jingyang and Sun, Maosong and Zhang, Xian},\n  booktitle = {proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics},\n  pages = {545--552},\n  title = {A comparison and semi-quantitative analysis of words and character-bigrams as features in chinese text categorization},\n  year = {2006},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#thunewsclusterings2sv2","title":"ThuNewsClusteringS2S.v2","text":"<p>Clustering of titles from the THUCNews dataset</p> <p>Dataset: <code>C-MTEB/ThuNewsClusteringS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure cmn News, Written derived found Citation <pre><code>@software{sun2016thuctc,\n  author = {Sun, M. and Li, J. and Guo, Z. and Yu, Z. and Zheng, Y. and Si, X. and Liu, Z.},\n  note = {THU Chinese Text Classification Toolkit},\n  publisher = {THU Natural Language Processing Lab},\n  title = {THUCTC: An Efficient Chinese Text Classifier},\n  url = {https://github.com/thunlp/THUCTC},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclustering","title":"TwentyNewsgroupsClustering","text":"<p>Clustering of the 20 Newsgroups dataset (subject only).</p> <p>Dataset: <code>mteb/twentynewsgroups-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng News, Written derived found Citation <pre><code>@incollection{LANG1995331,\n  address = {San Francisco (CA)},\n  author = {Ken Lang},\n  booktitle = {Machine Learning Proceedings 1995},\n  doi = {https://doi.org/10.1016/B978-1-55860-377-6.50048-7},\n  editor = {Armand Prieditis and Stuart Russell},\n  isbn = {978-1-55860-377-6},\n  pages = {331-339},\n  publisher = {Morgan Kaufmann},\n  title = {NewsWeeder: Learning to Filter Netnews},\n  url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500487},\n  year = {1995},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclustering-vn","title":"TwentyNewsgroupsClustering-VN","text":"<p>A translated dataset from Clustering of the 20 Newsgroups dataset (subject only). The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twentynewsgroups-clustering-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure vie News, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#twentynewsgroupsclusteringv2","title":"TwentyNewsgroupsClustering.v2","text":"<p>Clustering of the 20 Newsgroups dataset (subject only).</p> <p>Dataset: <code>mteb/twentynewsgroups-clustering</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng News, Written derived found Citation <pre><code>@incollection{LANG1995331,\n  address = {San Francisco (CA)},\n  author = {Ken Lang},\n  booktitle = {Machine Learning Proceedings 1995},\n  doi = {https://doi.org/10.1016/B978-1-55860-377-6.50048-7},\n  editor = {Armand Prieditis and Stuart Russell},\n  isbn = {978-1-55860-377-6},\n  pages = {331-339},\n  publisher = {Morgan Kaufmann},\n  title = {NewsWeeder: Learning to Filter Netnews},\n  url = {https://www.sciencedirect.com/science/article/pii/B9781558603776500487},\n  year = {1995},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vabbclusteringp2p","title":"VABBClusteringP2P","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vabbclusterings2s","title":"VABBClusteringS2S","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-cls</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vgclustering","title":"VGClustering","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vghierarchicalclusteringp2p","title":"VGHierarchicalClusteringP2P","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#vghierarchicalclusterings2s","title":"VGHierarchicalClusteringS2S","text":"<p>Articles and their classes (e.g. sports) from VG news articles extracted from Norsk Aviskorpus.</p> <p>Dataset: <code>navjordj/VG_summarization</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure nob News, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikicitiesclustering","title":"WikiCitiesClustering","text":"<p>Clustering of Wikipedia articles of cities by country from https://huggingface.co/datasets/wikipedia. Test set includes 126 countries, and a total of 3531 cities.</p> <p>Dataset: <code>mteb/WikiCitiesClustering</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Encyclopaedic, Written derived found Citation <pre><code>@online{wikidump2024,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikiclusteringp2p","title":"WikiClusteringP2P","text":"<p>Clustering of wikipedia articles inspired by BlubrbsClusteringP2P. Labels are taken from top-level categories of the respective languages (e.g., https://lv.wikipedia.org/wiki/Kategorija:Pamatkategorijas).</p> <p>Dataset: <code>ryzzlestrizzle/multi-wiki-clustering-p2p</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure bos, cat, ces, dan, eus, ... (14) Encyclopaedic, Written derived created"},{"location":"overview/available_tasks/clustering/#wikiclusteringp2pv2","title":"WikiClusteringP2P.v2","text":"<p>Clustering of wikipedia articles inspired by BlubrbsClusteringP2P. Labels are taken from top-level categories of the respective languages (e.g., https://lv.wikipedia.org/wiki/Kategorija:Pamatkategorijas).</p> <p>Dataset: <code>mteb/WikiClusteringP2P.v2</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure bos, cat, ces, dan, eus, ... (14) Encyclopaedic, Written derived created"},{"location":"overview/available_tasks/clustering/#wikipediachemistrytopicsclustering","title":"WikipediaChemistryTopicsClustering","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaEasy10Clustering</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/clustering/#wikipediaspecialtiesinchemistryclustering","title":"WikipediaSpecialtiesInChemistryClustering","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/WikipediaMedium5Clustering</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) v_measure eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/","title":"Compositionality","text":"<ul> <li>Number of tasks: 7</li> </ul>"},{"location":"overview/available_tasks/compositionality/#arococoorder","title":"AROCocoOrder","text":"<p>Compositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.</p> <p>Dataset: <code>mteb/ARO-COCO-order</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#aroflickrorder","title":"AROFlickrOrder","text":"<p>Compositionality Evaluation of images to their captions.Each capation has four hard negatives created by order permutations.</p> <p>Dataset: <code>mteb/ARO-Flickr-Order</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#arovisualattribution","title":"AROVisualAttribution","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>mteb/ARO-Visual-Attribution</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#arovisualrelation","title":"AROVisualRelation","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>mteb/ARO-Visual-Relation</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@inproceedings{yuksekgonul2023and,\n  author = {Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and Jurafsky, Dan and Zou, James},\n  booktitle = {The Eleventh International Conference on Learning Representations},\n  title = {When and why vision-language models behave like bags-of-words, and what to do about it?},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#imagecode","title":"ImageCoDe","text":"<p>Identify the correct image from a set of similar images based on a precise caption.</p> <p>Dataset: <code>mteb/imagecode-multi</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) image_acc eng Web, Written derived found Citation <pre><code>@article{krojer2022image,\n  author = {Krojer, Benno and Adlakha, Vaibhav and Vineet, Vibhav and Goyal, Yash and Ponti, Edoardo and Reddy, Siva},\n  journal = {arXiv preprint arXiv:2203.15867},\n  title = {Image retrieval from contextual descriptions},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#sugarcrepe","title":"SugarCrepe","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>mteb/SUGARCREPE_fmt</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) text_acc eng Encyclopaedic expert-annotated created Citation <pre><code>@article{hsieh2024sugarcrepe,\n  author = {Hsieh, Cheng-Yu and Zhang, Jieyu and Ma, Zixian and Kembhavi, Aniruddha and Krishna, Ranjay},\n  journal = {Advances in neural information processing systems},\n  title = {Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality},\n  volume = {36},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/compositionality/#winoground","title":"Winoground","text":"<p>Compositionality Evaluation of images to their captions.</p> <p>Dataset: <code>facebook/winoground</code> \u2022 License: https://huggingface.co/datasets/facebook/winoground/blob/main/license_agreement.txt \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Social expert-annotated created Citation <pre><code>@misc{thrush2022winogroundprobingvisionlanguage,\n  archiveprefix = {arXiv},\n  author = {Tristan Thrush and Ryan Jiang and Max Bartolo and Amanpreet Singh and Adina Williams and Douwe Kiela and Candace Ross},\n  eprint = {2204.03162},\n  primaryclass = {cs.CV},\n  title = {Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},\n  url = {https://arxiv.org/abs/2204.03162},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/","title":"DocumentUnderstanding","text":"<ul> <li>Number of tasks: 72</li> </ul>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrairbnbsyntheticretrieval","title":"JinaVDRAirbnbSyntheticRetrieval","text":"<p>Retrieve rendered tables from Airbnb listings based on templated queries. This dataset is created from the original Kaggle New York City Airbnb Open Data dataset.</p> <p>Dataset: <code>jinaai/airbnb-synthetic-retrieval_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, deu, eng, fra, hin, ... (10) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarabicchartqaretrieval","title":"JinaVDRArabicChartQARetrieval","text":"<p>Retrieve Arabic charts based on queries. This dataset is derived from the Arabic ChartQA dataset, reformatting the train split as a test split with modified field names such that it is compatible with the ViDoRe evaluation benchmark.</p> <p>Dataset: <code>jinaai/arabic_chartqa_ar_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarabicinfographicsvqaretrieval","title":"JinaVDRArabicInfographicsVQARetrieval","text":"<p>Retrieve Arabic infographics based on queries. This dataset is derived from the Arabic Infographics VQA dataset, reformatting the train split as a test split with modified field names so it can be used in the ViDoRe evaluation benchmark.</p> <p>Dataset: <code>jinaai/arabic_infographicsvqa_ar_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrarxivqaretrieval","title":"JinaVDRArxivQARetrieval","text":"<p>Retrieve figures from scientific papers from arXiv based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/arxivqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrautomobilecatelogretrieval","title":"JinaVDRAutomobileCatelogRetrieval","text":"<p>Retrieve automobile marketing documents based on LLM generated queries. Marketing document from Toyota Japanese website featuring RAV4 and Corolla. The <code>text_description</code> column contains OCR text extracted from the images using EasyOCR.</p> <p>Dataset: <code>jinaai/automobile_catalogue_jp_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Engineering, Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrbeveragescatalogueretrieval","title":"JinaVDRBeveragesCatalogueRetrieval","text":"<p>Retrieve beverages marketing documents based on LLM generated queries. This dataset was self-curated by searching beverage catalogs on Google search and downloading PDFs.</p> <p>Dataset: <code>jinaai/beverages_catalogue_ru_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 rus Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrcharxivocrretrieval","title":"JinaVDRCharXivOCRRetrieval","text":"<p>Retrieve charts from scientific papers based on human annotated queries. This dataset is derived from the CharXiv dataset, reformatting the test split with modified field names, so that it can be used in the ViDoRe benchmark.</p> <p>Dataset: <code>jinaai/CharXiv-en_beir</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrchartqaretrieval","title":"JinaVDRChartQARetrieval","text":"<p>Retrieve charts based on LLM generated queries. Source datasets https://huggingface.co/datasets/HuggingFaceM4/ChartQA</p> <p>Dataset: <code>jinaai/ChartQA_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqaai","title":"JinaVDRDocQAAI","text":"<p>Retrieve AI documents based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/docqa_artificial_intelligence_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqaenergyretrieval","title":"JinaVDRDocQAEnergyRetrieval","text":"<p>Retrieve energy industry documents based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/docqa_energy_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqagovreportretrieval","title":"JinaVDRDocQAGovReportRetrieval","text":"<p>Retrieve government reports based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/docqa_gov_report_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Government derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocqahealthcareindustryretrieval","title":"JinaVDRDocQAHealthcareIndustryRetrieval","text":"<p>Retrieve healthcare industry documents based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark. For more information regarding the filtering please read our paper or this discussion on github.</p> <p>Dataset: <code>jinaai/docqa_healthcare_industry_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdocvqaretrieval","title":"JinaVDRDocVQARetrieval","text":"<p>Retrieve industry documents based on human annotated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/docvqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrdonutvqaisynhmpretrieval","title":"JinaVDRDonutVQAISynHMPRetrieval","text":"<p>Retrieve medical records based on templated queries. Source dataset https://huggingface.co/datasets/warshakhan/donut_vqa_ISynHMP</p> <p>Dataset: <code>jinaai/donut_vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanadenewsretrieval","title":"JinaVDREuropeanaDeNewsRetrieval","text":"<p>Retrieve German news articles based on LLM generated queries. This dataset was created from records of the Europeana online collection by selecting scans of German news articles</p> <p>Dataset: <code>jinaai/europeana-de-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanaesnewsretrieval","title":"JinaVDREuropeanaEsNewsRetrieval","text":"<p>Retrieve Spanish news articles based on LLM generated queries. This dataset was created from records of the Europeana online collection by selecting scans of Spanish news articles</p> <p>Dataset: <code>jinaai/europeana-es-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 spa News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanafrnewsretrieval","title":"JinaVDREuropeanaFrNewsRetrieval","text":"<p>Retrieve French news articles from Europeana based on LLM generated queries. This dataset was created from records of the Europeana online collection by selecting scans of French news articles.</p> <p>Dataset: <code>jinaai/europeana-fr-news_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 fra News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeanaitscansretrieval","title":"JinaVDREuropeanaItScansRetrieval","text":"<p>Retrieve Italian historical articles based on LLM generated queries. This dataset was created from records of the Europeana online collection by selecting scans of Italian news articles</p> <p>Dataset: <code>jinaai/europeana-it-scans_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ita News LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdreuropeananllegalretrieval","title":"JinaVDREuropeanaNlLegalRetrieval","text":"<p>Retrieve Dutch historical legal documents based on LLM generated queries.  This dataset was created from records of the Europeana online collection by selecting scans of Dutch news articles</p> <p>Dataset: <code>jinaai/europeana-nl-legal_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 nld Legal LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrgithubreadmeretrieval","title":"JinaVDRGitHubReadmeRetrieval","text":"<p>Retrieve GitHub readme files based their description. This dataset consists of rendered GitHub readmes in a variety of different languages, together with their accompanying descriptions as queries and their license in the <code>license_type</code> and <code>license_text</code> columns. This particular dataset is a subsample of 1000 random rows per language from the full dataset which can be found here.</p> <p>Dataset: <code>jinaai/github-readme-retrieval-multilingual_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fra, ... (17) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrhindigovvqaretrieval","title":"JinaVDRHindiGovVQARetrieval","text":"<p>Retrieve Hindi government documents based on LLM generated queries.</p> <p>Dataset: <code>jinaai/hindi-gov-vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 hin Government LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrhungariandocqaretrieval","title":"JinaVDRHungarianDocQARetrieval","text":"<p>Retrieve Hungarian documents in various formats based on human annotated queries. Document Question answering from Hungurian doc qa dataset, test split.</p> <p>Dataset: <code>jinaai/hungarian_doc_qa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 hun Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrinfovqaretrieval","title":"JinaVDRInfovqaRetrieval","text":"<p>Retrieve infographics based on human annotated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/infovqa_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrjdocqaretrieval","title":"JinaVDRJDocQARetrieval","text":"<p>Retrieve Japanese documents in various formats based on human annotated queries. Document Question answering from JDocQAJP dataset, test split.</p> <p>Dataset: <code>jinaai/jdocqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrjina2024yearlybookretrieval","title":"JinaVDRJina2024YearlyBookRetrieval","text":"<p>Retrieve pages from the 2024 Jina yearbook based on human annotated questions. 75 human annotated questions created from digital version of Jina AI yearly book 2024, 166 pages in total. </p> <p>Dataset: <code>jinaai/jina_2024_yearly_book_beir</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmmtabretrieval","title":"JinaVDRMMTabRetrieval","text":"<p>Retrieve tables from the MMTab dataset based on queries. This dataset is a copy of the original test split from MMTab, taking only items where an 'original_query' is present, and removing the 'input' and 'output' columns, as they are unnecessary for retrieval tasks.</p> <p>Dataset: <code>jinaai/MMTab_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmpmqaretrieval","title":"JinaVDRMPMQARetrieval","text":"<p>Retrieve product manuals based on human annotated queries. 155 questions and 782 document images cleaned from jinaai/MPMQA, test set.</p> <p>Dataset: <code>jinaai/mpmqa_small_beir</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrmedicalprescriptionsretrieval","title":"JinaVDRMedicalPrescriptionsRetrieval","text":"<p>Retrieve medical prescriptions based on templated queries. Source dataset https://huggingface.co/datasets/Technoculture/medical-prescriptions</p> <p>Dataset: <code>jinaai/medical-prescriptions_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Medical derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrowidchartsretrieval","title":"JinaVDROWIDChartsRetrieval","text":"<p>Retrieve charts from the OWID dataset based on accompanied text snippets. We sampled a set of ~5k charts and articles from Our World In Data to produce this evaluation set. This particular dataset is a subsample of 1000 random charts from the full dataset which can be found here.</p> <p>Dataset: <code>jinaai/owid_charts_en_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdropenainewsretrieval","title":"JinaVDROpenAINewsRetrieval","text":"<p>Retrieve news articles from the OpenAI news website based on human annotated queries. News taken from https://openai.com/news/</p> <p>Dataset: <code>jinaai/openai-news_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng News, Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrplotqaretrieval","title":"JinaVDRPlotQARetrieval","text":"<p>Retrieve plots from the PlotQA dataset based on LLM generated queries. Questions subsampled from PlotQA test set. It is following a subsample + LLM-based classification process, using LLM to verify the question quality, e.g. queries like <code>How many different coloured dotlines are there</code> will be filtered out.</p> <p>Dataset: <code>jinaai/plotqa_beir</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrramensbenchmarkretrieval","title":"JinaVDRRamensBenchmarkRetrieval","text":"<p>Retrieve ramen restaurant marketing documents based on LLM generated queries. Marketing document from Ramen restaurants.</p> <p>Dataset: <code>jinaai/ramen_benchmark_jp_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 jpn Web LM-generated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrshanghaimasterplanretrieval","title":"JinaVDRShanghaiMasterPlanRetrieval","text":"<p>Retrieve pages from the Shanghai Master Plan based on human annotated queries. The master plan document is taken from here.</p> <p>Dataset: <code>jinaai/shanghai_master_plan_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 zho Web human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrshiftprojectretrieval","title":"JinaVDRShiftProjectRetrieval","text":"<p>Retrieve documents with graphs from the Shift Project based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/shiftproject_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrstanfordslideretrieval","title":"JinaVDRStanfordSlideRetrieval","text":"<p>Retrieve scientific and engineering slides based on human annotated queries. Source dataset https://exhibits.stanford.edu/data/catalog/mv327tb8364</p> <p>Dataset: <code>jinaai/stanford_slide_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic human-annotated found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrstudentenrollmentsyntheticretrieval","title":"JinaVDRStudentEnrollmentSyntheticRetrieval","text":"<p>Retrieve student enrollment data based on templated queries. This dataset is created from the original Kaggle Delaware Student Enrollment dataset. The charts are rendered and queries created using templates.</p> <p>Dataset: <code>jinaai/student-enrollment_beir</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtqaretrieval","title":"JinaVDRTQARetrieval","text":"<p>Retrieve textbook pages (images and text) based on LLM generated queries from the text. Source datasets https://prior.allenai.org/projects/tqa</p> <p>Dataset: <code>jinaai/tqa_beir</code> \u2022 License: cc-by-nc-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtabfquadretrieval","title":"JinaVDRTabFQuadRetrieval","text":"<p>Retrieve tables from industry documents based on LLM generated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/tabfquad_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtablevqaretrieval","title":"JinaVDRTableVQARetrieval","text":"<p>Retrieve scientific tables based on LLM generated queries. Source datasets https://huggingface.co/datasets/HuggingFaceM4/ChartQA or https://huggingface.co/datasets/cmarkea/aftdb</p> <p>Dataset: <code>jinaai/table-vqa_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtatqaretrieval","title":"JinaVDRTatQARetrieval","text":"<p>Retrieve financial reports based on human annotated queries. This dataset is build upon the corresponding dataset from the ViDoRe Benchmark.</p> <p>Dataset: <code>jinaai/tatqa_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrtweetstocksyntheticsretrieval","title":"JinaVDRTweetStockSyntheticsRetrieval","text":"<p>Retrieve rendered tables of stock prices based on templated queries. This dataset is created from the original Kaggle Tweet Sentiment's Impact on Stock Returns dataset.</p> <p>Dataset: <code>jinaai/tweet-stock-synthetic-retrieval_beir</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, deu, eng, fra, hin, ... (10) Social derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrwikimediacommonsdocumentsretrieval","title":"JinaVDRWikimediaCommonsDocumentsRetrieval","text":"<p>Retrieve historical documents from Wikimedia Commons based on their description. Wikimedia Commons Documents. It contains images of (mostly historic) documents which should be identified based on their description. We extracted those descriptions from Wikimedia Commons. We have included the license type and a link (<code>license_text</code>) to the original Wikimedia Commons page for each extracted image.</p> <p>Dataset: <code>jinaai/wikimedia-commons-documents-ml_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fra, ... (20) Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#jinavdrwikimediacommonsmapsretrieval","title":"JinaVDRWikimediaCommonsMapsRetrieval","text":"<p>Retrieve maps from Wikimedia Commons based on their description. It contains images of (mostly historic) maps which should be identified based on their description. We extracted those descriptions from Wikimedia Commons. We have included the license type and a link (license_text) to the original Wikimedia Commons page for each extracted image.</p> <p>Dataset: <code>jinaai/wikimedia-commons-maps_beir</code> \u2022 License: multiple \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Web derived found Citation <pre><code>@misc{g\u00fcnther2025jinaembeddingsv4universalembeddingsmultimodal,\n  archiveprefix = {arXiv},\n  author = {Michael G\u00fcnther and Saba Sturua and Mohammad Kalim Akram and Isabelle Mohr and Andrei Ungureanu and Bo Wang and Sedigheh Eslami and Scott Martens and Maximilian Werk and Nan Wang and Han Xiao},\n  eprint = {2506.18902},\n  primaryclass = {cs.AI},\n  title = {jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval},\n  url = {https://arxiv.org/abs/2506.18902},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#kovidore2cybersecurityretrieval","title":"KoVidore2CybersecurityRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Cybersecurity, is a corpus of technical reports on cyber threat trends and security incident responses in Korea, intended for complex-document understanding tasks.</p> <p>Dataset: <code>whybe-choi/kovidore-v2-cybersecurity-mteb</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 kor Social derived created Citation <pre><code>@misc{choi2026kovidorev2,\n  author = {Yongbin Choi},\n  note = {A benchmark for evaluating Korean vision document retrieval with multi-page reasoning queries in practical domains},\n  title = {KoViDoRe v2: a comprehensive evaluation of vision document retrieval for enterprise use-cases},\n  url = {https://github.com/whybe-choi/kovidore-data-generator},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#kovidore2economicretrieval","title":"KoVidore2EconomicRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Economic trends, is a corpus of periodic reports on major economic indicators in Korea, intended for complex-document understanding tasks.</p> <p>Dataset: <code>whybe-choi/kovidore-v2-economic-mteb</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 kor Social derived created Citation <pre><code>@misc{choi2026kovidorev2,\n  author = {Yongbin Choi},\n  note = {A benchmark for evaluating Korean vision document retrieval with multi-page reasoning queries in practical domains},\n  title = {KoViDoRe v2: a comprehensive evaluation of vision document retrieval for enterprise use-cases},\n  url = {https://github.com/whybe-choi/kovidore-data-generator},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#kovidore2energyretrieval","title":"KoVidore2EnergyRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Energy, is a corpus of reports on energy market trends, policy planning, and industry statistics, intended for complex-document understanding tasks.</p> <p>Dataset: <code>whybe-choi/kovidore-v2-energy-mteb</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 kor Social derived created Citation <pre><code>@misc{choi2026kovidorev2,\n  author = {Yongbin Choi},\n  note = {A benchmark for evaluating Korean vision document retrieval with multi-page reasoning queries in practical domains},\n  title = {KoViDoRe v2: a comprehensive evaluation of vision document retrieval for enterprise use-cases},\n  url = {https://github.com/whybe-choi/kovidore-data-generator},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#kovidore2hrretrieval","title":"KoVidore2HrRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, HR, is a corpus of reports on workforce outlook and employment policy in korea, intended for complex-document understanding tasks.</p> <p>Dataset: <code>whybe-choi/kovidore-v2-hr-mteb</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 kor Social derived created Citation <pre><code>@misc{choi2026kovidorev2,\n  author = {Yongbin Choi},\n  note = {A benchmark for evaluating Korean vision document retrieval with multi-page reasoning queries in practical domains},\n  title = {KoViDoRe v2: a comprehensive evaluation of vision document retrieval for enterprise use-cases},\n  url = {https://github.com/whybe-choi/kovidore-data-generator},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#miraclvisionretrieval","title":"MIRACLVisionRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>nvidia/miracl-vision</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 ara, ben, deu, eng, fas, ... (18) Encyclopaedic derived created Citation <pre><code>@article{osmulski2025miraclvisionlargemultilingualvisual,\n  author = {Radek Osmulski and Gabriel de Souza P. Moreira and Ronay Ak and Mengyao Xu and Benedikt Schifferer and Even Oldridge},\n  eprint = {2505.11651},\n  journal = {arxiv},\n  title = {{MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark}},\n  url = {https://arxiv.org/abs/2505.11651},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2biomedicallecturesretrieval","title":"Vidore2BioMedicalLecturesRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/biomedical_lectures_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2esgreportshlretrieval","title":"Vidore2ESGReportsHLRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/esg_reports_human_labeled_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2esgreportsretrieval","title":"Vidore2ESGReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/esg_reports_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore2economicsreportsretrieval","title":"Vidore2EconomicsReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>vidore/economics_reports_v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, spa Academic derived found Citation <pre><code>@article{mace2025vidorev2,\n  author = {Mac\u00e9, Quentin and Loison Ant\u00f3nio and Faysse, Manuel},\n  journal = {arXiv preprint arXiv:2505.17166},\n  title = {ViDoRe Benchmark V2: Raising the Bar for Visual Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3computerscienceretrieval","title":"Vidore3ComputerScienceRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Computer Science, is a corpus of textbooks from the openstacks website, intended for long-document understanding tasks. Original queries were created in english, then translated to french, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_computer_science_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Engineering, Programming derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3energyretrieval","title":"Vidore3EnergyRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Energy Fr, is a corpus of reports on energy supply in europe, intended for complex-document understanding tasks. Original queries were created in french, then translated to english, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_energy_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Academic, Chemistry, Engineering derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3financeenretrieval","title":"Vidore3FinanceEnRetrieval","text":"<p>Retrieve associated pages according to questions. This task, Finance - EN, is a corpus of reports from american banking companies, intended for long-document understanding tasks. Original queries were created in english, then translated to french, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_finance_en_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Financial derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3financefrretrieval","title":"Vidore3FinanceFrRetrieval","text":"<p>Retrieve associated pages according to questions. This task, Finance - FR, is a corpus of reports from french companies in the luxury domain, intended for long-document understanding tasks. Original queries were created in french, then translated to english, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_finance_fr_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Financial derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3hrretrieval","title":"Vidore3HrRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, HR, is a corpus of reports released by the european union, intended for complex-document understanding tasks. Original queries were created in english, then translated to french, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_hr_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Social derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3industrialretrieval","title":"Vidore3IndustrialRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Industrial reports, is a corpus of technical documents on military aircraft (fueling, mechanics...), intended for complex-document understanding tasks. Original queries were created in english, then translated to french, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_industrial_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Engineering derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3nuclearretrieval","title":"Vidore3NuclearRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb-private/Vidore3NuclearRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Chemistry, Engineering derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3pharmaceuticalsretrieval","title":"Vidore3PharmaceuticalsRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Pharmaceutical, is a corpus of slides from the FDA, intended for long-document understanding tasks. Original queries were created in english, then translated to french, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_pharmaceuticals_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Medical derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3physicsretrieval","title":"Vidore3PhysicsRetrieval","text":"<p>Retrieve associated pages according to questions. This dataset, Physics, is a corpus of course slides on french bachelor level physics lectures, intended for complex visual understanding tasks. Original queries were created in french, then translated to english, german, italian, portuguese and spanish.</p> <p>Dataset: <code>vidore/vidore_v3_physics_mteb_format</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Academic, Engineering derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidore3telecomretrieval","title":"Vidore3TelecomRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb-private/Vidore3TelecomRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_10 deu, eng, fra, ita, por, ... (6) Engineering, Programming derived created and machine-translated Citation <pre><code>@article{loison2026vidorev3comprehensiveevaluation,\n  archiveprefix = {arXiv},\n  author = {Ant\u00f3nio Loison and Quentin Mac\u00e9 and Antoine Edy and Victor Xing and Tom Balough and Gabriel Moreira and Bo Liu and Manuel Faysse and C\u00e9line Hudelot and Gautier Viaud},\n  eprint = {2601.08620},\n  primaryclass = {cs.AI},\n  title = {ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios},\n  url = {https://arxiv.org/abs/2601.08620},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidorearxivqaretrieval","title":"VidoreArxivQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/arxivqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoredocvqaretrieval","title":"VidoreDocVQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/docvqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoreinfovqaretrieval","title":"VidoreInfoVQARetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/infovqa_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoreshiftprojectretrieval","title":"VidoreShiftProjectRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/shiftproject_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqaairetrieval","title":"VidoreSyntheticDocQAAIRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/syntheticDocQA_artificial_intelligence_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqaenergyretrieval","title":"VidoreSyntheticDocQAEnergyRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/syntheticDocQA_energy_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqagovernmentreportsretrieval","title":"VidoreSyntheticDocQAGovernmentReportsRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/syntheticDocQA_government_reports_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoresyntheticdocqahealthcareindustryretrieval","title":"VidoreSyntheticDocQAHealthcareIndustryRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/syntheticDocQA_healthcare_industry_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoretabfquadretrieval","title":"VidoreTabfquadRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/tabfquad_test_subsampled_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/documentunderstanding/#vidoretatdqaretrieval","title":"VidoreTatdqaRetrieval","text":"<p>Retrieve associated pages according to questions.</p> <p>Dataset: <code>mteb/tatdqa_test_beir</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 eng Academic derived found Citation <pre><code>@article{faysse2024colpali,\n  author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Viaud, Gautier and Hudelot, C{\\'e}line and Colombo, Pierre},\n  journal = {arXiv preprint arXiv:2407.01449},\n  title = {ColPali: Efficient Document Retrieval with Vision Language Models},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/","title":"ImageClassification","text":"<ul> <li>Number of tasks: 22</li> </ul>"},{"location":"overview/available_tasks/imageclassification/#birdsnap","title":"Birdsnap","text":"<p>Classifying bird images from 500 species.</p> <p>Dataset: <code>mteb/birdsnap</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Berg_2014_CVPR,\n  author = {Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L. and Jacobs, David W. and Belhumeur, Peter N.},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Birdsnap: Large-scale Fine-grained Visual Categorization of Birds},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#cifar10","title":"CIFAR10","text":"<p>Classifying images from 10 classes.</p> <p>Dataset: <code>mteb/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#cifar100","title":"CIFAR100","text":"<p>Classifying images from 100 classes.</p> <p>Dataset: <code>mteb/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#caltech101","title":"Caltech101","text":"<p>Classifying images of 101 widely varied objects.</p> <p>Dataset: <code>mteb/Caltech101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{1384978,\n  author = {Li Fei-Fei and Fergus, R. and Perona, P.},\n  booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n  doi = {10.1109/CVPR.2004.383},\n  keywords = {Bayesian methods;Testing;Humans;Maximum likelihood estimation;Assembly;Shape;Machine vision;Image recognition;Parameter estimation;Image databases},\n  number = {},\n  pages = {178-178},\n  title = {Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},\n  volume = {},\n  year = {2004},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#country211","title":"Country211","text":"<p>Classifying images of 211 countries.</p> <p>Dataset: <code>mteb/wds_country211</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@article{radford2021learning,\n  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},\n  journal = {arXiv preprint arXiv:2103.00020},\n  title = {Learning Transferable Visual Models From Natural Language Supervision},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#dtd","title":"DTD","text":"<p>Describable Textures Dataset in 47 categories.</p> <p>Dataset: <code>mteb/dtd</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{cimpoi14describing,\n  author = {M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and and A. Vedaldi},\n  booktitle = {Proceedings of the {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},\n  title = {Describing Textures in the Wild},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#eurosat","title":"EuroSAT","text":"<p>Classifying satellite images.</p> <p>Dataset: <code>mteb/eurosat-rgb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{8736785,\n  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n  doi = {10.1109/JSTARS.2019.2918242},\n  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n  keywords = {Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing;Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images},\n  number = {7},\n  pages = {2217-2226},\n  title = {EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},\n  volume = {12},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#fer2013","title":"FER2013","text":"<p>Classifying facial emotions.</p> <p>Dataset: <code>mteb/wds_fer2013</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{goodfellow2015explainingharnessingadversarialexamples,\n  archiveprefix = {arXiv},\n  author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},\n  eprint = {1412.6572},\n  primaryclass = {stat.ML},\n  title = {Explaining and Harnessing Adversarial Examples},\n  url = {https://arxiv.org/abs/1412.6572},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#fgvcaircraft","title":"FGVCAircraft","text":"<p>Classifying aircraft images from 41 manufacturers and 102 variants.</p> <p>Dataset: <code>mteb/FGVCAircraft</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#food101classification","title":"Food101Classification","text":"<p>Classifying food.</p> <p>Dataset: <code>mteb/food101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Web derived created Citation <pre><code>@inproceedings{bossard14,\n  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n  booktitle = {European Conference on Computer Vision},\n  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#gtsrb","title":"GTSRB","text":"<p>The German Traffic Sign Recognition Benchmark (GTSRB) is a multi-class classification dataset for traffic signs. It consists of dataset of more than 50,000 traffic sign images. The dataset comprises 43 classes with unbalanced class frequencies.</p> <p>Dataset: <code>mteb/wds_gtsrb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@inproceedings{6033395,\n  author = {Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},\n  booktitle = {The 2011 International Joint Conference on Neural Networks},\n  doi = {10.1109/IJCNN.2011.6033395},\n  keywords = {Humans;Training;Image color analysis;Benchmark testing;Lead;Histograms;Image resolution},\n  number = {},\n  pages = {1453-1460},\n  title = {The German Traffic Sign Recognition Benchmark: A multi-class classification competition},\n  volume = {},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#imagenet1k","title":"Imagenet1k","text":"<p>ImageNet, a large-scale ontology of images built upon the backbone of the WordNet structure.</p> <p>Dataset: <code>mteb/wds_imagenet1k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene human-annotated created Citation <pre><code>@inproceedings{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#mnist","title":"MNIST","text":"<p>Classifying handwritten digits.</p> <p>Dataset: <code>mteb/mnist</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{lecun2010mnist,\n  author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},\n  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n  title = {MNIST handwritten digit database},\n  volume = {2},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#oxfordflowersclassification","title":"OxfordFlowersClassification","text":"<p>Classifying flowers</p> <p>Dataset: <code>mteb/oxford-flowers</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Reviews derived found Citation <pre><code>@inproceedings{4756141,\n  author = {Nilsback, Maria-Elena and Zisserman, Andrew},\n  booktitle = {2008 Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing},\n  doi = {10.1109/ICVGIP.2008.47},\n  keywords = {Shape;Kernel;Distributed computing;Support vector machines;Support vector machine classification;object classification;segmentation},\n  number = {},\n  pages = {722-729},\n  title = {Automated Flower Classification over a Large Number of Classes},\n  volume = {},\n  year = {2008},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#oxfordpets","title":"OxfordPets","text":"<p>Classifying animal images.</p> <p>Dataset: <code>mteb/OxfordPets</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{6248092,\n  author = {Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},\n  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2012.6248092},\n  keywords = {Positron emission tomography;Image segmentation;Cats;Dogs;Layout;Deformable models;Head},\n  number = {},\n  pages = {3498-3505},\n  title = {Cats and dogs},\n  volume = {},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#patchcamelyon","title":"PatchCamelyon","text":"<p>Histopathology diagnosis classification dataset.</p> <p>Dataset: <code>mteb/wds_vtab-pcam</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Medical derived created Citation <pre><code>@inproceedings{10.1007/978-3-030-00934-2_24,\n  address = {Cham},\n  author = {Veeling, Bastiaan S.\nand Linmans, Jasper\nand Winkens, Jim\nand Cohen, Taco\nand Welling, Max},\n  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},\n  editor = {Frangi, Alejandro F.\nand Schnabel, Julia A.\nand Davatzikos, Christos\nand Alberola-L{\\'o}pez, Carlos\nand Fichtinger, Gabor},\n  isbn = {978-3-030-00934-2},\n  pages = {210--218},\n  publisher = {Springer International Publishing},\n  title = {Rotation Equivariant CNNs for Digital Pathology},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#resisc45","title":"RESISC45","text":"<p>Remote Sensing Image Scene Classification by Northwestern Polytechnical University (NWPU).</p> <p>Dataset: <code>mteb/resisc45</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{7891544,\n  author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},\n  doi = {10.1109/JPROC.2017.2675998},\n  journal = {Proceedings of the IEEE},\n  keywords = {Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification;Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning},\n  number = {10},\n  pages = {1865-1883},\n  title = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},\n  volume = {105},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#stl10","title":"STL10","text":"<p>Classifying 96x96 images from 10 classes.</p> <p>Dataset: <code>mteb/stl10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{pmlr-v15-coates11a,\n  address = {Fort Lauderdale, FL, USA},\n  author = {Coates, Adam and Ng, Andrew and Lee, Honglak},\n  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},\n  editor = {Gordon, Geoffrey and Dunson, David and Dud\u00edk, Miroslav},\n  month = {11--13 Apr},\n  pages = {215--223},\n  pdf = {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},\n  publisher = {PMLR},\n  series = {Proceedings of Machine Learning Research},\n  title = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},\n  url = {https://proceedings.mlr.press/v15/coates11a.html},\n  volume = {15},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#sun397","title":"SUN397","text":"<p>Large scale scene recognition in 397 categories.</p> <p>Dataset: <code>mteb/sun397</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{5539970,\n  author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},\n  booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2010.5539970},\n  number = {},\n  pages = {3485-3492},\n  title = {SUN database: Large-scale scene recognition from abbey to zoo},\n  volume = {},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#stanfordcars","title":"StanfordCars","text":"<p>Classifying car images from 196 makes.</p> <p>Dataset: <code>mteb/StanfordCars</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#ucf101","title":"UCF101","text":"<p>UCF101 is an action recognition data set of realistic action videos collected from YouTube, having 101 action categories. This version of the dataset does not contain images but images saved frame by frame. Train and test splits are generated based on the authors' first version train/test list.</p> <p>Dataset: <code>mteb/ucf101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) accuracy eng Scene derived created Citation <pre><code>@misc{soomro2012ucf101dataset101human,\n  archiveprefix = {arXiv},\n  author = {Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},\n  eprint = {1212.0402},\n  primaryclass = {cs.CV},\n  title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\n  url = {https://arxiv.org/abs/1212.0402},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclassification/#voc2007","title":"VOC2007","text":"<p>Classifying bird images from 500 species.</p> <p>Dataset: <code>mteb/VOC2007</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) lrap eng Encyclopaedic derived created Citation <pre><code>@article{Everingham10,\n  author = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},\n  journal = {International Journal of Computer Vision},\n  month = jun,\n  number = {2},\n  pages = {303--338},\n  title = {The Pascal Visual Object Classes (VOC) Challenge},\n  volume = {88},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/","title":"ImageClustering","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/imageclustering/#cifar100clustering","title":"CIFAR100Clustering","text":"<p>Clustering images from 100 classes.</p> <p>Dataset: <code>mteb/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) nmi eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#cifar10clustering","title":"CIFAR10Clustering","text":"<p>Clustering images from 10 classes.</p> <p>Dataset: <code>mteb/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#imagenet10clustering","title":"ImageNet10Clustering","text":"<p>Clustering images from an 10-class subset of ImageNet which are generally easy to distinguish.</p> <p>Dataset: <code>mteb/imagenet-10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) nmi eng Web derived created Citation <pre><code>@inproceedings{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#imagenetdog15clustering","title":"ImageNetDog15Clustering","text":"<p>Clustering images from a 15-class dogs-only subset of the dog classes in ImageNet.</p> <p>Dataset: <code>mteb/imagenet-dog-15</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Web derived created Citation <pre><code>@inproceedings{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/imageclustering/#tinyimagenetclustering","title":"TinyImageNetClustering","text":"<p>Clustering over 200 classes.</p> <p>Dataset: <code>mteb/tiny-imagenet</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to category (i2c) nmi eng Reviews derived found"},{"location":"overview/available_tasks/instructionreranking/","title":"InstructionReranking","text":"<ul> <li>Number of tasks: 5</li> </ul>"},{"location":"overview/available_tasks/instructionreranking/#core17instructionretrieval","title":"Core17InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on Core17 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/core17-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#news21instructionretrieval","title":"News21InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on News21 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/news21-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#robust04instructionretrieval","title":"Robust04InstructionRetrieval","text":"<p>Measuring retrieval instruction following ability on Robust04 narratives for the FollowIR benchmark.</p> <p>Dataset: <code>jhu-clsp/robust04-instructions-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng News, Written derived found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#mfollowir","title":"mFollowIR","text":"<p>This tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages.</p> <p>Dataset: <code>jhu-clsp/mFollowIR-parquet-mteb</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{weller2024mfollowir,\n  author = {Weller, Orion and Chang, Benjamin and Yang, Eugene and Yarmohammadi, Mahsa and Barham, Sam and MacAvaney, Sean and Cohan, Arman and Soldaini, Luca and Van Durme, Benjamin and Lawrie, Dawn},\n  journal = {arXiv preprint TODO},\n  title = {{mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionreranking/#mfollowircrosslingual","title":"mFollowIRCrossLingual","text":"<p>This tasks measures retrieval instruction following ability on NeuCLIR narratives for the mFollowIR benchmark on the Farsi, Russian, and Chinese languages with English queries/instructions.</p> <p>Dataset: <code>jhu-clsp/mFollowIR-cross-lingual-parquet-mteb</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) p-MRR eng, fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{weller2024mfollowir,\n  author = {Weller, Orion and Chang, Benjamin and Yang, Eugene and Yarmohammadi, Mahsa and Barham, Sam and MacAvaney, Sean and Cohan, Arman and Soldaini, Luca and Van Durme, Benjamin and Lawrie, Dawn},\n  journal = {arXiv preprint TODO},\n  title = {{mFollowIR: a Multilingual Benchmark for Instruction Following in Retrieval}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/","title":"InstructionRetrieval","text":"<ul> <li>Number of tasks: 8</li> </ul>"},{"location":"overview/available_tasks/instructionretrieval/#ifiraila","title":"IFIRAila","text":"<p>Benchmark aila subset in aila within instruction following abilities. The instructions simulate lawyers' or legal assistants' nuanced queries to retrieve relevant legal documents. </p> <p>Dataset: <code>if-ir/aila</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Legal, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifircds","title":"IFIRCds","text":"<p>Benchmark IFIR cds subset within instruction following abilities. The instructions simulate a doctor's nuanced queries to retrieve suitable clinical trails, treatment and diagnosis information. </p> <p>Dataset: <code>if-ir/cds</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirfiqa","title":"IFIRFiQA","text":"<p>Benchmark IFIR fiqa subset within instruction following abilities. The instructions simulate people's daily life queries to retrieve suitable financial suggestions. </p> <p>Dataset: <code>if-ir/fiqa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Financial, Written human-annotated created Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirfire","title":"IFIRFire","text":"<p>Benchmark IFIR fire subset within instruction following abilities. The instructions simulate lawyers' or legal assistants' nuanced queries to retrieve relevant legal documents. </p> <p>Dataset: <code>if-ir/fire</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Legal, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirnfcorpus","title":"IFIRNFCorpus","text":"<p>Benchmark IFIR nfcorpus subset within instruction following abilities. The instructions in this dataset simulate nuanced queries from students or researchers to retrieve relevant science literature in the medical and biological domains. </p> <p>Dataset: <code>if-ir/nfcorpus</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Academic, Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirpm","title":"IFIRPm","text":"<p>Benchmark IFIR pm subset within instruction following abilities. The instructions simulate a doctor's nuanced queries to retrieve suitable clinical trails, treatment and diagnosis information. </p> <p>Dataset: <code>if-ir/pm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Medical, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#ifirscifact","title":"IFIRScifact","text":"<p>Benchmark IFIR scifact_open subset within instruction following abilities. The instructions in this dataset simulate nuanced queries from students or researchers to retrieve relevant science literature. </p> <p>Dataset: <code>if-ir/scifact_open</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 eng Academic, Written human-annotated found Citation <pre><code>@inproceedings{song2025ifir,\n  author = {Song, Tingyu and Gan, Guo and Shang, Mingsheng and Zhao, Yilun},\n  booktitle = {Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},\n  pages = {10186--10204},\n  title = {IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/instructionretrieval/#instructir","title":"InstructIR","text":"<p>A benchmark specifically designed to evaluate the instruction following ability in information retrieval models. Our approach focuses on user-aligned instructions tailored to each query instance, reflecting the diverse characteristics inherent in real-world search scenarios. NOTE: scores on this may differ unless you include instruction first, then \"[SEP]\" and then the query via redefining <code>combine_query_and_instruction</code> in your model.</p> <p>Dataset: <code>mteb/InstructIR-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) robustness_at_10 eng Web human-annotated created Citation <pre><code>@article{oh2024instructir,\n  archiveprefix = {{arXiv}},\n  author = {{Hanseok Oh and Hyunji Lee and Seonghyeon Ye and Haebin Shin and Hansol Jang and Changwook Jun and Minjoon Seo}},\n  eprint = {{2402.14334}},\n  primaryclass = {{cs.CL}},\n  title = {{INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models}},\n  year = {{2024}},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/","title":"MultilabelClassification","text":"<ul> <li>Number of tasks: 11</li> </ul>"},{"location":"overview/available_tasks/multilabelclassification/#braziliantoxictweetsclassification","title":"BrazilianToxicTweetsClassification","text":"<p>ToLD-Br is the biggest dataset for toxic tweets in Brazilian Portuguese, crowdsourced by 42 annotators selected from a pool of 129 volunteers. Annotators were selected aiming to create a plural group in terms of demographics (ethnicity, sexual orientation, age, gender). Each tweet was labeled by three annotators in 6 possible categories: LGBTQ+phobia, Xenophobia, Obscene, Insult, Misogyny and Racism.</p> <p>Dataset: <code>mteb/BrazilianToxicTweetsClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy por Constructed, Written expert-annotated found Citation <pre><code>@article{DBLP:journals/corr/abs-2010-04543,\n  author = {Joao Augusto Leite and\nDiego F. Silva and\nKalina Bontcheva and\nCarolina Scarton},\n  eprint = {2010.04543},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Tue, 15 Dec 2020 16:10:16 +0100},\n  title = {Toxic Language Detection in Social Media for Brazilian Portuguese:\nNew Dataset and Multilingual Analysis},\n  url = {https://arxiv.org/abs/2010.04543},\n  volume = {abs/2010.04543},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#cedrclassification","title":"CEDRClassification","text":"<p>Classification of sentences by emotions, labeled into 5 categories (joy, sadness, surprise, fear, and anger).</p> <p>Dataset: <code>mteb/CEDRClassification</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Blog, Social, Web, Written human-annotated found Citation <pre><code>@article{sboev2021data,\n  author = {Sboev, Alexander and Naumov, Aleksandr and Rybka, Roman},\n  journal = {Procedia Computer Science},\n  pages = {637--642},\n  publisher = {Elsevier},\n  title = {Data-Driven Model for Emotion Detection in Russian Texts},\n  volume = {190},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#coviddisinformationnlmultilabelclassification","title":"CovidDisinformationNLMultiLabelClassification","text":"<p>The dataset is curated to address questions of interest to journalists, fact-checkers, social media platforms, policymakers, and the general public.</p> <p>Dataset: <code>clips/mteb-nl-COVID-19-disinformation</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{alam-etal-2021-fighting-covid,\n  address = {Punta Cana, Dominican Republic},\n  author = {Alam, Firoj  and\nShaar, Shaden  and\nDalvi, Fahim  and\nSajjad, Hassan  and\nNikolov, Alex  and\nMubarak, Hamdy  and\nDa San Martino, Giovanni  and\nAbdelali, Ahmed  and\nDurrani, Nadir  and\nDarwish, Kareem  and\nAl-Homaid, Abdulaziz  and\nZaghouani, Wajdi  and\nCaselli, Tommaso  and\nDanoe, Gijs  and\nStolk, Friso  and\nBruntink, Britt  and\nNakov, Preslav},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},\n  doi = {10.18653/v1/2021.findings-emnlp.56},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {611--649},\n  publisher = {Association for Computational Linguistics},\n  title = {Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society},\n  url = {https://aclanthology.org/2021.findings-emnlp.56/},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#emitclassification","title":"EmitClassification","text":"<p>The EMit dataset is a comprehensive resource for the detection of emotions in Italian social media texts. The EMit dataset consists of social media messages about TV shows, TV series, music videos, and advertisements. Each message is annotated with one or more of the 8 primary emotions defined by Plutchik (anger, anticipation, disgust, fear, joy, sadness, surprise, trust), as well as an additional label \u201clove.\u201d</p> <p>Dataset: <code>MattiaSangermano/emit</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy ita Social, Written expert-annotated found Citation <pre><code>@inproceedings{araque2023emit,\n  author = {Araque, O and Frenda, S and Sprugnoli, R and Nozza, D and Patti, V and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {CEUR-WS},\n  pages = {1--8},\n  title = {EMit at EVALITA 2023: Overview of the Categorical Emotion Detection in Italian Social Media Task},\n  volume = {3473},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#korhatespeechmlclassification","title":"KorHateSpeechMLClassification","text":"<p>The Korean Multi-label Hate Speech Dataset, K-MHaS, consists of 109,692 utterances from Korean online news comments, labelled with 8 fine-grained hate speech classes (labels: Politics, Origin, Physical, Age, Gender, Religion, Race, Profanity) or Not Hate Speech class. Each utterance provides from a single to four labels that can handles Korean language patterns effectively. For more details, please refer to the paper about K-MHaS, published at COLING 2022. This dataset is based on the Korean online news comments available on Kaggle and Github. The unlabeled raw data was collected between January 2018 and June 2020. The language producers are users who left the comments on the Korean online news platform between 2018 and 2020.</p> <p>Dataset: <code>mteb/KorHateSpeechMLClassification</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy kor Social, Written expert-annotated found Citation <pre><code>@inproceedings{lee-etal-2022-k,\n  address = {Gyeongju, Republic of Korea},\n  author = {Lee, Jean  and\nLim, Taejun  and\nLee, Heejun  and\nJo, Bogeun  and\nKim, Yangsok  and\nYoon, Heegeun  and\nHan, Soyeon Caren},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {3530--3538},\n  publisher = {International Committee on Computational Linguistics},\n  title = {K-{MH}a{S}: A Multi-label Hate Speech Detection Dataset in {K}orean Online News Comment},\n  url = {https://aclanthology.org/2022.coling-1.311},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#maltesenewsclassification","title":"MalteseNewsClassification","text":"<p>A multi-label topic classification dataset for Maltese News Articles. The data was collected from the press_mt subset from Korpus Malti v4.0. Article contents were cleaned to filter out JavaScript, CSS, &amp; repeated non-Maltese sub-headings. The labels are based on the category field from this corpus.</p> <p>Dataset: <code>mteb/MalteseNewsClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy mlt Constructed, Written expert-annotated found Citation <pre><code>@inproceedings{maltese-news-datasets,\n  author = {Chaudhary, Amit Kumar  and\nMicallef, Kurt  and\nBorg, Claudia},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},\n  month = may,\n  publisher = {Association for Computational Linguistics},\n  title = {Topic Classification and Headline Generation for {M}altese using a Public News Corpus},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#multieurlexmultilabelclassification","title":"MultiEURLEXMultilabelClassification","text":"<p>EU laws in 23 EU languages containing annotated labels for 21 EUROVOC concepts.</p> <p>Dataset: <code>mteb/eurlex-multilingual</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy bul, ces, dan, deu, ell, ... (23) Government, Legal, Written expert-annotated found Citation <pre><code>@inproceedings{chalkidis-etal-2021-multieurlex,\n  author = {Chalkidis, Ilias\nand Fergadiotis, Manos\nand Androutsopoulos, Ion},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Punta Cana, Dominican Republic},\n  publisher = {Association for Computational Linguistics},\n  title = {MultiEURLEX -- A multi-lingual and multi-label legal document\nclassification dataset for zero-shot cross-lingual transfer},\n  url = {https://arxiv.org/abs/2109.00904},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#sensitivetopicsclassification","title":"SensitiveTopicsClassification","text":"<p>Multilabel classification of sentences across 18 sensitive topics.</p> <p>Dataset: <code>ai-forever/sensitive-topics-classification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) accuracy rus Social, Web, Written human-annotated found Citation <pre><code>@inproceedings{babakov-etal-2021-detecting,\n  address = {Kiyv, Ukraine},\n  author = {Babakov, Nikolay  and\nLogacheva, Varvara  and\nKozlova, Olga  and\nSemenov, Nikita  and\nPanchenko, Alexander},\n  booktitle = {Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing},\n  editor = {Babych, Bogdan  and\nKanishcheva, Olga  and\nNakov, Preslav  and\nPiskorski, Jakub  and\nPivovarova, Lidia  and\nStarko, Vasyl  and\nSteinberger, Josef  and\nYangarber, Roman  and\nMarci{\\'n}czuk, Micha{\\l}  and\nPollak, Senja  and\nP{\\v{r}}ib{\\'a}{\\v{n}}, Pavel  and\nRobnik-{\\v{S}}ikonja, Marko},\n  month = apr,\n  pages = {26--36},\n  publisher = {Association for Computational Linguistics},\n  title = {Detecting Inappropriate Messages on Sensitive Topics that Could Harm a Company{'}s Reputation},\n  url = {https://aclanthology.org/2021.bsnlp-1.4},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#swedishpatentcpcgroupclassification","title":"SwedishPatentCPCGroupClassification","text":"<p>This dataset contains historical Swedish patent documents (1885-1972) classified according to the Cooperative Patent Classification (CPC) system at the group level. Each document can have multiple labels, making this a challenging multi-label classification task with significant class imbalance and data sparsity characteristics. The dataset includes patent claims text extracted from digitally recreated versions of historical Swedish patents, generated using Optical Character Recognition (OCR) from original paper documents. The text quality varies due to OCR limitations, but all CPC labels were manually assigned by patent engineers at PRV (Swedish Patent and Registration Office), ensuring high reliability for machine learning applications.</p> <p>Dataset: <code>atheer2104/swedish-patent-cpc-group-new</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy swe Government, Legal expert-annotated found Citation <pre><code>@mastersthesis{Salim1987995,\n  author = {Salim, Atheer},\n  institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  keywords = {Multi-label Text Classification, Machine Learning, Patent Classification, Deep Learning, Natural Language Processing, Textklassificering med flera Klasser, Maskininl\u00e4rning, Patentklassificering, Djupinl\u00e4rning, Spr\u00e5kteknologi},\n  number = {2025:571},\n  pages = {70},\n  school = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  series = {TRITA-EECS-EX},\n  title = {Machine Learning for Classifying Historical Swedish Patents : A Comparison of Textual and Combined Data Approaches},\n  url = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-368254},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#swedishpatentcpcsubclassclassification","title":"SwedishPatentCPCSubclassClassification","text":"<p>This dataset contains historical Swedish patent documents (1885-1972) classified according to the Cooperative Patent Classification (CPC) system. Each document can have multiple labels, making this a multi-label classification task with significant implications for patent retrieval and prior art search. The dataset includes patent claims text extracted from digitally recreated versions of historical Swedish patents, generated using Optical Character Recognition (OCR) from original paper documents. The text quality varies due to OCR limitations, but all CPC labels were manually assigned by patent engineers at PRV (Swedish Patent and Registration Office), ensuring high reliability for machine learning applications.</p> <p>Dataset: <code>atheer2104/swedish-patent-cpc-subclass-new</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) accuracy swe Government, Legal expert-annotated found Citation <pre><code>@mastersthesis{Salim1987995,\n  author = {Salim, Atheer},\n  institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  keywords = {Multi-label Text Classification, Machine Learning, Patent Classification, Deep Learning, Natural Language Processing, Textklassificering med flera Klasser, Maskininl\u00e4rning, Patentklassificering, Djupinl\u00e4rning, Spr\u00e5kteknologi},\n  number = {2025:571},\n  pages = {70},\n  school = {KTH, School of Electrical Engineering and Computer Science (EECS)},\n  series = {TRITA-EECS-EX},\n  title = {Machine Learning for Classifying Historical Swedish Patents : A Comparison of Textual and Combined Data Approaches},\n  url = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-368254},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/multilabelclassification/#vabbmultilabelclassification","title":"VABBMultiLabelClassification","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-mlcls-pr</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) f1 nld Academic, Written human-annotated found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/","title":"PairClassification","text":"<ul> <li>Number of tasks: 47</li> </ul>"},{"location":"overview/available_tasks/pairclassification/#arentail","title":"ArEntail","text":"<p>A manually-curated Arabic natural language inference dataset from news headlines.</p> <p>Dataset: <code>arbml/ArEntail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ara News, Written human-annotated found Citation <pre><code>@article{obeidat2024arentail,\n  author = {Obeidat, Rasha and Al-Harahsheh, Yara and Al-Ayyoub, Mahmoud and Gharaibeh, Maram},\n  journal = {Language Resources and Evaluation},\n  pages = {1--27},\n  publisher = {Springer},\n  title = {ArEntail: manually-curated Arabic natural language inference dataset from news headlines},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#armenianparaphrasepc","title":"ArmenianParaphrasePC","text":"<p>asparius/Armenian-Paraphrase-PC</p> <p>Dataset: <code>mteb/ArmenianParaphrasePC</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap hye News, Written derived found Citation <pre><code>@misc{malajyan2020arpa,\n  archiveprefix = {arXiv},\n  author = {Arthur Malajyan and Karen Avetisyan and Tsolak Ghukasyan},\n  eprint = {2009.12615},\n  primaryclass = {cs.CL},\n  title = {ARPA: Armenian Paraphrase Detection Corpus and Models},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#assin2rte","title":"Assin2RTE","text":"<p>Recognizing Textual Entailment part of the ASSIN 2, an evaluation shared task collocated with STIL 2019.</p> <p>Dataset: <code>nilc-nlp/assin2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap por Written human-annotated found Citation <pre><code>@inproceedings{real2020assin,\n  author = {Real, Livy and Fonseca, Erick and Oliveira, Hugo Goncalo},\n  booktitle = {International Conference on Computational Processing of the Portuguese Language},\n  organization = {Springer},\n  pages = {406--412},\n  title = {The assin 2 shared task: a quick overview},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cdsc-e","title":"CDSC-E","text":"<p>Compositional Distributional Semantics Corpus for textual entailment.</p> <p>Dataset: <code>PL-MTEB/cdsce-pairclassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Written human-annotated found Citation <pre><code>@inproceedings{wroblewska-krasnowska-kieras-2017-polish,\n  address = {Vancouver, Canada},\n  author = {Wr{\\'o}blewska, Alina  and\nKrasnowska-Kiera{\\'s}, Katarzyna},\n  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/P17-1073},\n  editor = {Barzilay, Regina  and\nKan, Min-Yen},\n  month = jul,\n  pages = {784--792},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}olish evaluation dataset for compositional distributional semantics models},\n  url = {https://aclanthology.org/P17-1073},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cexappc","title":"CExaPPC","text":"<p>ExaPPC is a large paraphrase corpus consisting of monolingual sentence-level paraphrases using different sources.</p> <p>Dataset: <code>PNLPhub/C-ExaPPC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Social, Web derived found Citation <pre><code>@inproceedings{9786243,\n  author = {Sadeghi, Reyhaneh and Karbasi, Hamed and Akbari, Ahmad},\n  booktitle = {2022 8th International Conference on Web Research (ICWR)},\n  doi = {10.1109/ICWR54782.2022.9786243},\n  keywords = {Data mining;Task analysis;Paraphrase Identification;Semantic Similarity;Deep Learning;Paraphrasing Corpora},\n  number = {},\n  pages = {168-175},\n  title = {ExaPPC: a Large-Scale Persian Paraphrase Detection Corpus},\n  volume = {},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ctkfactsnli","title":"CTKFactsNLI","text":"<p>Czech Natural Language Inference dataset of around 3K evidence-claim pairs labelled with SUPPORTS, REFUTES or NOT ENOUGH INFO veracity labels. Extracted from a round of fact-checking experiments.</p> <p>Dataset: <code>mteb/CTKFactsNLI</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ces News, Written human-annotated found Citation <pre><code>@article{ullrich2023csfever,\n  author = {Ullrich, Herbert and Drchal, Jan and R{\\\\`y}par, Martin and Vincourov{\\\\'a}, Hana and Moravec, V{\\\\'a}clav},\n  journal = {Language Resources and Evaluation},\n  number = {4},\n  pages = {1571--1605},\n  publisher = {Springer},\n  title = {CsFEVER and CTKFacts: acquiring Czech data for fact verification},\n  volume = {57},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#cmnli","title":"Cmnli","text":"<p>Chinese Multi-Genre NLI</p> <p>Dataset: <code>C-MTEB/CMNLI</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy cmn Web, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2020-clue,\n  address = {Barcelona, Spain (Online)},\n  author = {Xu, Liang  and\nHu, Hai  and\nZhang, Xuanwei  and\nLi, Lu  and\nCao, Chenjie  and\nLi, Yudong  and\nXu, Yechen  and\nSun, Kai  and\nYu, Dian  and\nYu, Cong  and\nTian, Yin  and\nDong, Qianqian  and\nLiu, Weitang  and\nShi, Bo  and\nCui, Yiming  and\nLi, Junyi  and\nZeng, Jun  and\nWang, Rongzhao  and\nXie, Weijian  and\nLi, Yanting  and\nPatterson, Yina  and\nTian, Zuoyu  and\nZhang, Yiwen  and\nZhou, He  and\nLiu, Shaoweihua  and\nZhao, Zhe  and\nZhao, Qipeng  and\nYue, Cong  and\nZhang, Xinrui  and\nYang, Zhengliang  and\nRichardson, Kyle  and\nLan, Zhenzhong},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.419},\n  month = dec,\n  pages = {4762--4772},\n  publisher = {International Committee on Computational Linguistics},\n  title = {{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark},\n  url = {https://aclanthology.org/2020.coling-main.419},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#discotexpairclassification","title":"DisCoTexPairClassification","text":"<p>The DisCoTEX dataset aims at assessing discourse coherence in Italian texts. This dataset focuses on Italian real-world texts and provides resources to model coherence in natural language.</p> <p>Dataset: <code>MattiaSangermano/DisCoTex-last-sentence</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ita Social, Written derived found Citation <pre><code>@inproceedings{brunato2023discotex,\n  author = {Brunato, Dominique and Colla, Davide and Dell'Orletta, Felice and Dini, Irene and Radicioni, Daniele Paolo and Ravelli, Andrea Amelio and others},\n  booktitle = {CEUR WORKSHOP PROCEEDINGS},\n  organization = {CEUR},\n  pages = {1--8},\n  title = {DisCoTex at EVALITA 2023: overview of the assessing discourse coherence in Italian texts task},\n  volume = {3473},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#falsefriendsgermanenglish","title":"FalseFriendsGermanEnglish","text":"<p>A dataset to identify False Friends / false cognates between English and German. A generally challenging task for multilingual models.</p> <p>Dataset: <code>aari1995/false_friends_de_en_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu Written human-annotated created Citation <pre><code>@misc{Chibb_2022,\n  author = {Chibb, Aaron},\n  month = {Sep},\n  title = {{German-English False Friends in Multilingual Transformer Models: An Evaluation on Robustness and Word-to-Word Fine-Tuning}},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#farstail","title":"FarsTail","text":"<p>This dataset, named FarsTail, includes 10,367 samples which are provided in both the Persian language as well as the indexed format to be useful for non-Persian researchers. The samples are generated from 3,539 multiple-choice questions with the least amount of annotator interventions in a way similar to the SciTail dataset</p> <p>Dataset: <code>mteb/FarsTail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Academic, Written human-annotated found Citation <pre><code>@article{amirkhani2023farstail,\n  author = {Amirkhani, Hossein and AzariJafari, Mohammad and Faridan-Jahromi, Soroush and Kouhkan, Zeinab and Pourjafari, Zohreh and Amirak, Azadeh},\n  doi = {10.1007/s00500-023-08959-3},\n  journal = {Soft Computing},\n  publisher = {Springer},\n  title = {FarsTail: a Persian natural language inference dataset},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#farsiparaphrasedetection","title":"FarsiParaphraseDetection","text":"<p>Farsi Paraphrase Detection</p> <p>Dataset: <code>alighasemi/farsi_paraphrase_detection</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#indicxnlipairclassification","title":"IndicXnliPairClassification","text":"<p>INDICXNLI is similar to existing XNLI dataset in shape/form, but focuses on Indic language family. The train (392,702), validation (2,490), and evaluation sets (5,010) of English XNLI were translated from English into each of the eleven Indic languages. IndicTrans is a large Transformer-based sequence to sequence model. It is trained on Samanantar dataset (Ramesh et al., 2021), which is the largest parallel multi- lingual corpus over eleven Indic languages.</p> <p>Dataset: <code>mteb/IndicXnliPairClassification</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap asm, ben, guj, hin, kan, ... (11) Fiction, Government, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{aggarwal_gupta_kunch_22,\n  author = {Aggarwal, Divyanshu and Gupta, Vivek and Kunchukuttan, Anoop},\n  copyright = {Creative Commons Attribution 4.0 International},\n  doi = {10.48550/ARXIV.2204.08776},\n  publisher = {arXiv},\n  title = {IndicXNLI: Evaluating Multilingual Inference for Indian Languages},\n  url = {https://arxiv.org/abs/2204.08776},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#klue-nli","title":"KLUE-NLI","text":"<p>Textual Entailment between a hypothesis sentence and a premise sentence. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap kor Encyclopaedic, News, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#legalbenchpc","title":"LegalBenchPC","text":"<p>This LegalBench pair classification task is a combination of the following datasets: - Citation Prediction Classification: Given a legal statement and a case citation, determine if the citation is supportive of the legal statement. - Consumer Contracts QA: The task consists of 400 yes/no questions relating to consumer contracts (specifically, online terms of service) and is relevant to the legal skill of contract interpretation. - Contract QA: Answer yes/no questions about whether contractual clauses discuss particular issues like confidentiality requirements, BIPA consent, PII data breaches, breach of contract etc. - Hearsay: Classify if a particular piece of evidence qualifies as hearsay. Each sample in the dataset describes (1) an issue being litigated or an assertion a party wishes to prove, and (2) a piece of evidence a party wishes to introduce. The goal is to determine if\u2014as it relates to the issue\u2014the evidence would be considered hearsay under the definition provided above. - Privacy Policy Entailment: Given a privacy policy clause and a description of the clause, determine if the description is correct. This is a binary classification task in which the LLM is provided with a clause from a privacy policy, and a description of that clause (e.g., \u201cThe policy describes collection of the user\u2019s HTTP cookies, flash cookies, pixel tags, or similar identifiers by a party to the contract.\u201d). - Privacy Policy QA: Given a question and a clause from a privacy policy, determine if the clause contains enough information to answer the question. This is a binary classification task in which the LLM is provided with a question (e.g., \u201cdo you publish my data\u201d) and a clause from a privacy policy. The LLM must determine if the clause contains an answer to the question, and classify the question-clause pair.</p> <p>Dataset: <code>mteb/LegalBenchPC</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy eng Legal, Written expert-annotated found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{kolt2022predicting,\n  author = {Kolt, Noam},\n  journal = {Berkeley Tech. LJ},\n  pages = {71},\n  publisher = {HeinOnline},\n  title = {Predicting consumer contracts},\n  volume = {37},\n  year = {2022},\n}\n\n@article{ravichander2019question,\n  author = {Ravichander, Abhilasha and Black, Alan W and Wilson, Shomir and Norton, Thomas and Sadeh, Norman},\n  journal = {arXiv preprint arXiv:1911.00841},\n  title = {Question answering for privacy policies: Combining computational and legal perspectives},\n  year = {2019},\n}\n\n@article{zimmeck2019maps,\n  author = {Zimmeck, Sebastian and Story, Peter and Smullen, Daniel and Ravichander, Abhilasha and Wang, Ziqi and Reidenberg, Joel R and Russell, N Cameron and Sadeh, Norman},\n  journal = {Proc. Priv. Enhancing Tech.},\n  pages = {66},\n  title = {Maps: Scaling privacy compliance analysis to a million apps},\n  volume = {2019},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ocnli","title":"Ocnli","text":"<p>Original Chinese Natural Language Inference dataset</p> <p>Dataset: <code>C-MTEB/OCNLI</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy cmn Web, Written human-annotated created Citation <pre><code>@misc{hu2020ocnli,\n  archiveprefix = {arXiv},\n  author = {Hai Hu and Kyle Richardson and Liang Xu and Lu Li and Sandra Kuebler and Lawrence S. Moss},\n  eprint = {2010.05444},\n  primaryclass = {cs.CL},\n  title = {OCNLI: Original Chinese Natural Language Inference},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#opusparcuspc","title":"OpusparcusPC","text":"<p>Opusparcus is a paraphrase corpus for six European language: German, English, Finnish, French, Russian, and Swedish. The paraphrases consist of subtitles from movies and TV shows.</p> <p>Dataset: <code>mteb/OpusparcusPC</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, eng, fin, fra, rus, ... (6) Spoken, Spoken human-annotated created Citation <pre><code>@misc{creutz2018open,\n  archiveprefix = {arXiv},\n  author = {Mathias Creutz},\n  eprint = {1809.06142},\n  primaryclass = {cs.CL},\n  title = {Open Subtitles Paraphrase Corpus for Six Languages},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#psc","title":"PSC","text":"<p>Polish Summaries Corpus</p> <p>Dataset: <code>PL-MTEB/psc-pairclassification</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol News, Written derived found Citation <pre><code>@inproceedings{ogrodniczuk-kopec-2014-polish,\n  address = {Reykjavik, Iceland},\n  author = {Ogrodniczuk, Maciej  and\nKope{\\'c}, Mateusz},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {3712--3715},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {The {P}olish Summaries Corpus},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/1211_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#parsinluentail","title":"ParsinluEntail","text":"<p>A Persian textual entailment task (deciding sent1 entails sent2). The questions are partially translated from the SNLI dataset and partially generated by expert annotators.</p> <p>Dataset: <code>mteb/ParsinluEntail</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Reviews, Written derived found Citation <pre><code>@misc{khashabi2021parsinlusuitelanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Daniel Khashabi and Arman Cohan and Siamak Shakeri and Pedram Hosseini and Pouya Pezeshkpour and Malihe Alikhani and Moin Aminnaseri and Marzieh Bitaab and Faeze Brahman and Sarik Ghazarian and Mozhdeh Gheini and Arman Kabiri and Rabeeh Karimi Mahabadi and Omid Memarrast and Ahmadreza Mosallanezhad and Erfan Noury and Shahab Raji and Mohammad Sadegh Rasooli and Sepideh Sadeghi and Erfan Sadeqi Azer and Niloofar Safi Samghabadi and Mahsa Shafaei and Saber Sheybani and Ali Tazarv and Yadollah Yaghoobzadeh},\n  eprint = {2012.06154},\n  primaryclass = {cs.CL},\n  title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n  url = {https://arxiv.org/abs/2012.06154},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#parsinluqueryparaphpc","title":"ParsinluQueryParaphPC","text":"<p>A Persian query paraphrasng task (deciding whether two questions are paraphrases of each other). The questions are partially generated from Google auto-complete, and partially translated from the Quora paraphrasing dataset.</p> <p>Dataset: <code>mteb/ParsinluQueryParaphPC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Reviews, Written derived found Citation <pre><code>@misc{khashabi2021parsinlusuitelanguageunderstanding,\n  archiveprefix = {arXiv},\n  author = {Daniel Khashabi and Arman Cohan and Siamak Shakeri and Pedram Hosseini and Pouya Pezeshkpour and Malihe Alikhani and Moin Aminnaseri and Marzieh Bitaab and Faeze Brahman and Sarik Ghazarian and Mozhdeh Gheini and Arman Kabiri and Rabeeh Karimi Mahabadi and Omid Memarrast and Ahmadreza Mosallanezhad and Erfan Noury and Shahab Raji and Mohammad Sadegh Rasooli and Sepideh Sadeghi and Erfan Sadeqi Azer and Niloofar Safi Samghabadi and Mahsa Shafaei and Saber Sheybani and Ali Tazarv and Yadollah Yaghoobzadeh},\n  eprint = {2012.06154},\n  primaryclass = {cs.CL},\n  title = {ParsiNLU: A Suite of Language Understanding Challenges for Persian},\n  url = {https://arxiv.org/abs/2012.06154},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pawsxpairclassification","title":"PawsXPairClassification","text":"<p>PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification</p> <p>Dataset: <code>mteb/PawsXPairClassification</code> \u2022 License: https://huggingface.co/datasets/google-research-datasets/paws-x#licensing-information \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap cmn, deu, eng, fra, jpn, ... (7) Encyclopaedic, Web, Written human-annotated human-translated Citation <pre><code>@misc{yang2019pawsx,\n  archiveprefix = {arXiv},\n  author = {Yinfei Yang and Yuan Zhang and Chris Tar and Jason Baldridge},\n  eprint = {1908.11828},\n  primaryclass = {cs.CL},\n  title = {PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#ppcpc","title":"PpcPC","text":"<p>Polish Paraphrase Corpus</p> <p>Dataset: <code>mteb/PpcPC</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Fiction, News, Non-fiction, Social, Spoken, ... (7) derived found Citation <pre><code>@misc{dadas2022training,\n  archiveprefix = {arXiv},\n  author = {S\u0142awomir Dadas},\n  eprint = {2207.12759},\n  primaryclass = {cs.CL},\n  title = {Training Effective Neural Sentence Encoders from Automatically Mined Paraphrases},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemaisentenceparaphrasepc","title":"PubChemAISentenceParaphrasePC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemAISentenceParaphrasePC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry LM-generated created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemsmilespc","title":"PubChemSMILESPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSMILESPairClassification</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemsynonympc","title":"PubChemSynonymPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemSynonymPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemwikipairclassification","title":"PubChemWikiPairClassification","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemWikiMultilingualPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ces, deu, eng, fra, hin, ... (13) Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#pubchemwikiparagraphspc","title":"PubChemWikiParagraphsPC","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/PubChemWikiParagraphsPC</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Chemistry derived created Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@article{kim2023pubchem,\n  author = {Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},\n  journal = {Nucleic acids research},\n  number = {D1},\n  pages = {D1373--D1380},\n  publisher = {Oxford University Press},\n  title = {PubChem 2023 update},\n  volume = {51},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#rte3","title":"RTE3","text":"<p>Recognising Textual Entailment Challenge (RTE-3) aim to provide the NLP community with a benchmark to test progress in recognizing textual entailment</p> <p>Dataset: <code>mteb/RTE3</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, eng, fra, ita Encyclopaedic, News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{giampiccolo-etal-2007-third,\n  address = {Prague},\n  author = {Giampiccolo, Danilo  and\nMagnini, Bernardo  and\nDagan, Ido  and\nDolan, Bill},\n  booktitle = {Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing},\n  month = jun,\n  pages = {1--9},\n  publisher = {Association for Computational Linguistics},\n  title = {The Third {PASCAL} Recognizing Textual Entailment Challenge},\n  url = {https://aclanthology.org/W07-1401},\n  year = {2007},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sick-br-pc","title":"SICK-BR-PC","text":"<p>SICK-BR is a Portuguese inference corpus, human translated from SICK</p> <p>Dataset: <code>eduagarcia/sick-br</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap por Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{real18,\n  author = {Real, Livy\nand Rodrigues, Ana\nand Vieira e Silva, Andressa\nand Albiero, Beatriz\nand Thalenberg, Bruna\nand Guide, Bruno\nand Silva, Cindy\nand de Oliveira Lima, Guilherme\nand C{\\^a}mara, Igor C. S.\nand Stanojevi{\\'{c}}, Milo{\\v{s}}\nand Souza, Rodrigo\nand de Paiva, Valeria},\n  booktitle = {{Computational Processing of the Portuguese Language. PROPOR 2018.}},\n  doi = {10.1007/978-3-319-99722-3_31},\n  isbn = {978-3-319-99722-3},\n  title = {{SICK-BR: A Portuguese Corpus for Inference}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sick-e-pl","title":"SICK-E-PL","text":"<p>Polish version of SICK dataset for textual entailment.</p> <p>Dataset: <code>PL-MTEB/sicke-pl-pairclassification</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap pol Reviews, Written human-annotated machine-translated Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPere{\\l}kiewicz, Micha{\\l}  and\nPo{\\'s}wiata, Rafa{\\l}},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sicknlpairclassification","title":"SICKNLPairClassification","text":"<p>SICK-NL is a Dutch translation of SICK </p> <p>Dataset: <code>clips/mteb-nl-sick-pcls-pr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap nld Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{wijnholds2021sick,\n  author = {Wijnholds, Gijs and Moortgat, Michael},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  pages = {1474--1479},\n  title = {SICK-NL: A Dataset for Dutch Natural Language Inference},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sprintduplicatequestions","title":"SprintDuplicateQuestions","text":"<p>Duplicate questions from the Sprint community.</p> <p>Dataset: <code>mteb/sprintduplicatequestions-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Programming, Written derived found Citation <pre><code>@inproceedings{shah-etal-2018-adversarial,\n  address = {Brussels, Belgium},\n  author = {Shah, Darsh  and\nLei, Tao  and\nMoschitti, Alessandro  and\nRomeo, Salvatore  and\nNakov, Preslav},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1131},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {1056--1063},\n  publisher = {Association for Computational Linguistics},\n  title = {Adversarial Domain Adaptation for Duplicate Question Detection},\n  url = {https://aclanthology.org/D18-1131},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#sprintduplicatequestions-vn","title":"SprintDuplicateQuestions-VN","text":"<p>A translated dataset from Duplicate questions from the Sprint community. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/sprintduplicatequestions-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Programming, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synperchatbotragfaqpc","title":"SynPerChatbotRAGFAQPC","text":"<p>Synthetic Persian Chatbot RAG FAQ Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-faq-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Spoken LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synperqapc","title":"SynPerQAPC","text":"<p>Synthetic Persian QA Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-qa-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#synpertextkeywordspc","title":"SynPerTextKeywordsPC","text":"<p>Synthetic Persian Text Keywords Pair Classification</p> <p>Dataset: <code>MCINext/synthetic-persian-text-keyword-pair-classification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#terra","title":"TERRa","text":"<p>Textual Entailment Recognition for Russian. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text.</p> <p>Dataset: <code>mteb/TERRa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap rus News, Web, Written human-annotated found Citation <pre><code>@article{shavrina2020russiansuperglue,\n  author = {Shavrina, Tatiana\nand Fenogenova, Alena\nand Emelyanov, Anton\nand Shevelev, Denis\nand Artemova, Ekaterina\nand Malykh, Valentin\nand Mikhailov, Vladislav\nand Tikhonova, Maria\nand Chertok, Andrey\nand Evlampiev, Andrey},\n  journal = {arXiv preprint arXiv:2010.15925},\n  title = {RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#terrav2","title":"TERRa.V2","text":"<p>Textual Entailment Recognition for Russian. This task requires to recognize, given two text fragments, whether the meaning of one text is entailed (can be inferred) from the other text. Version 2 uses different prompt types for the two inputs.</p> <p>Dataset: <code>mteb/TERRa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap rus News, Web, Written human-annotated found Citation <pre><code>@article{shavrina2020russiansuperglue,\n  author = {Shavrina, Tatiana\nand Fenogenova, Alena\nand Emelyanov, Anton\nand Shevelev, Denis\nand Artemova, Ekaterina\nand Malykh, Valentin\nand Mikhailov, Vladislav\nand Tikhonova, Maria\nand Chertok, Andrey\nand Evlampiev, Andrey},\n  journal = {arXiv preprint arXiv:2010.15925},\n  title = {RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#talemaaderpc","title":"TalemaaderPC","text":"<p>\\ The Danish Language and Literature Society has developed a dataset for evaluating language models in Danish. The dataset contains a total of 1000 Danish idioms and fixed expressions with transferred meanings based on the Danish Dictionary's collection of fixed expressions with associated definitions. For each of the 1000 idioms and fixed expressions, three false definitions have also been prepared. The dataset can be used to test the performance of language models in identifying correct definitions for Danish idioms and fixed expressions.</p> <p>Dataset: <code>mteb/talemaader_pc</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_accuracy dan Academic, Written derived created Citation <pre><code>@misc{DSLDK1000Talemader,\n  author = {{Det Danske Sprog- og Litteraturselskab}},\n  howpublished = {Sprogteknologi.dk},\n  language = {Danish},\n  note = {CC-BY licensed dataset of 1000 Danish sayings and expressions},\n  publisher = {Digitaliseringsstyrelsen \\&amp; Det Danske Sprog- og Litteraturselskab},\n  title = {1000 danske talem\u00e5der - evalueringsdatas\u00e6t},\n  url = {https://sprogteknologi.dk/dataset/1000-talemader-evalueringsdatasaet},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twittersemeval2015","title":"TwitterSemEval2015","text":"<p>Paraphrase-Pairs of Tweets from the SemEval 2015 workshop.</p> <p>Dataset: <code>mteb/twittersemeval2015-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Social, Written human-annotated found Citation <pre><code>@inproceedings{xu-etal-2015-semeval,\n  address = {Denver, Colorado},\n  author = {Xu, Wei  and\nCallison-Burch, Chris  and\nDolan, Bill},\n  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)},\n  doi = {10.18653/v1/S15-2001},\n  editor = {Nakov, Preslav  and\nZesch, Torsten  and\nCer, Daniel  and\nJurgens, David},\n  month = jun,\n  pages = {1--11},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2015 Task 1: Paraphrase and Semantic Similarity in {T}witter ({PIT})},\n  url = {https://aclanthology.org/S15-2001},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twittersemeval2015-vn","title":"TwitterSemEval2015-VN","text":"<p>A translated dataset from Paraphrase-Pairs of Tweets from the SemEval 2015 workshop. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twittersemeval2015-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twitterurlcorpus","title":"TwitterURLCorpus","text":"<p>Paraphrase-Pairs of Tweets.</p> <p>Dataset: <code>mteb/twitterurlcorpus-pairclassification</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap eng Social, Written derived found Citation <pre><code>@inproceedings{lan-etal-2017-continuously,\n  address = {Copenhagen, Denmark},\n  author = {Lan, Wuwei  and\nQiu, Siyu  and\nHe, Hua  and\nXu, Wei},\n  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D17-1126},\n  editor = {Palmer, Martha  and\nHwa, Rebecca  and\nRiedel, Sebastian},\n  month = sep,\n  pages = {1224--1234},\n  publisher = {Association for Computational Linguistics},\n  title = {A Continuously Growing Dataset of Sentential Paraphrases},\n  url = {https://aclanthology.org/D17-1126},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#twitterurlcorpus-vn","title":"TwitterURLCorpus-VN","text":"<p>A translated dataset from Paraphrase-Pairs of Tweets. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/twitterurlcorpus-pairclassification-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) max_ap vie Social, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xlwicnlpairclassification","title":"XLWICNLPairClassification","text":"<p>The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. </p> <p>Dataset: <code>clips/mteb-nl-xlwic</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap nld Written derived created Citation <pre><code>@inproceedings{raganato2020xl,\n  author = {Raganato, A and Pasini, T and Camacho-Collados, J and Pilehvar, M and others},\n  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},\n  organization = {Association for Computational Linguistics (ACL)},\n  pages = {7193--7206},\n  title = {XL-WiC: A multilingual benchmark for evaluating semantic contextualization},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xnli","title":"XNLI","text":"<p>Dataset: <code>mteb/xnli</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ara, bul, deu, ell, eng, ... (14) Fiction, Government, Non-fiction, Written expert-annotated created Citation <pre><code>@inproceedings{conneau2018xnli,\n  author = {Conneau, Alexis\nand Rinott, Ruty\nand Lample, Guillaume\nand Williams, Adina\nand Bowman, Samuel R.\nand Schwenk, Holger\nand Stoyanov, Veselin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xnliv2","title":"XNLIV2","text":"<p>This is subset of 'XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding' with languages that were not part of the original XNLI plus three (verified) languages that are not strongly covered in MTEB</p> <p>Dataset: <code>mteb/XNLIV2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap asm, ben, bho, ell, guj, ... (13) Fiction, Government, Non-fiction, Written expert-annotated machine-translated and verified Citation <pre><code>@inproceedings{upadhyay2023xnli,\n  author = {Upadhyay, Ankit Kumar and Upadhya, Harsit Kumar},\n  booktitle = {2023 IEEE 8th International Conference for Convergence in Technology (I2CT)},\n  organization = {IEEE},\n  pages = {1--6},\n  title = {XNLI 2.0: Improving XNLI dataset and performance on Cross Lingual Understanding (XLU)},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#xstance","title":"XStance","text":"<p>A Multilingual Multi-Target Dataset for Stance Detection in French, German, and Italian.</p> <p>Dataset: <code>mteb/XStance</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap deu, fra, ita Social, Written human-annotated created Citation <pre><code>@inproceedings{vamvas2020xstance,\n  address = {Zurich, Switzerland},\n  author = {Vamvas, Jannis and Sennrich, Rico},\n  booktitle = {Proceedings of the 5th Swiss Text Analytics Conference (SwissText)  16th Conference on Natural Language Processing (KONVENS)},\n  month = {jun},\n  title = {{X-Stance}: A Multilingual Multi-Target Dataset for Stance Detection},\n  url = {http://ceur-ws.org/Vol-2624/paper9.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/pairclassification/#indonli","title":"indonli","text":"<p>IndoNLI is the first human-elicited Natural Language Inference (NLI) dataset for Indonesian. IndoNLI is annotated by both crowd workers and experts.</p> <p>Dataset: <code>mteb/indonli</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_ap ind Encyclopaedic, News, Web, Written expert-annotated found Citation <pre><code>@inproceedings{mahendra-etal-2021-indonli,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Mahendra, Rahmad and Aji, Alham Fikri and Louvan, Samuel and Rahman, Fahrurrozi and Vania, Clara},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  month = nov,\n  pages = {10511--10527},\n  publisher = {Association for Computational Linguistics},\n  title = {{I}ndo{NLI}: A Natural Language Inference Dataset for {I}ndonesian},\n  url = {https://aclanthology.org/2021.emnlp-main.821},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/regression/","title":"Regression","text":"<ul> <li>Number of tasks: 2</li> </ul>"},{"location":"overview/available_tasks/regression/#ruscibenchcitedcountregression","title":"RuSciBenchCitedCountRegression","text":"<p>Predicts the number of times a scientific article has been cited by other papers. The prediction is based on the article's title and abstract. The data is sourced from the Russian electronic library of scientific publications (eLibrary.ru) and includes papers with both Russian and English abstracts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) kendalltau eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/regression/#ruscibenchyearpublregression","title":"RuSciBenchYearPublRegression","text":"<p>Predicts the publication year of a scientific article. The prediction is based on the article's title and abstract. The data is sourced from the Russian electronic library of scientific publications (eLibrary.ru) and includes papers with both Russian and English abstracts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) kendalltau eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/","title":"Reranking","text":"<ul> <li>Number of tasks: 43</li> </ul>"},{"location":"overview/available_tasks/reranking/#alloprofreranking","title":"AlloprofReranking","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>mteb/AlloprofReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 fra Academic, Web, Written expert-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#askubuntudupquestions","title":"AskUbuntuDupQuestions","text":"<p>AskUbuntu Question Dataset - Questions from AskUbuntu with manual annotations marking pairs of questions as similar or non-similar</p> <p>Dataset: <code>mteb/AskUbuntuDupQuestions</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Programming, Web human-annotated found Citation <pre><code>@article{wang-2021-TSDAE,\n  author = {Wang, Kexin and Reimers, Nils and  Gurevych, Iryna},\n  journal = {arXiv preprint arXiv:2104.06979},\n  month = {4},\n  title = {TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning},\n  url = {https://arxiv.org/abs/2104.06979},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#askubuntudupquestions-vn","title":"AskUbuntuDupQuestions-VN","text":"<p>A translated dataset from AskUbuntu Question Dataset - Questions from AskUbuntu with manual annotations marking pairs of questions as similar or non-similar The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/AskUbuntuDupQuestions-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Programming, Web derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#builtbenchreranking","title":"BuiltBenchReranking","text":"<p>Reranking of built asset entity type/class descriptions given a query describing an entity as represented in well-established industry classification systems such as Uniclass, IFC, etc.</p> <p>Dataset: <code>mteb/BuiltBenchReranking</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#cmedqav1-reranking","title":"CMedQAv1-reranking","text":"<p>Chinese community medical question answering</p> <p>Dataset: <code>mteb/CMedQAv1-reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Medical, Written expert-annotated found Citation <pre><code>@article{zhang2017chinese,\n  author = {Zhang, Sheng and Zhang, Xin and Wang, Hui and Cheng, Jiajun and Li, Pei and Ding, Zhaoyun},\n  journal = {Applied Sciences},\n  number = {8},\n  pages = {767},\n  publisher = {Multidisciplinary Digital Publishing Institute},\n  title = {Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs},\n  volume = {7},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#cmedqav2-reranking","title":"CMedQAv2-reranking","text":"<p>Chinese community medical question answering</p> <p>Dataset: <code>mteb/CMedQAv2-reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Medical, Written expert-annotated found Citation <pre><code>@article{8548603,\n  author = {S. Zhang and X. Zhang and H. Wang and L. Guo and S. Liu},\n  doi = {10.1109/ACCESS.2018.2883637},\n  issn = {2169-3536},\n  journal = {IEEE Access},\n  keywords = {Biomedical imaging;Data mining;Semantics;Medical services;Feature extraction;Knowledge discovery;Medical question answering;interactive attention;deep learning;deep neural networks},\n  month = {},\n  number = {},\n  pages = {74061-74071},\n  title = {Multi-Scale Attentive Interaction Networks for Chinese Medical Question Answer Selection},\n  volume = {6},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderaglibrarydocumentationsolutions","title":"CodeRAGLibraryDocumentationSolutions","text":"<p>Evaluation of code library documentation retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant Python library documentation sections given code-related queries.</p> <p>Dataset: <code>code-rag-bench/library-documentation</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagonlinetutorials","title":"CodeRAGOnlineTutorials","text":"<p>Evaluation of online programming tutorial retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant tutorials from online platforms given code-related queries.</p> <p>Dataset: <code>code-rag-bench/online-tutorials</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagprogrammingsolutions","title":"CodeRAGProgrammingSolutions","text":"<p>Evaluation of programming solution retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant programming solutions given code-related queries.</p> <p>Dataset: <code>code-rag-bench/programming-solutions</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#coderagstackoverflowposts","title":"CodeRAGStackoverflowPosts","text":"<p>Evaluation of StackOverflow post retrieval using CodeRAG-Bench. Tests the ability to retrieve relevant StackOverflow posts given code-related queries.</p> <p>Dataset: <code>code-rag-bench/stackoverflow-posts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming derived found Citation <pre><code>    @misc{wang2024coderagbenchretrievalaugmentcode,\n  archiveprefix = {arXiv},\n  author = {Zora Zhiruo Wang and Akari Asai and Xinyan Velocity Yu and Frank F. Xu and Yiqing Xie and Graham Neubig and Daniel Fried},\n  eprint = {2406.14497},\n  primaryclass = {cs.SE},\n  title = {CodeRAG-Bench: Can Retrieval Augment Code Generation?},\n  url = {https://arxiv.org/abs/2406.14497},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#eressreranking","title":"ERESSReranking","text":"<p>ERESS is a comprehensive e-commerce reranking dataset designed for holistic     evaluation of reranking models. It includes diverse query intents including     attribute-rich queries, navigational queries, gift/audience-specific queries,     utility queries, and more.</p> <p>Dataset: <code>thebajajra/ERESSReranking</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_5 eng E-commerce, Web LM-generated found Citation <pre><code>@article{Bajaj2026RexRerankers,\n  author = {Bajaj, Rahul and Garg, Anuj and Nupur, Jaya},\n  journal = {Hugging Face Blog (Community Article)},\n  month = jan,\n  title = {{RexRerankers}: {SOTA} Rankers for Product Discovery and {AI} Assistants},\n  url = {https://huggingface.co/blog/thebajajra/rexrerankers},\n  urldate = {2026-01-24},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#escireranking","title":"ESCIReranking","text":"<p>Dataset: <code>mteb/ESCIReranking</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng, jpn, spa Written derived created Citation <pre><code>@article{reddy2022shopping,\n  archiveprefix = {arXiv},\n  author = {Chandan K. Reddy and Llu\u00eds M\u00e0rquez and Fran Valero and Nikhil Rao and Hugo Zaragoza and Sambaran Bandyopadhyay and Arnab Biswas and Anlu Xing and Karthik Subbian},\n  eprint = {2206.06588},\n  title = {Shopping Queries Dataset: A Large-Scale {ESCI} Benchmark for Improving Product Search},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humecore17instructionreranking","title":"HUMECore17InstructionReranking","text":"<p>Human evaluation subset of Core17 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMECore17InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@misc{weller2024followir,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini},\n  eprint = {2403.15246},\n  primaryclass = {cs.IR},\n  title = {FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humenews21instructionreranking","title":"HUMENews21InstructionReranking","text":"<p>Human evaluation subset of News21 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMENews21InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@inproceedings{soboroff2021trec,\n  author = {Soboroff, Ian and Macdonald, Craig and McCreadie, Richard},\n  booktitle = {TREC},\n  title = {TREC 2021 News Track Overview},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humerobust04instructionreranking","title":"HUMERobust04InstructionReranking","text":"<p>Human evaluation subset of Robust04 instruction retrieval dataset for reranking evaluation.</p> <p>Dataset: <code>mteb/HUMERobust04InstructionReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng News, Written human-annotated found Citation <pre><code>@inproceedings{voorhees2005trec,\n  author = {Voorhees, Ellen M},\n  booktitle = {TREC},\n  title = {TREC 2004 Robust Retrieval Track Overview},\n  year = {2005},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#humewikipediarerankingmultilingual","title":"HUMEWikipediaRerankingMultilingual","text":"<p>Human evaluation subset of Wikipedia reranking dataset across multiple languages.</p> <p>Dataset: <code>mteb/HUMEWikipediaRerankingMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 dan, eng, nob Encyclopaedic, Written derived found Citation <pre><code>@misc{wikipedia_reranking_2023,\n  author = {Ellamind},\n  title = {Wikipedia 2023-11 Reranking Multilingual Dataset},\n  url = {https://github.com/ellamind/wikipedia-2023-11-reranking-multilingual},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jqarareranking","title":"JQaRAReranking","text":"<p>JQaRA: Japanese Question Answering with Retrieval Augmentation  - \u691c\u7d22\u62e1\u5f35(RAG)\u8a55\u4fa1\u306e\u305f\u3081\u306e\u65e5\u672c\u8a9e Q&amp;A \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8. JQaRA is an information retrieval task for questions against 100 candidate data (including one or more correct answers).</p> <p>Dataset: <code>mteb/JQaRAReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jqara,\n  author = {Yuichi Tateno},\n  title = {JQaRA: Japanese Question Answering with Retrieval Augmentation - \u691c\u7d22\u62e1\u5f35(RAG)\u8a55\u4fa1\u306e\u305f\u3081\u306e\u65e5\u672c\u8a9eQ&amp;A\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JQaRA},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jqararerankinglite","title":"JQaRARerankingLite","text":"<p>JQaRA (Japanese Question Answering with Retrieval Augmentation) is a reranking dataset consisting of questions from JAQKET and corpus from Japanese Wikipedia. This is the lightweight version with a reduced corpus (172,897 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/JQaRARerankingLite</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n\n@misc{yuichi-tateno-2024-jqara,\n  author = {Yuichi Tateno},\n  title = {JQaRA: Japanese Question Answering with Retrieval Augmentation\n- \u691c\u7d22\u62e1\u5f35(RAG)\u8a55\u4fa1\u306e\u305f\u3081\u306e\u65e5\u672c\u8a9eQ&amp;A\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JQaRA},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jacwirreranking","title":"JaCWIRReranking","text":"<p>JaCWIR is a small-scale Japanese information retrieval evaluation dataset consisting of 5000 question texts and approximately 500k web page titles and web page introductions or summaries (meta descriptions, etc.). The question texts are created based on one of the 500k web pages, and that data is used as a positive example for the question text.</p> <p>Dataset: <code>mteb/JaCWIRReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Web, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#jacwirrerankinglite","title":"JaCWIRRerankingLite","text":"<p>JaCWIR (Japanese Casual Web IR) is a dataset consisting of questions and webpage meta descriptions collected from Hatena Bookmark. This is the lightweight reranking version with a reduced corpus (188,033 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/JaCWIRRerankingLite</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found Citation <pre><code>@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n\n@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#locbenchrr","title":"LocBenchRR","text":"<p>Software Issue Localization.</p> <p>Dataset: <code>mteb/LocBenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{chen2025locagentgraphguidedllmagents,\n  archiveprefix = {arXiv},\n  author = {Zhaoling Chen and Xiangru Tang and Gangda Deng and Fang Wu and Jialong Wu and Zhiwei Jiang and Viktor Prasanna and Arman Cohan and Xingyao Wang},\n  eprint = {2503.09089},\n  primaryclass = {cs.SE},\n  title = {LocAgent: Graph-Guided LLM Agents for Code Localization},\n  url = {https://arxiv.org/abs/2503.09089},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#miraclreranking","title":"MIRACLReranking","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages.</p> <p>Dataset: <code>mteb/MIRACLReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#mmarcoreranking","title":"MMarcoReranking","text":"<p>mMARCO is a multilingual version of the MS MARCO passage ranking dataset</p> <p>Dataset: <code>mteb/MMarcoReranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Web, Written human-annotated machine-translated Citation <pre><code>@article{DBLP:journals/corr/abs-2108-13897,\n  author = {Luiz Bonifacio and\nIsrael Campiotti and\nRoberto de Alencar Lotufo and\nRodrigo Frassetto Nogueira},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},\n  eprint = {2108.13897},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 20 Mar 2023 15:35:34 +0100},\n  title = {mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset},\n  url = {https://arxiv.org/abs/2108.13897},\n  volume = {abs/2108.13897},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#mindsmallreranking","title":"MindSmallReranking","text":"<p>Microsoft News Dataset: A Large-Scale English Dataset for News Recommendation Research</p> <p>Dataset: <code>mteb/MindSmallReranking</code> \u2022 License: https://github.com/msnews/MIND/blob/master/MSR%20License_Data.pdf \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) max_over_subqueries_map_at_1000 eng News, Written expert-annotated found Citation <pre><code>@inproceedings{wu-etal-2020-mind,\n  address = {Online},\n  author = {Wu, Fangzhao  and Qiao, Ying  and Chen, Jiun-Hung  and Wu, Chuhan  and Qi,\nTao  and Lian, Jianxun  and Liu, Danyang  and Xie, Xing  and Gao, Jianfeng  and Wu, Winnie  and Zhou, Ming},\n  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2020.acl-main.331},\n  editor = {Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel},\n  month = jul,\n  pages = {3597--3606},\n  publisher = {Association for Computational Linguistics},\n  title = {{MIND}: A Large-scale Dataset for News\nRecommendation},\n  url = {https://aclanthology.org/2020.acl-main.331},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#multilongdocreranking","title":"MultiLongDocReranking","text":"<p>Reranking version of MultiLongDocRetrieval (MLDR). MLDR is a Multilingual Long-Document Retrieval dataset built on Wikipedia, Wudao and mC4, covering 13 typologically diverse languages. Specifically, we sample lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the dataset.</p> <p>Dataset: <code>mteb/MultiLongDocReranking</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, deu, eng, fra, hin, ... (13) Encyclopaedic, Fiction, Non-fiction, Web, Written LM-generated found Citation <pre><code>@misc{bge-m3,\n  archiveprefix = {arXiv},\n  author = {Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n  eprint = {2402.03216},\n  primaryclass = {cs.CL},\n  title = {BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#multiswebenchrr","title":"MultiSWEbenchRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/MultiSWEbenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{zan2025multiswebench,\n  archiveprefix = {arXiv},\n  author = {Daoguang Zan and Zhirong Huang and Wei Liu and Hanwu Chen and Linhao Zhang and Shulin Xin and Lu Chen and Qi Liu and Xiaojian Zhong and Aoyan Li and Siyao Liu and Yongsheng Xiao and Liangqiang Chen and Yuyu Zhang and Jing Su and Tianyu Liu and Rui Long and Kai Shen and Liang Xiang},\n  eprint = {2504.02605},\n  primaryclass = {cs.SE},\n  title = {Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving},\n  url = {https://arxiv.org/abs/2504.02605},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#namaamrtydireranking","title":"NamaaMrTydiReranking","text":"<p>Mr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations. This dataset adapts the arabic test split for Reranking evaluation purposes by the addition of multiple (Hard) Negatives to each query and positive</p> <p>Dataset: <code>mteb/NamaaMrTydiReranking</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 ara Encyclopaedic, Written human-annotated found Citation <pre><code>@article{muennighoff2022mteb,\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\\\"\\\\i}c and Reimers, Nils},\n  doi = {10.48550/ARXIV.2210.07316},\n  journal = {arXiv preprint arXiv:2210.07316},\n  publisher = {arXiv},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2210.07316},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#nevir","title":"NevIR","text":"<p>Paired evaluation of real world negation in retrieval, with questions and passages. Since models generally prefer one passage over the other always, there are two questions that the model must get right to understand the negation (hence the <code>paired_accuracy</code> metric).</p> <p>Dataset: <code>orionweller/NevIR-mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) paired_accuracy eng Web human-annotated created Citation <pre><code>@inproceedings{Weller2023NevIRNI,\n  author = {{Orion Weller and Dawn J Lawrie and Benjamin Van Durme}},\n  booktitle = {{Conference of the European Chapter of the Association for Computational Linguistics}},\n  title = {{NevIR: Negation in Neural Information Retrieval}},\n  url = {{https://api.semanticscholar.org/CorpusID:258676146}},\n  year = {{2023}},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#rubqreranking","title":"RuBQReranking","text":"<p>Paragraph reranking based on RuBQ 2.0. Give paragraphs that answer the question higher scores.</p> <p>Dataset: <code>mteb/RuBQReranking</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 rus Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{RuBQ2021,\n  author = {Ivan Rybin and Vladislav Korablinov and Pavel Efimov and Pavel Braslavski},\n  booktitle = {ESWC},\n  pages = {532--547},\n  title = {RuBQ 2.0: An Innovated Russian Question Answering Dataset},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swepolybenchrr","title":"SWEPolyBenchRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEPolyBenchRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{rashid2025swepolybenchmultilanguagebenchmarkrepository,\n  archiveprefix = {arXiv},\n  author = {Muhammad Shihab Rashid and Christian Bock and Yuan Zhuang and Alexander Buchholz and Tim Esler and Simon Valentin and Luca Franceschi and Martin Wistuba and Prabhu Teja Sivaprasad and Woo Jung Kim and Anoop Deoras and Giovanni Zappella and Laurent Callot},\n  eprint = {2504.08703},\n  primaryclass = {cs.SE},\n  title = {SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents},\n  url = {https://arxiv.org/abs/2504.08703},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchliterr","title":"SWEbenchLiteRR","text":"<p>Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEbenchLiteRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{jimenez2024swebenchlanguagemodelsresolve,\n  archiveprefix = {arXiv},\n  author = {Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik Narasimhan},\n  eprint = {2310.06770},\n  primaryclass = {cs.CL},\n  title = {SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},\n  url = {https://arxiv.org/abs/2310.06770},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchmultilingualrr","title":"SWEbenchMultilingualRR","text":"<p>Multilingual Software Issue Localization.</p> <p>Dataset: <code>mteb/SWEbenchMultilingualRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{yang2025swesmith,\n  archiveprefix = {arXiv},\n  author = {John Yang and Kilian Lieret and Carlos E. Jimenez and Alexander Wettig and Kabir Khandpur and Yanzhe Zhang and Binyuan Hui and Ofir Press and Ludwig Schmidt and Diyi Yang},\n  eprint = {2504.21798},\n  primaryclass = {cs.SE},\n  title = {SWE-smith: Scaling Data for Software Engineering Agents},\n  url = {https://arxiv.org/abs/2504.21798},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#swebenchverifiedrr","title":"SWEbenchVerifiedRR","text":"<p>Software Issue Localization for SWE-bench Verified</p> <p>Dataset: <code>mteb/SWEbenchVerifiedRR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{openai2024swebenchverified,\n  author = {OpenAI},\n  title = {Introducing swe-bench verified},\n  url = {https://openai.com/index/introducing-swe-bench-verified/},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#scidocsrr","title":"SciDocsRR","text":"<p>Ranking of related scientific papers based on their title.</p> <p>Dataset: <code>mteb/SciDocsRR</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#scidocsrr-vn","title":"SciDocsRR-VN","text":"<p>A translated dataset from Ranking of related scientific papers based on their title. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/SciDocsRR-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#stackoverflowdupquestions","title":"StackOverflowDupQuestions","text":"<p>Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python</p> <p>Dataset: <code>mteb/StackOverflowDupQuestions</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 eng Blog, Programming, Written derived found Citation <pre><code>@article{Liu2018LinkSOAD,\n  author = {Xueqing Liu and Chi Wang and Yue Leng and ChengXiang Zhai},\n  journal = {Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software Engineering},\n  title = {LinkSO: a dataset for learning to retrieve similar question answer pairs on software development forums},\n  url = {https://api.semanticscholar.org/CorpusID:53111679},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#stackoverflowdupquestions-vn","title":"StackOverflowDupQuestions-VN","text":"<p>A translated dataset from Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>mteb/StackOverflowDupQuestions-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#syntecreranking","title":"SyntecReranking","text":"<p>This dataset has been built from the Syntec Collective bargaining agreement.</p> <p>Dataset: <code>mteb/SyntecReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 fra Legal, Written human-annotated found Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#t2reranking","title":"T2Reranking","text":"<p>T2Ranking: A large-scale Chinese Benchmark for Passage Ranking</p> <p>Dataset: <code>mteb/T2Reranking</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 cmn Web, Written human-annotated found Citation <pre><code>@misc{xie2023t2ranking,\n  archiveprefix = {arXiv},\n  author = {Xiaohui Xie and Qian Dong and Bingning Wang and Feiyang Lv and Ting Yao and Weinan Gan and Zhijing Wu and Xiangsheng Li and Haitao Li and Yiqun Liu and Jin Ma},\n  eprint = {2304.03679},\n  primaryclass = {cs.IR},\n  title = {T2Ranking: A large-scale Chinese Benchmark for Passage Ranking},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#voyagemmarcoreranking","title":"VoyageMMarcoReranking","text":"<p>a hard-negative augmented version of the Japanese MMARCO dataset as used in Voyage AI Evaluation Suite</p> <p>Dataset: <code>mteb/VoyageMMarcoReranking</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 jpn Academic, Non-fiction, Written derived found Citation <pre><code>@misc{clavi\u00e92023jacolbert,\n  archiveprefix = {arXiv},\n  author = {Benjamin Clavi\u00e9},\n  eprint = {2312.16144},\n  title = {JaColBERT and Hard Negatives, Towards Better Japanese-First Embeddings for Retrieval: Early Technical Report},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#weblinxcandidatesreranking","title":"WebLINXCandidatesReranking","text":"<p>WebLINX is a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. The reranking task focuses on finding relevant elements at every given step in the trajectory.</p> <p>Dataset: <code>mteb/WebLINXCandidatesReranking</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) mrr_at_10 eng Academic, Web, Written expert-annotated created Citation <pre><code>@misc{l\u00f92024weblinx,\n  archiveprefix = {arXiv},\n  author = {Xing Han L\u00f9 and Zden\u011bk Kasner and Siva Reddy},\n  eprint = {2402.05930},\n  primaryclass = {cs.CL},\n  title = {WebLINX: Real-World Website Navigation with Multi-Turn Dialogue},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#wikipediarerankingmultilingual","title":"WikipediaRerankingMultilingual","text":"<p>The dataset is derived from Cohere's wikipedia-2023-11 dataset and contains synthetically generated queries.</p> <p>Dataset: <code>mteb/WikipediaRerankingMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 ben, bul, ces, dan, deu, ... (18) Encyclopaedic, Written LM-generated and reviewed LM-generated and verified Citation <pre><code>@online{wikidump2024,\n  author = {Wikimedia Foundation},\n  title = {Wikimedia Downloads},\n  url = {https://dumps.wikimedia.org},\n}\n</code></pre>"},{"location":"overview/available_tasks/reranking/#xgluewprreranking","title":"XGlueWPRReranking","text":"<p>XGLUE is a new benchmark dataset to evaluate the performance of cross-lingual pre-trained models with respect to cross-lingual natural language understanding and generation. XGLUE is composed of 11 tasks spans 19 languages.</p> <p>Dataset: <code>mteb/XGlueWPRReranking</code> \u2022 License: http://hdl.handle.net/11234/1-3105 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) map_at_1000 deu, eng, fra, ita, por, ... (7) Written human-annotated found Citation <pre><code>@misc{11234/1-3105,\n  author = {Zeman, Daniel and Nivre, Joakim and Abrams, Mitchell and Aepli, No{\\\"e}mi and Agi{\\'c}, {\\v Z}eljko and Ahrenberg, Lars and Aleksandravi{\\v c}i{\\=u}t{\\.e}, Gabriel{\\.e} and Antonsen, Lene and Aplonova, Katya and Aranzabe, Maria Jesus and Arutie, Gashaw and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Augustinus, Liesbeth and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Barbu Mititelu, Verginica and Basmov, Victoria and Batchelor, Colin and Bauer, John and Bellato, Sandra and Bengoetxea, Kepa and Berzak, Yevgeni and Bhat, Irshad Ahmad and Bhat, Riyaz Ahmad and Biagetti, Erica and Bick, Eckhard and Bielinskien{\\.e}, Agn{\\.e} and Blokland, Rogier and Bobicev, Victoria and Boizou, Lo{\\\"{\\i}}c and Borges V{\\\"o}lker, Emanuel and B{\\\"o}rstell, Carl and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Boyd, Adriane and Brokait{\\.e}, Kristina and Burchardt, Aljoscha and Candito, Marie and Caron, Bernard and Caron, Gauthier and Cavalcanti, Tatiana and Cebiro{\\u g}lu Eryi{\\u g}it, G{\\\"u}l{\\c s}en and Cecchini, Flavio Massimiliano and Celano, Giuseppe G. A. and {\\v C}{\\'e}pl{\\\"o}, Slavom{\\'{\\i}}r and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Chun, Jayeol and Cignarella, Alessandra T. and Cinkov{\\'a}, Silvie and Collomb, Aur{\\'e}lie and {\\c C}{\\\"o}ltekin, {\\c C}a{\\u g}r{\\i} and Connor, Miriam and Courtin, Marine and Davidson, Elizabeth and de Marneffe, Marie-Catherine and de Paiva, Valeria and de Souza, Elvis and Diaz de Ilarraza, Arantza and Dickerson, Carly and Dione, Bamba and Dirix, Peter and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Dwivedi, Puneet and Eckhoff, Hanne and Eli, Marhaba and Elkahky, Ali and Ephrem, Binyam and Erina, Olga and Erjavec, Toma{\\v z} and Etienne, Aline and Evelyn, Wograine and Farkas, Rich{\\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\\'a}udia and Fujita, Kazunori and Gajdo{\\v s}ov{\\'a}, Katar{\\'{\\i}}na and Galbraith, Daniel and Garcia, Marcos and G{\\\"a}rdenfors, Moa and Garza, Sebastian and Gerdes, Kim and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\\\"o}k{\\i}rmak, Memduh and Goldberg, Yoav and G{\\'o}mez Guinovart, Xavier and Gonz{\\'a}lez Saavedra, Berta and Grici{\\=u}t{\\.e}, Bernadeta and Grioni, Matias and Gr{\\=u}z{\\={\\i}}tis, Normunds and Guillaume, Bruno and Guillot-Barbance, C{\\'e}line and Habash, Nizar and Haji{\\v c}, Jan and Haji{\\v c} jr., Jan and H{\\\"a}m{\\\"a}l{\\\"a}inen, Mika and H{\\`a} M{\\~y}, Linh and Han, Na-Rae and Harris, Kim and Haug, Dag and Heinecke, Johannes and Hennig, Felix and Hladk{\\'a}, Barbora and Hlav{\\'a}{\\v c}ov{\\'a}, Jaroslava and Hociung, Florinel and Hohle, Petter and Hwang, Jena and Ikeda, Takumi and Ion, Radu and Irimia, Elena and Ishola, {\\d O}l{\\'a}j{\\'{\\i}}d{\\'e} and Jel{\\'{\\i}}nek, Tom{\\'a}{\\v s} and Johannsen, Anders and J{\\o}rgensen, Fredrik and Juutinen, Markus and Ka{\\c s}{\\i}kara, H{\\\"u}ner and Kaasen, Andre and Kabaeva, Nadezhda and Kahane, Sylvain and Kanayama, Hiroshi and Kanerva, Jenna and Katz, Boris and Kayadelen, Tolga and Kenney, Jessica and Kettnerov{\\'a}, V{\\'a}clava and Kirchner, Jesse and Klementieva, Elena and K{\\\"o}hn, Arne and Kopacewicz, Kamil and Kotsyba, Natalia and Kovalevskait{\\.e}, Jolanta and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lam, Lucia and Lando, Tatiana and Larasati, Septina Dian and Lavrentiev, Alexei and Lee, John and L{\\^e} H{\\`{\\^o}}ng, Ph\u01b0\u01a1ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Li, Keying and Lim, {KyungTae} and Liovina, Maria and Li, Yuan and Ljube{\\v s}i{\\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\\u a}r{\\u a}nduc, C{\\u a}t{\\u a}lina and Mare{\\v c}ek, David and Marheinecke, Katrin and Mart{\\'{\\i}}nez Alonso, H{\\'e}ctor and Martins, Andr{\\'e} and Ma{\\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and {McGuinness}, Sarah and Mendon{\\c c}a, Gustavo and Miekka, Niko and Misirpashayeva, Margarita and Missil{\\\"a}, Anna and Mititelu, C{\\u a}t{\\u a}lin and Mitrofan, Maria and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Keiko Sophie and Morioka, Tomohiko and Mori, Shinsuke and Moro, Shigeki and Mortensen, Bjartur and Moskalevskyi, Bohdan and Muischnek, Kadri and Munro, Robert and Murawaki, Yugo and M{\\\"u}{\\\"u}risep, Kaili and Nainwani, Pinkey and Navarro Hor{\\~n}iacek, Juan Ignacio and Nedoluzhko, Anna and Ne{\\v s}pore-B{\\=e}rzkalne, Gunta and Nguy{\\~{\\^e}}n Th{\\d i}, L\u01b0\u01a1ng and Nguy{\\~{\\^e}}n Th{\\d i} Minh, Huy{\\`{\\^e}}n and Nikaido, Yoshihiro and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Ojha, Atul Kr. and Ol{\\'u}{\\`o}kun, Ad{\\'e}day{\\d o}\u0300 and Omura, Mai and Osenova, Petya and {\\\"O}stling, Robert and {\\O}vrelid, Lilja and Partanen, Niko and Pascual, Elena and Passarotti, Marco and Patejuk, Agnieszka and Paulino-Passos, Guilherme and Peljak-{\\L}api{\\'n}ska, Angelika and Peng, Siyao and Perez, Cenel-Augusto and Perrier, Guy and Petrova, Daria and Petrov, Slav and Phelan, Jason and Piitulainen, Jussi and Pirinen, Tommi A and Pitler, Emily and Plank, Barbara and Poibeau, Thierry and Ponomareva, Larisa and Popel, Martin and Pretkalni{\\c n}a, Lauma and Pr{\\'e}vost, Sophie and Prokopidis, Prokopis and Przepi{\\'o}rkowski, Adam and Puolakainen, Tiina and Pyysalo, Sampo and Qi, Peng and R{\\\"a}{\\\"a}bis, Andriela and Rademaker, Alexandre and Ramasamy, Loganathan and Rama, Taraka and Ramisch, Carlos and Ravishankar, Vinit and Real, Livy and Reddy, Siva and Rehm, Georg and Riabov, Ivan and Rie{\\ss}ler, Michael and Rimkut{\\.e}, Erika and Rinaldi, Larissa and Rituma, Laura and Rocha, Luisa and Romanenko, Mykhailo and Rosa, Rudolf and Rovati, Davide and Ro\u0219ca, Valentin and Rudina, Olga and Rueter, Jack and Sadde, Shoval and Sagot, Beno{\\^{\\i}}t and Saleh, Shadi and Salomoni, Alessio and Samard{\\v z}i{\\'c}, Tanja and Samson, Stephanie and Sanguinetti, Manuela and S{\\\"a}rg, Dage and Saul{\\={\\i}}te, Baiba and Sawanakunanon, Yanin and Schneider, Nathan and Schuster, Sebastian and Seddah, Djam{\\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shen, Mo and Shimada, Atsuko and Shirasu, Hiroyuki and Shohibussirri, Muh and Sichinava, Dmitry and Silveira, Aline and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\\'o}, Katalin and {\\v S}imkov{\\'a}, M{\\'a}ria and Simov, Kiril and Smith, Aaron and Soares-Bastos, Isabela and Spadine, Carolyn and Stella, Antonio and Straka, Milan and Strnadov{\\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Suzuki, Shingo and Sz{\\'a}nt{\\'o}, Zsolt and Taji, Dima and Takahashi, Yuta and Tamburini, Fabio and Tanaka, Takaaki and Tellier, Isabelle and Thomas, Guillaume and Torga, Liisi and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\\v s}ov{\\'a}, Zde{\\v n}ka and Uria, Larraitz and Uszkoreit, Hans and Utka, Andrius and Vajjala, Sowmya and van Niekerk, Daniel and van Noord, Gertjan and Varga, Viktor and Villemonte de la Clergerie, Eric and Vincze, Veronika and Wallin, Lars and Walsh, Abigail and Wang, Jing Xian and Washington, Jonathan North and Wendt, Maximilan and Williams, Seyi and Wir{\\'e}n, Mats and Wittern, Christian and Woldemariam, Tsegay and Wong, Tak-sum and Wr{\\'o}blewska, Alina and Yako, Mary and Yamazaki, Naoki and Yan, Chunxiao and Yasuoka, Koichi and Yavrumyan, Marat M. and Yu, Zhuoran and {\\v Z}abokrtsk{\\'y}, Zden{\\v e}k and Zeldes, Amir and Zhang, Manying and Zhu, Hanzhi},\n  copyright = {Licence Universal Dependencies v2.5},\n  note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\\'U}FAL}), Faculty of Mathematics and Physics, Charles University},\n  title = {Universal Dependencies 2.5},\n  url = {http://hdl.handle.net/11234/1-3105},\n  year = {2019},\n}\n\n@inproceedings{conneau2018xnli,\n  author = {Conneau, Alexis\nand Rinott, Ruty\nand Lample, Guillaume\nand Williams, Adina\nand Bowman, Samuel R.\nand Schwenk, Holger\nand Stoyanov, Veselin},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing},\n  location = {Brussels, Belgium},\n  publisher = {Association for Computational Linguistics},\n  title = {XNLI: Evaluating Cross-lingual Sentence Representations},\n  year = {2018},\n}\n\n@article{lewis2019mlqa,\n  author = {Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  eid = {arXiv: 1910.07475},\n  journal = {arXiv preprint arXiv:1910.07475},\n  title = {MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  year = {2019},\n}\n\n@article{Liang2020XGLUEAN,\n  author = {Yaobo Liang and Nan Duan and Yeyun Gong and Ning Wu and Fenfei Guo and Weizhen Qi and Ming Gong and Linjun Shou and Daxin Jiang and Guihong Cao and Xiaodong Fan and Ruofei Zhang and Rahul Agrawal and Edward Cui and Sining Wei and Taroon Bharti and Ying Qiao and Jiun-Hung Chen and Winnie Wu and Shuguang Liu and Fan Yang and Daniel Campos and Rangan Majumder and Ming Zhou},\n  journal = {arXiv},\n  title = {XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation},\n  volume = {abs/2004.01401},\n  year = {2020},\n}\n\n@article{Sang2002IntroductionTT,\n  author = {Erik F. Tjong Kim Sang},\n  journal = {ArXiv},\n  title = {Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition},\n  volume = {cs.CL/0209010},\n  year = {2002},\n}\n\n@article{Sang2003IntroductionTT,\n  author = {Erik F. Tjong Kim Sang and Fien De Meulder},\n  journal = {ArXiv},\n  title = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},\n  volume = {cs.CL/0306050},\n  year = {2003},\n}\n\n@misc{yang2019pawsx,\n  archiveprefix = {arXiv},\n  author = {Yinfei Yang and Yuan Zhang and Chris Tar and Jason Baldridge},\n  eprint = {1908.11828},\n  primaryclass = {cs.CL},\n  title = {PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/","title":"Retrieval","text":"<ul> <li>Number of tasks: 431</li> </ul>"},{"location":"overview/available_tasks/retrieval/#ailacasedocs","title":"AILACasedocs","text":"<p>The task is to retrieve the case document that most closely matches or is most relevant to the scenario described in the provided query.</p> <p>Dataset: <code>mteb/AILA_casedocs</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@dataset{paheli_bhattacharya_2020_4063986,\n  author = {Paheli Bhattacharya and\nKripabandhu Ghosh and\nSaptarshi Ghosh and\nArindam Pal and\nParth Mehta and\nArnab Bhattacharya and\nPrasenjit Majumder},\n  doi = {10.5281/zenodo.4063986},\n  month = oct,\n  publisher = {Zenodo},\n  title = {AILA 2019 Precedent \\&amp; Statute Retrieval Task},\n  url = {https://doi.org/10.5281/zenodo.4063986},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ailastatutes","title":"AILAStatutes","text":"<p>This dataset is structured for the task of identifying the most relevant statutes for a given situation.</p> <p>Dataset: <code>mteb/AILA_statutes</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@dataset{paheli_bhattacharya_2020_4063986,\n  author = {Paheli Bhattacharya and\nKripabandhu Ghosh and\nSaptarshi Ghosh and\nArindam Pal and\nParth Mehta and\nArnab Bhattacharya and\nPrasenjit Majumder},\n  doi = {10.5281/zenodo.4063986},\n  month = oct,\n  publisher = {Zenodo},\n  title = {AILA 2019 Precedent \\&amp; Statute Retrieval Task},\n  url = {https://doi.org/10.5281/zenodo.4063986},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arcchallenge","title":"ARCChallenge","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on ARC-Challenge.</p> <p>Dataset: <code>mteb/ARCChallenge</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{clark2018think,\n  author = {Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},\n  journal = {arXiv preprint arXiv:1803.05457},\n  title = {Think you have solved question answering? try arc, the ai2 reasoning challenge},\n  year = {2018},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#alloprofretrieval","title":"AlloprofRetrieval","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>mteb/AlloprofRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{lef23,\n  author = {Lefebvre-Brossard, Antoine and Gazaille, Stephane and Desmarais, Michel C.},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},\n  doi = {10.48550/ARXIV.2302.07738},\n  keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  publisher = {arXiv},\n  title = {Alloprof: a new French question-answer education dataset and its use in an information retrieval case study},\n  url = {https://arxiv.org/abs/2302.07738},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#alphanli","title":"AlphaNLI","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on AlphaNLI.</p> <p>Dataset: <code>mteb/AlphaNLI</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{bhagavatula2019abductive,\n  author = {Bhagavatula, Chandra and Bras, Ronan Le and Malaviya, Chaitanya and Sakaguchi, Keisuke and Holtzman, Ari and Rashkin, Hannah and Downey, Doug and Yih, Scott Wen-tau and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1908.05739},\n  title = {Abductive commonsense reasoning},\n  year = {2019},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#appsretrieval","title":"AppsRetrieval","text":"<p>The dataset is a collection of natural language queries and their corresponding code snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/apps</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming, Written derived found Citation <pre><code>@article{hendrycksapps2021,\n  author = {Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},\n  journal = {NeurIPS},\n  title = {Measuring Coding Challenge Competence With APPS},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana","title":"ArguAna","text":"<p>ArguAna: Retrieval of the Best Counterargument without Prior Topic Knowledge</p> <p>Dataset: <code>mteb/arguana</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social, Web, Written derived found Citation <pre><code>@inproceedings{wachsmuth2018retrieval,\n  author = {Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},\n  booktitle = {ACL},\n  title = {Retrieval of the Best Counterargument without Prior Topic Knowledge},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-fa","title":"ArguAna-Fa","text":"<p>ArguAna-Fa</p> <p>Dataset: <code>MCINext/arguana-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Blog derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-fav2","title":"ArguAna-Fa.v2","text":"<p>ArguAna-Fa.v2</p> <p>Dataset: <code>MCINext/arguana-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Blog derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-nl","title":"ArguAna-NL","text":"<p>ArguAna involves the task of retrieval of the best counterargument to an argument. ArguAna-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-arguana</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-nlv2","title":"ArguAna-NL.v2","text":"<p>ArguAna involves the task of retrieval of the best counterargument to an argument. ArguAna-NL is a Dutch translation. This version adds a Dutch prompt to the dataset.</p> <p>Dataset: <code>clips/beir-nl-arguana</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-pl","title":"ArguAna-PL","text":"<p>ArguAna-PL</p> <p>Dataset: <code>mteb/ArguAna-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Medical, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#arguana-vn","title":"ArguAna-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/arguana-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#autoragretrieval","title":"AutoRAGRetrieval","text":"<p>This dataset enables the evaluation of Korean RAG performance across various domains\u2014finance, public sector, healthcare, legal, and commerce\u2014by providing publicly accessible documents, questions, and answers.</p> <p>Dataset: <code>yjoonjang/markers_bm</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor Financial, Government, Legal, Medical, Social human-annotated created Citation <pre><code>@misc{kim2024autoragautomatedframeworkoptimization,\n  archiveprefix = {arXiv},\n  author = {Dongkyu Kim and Byoungwook Kim and Donggeon Han and Matou\u0161 Eibich},\n  eprint = {2410.20878},\n  primaryclass = {cs.CL},\n  title = {AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline},\n  url = {https://arxiv.org/abs/2410.20878},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-arguana","title":"BIRCO-ArguAna","text":"<p>Retrieval task using the ArguAna dataset from BIRCO. This dataset contains 100 queries where both queries and passages are complex one-paragraph arguments about current affairs. The objective is to retrieve the counter-argument that directly refutes the query\u2019s stance.</p> <p>Dataset: <code>mteb/BIRCO-ArguAna-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Written expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-clinicaltrial","title":"BIRCO-ClinicalTrial","text":"<p>Retrieval task using the Clinical-Trial dataset from BIRCO. This dataset contains 50 queries that are patient case reports. Each query has a candidate pool comprising 30-110 clinical trial descriptions. Relevance is graded (0, 1, 2), where 1 and 2 are considered relevant.</p> <p>Dataset: <code>mteb/BIRCO-ClinicalTrial-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-dorismae","title":"BIRCO-DorisMae","text":"<p>Retrieval task using the DORIS-MAE dataset from BIRCO. This dataset contains 60 queries that are complex research questions from computer scientists. Each query has a candidate pool of approximately 110 abstracts. Relevance is graded from 0 to 2 (scores of 1 and 2 are considered relevant).</p> <p>Dataset: <code>mteb/BIRCO-DorisMae-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-relic","title":"BIRCO-Relic","text":"<p>Retrieval task using the RELIC dataset from BIRCO. This dataset contains 100 queries which are excerpts from literary analyses with a missing quotation (indicated by [masked sentence(s)]). Each query has a candidate pool of 50 passages. The objective is to retrieve the passage that best completes the literary analysis.</p> <p>Dataset: <code>mteb/BIRCO-Relic-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#birco-wtb","title":"BIRCO-WTB","text":"<p>Retrieval task using the WhatsThatBook dataset from BIRCO. This dataset contains 100 queries where each query is an ambiguous description of a book. Each query has a candidate pool of 50 book descriptions. The objective is to retrieve the correct book description.</p> <p>Dataset: <code>mteb/BIRCO-WTB-Test</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction expert-annotated found Citation <pre><code>@misc{wang2024bircobenchmarkinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Xiaoyue Wang and Jianyou Wang and Weili Cao and Kaicheng Wang and Ramamohan Paturi and Leon Bergen},\n  eprint = {2402.14151},\n  primaryclass = {cs.IR},\n  title = {BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives},\n  url = {https://arxiv.org/abs/2402.14151},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#bsardretrieval","title":"BSARDRetrieval","text":"<p>The Belgian Statutory Article Retrieval Dataset (BSARD) is a French native dataset for studying legal information retrieval. BSARD consists of more than 22,600 statutory articles from Belgian law and about 1,100 legal questions posed by Belgian citizens and labeled by experienced jurists with relevant articles from the corpus.</p> <p>Dataset: <code>mteb/BSARDRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_100 fra Legal, Spoken expert-annotated found Citation <pre><code>@inproceedings{louis2022statutory,\n  address = {Dublin, Ireland},\n  author = {Louis, Antoine and Spanakis, Gerasimos},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2022.acl-long.468},\n  month = may,\n  pages = {6789\u20136803},\n  publisher = {Association for Computational Linguistics},\n  title = {A Statutory Article Retrieval Dataset in French},\n  url = {https://aclanthology.org/2022.acl-long.468/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#bsardretrievalv2","title":"BSARDRetrieval.v2","text":"<p>BSARD is a French native dataset for legal information retrieval. BSARDRetrieval.v2 covers multi-article queries, fixing issues (#2906) with the previous data loading. </p> <p>Dataset: <code>mteb/BSARDRetrieval.v2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_100 fra Legal, Spoken expert-annotated found Citation <pre><code>@inproceedings{louis2022statutory,\n  address = {Dublin, Ireland},\n  author = {Louis, Antoine and Spanakis, Gerasimos},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},\n  doi = {10.18653/v1/2022.acl-long.468},\n  month = may,\n  pages = {6789\u20136803},\n  publisher = {Association for Computational Linguistics},\n  title = {A Statutory Article Retrieval Dataset in French},\n  url = {https://aclanthology.org/2022.acl-long.468/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#barexamqa","title":"BarExamQA","text":"<p>A benchmark for retrieving legal provisions that answer US bar exam questions.</p> <p>Dataset: <code>isaacus/mteb-barexam-qa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Legal expert-annotated found Citation <pre><code>@inproceedings{Zheng_2025,\n  author = {Zheng, Lucia and Guha, Neel and Arifov, Javokhir and Zhang, Sarah and Skreta, Michal and Manning, Christopher D. and Henderson, Peter and Ho, Daniel E.},\n  booktitle = {Proceedings of the Symposium on Computer Science and Law on ZZZ},\n  collection = {CSLAW \u201925},\n  doi = {10.1145/3709025.3712219},\n  eprint = {2505.03970},\n  month = mar,\n  pages = {169\u2013193},\n  publisher = {ACM},\n  series = {CSLAW \u201925},\n  title = {A Reasoning-Focused Legal Retrieval Benchmark},\n  url = {http://dx.doi.org/10.1145/3709025.3712219},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#belebeleretrieval","title":"BelebeleRetrieval","text":"<p>Belebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants (including 115 distinct languages and their scripts)</p> <p>Dataset: <code>mteb/belebele</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 acm, afr, als, amh, apc, ... (115) News, Web, Written expert-annotated created Citation <pre><code>@article{bandarkar2023belebele,\n  author = {Lucas Bandarkar and Davis Liang and Benjamin Muller and Mikel Artetxe and Satya Narayan Shukla and Donald Husa and Naman Goyal and Abhinandan Krishnan and Luke Zettlemoyer and Madian Khabsa},\n  journal = {arXiv preprint arXiv:2308.16884},\n  title = {The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#billsumca","title":"BillSumCA","text":"<p>A benchmark for retrieving Californian bills based on their summaries.</p> <p>Dataset: <code>isaacus/mteb-BillSumCA</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{Eidelman_2019,\n  author = {Eidelman, Vladimir},\n  booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},\n  doi = {10.18653/v1/d19-5406},\n  pages = {48\u201356},\n  publisher = {Association for Computational Linguistics},\n  title = {BillSum: A Corpus for Automatic Summarization of US Legislation},\n  url = {http://dx.doi.org/10.18653/v1/D19-5406},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#billsumus","title":"BillSumUS","text":"<p>A benchmark for retrieving US federal bills based on their summaries.</p> <p>Dataset: <code>isaacus/mteb-BillSumUS</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{Eidelman_2019,\n  author = {Eidelman, Vladimir},\n  booktitle = {Proceedings of the 2nd Workshop on New Frontiers in Summarization},\n  doi = {10.18653/v1/d19-5406},\n  pages = {48\u201356},\n  publisher = {Association for Computational Linguistics},\n  title = {BillSum: A Corpus for Automatic Summarization of US Legislation},\n  url = {http://dx.doi.org/10.18653/v1/D19-5406},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightaopsretrieval","title":"BrightAopsRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of similar Math Olympiad problems from Art of Problem Solving.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightbiologylongretrieval","title":"BrightBiologyLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Biology StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightbiologyretrieval","title":"BrightBiologyRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Biology StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightearthsciencelongretrieval","title":"BrightEarthScienceLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Earth Science StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightearthscienceretrieval","title":"BrightEarthScienceRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Earth Science StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brighteconomicslongretrieval","title":"BrightEconomicsLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Economics StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brighteconomicsretrieval","title":"BrightEconomicsRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Economics StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightleetcoderetrieval","title":"BrightLeetcodeRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of similar algorithmic problems based on shared solution techniques.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightlongretrieval","title":"BrightLongRetrieval","text":"<p>Bright retrieval dataset with long documents.</p> <p>Dataset: <code>xlangai/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightponylongretrieval","title":"BrightPonyLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of Pony programming language syntax documentation with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightponyretrieval","title":"BrightPonyRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of Pony programming language syntax documentation.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightpsychologylongretrieval","title":"BrightPsychologyLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Psychology StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightpsychologyretrieval","title":"BrightPsychologyRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Psychology StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightretrieval","title":"BrightRetrieval","text":"<p>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</p> <p>Dataset: <code>xlangai/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightroboticslongretrieval","title":"BrightRoboticsLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Robotics StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightroboticsretrieval","title":"BrightRoboticsRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Robotics StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightstackoverflowlongretrieval","title":"BrightStackoverflowLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Stack Overflow answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightstackoverflowretrieval","title":"BrightStackoverflowRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Stack Overflow answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightsustainablelivinglongretrieval","title":"BrightSustainableLivingLongRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Sustainable Living StackExchange answers with long documents.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brightsustainablelivingretrieval","title":"BrightSustainableLivingRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of web documents cited in Sustainable Living StackExchange answers.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brighttheoremqaquestionsretrieval","title":"BrightTheoremQAQuestionsRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of theorem definitions from ProofWiki given questions rephrased as real-world scenarios.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#brighttheoremqatheoremsretrieval","title":"BrightTheoremQATheoremsRetrieval","text":"<p>Part of the BRIGHT benchmark for reasoning-intensive retrieval. Retrieval of theorem definitions and proofs from ProofWiki.</p> <p>Dataset: <code>mteb/BRIGHT</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@article{su2024bright,\n  author = {Su, Hongjin and Yen, Howard and Xia, Mengzhou and Shi, Weijia and Muennighoff, Niklas and Wang, Han-yu and Liu, Haisu and Shi, Quan and Siegel, Zachary S and Tang, Michael and others},\n  journal = {arXiv preprint arXiv:2407.12883},\n  title = {Bright: A realistic and challenging benchmark for reasoning-intensive retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#builtbenchretrieval","title":"BuiltBenchRetrieval","text":"<p>Retrieval of built asset entity type/class descriptions given a query describing an entity as represented in well-established industry classification systems such as Uniclass, IFC, etc.</p> <p>Dataset: <code>mteb/BuiltBenchRetrieval</code> \u2022 License: cc-by-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Engineering, Written derived created Citation <pre><code>@article{shahinmoghadam2024benchmarking,\n  author = {Shahinmoghadam, Mehrzad and Motamedi, Ali},\n  journal = {arXiv preprint arXiv:2411.12056},\n  title = {Benchmarking pre-trained text embedding models in aligning built asset information},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#coircodesearchnetretrieval","title":"COIRCodeSearchNetRetrieval","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code summary given a code snippet.</p> <p>Dataset: <code>CoIR-Retrieval/CodeSearchNet</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-android-pl","title":"CQADupstack-Android-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Android-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-english-pl","title":"CQADupstack-English-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-English-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-gaming-pl","title":"CQADupstack-Gaming-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Gaming-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-gis-pl","title":"CQADupstack-Gis-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Gis-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-mathematica-pl","title":"CQADupstack-Mathematica-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Mathematica-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-nl","title":"CQADupstack-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages CQADupstackAndroid-NL Retrieval text nld CQADupstackEnglish-NL Retrieval text nld CQADupstackGaming-NL Retrieval text nld CQADupstackGis-NL Retrieval text nld CQADupstackMathematica-NL Retrieval text nld CQADupstackPhysics-NL Retrieval text nld CQADupstackProgrammers-NL Retrieval text nld CQADupstackStats-NL Retrieval text nld CQADupstackTex-NL Retrieval text nld CQADupstackUnix-NL Retrieval text nld CQADupstackWebmasters-NL Retrieval text nld CQADupstackWordpress-NL Retrieval text nld"},{"location":"overview/available_tasks/retrieval/#cqadupstack-physics-pl","title":"CQADupstack-Physics-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Physics-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-programmers-pl","title":"CQADupstack-Programmers-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Programmers-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Programming, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-stats-pl","title":"CQADupstack-Stats-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Stats-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-tex-pl","title":"CQADupstack-Tex-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Tex-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-unix-pl","title":"CQADupstack-Unix-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Unix-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-webmasters-pl","title":"CQADupstack-Webmasters-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Webmasters-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstack-wordpress-pl","title":"CQADupstack-Wordpress-PL","text":"<p>CQADupStack: A Stack Exchange Question Duplicate Pairs Dataset</p> <p>Dataset: <code>mteb/CQADupstack-Wordpress-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroid-nl","title":"CQADupstackAndroid-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroid-vn","title":"CQADupstackAndroid-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-android-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroidretrieval","title":"CQADupstackAndroidRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/CQADupstackAndroidRetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackandroidretrieval-fa","title":"CQADupstackAndroidRetrieval-Fa","text":"<p>CQADupstackAndroidRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-android-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglish-nl","title":"CQADupstackEnglish-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglishretrieval","title":"CQADupstackEnglishRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-english</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackenglishretrieval-fa","title":"CQADupstackEnglishRetrieval-Fa","text":"<p>CQADupstackEnglishRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-english-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgaming-nl","title":"CQADupstackGaming-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgamingretrieval","title":"CQADupstackGamingRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-gaming</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgamingretrieval-fa","title":"CQADupstackGamingRetrieval-Fa","text":"<p>CQADupstackGamingRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-gaming-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgis-nl","title":"CQADupstackGis-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgis-vn","title":"CQADupstackGis-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-gis-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgisretrieval","title":"CQADupstackGisRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-gis</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackgisretrieval-fa","title":"CQADupstackGisRetrieval-Fa","text":"<p>CQADupstackGisRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-gis-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematica-nl","title":"CQADupstackMathematica-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematica-vn","title":"CQADupstackMathematica-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-mathematica-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematicaretrieval","title":"CQADupstackMathematicaRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-mathematica</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackmathematicaretrieval-fa","title":"CQADupstackMathematicaRetrieval-Fa","text":"<p>CQADupstackMathematicaRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-mathematica-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysics-nl","title":"CQADupstackPhysics-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysics-vn","title":"CQADupstackPhysics-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-physics-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysicsretrieval","title":"CQADupstackPhysicsRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-physics</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackphysicsretrieval-fa","title":"CQADupstackPhysicsRetrieval-Fa","text":"<p>CQADupstackPhysicsRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-physics-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammers-nl","title":"CQADupstackProgrammers-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammers-vn","title":"CQADupstackProgrammers-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-programmers-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Programming, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammersretrieval","title":"CQADupstackProgrammersRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-programmers</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Programming, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackprogrammersretrieval-fa","title":"CQADupstackProgrammersRetrieval-Fa","text":"<p>CQADupstackProgrammersRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-programmers-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackretrieval","title":"CQADupstackRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre> Tasks name type modalities languages CQADupstackAndroidRetrieval Retrieval text eng CQADupstackEnglishRetrieval Retrieval text eng CQADupstackGamingRetrieval Retrieval text eng CQADupstackGisRetrieval Retrieval text eng CQADupstackMathematicaRetrieval Retrieval text eng CQADupstackPhysicsRetrieval Retrieval text eng CQADupstackProgrammersRetrieval Retrieval text eng CQADupstackStatsRetrieval Retrieval text eng CQADupstackTexRetrieval Retrieval text eng CQADupstackUnixRetrieval Retrieval text eng CQADupstackWebmastersRetrieval Retrieval text eng CQADupstackWordpressRetrieval Retrieval text eng"},{"location":"overview/available_tasks/retrieval/#cqadupstackretrieval-fa","title":"CQADupstackRetrieval-Fa","text":"<p>CQADupstackRetrieval-Fa</p> <p>License: not specified \u2022 Learn more \u2192 not specified</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation Tasks name type modalities languages CQADupstackAndroidRetrieval-Fa Retrieval text fas CQADupstackEnglishRetrieval-Fa Retrieval text fas CQADupstackGamingRetrieval-Fa Retrieval text fas CQADupstackGisRetrieval-Fa Retrieval text fas CQADupstackMathematicaRetrieval-Fa Retrieval text fas CQADupstackPhysicsRetrieval-Fa Retrieval text fas CQADupstackProgrammersRetrieval-Fa Retrieval text fas CQADupstackStatsRetrieval-Fa Retrieval text fas CQADupstackTexRetrieval-Fa Retrieval text fas CQADupstackUnixRetrieval-Fa Retrieval text fas CQADupstackWebmastersRetrieval-Fa Retrieval text fas CQADupstackWordpressRetrieval-Fa Retrieval text fas"},{"location":"overview/available_tasks/retrieval/#cqadupstackretrieval-pl","title":"CQADupstackRetrieval-PL","text":"<p>CQADupstackRetrieval-PL</p> <p>License: not specified \u2022 Learn more \u2192 not specified</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Non-fiction, Programming, Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages CQADupstack-Android-PL Retrieval text pol CQADupstack-English-PL Retrieval text pol CQADupstack-Gaming-PL Retrieval text pol CQADupstack-Gis-PL Retrieval text pol CQADupstack-Mathematica-PL Retrieval text pol CQADupstack-Physics-PL Retrieval text pol CQADupstack-Programmers-PL Retrieval text pol CQADupstack-Stats-PL Retrieval text pol CQADupstack-Tex-PL Retrieval text pol CQADupstack-Unix-PL Retrieval text pol CQADupstack-Webmasters-PL Retrieval text pol CQADupstack-Wordpress-PL Retrieval text pol"},{"location":"overview/available_tasks/retrieval/#cqadupstackstats-nl","title":"CQADupstackStats-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstats-vn","title":"CQADupstackStats-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-stats-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstatsretrieval","title":"CQADupstackStatsRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-stats</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackstatsretrieval-fa","title":"CQADupstackStatsRetrieval-Fa","text":"<p>CQADupstackStatsRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-stats-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktex-nl","title":"CQADupstackTex-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktex-vn","title":"CQADupstackTex-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-tex-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktexretrieval","title":"CQADupstackTexRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-tex</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstacktexretrieval-fa","title":"CQADupstackTexRetrieval-Fa","text":"<p>CQADupstackTexRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-tex-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunix-nl","title":"CQADupstackUnix-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunix-vn","title":"CQADupstackUnix-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-unix-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunixretrieval","title":"CQADupstackUnixRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-unix</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackunixretrieval-fa","title":"CQADupstackUnixRetrieval-Fa","text":"<p>CQADupstackUnixRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-unix-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmasters-nl","title":"CQADupstackWebmasters-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmasters-vn","title":"CQADupstackWebmasters-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-webmasters-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmastersretrieval","title":"CQADupstackWebmastersRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-webmasters</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwebmastersretrieval-fa","title":"CQADupstackWebmastersRetrieval-Fa","text":"<p>CQADupstackWebmastersRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-webmasters-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpress-nl","title":"CQADupstackWordpress-NL","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research. This a Dutch-translated version.</p> <p>Dataset: <code>clips/beir-nl-cqadupstack</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpress-vn","title":"CQADupstackWordpress-VN","text":"<p>A translated dataset from CQADupStack: A Benchmark Data Set for Community Question-Answering Research The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/cqadupstack-wordpress-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Programming, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpressretrieval","title":"CQADupstackWordpressRetrieval","text":"<p>CQADupStack: A Benchmark Data Set for Community Question-Answering Research</p> <p>Dataset: <code>mteb/cqadupstack-wordpress</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Web, Written derived found Citation <pre><code>@inproceedings{hoogeveen2015,\n  acmid = {2838934},\n  address = {New York, NY, USA},\n  articleno = {3},\n  author = {Hoogeveen, Doris and Verspoor, Karin M. and Baldwin, Timothy},\n  booktitle = {Proceedings of the 20th Australasian Document Computing Symposium (ADCS)},\n  doi = {10.1145/2838931.2838934},\n  isbn = {978-1-4503-4040-3},\n  location = {Parramatta, NSW, Australia},\n  numpages = {8},\n  pages = {3:1--3:8},\n  publisher = {ACM},\n  series = {ADCS '15},\n  title = {CQADupStack: A Benchmark Data Set for Community Question-Answering Research},\n  url = {http://doi.acm.org/10.1145/2838931.2838934},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cqadupstackwordpressretrieval-fa","title":"CQADupstackWordpressRetrieval-Fa","text":"<p>CQADupstackWordpressRetrieval-Fa</p> <p>Dataset: <code>MCINext/cqadupstack-wordpress-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#curev1","title":"CUREv1","text":"<p>Collection of query-passage pairs curated by medical professionals, across 10 disciplines and 3 cross-lingual settings.</p> <p>Dataset: <code>clinia/CUREv1</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, fra, spa Academic, Medical, Written expert-annotated created"},{"location":"overview/available_tasks/retrieval/#chatdoctorretrieval","title":"ChatDoctorRetrieval","text":"<p>A medical retrieval task based on ChatDoctor_HealthCareMagic dataset containing 112,000 real-world medical question-and-answer pairs. Each query is a medical question from patients (e.g., 'What are the symptoms of diabetes?'), and the corpus contains medical responses and healthcare information. The task is to retrieve the correct medical information that answers the patient's question. The dataset includes grammatical inconsistencies which help separate strong healthcare retrieval models from weak ones. Queries are patient medical questions while the corpus contains relevant medical responses, diagnoses, and treatment information from healthcare professionals.</p> <p>Dataset: <code>embedding-benchmark/ChatDoctor_HealthCareMagic</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical expert-annotated found Citation <pre><code>@misc{chatdoctor_healthcaremagic,\n  title = {ChatDoctor HealthCareMagic: Medical Question-Answer Retrieval Dataset},\n  url = {https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#chemhotpotqaretrieval","title":"ChemHotpotQARetrieval","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/ChemHotpotQARetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Chemistry derived found Citation <pre><code>@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n\n@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#chemnqretrieval","title":"ChemNQRetrieval","text":"<p>ChemTEB evaluates the performance of text embedding models on chemical domain data.</p> <p>Dataset: <code>BASF-AI/ChemNQRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Chemistry derived found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational Linguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n\n@article{kasmaee2024chemteb,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Saloot, Mohammad Arshi and Sherck, Nick and Dokas, Stephen and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2412.00532},\n  title = {ChemTEB: Chemical Text Embedding Benchmark, an Overview of Embedding Models Performance \\&amp; Efficiency on a Specific Domain},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#chemrxivretrieval","title":"ChemRxivRetrieval","text":"<p>A retrieval task based on ChemRxiv papers where queries are LLM-synthesized to match specific paragraphs.</p> <p>Dataset: <code>BASF-AI/ChemRxivRetrieval</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Chemistry LM-generated and reviewed found Citation <pre><code>@article{kasmaee2025chembed,\n  author = {Kasmaee, Ali Shiraee and Khodadad, Mohammad and Astaraki, Mahdi and Saloot, Mohammad Arshi and Sherck, Nicholas and Mahyar, Hamidreza and Samiee, Soheila},\n  journal = {arXiv preprint arXiv:2508.01643},\n  title = {Chembed: Enhancing chemical literature search through domain-specific text embeddings},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever","title":"ClimateFEVER","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims (queries) regarding climate-change. The underlying corpus is the same as FEVER.</p> <p>Dataset: <code>mteb/climate-fever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-fa","title":"ClimateFEVER-Fa","text":"<p>ClimateFEVER-Fa</p> <p>Dataset: <code>MCINext/climate-fever-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-nl","title":"ClimateFEVER-NL","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. ClimateFEVER-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-climate-fever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefever-vn","title":"ClimateFEVER-VN","text":"<p>A translated dataset from CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/climate-fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverv2","title":"ClimateFEVER.v2","text":"<p>CLIMATE-FEVER is a dataset following the FEVER methodology, containing 1,535 real-world climate change claims. This updated version addresses corpus mismatches and qrel inconsistencies in MTEB, restoring labels while refining corpus-query alignment for better accuracy.</p> <p>Dataset: <code>mteb/climate-fever-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverhardnegatives","title":"ClimateFEVERHardNegatives","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/ClimateFEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#climatefeverhardnegativesv2","title":"ClimateFEVERHardNegatives.v2","text":"<p>CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/ClimateFEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cmedqaretrieval","title":"CmedqaRetrieval","text":"<p>Online medical consultation text. Used the CMedQAv2 as its underlying dataset.</p> <p>Dataset: <code>mteb/CmedqaRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Medical, Written human-annotated found Citation <pre><code>@misc{qiu2022dureaderretrievallargescalechinesebenchmark,\n  archiveprefix = {arXiv},\n  author = {Yifu Qiu and Hongyu Li and Yingqi Qu and Ying Chen and Qiaoqiao She and Jing Liu and Hua Wu and Haifeng Wang},\n  eprint = {2203.10232},\n  primaryclass = {cs.CL},\n  title = {DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine},\n  url = {https://arxiv.org/abs/2203.10232},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#code1retrieval","title":"Code1Retrieval","text":"<p>Code retrieval dataset with programming questions paired with C/Python/Go/Ruby code snippets for multi-language code retrieval evaluation.</p> <p>Dataset: <code>mteb-private/Code1Retrieval</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found"},{"location":"overview/available_tasks/retrieval/#codeeditsearchretrieval","title":"CodeEditSearchRetrieval","text":"<p>The dataset is a collection of unified diffs of code changes, paired with a short instruction that describes the change. The dataset is derived from the CommitPackFT dataset.</p> <p>Dataset: <code>cassanof/CodeEditSearch</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 c, c++, go, java, javascript, ... (13) Programming, Written derived found Citation <pre><code>@article{muennighoff2023octopack,\n  author = {Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},\n  journal = {arXiv preprint arXiv:2308.07124},\n  title = {OctoPack: Instruction Tuning Code Large Language Models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codefeedbackmt","title":"CodeFeedbackMT","text":"<p>The dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/codefeedback-mt</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{zheng2024opencodeinterpreterintegratingcodegeneration,\n  archiveprefix = {arXiv},\n  author = {Tianyu Zheng and Ge Zhang and Tianhao Shen and Xueling Liu and Bill Yuchen Lin and Jie Fu and Wenhu Chen and Xiang Yue},\n  eprint = {2402.14658},\n  primaryclass = {cs.SE},\n  title = {OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement},\n  url = {https://arxiv.org/abs/2402.14658},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codefeedbackst","title":"CodeFeedbackST","text":"<p>The dataset is a collection of user queries and assistant responses. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/codefeedback-st</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codesearchnetccretrieval","title":"CodeSearchNetCCRetrieval","text":"<p>The dataset is a collection of code snippets. The task is to retrieve the most relevant code snippet for a given code snippet.</p> <p>Dataset: <code>CoIR-Retrieval/CodeSearchNet-ccr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codesearchnetretrieval","title":"CodeSearchNetRetrieval","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>mteb/CodeSearchNetRetrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 go, java, javascript, php, python, ... (6) Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codetransoceancontest","title":"CodeTransOceanContest","text":"<p>The dataset is a collection of code snippets and their corresponding natural language queries. The task is to retrieve the most relevant code snippet</p> <p>Dataset: <code>CoIR-Retrieval/codetrans-contest</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 c++, python Programming, Written derived found Citation <pre><code>@misc{yan2023codetransoceancomprehensivemultilingualbenchmark,\n  archiveprefix = {arXiv},\n  author = {Weixiang Yan and Yuchen Tian and Yunzhe Li and Qian Chen and Wen Wang},\n  eprint = {2310.04951},\n  primaryclass = {cs.AI},\n  title = {CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation},\n  url = {https://arxiv.org/abs/2310.04951},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#codetransoceandl","title":"CodeTransOceanDL","text":"<p>The dataset is a collection of equivalent Python Deep Learning code snippets written in different machine learning framework. The task is to retrieve the equivalent code snippet in another framework, given a query code snippet from one framework.</p> <p>Dataset: <code>CoIR-Retrieval/codetrans-dl</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 python Programming, Written derived found Citation <pre><code>@misc{yan2023codetransoceancomprehensivemultilingualbenchmark,\n  archiveprefix = {arXiv},\n  author = {Weixiang Yan and Yuchen Tian and Yunzhe Li and Qian Chen and Wen Wang},\n  eprint = {2310.04951},\n  primaryclass = {cs.AI},\n  title = {CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation},\n  url = {https://arxiv.org/abs/2310.04951},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#cosqa","title":"CosQA","text":"<p>The dataset is a collection of natural language queries and their corresponding code snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/cosqa</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming, Written derived found Citation <pre><code>@misc{huang2021cosqa20000webqueries,\n  archiveprefix = {arXiv},\n  author = {Junjie Huang and Duyu Tang and Linjun Shou and Ming Gong and Ke Xu and Daxin Jiang and Ming Zhou and Nan Duan},\n  eprint = {2105.13239},\n  primaryclass = {cs.CL},\n  title = {CoSQA: 20,000+ Web Queries for Code Search and Question Answering},\n  url = {https://arxiv.org/abs/2105.13239},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#covidretrieval","title":"CovidRetrieval","text":"<p>COVID-19 news articles</p> <p>Dataset: <code>mteb/CovidRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Entertainment, Medical human-annotated found Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#crosslingualsemanticdiscriminationwmt19","title":"CrossLingualSemanticDiscriminationWMT19","text":"<p>Evaluate a multilingual embedding model based on its ability to discriminate against the original parallel pair against challenging distractors - spawning from WMT19 DE-FR test set</p> <p>Dataset: <code>Andrianos/clsd_wmt19_21</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 deu, fra News, Written derived LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#crosslingualsemanticdiscriminationwmt21","title":"CrossLingualSemanticDiscriminationWMT21","text":"<p>Evaluate a multilingual embedding model based on its ability to discriminate against the original parallel pair against challenging distractors - spawning from WMT21 DE-FR test set</p> <p>Dataset: <code>Andrianos/clsd_wmt19_21</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_1 deu, fra News, Written derived LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtofulltextretrieval","title":"DAPFAMAllTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtotitlabsclmretrieval","title":"DAPFAMAllTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabsclmtotitlabsretrieval","title":"DAPFAMAllTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstofulltextretrieval","title":"DAPFAMAllTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstotitlabsclmretrieval","title":"DAPFAMAllTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamalltitlabstotitlabsretrieval","title":"DAPFAMAllTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, no International Patent Classification-based filtering is applied. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract across all technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtofulltextretrieval","title":"DAPFAMInTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtotitlabsclmretrieval","title":"DAPFAMInTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabsclmtotitlabsretrieval","title":"DAPFAMInTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstofulltextretrieval","title":"DAPFAMInTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstotitlabsclmretrieval","title":"DAPFAMInTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamintitlabstotitlabsretrieval","title":"DAPFAMInTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing at least one three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract within the same technical domain.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtofulltextretrieval","title":"DAPFAMOutTitlAbsClmToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Claims-augmented query patent family representations full-text target patent family representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtotitlabsclmretrieval","title":"DAPFAMOutTitlAbsClmToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval when both query and target patent families use Claims-augmented representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabsclmtotitlabsretrieval","title":"DAPFAMOutTitlAbsClmToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title, Abstract, and Claims, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to measure the effect of Claims-augmented query patent family representations when targets are limited to Title and Abstract across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstofulltextretrieval","title":"DAPFAMOutTitlAbsToFullTextRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, Claims, and Description. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to evaluate retrieval performance using Title and Abstract query patent family representations and full-text target patent family representations across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstotitlabsclmretrieval","title":"DAPFAMOutTitlAbsToTitlAbsClmRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title, Abstract, and Claims. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to assess how adding Claims text to target patent family representations improves retrieval of citation-linked patent families across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dapfamouttitlabstotitlabsretrieval","title":"DAPFAMOutTitlAbsToTitlAbsRetrieval","text":"<p>In this patent family retrieval task, query patent families are represented by Title and Abstract, and target patent families are represented by Title and Abstract. Relevant target families have a citation link (cited or citing) with the query family. Additionally, only targets sharing no three-character International Patent Classification code with the query family. Relevance and labelling scheme are described in detail in Section 3.4 and 3.5 of Ayaou et al. (2025), arXiv:2506.22141.Patents are aggregated and represented at the family level to reduce redundancy across jurisdictions. The goal of the task is to retrieve citation-linked patent families using query and target patent family representations of Title and Abstract across different technical domains.</p> <p>Dataset: <code>datalyes/DAPFAM_patent</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_100 eng Chemistry, Engineering, Legal derived created Citation <pre><code>@misc{ayaou2025dapfamdomainawarefamilyleveldataset,\n  archiveprefix = {arXiv},\n  author = {Iliass Ayaou and Denis Cavallucci and Hicham Chibane},\n  eprint = {2506.22141},\n  primaryclass = {cs.CL},\n  title = {DAPFAM: A Domain-Aware Family-level Dataset to benchmark cross domain patent retrieval},\n  url = {https://arxiv.org/abs/2506.22141},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia","title":"DBPedia","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base</p> <p>Dataset: <code>mteb/dbpedia</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-fa","title":"DBPedia-Fa","text":"<p>DBPedia-Fa</p> <p>Dataset: <code>MCINext/dbpedia-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-nl","title":"DBPedia-NL","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. DBPedia-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-dbpedia-entity</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-pl","title":"DBPedia-PL","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base</p> <p>Dataset: <code>mteb/DBPedia-PL</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written derived machine-translated Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-plhardnegatives","title":"DBPedia-PLHardNegatives","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/DBPedia-PLHardNegatives</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written derived machine-translated Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpedia-vn","title":"DBPedia-VN","text":"<p>A translated dataset from DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/dbpedia-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpediahardnegatives","title":"DBPediaHardNegatives","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/DBPedia_test_top_250_only_w_correct-v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dbpediahardnegativesv2","title":"DBPediaHardNegatives.v2","text":"<p>DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/DBPedia_test_top_250_only_w_correct-v2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{Hasibi:2017:DVT,\n  author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},\n  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  doi = {10.1145/3077136.3080751},\n  pages = {1265--1268},\n  publisher = {ACM},\n  series = {SIGIR '17},\n  title = {DBpedia-Entity V2: A Test Collection for Entity Search},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ds1000retrieval","title":"DS1000Retrieval","text":"<p>A code retrieval task based on 1,000 data science programming problems from DS-1000. Each query is a natural language description of a data science task (e.g., 'Create a scatter plot of column A vs column B with matplotlib'), and the corpus contains Python code implementations using libraries like pandas, numpy, matplotlib, scikit-learn, and scipy. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations focused on data science workflows.</p> <p>Dataset: <code>embedding-benchmark/DS1000</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming expert-annotated found Citation <pre><code>@article{lai2022ds,\n  author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},\n  journal = {arXiv preprint arXiv:2211.11501},\n  title = {DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#danfever","title":"DanFEVER","text":"<p>A Danish dataset intended for misinformation research. It follows the same format as the English FEVER dataset.</p> <p>Dataset: <code>mteb/DanFEVER</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Encyclopaedic, Non-fiction, Spoken human-annotated found Citation <pre><code>@inproceedings{norregaard-derczynski-2021-danfever,\n  address = {Reykjavik, Iceland (Online)},\n  author = {N{\\o}rregaard, Jeppe  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {422--428},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{D}an{FEVER}: claim verification dataset for {D}anish},\n  url = {https://aclanthology.org/2021.nodalida-main.47},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#danfeverretrieval","title":"DanFeverRetrieval","text":"<p>A Danish dataset intended for misinformation research. It follows the same format as the English FEVER dataset. DanFeverRetrieval fixed an issue in DanFever where some corpus entries were incorrectly removed.</p> <p>Dataset: <code>strombergnlp/danfever</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Encyclopaedic, Non-fiction, Spoken human-annotated found Citation <pre><code>@inproceedings{norregaard-derczynski-2021-danfever,\n  address = {Reykjavik, Iceland (Online)},\n  author = {N{\\o}rregaard, Jeppe  and\nDerczynski, Leon},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {422--428},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{D}an{FEVER}: claim verification dataset for {D}anish},\n  url = {https://aclanthology.org/2021.nodalida-main.47},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#duretrieval","title":"DuRetrieval","text":"<p>A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine</p> <p>Dataset: <code>mteb/DuRetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Web, Written human-annotated found Citation <pre><code>@misc{qiu2022dureaderretrievallargescalechinesebenchmark,\n  archiveprefix = {arXiv},\n  author = {Yifu Qiu and Hongyu Li and Yingqi Qu and Ying Chen and Qiaoqiao She and Jing Liu and Hua Wu and Haifeng Wang},\n  eprint = {2203.10232},\n  primaryclass = {cs.CL},\n  title = {DuReader_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine},\n  url = {https://arxiv.org/abs/2203.10232},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#dutchnewsarticlesretrieval","title":"DutchNewsArticlesRetrieval","text":"<p>This dataset contains all the articles published by the NOS as of the 1st of January 2010. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands.</p> <p>Dataset: <code>clips/mteb-nl-news-articles-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld News, Written derived found"},{"location":"overview/available_tasks/retrieval/#ecomretrieval","title":"EcomRetrieval","text":"<p>EcomRetrieval</p> <p>Dataset: <code>mteb/EcomRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Reviews, Written human-annotated found Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#englishfinance1retrieval","title":"EnglishFinance1Retrieval","text":"<p>Financial document retrieval dataset with queries about stock compensation, corporate governance, and SEC filing content.</p> <p>Dataset: <code>mteb-private/EnglishFinance1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found"},{"location":"overview/available_tasks/retrieval/#englishfinance2retrieval","title":"EnglishFinance2Retrieval","text":"<p>Financial performance retrieval dataset with queries about stock performance, S&amp;P 500 comparisons, and railroad industry metrics.</p> <p>Dataset: <code>mteb-private/EnglishFinance2Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found"},{"location":"overview/available_tasks/retrieval/#englishfinance3retrieval","title":"EnglishFinance3Retrieval","text":"<p>Personal finance Q&amp;A retrieval dataset with questions about tax codes, business expenses, and financial advice.</p> <p>Dataset: <code>mteb-private/EnglishFinance3Retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found"},{"location":"overview/available_tasks/retrieval/#englishfinance4retrieval","title":"EnglishFinance4Retrieval","text":"<p>Personal finance advice retrieval dataset with questions about car financing, investment strategies, and financial planning.</p> <p>Dataset: <code>mteb-private/EnglishFinance4Retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Non-fiction, Written derived found"},{"location":"overview/available_tasks/retrieval/#englishhealthcare1retrieval","title":"EnglishHealthcare1Retrieval","text":"<p>Medical research retrieval dataset with queries about HIV transmission, genetic variants, and biomedical research findings.</p> <p>Dataset: <code>mteb-private/EnglishHealthcare1Retrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written derived found"},{"location":"overview/available_tasks/retrieval/#estqa","title":"EstQA","text":"<p>EstQA is an Estonian question answering dataset based on Wikipedia.</p> <p>Dataset: <code>kardosdrur/estonian-qa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 est Encyclopaedic, Written human-annotated found Citation <pre><code>@mastersthesis{mastersthesis,\n  author = {Anu K\u00e4ver},\n  school = {Tallinn University of Technology (TalTech)},\n  title = {Extractive Question Answering for Estonian Language},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#europirqretrieval","title":"EuroPIRQRetrieval","text":"<p>The EuroPIRQ retrieval dataset is a multilingual collection designed for evaluating retrieval and cross-lingual retrieval tasks. Dataset contains 10,000 parallel passages &amp; 100 parallel queries (synthetic) in three languages: English, Portuguese, and Finnish, constructed from the European Union's DGT-Acquis corpus.</p> <p>Dataset: <code>eherra/EuroPIRQ-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, fin, por Legal LM-generated and reviewed found Citation <pre><code>@misc{eherra_2025_europirq,\n  author = { {Elias Herranen} },\n  publisher = { Hugging Face },\n  title = { EuroPIRQ: European Parallel Information Retrieval Queries },\n  url = { https://huggingface.co/datasets/eherra/EuroPIRQ-retrieval },\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever","title":"FEVER","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.</p> <p>Dataset: <code>mteb/fever</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-fahardnegatives","title":"FEVER-FaHardNegatives","text":"<p>FEVER-FaHardNegatives</p> <p>Dataset: <code>MCINext/FEVER_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic, Written human-annotated found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-nl","title":"FEVER-NL","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. FEVER-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-fever</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fever-vn","title":"FEVER-VN","text":"<p>A translated dataset from FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feverhardnegatives","title":"FEVERHardNegatives","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/FEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feverhardnegativesv2","title":"FEVERHardNegatives.v2","text":"<p>FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct. V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/FEVER_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fquadretrieval","title":"FQuADRetrieval","text":"<p>This dataset has been built from the French SQuad dataset.</p> <p>Dataset: <code>manu/fquad2_test</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{dhoffschmidt-etal-2020-fquad,\n  address = {Online},\n  author = {d{'}Hoffschmidt, Martin  and\nBelblidia, Wacim  and\nHeinrich, Quentin  and\nBrendl{\\'e}, Tom  and\nVidal, Maxime},\n  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},\n  doi = {10.18653/v1/2020.findings-emnlp.107},\n  editor = {Cohn, Trevor  and\nHe, Yulan  and\nLiu, Yang},\n  month = nov,\n  pages = {1193--1208},\n  publisher = {Association for Computational Linguistics},\n  title = {{FQ}u{AD}: {F}rench Question Answering Dataset},\n  url = {https://aclanthology.org/2020.findings-emnlp.107},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#faithdial","title":"FaithDial","text":"<p>FaithDial is a faithful knowledge-grounded dialogue benchmark.It was curated by asking annotators to amend hallucinated utterances in Wizard of Wikipedia (WoW). It consists of conversation histories along with manually labelled relevant passage. For the purpose of retrieval, we only consider the instances marked as 'Edification' in the VRM field, as the gold passage associated with these instances is non-ambiguous.</p> <p>Dataset: <code>mteb/FaithDial</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@article{dziri2022faithdial,\n  author = {Dziri, Nouha and Kamalloo, Ehsan and Milton, Sivan and Zaiane, Osmar and Yu, Mo and Ponti, Edoardo M and Reddy, Siva},\n  doi = {10.1162/tacl_a_00529},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {12},\n  pages = {1473--1490},\n  publisher = {MIT Press},\n  title = {{FaithDial: A Faithful Benchmark for Information-Seeking Dialogue}},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#feedbackqaretrieval","title":"FeedbackQARetrieval","text":"<p>Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment</p> <p>Dataset: <code>mteb/FeedbackQARetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) precision_at_1 eng Government, Medical, Web, Written human-annotated created Citation <pre><code>@inproceedings{li-etal-2022-using,\n  address = {Dublin, Ireland},\n  author = {Li, Zichao  and\nSharma, Prakhar  and\nLu, Xing Han  and\nCheung, Jackie  and\nReddy, Siva},\n  booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},\n  doi = {10.18653/v1/2022.findings-acl.75},\n  editor = {Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline},\n  month = may,\n  pages = {926--937},\n  publisher = {Association for Computational Linguistics},\n  title = {Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment},\n  url = {https://aclanthology.org/2022.findings-acl.75},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa-pl","title":"FiQA-PL","text":"<p>Financial Opinion Mining and Question Answering</p> <p>Dataset: <code>mteb/FiQA-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Financial, Written human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018","title":"FiQA2018","text":"<p>Financial Opinion Mining and Question Answering</p> <p>Dataset: <code>mteb/fiqa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial, Written human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-fa","title":"FiQA2018-Fa","text":"<p>FiQA2018-Fa</p> <p>Dataset: <code>MCINext/fiqa-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-fav2","title":"FiQA2018-Fa.v2","text":"<p>FiQA2018-Fa.v2</p> <p>Dataset: <code>MCINext/fiqa-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-nl","title":"FiQA2018-NL","text":"<p>Financial Opinion Mining and Question Answering. FiQA2018-NL is a Dutch translation</p> <p>Dataset: <code>clips/beir-nl-fiqa</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#fiqa2018-vn","title":"FiQA2018-VN","text":"<p>A translated dataset from Financial Opinion Mining and Question Answering The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/fiqa-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Financial, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#finqaretrieval","title":"FinQARetrieval","text":"<p>A financial retrieval task based on FinQA dataset containing numerical reasoning questions over financial documents. Each query is a financial question requiring numerical computation (e.g., 'What is the percentage change in operating expenses from 2019 to 2020?'), and the corpus contains financial document text with tables and numerical data. The task is to retrieve the correct financial information that enables answering the numerical question. Queries are numerical reasoning questions while the corpus contains financial text passages with embedded tables, figures, and quantitative financial data from earnings reports.</p> <p>Dataset: <code>embedding-benchmark/FinQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{chen2021finqa,\n  author = {Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},\n  journal = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  title = {FinQA: A Dataset of Numerical Reasoning over Financial Data},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#financebenchretrieval","title":"FinanceBenchRetrieval","text":"<p>A financial retrieval task based on FinanceBench dataset containing financial questions and answers. Each query is a financial question (e.g., 'What was the total revenue in Q3 2023?'), and the corpus contains financial document excerpts and annual reports. The task is to retrieve the correct financial information that answers the question. Queries are financial questions while the corpus contains relevant excerpts from financial documents, earnings reports, and SEC filings with detailed financial data and metrics.</p> <p>Dataset: <code>embedding-benchmark/FinanceBench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{islam2023financebench,\n  author = {Islam, Pranab and Kannappan, Anand and Kiela, Douwe and Fergus, Rob and Ott, Myle and Wang, Sam and Garimella, Aparna and Garcia, Nino},\n  journal = {arXiv preprint arXiv:2311.11944},\n  title = {FinanceBench: A New Benchmark for Financial Question Answering},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#french1retrieval","title":"French1Retrieval","text":"<p>French general knowledge retrieval dataset with queries about celebrities, historical figures, and cultural topics.</p> <p>Dataset: <code>mteb-private/French1Retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Encyclopaedic, Written derived found"},{"location":"overview/available_tasks/retrieval/#frenchlegal1retrieval","title":"FrenchLegal1Retrieval","text":"<p>French legal document retrieval dataset with queries about administrative law, court decisions, and legal proceedings.</p> <p>Dataset: <code>mteb-private/FrenchLegal1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Legal, Written derived found"},{"location":"overview/available_tasks/retrieval/#freshstackretrieval","title":"FreshStackRetrieval","text":"<p>A code retrieval task based on FreshStack dataset containing programming problems across multiple languages. Each query is a natural language description of a programming task (e.g., 'Write a function to reverse a string using recursion'), and the corpus contains code implementations in Python, JavaScript, and Go. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains function implementations with proper syntax and logic across different programming languages.</p> <p>Dataset: <code>embedding-benchmark/FreshStack_mteb</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, go, javascript, python Programming expert-annotated found Citation <pre><code>@misc{thakur2025freshstackbuildingrealisticbenchmarks,\n  archiveprefix = {arXiv},\n  author = {Nandan Thakur and Jimmy Lin and Sam Havens and Michael Carbin and Omar Khattab and Andrew Drozdov},\n  eprint = {2504.13128},\n  primaryclass = {cs.IR},\n  title = {FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents},\n  url = {https://arxiv.org/abs/2504.13128},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#georgianfaqretrieval","title":"GeorgianFAQRetrieval","text":"<p>Frequently asked questions (FAQs) and answers mined from Georgian websites via Common Crawl.</p> <p>Dataset: <code>jupyterjazz/georgian-faq</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kat Web, Written derived created"},{"location":"overview/available_tasks/retrieval/#gerdalir","title":"GerDaLIR","text":"<p>GerDaLIR is a legal information retrieval dataset created from the Open Legal Data platform.</p> <p>Dataset: <code>mteb/GerDaLIR</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found Citation <pre><code>@inproceedings{wrzalik-krechel-2021-gerdalir,\n  address = {Punta Cana, Dominican Republic},\n  author = {Wrzalik, Marco  and\nKrechel, Dirk},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  month = nov,\n  pages = {123--128},\n  publisher = {Association for Computational Linguistics},\n  title = {{G}er{D}a{LIR}: A {G}erman Dataset for Legal Information Retrieval},\n  url = {https://aclanthology.org/2021.nllp-1.13},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#gerdalirsmall","title":"GerDaLIRSmall","text":"<p>The dataset consists of documents, passages and relevance labels in German. In contrast to the original dataset, only documents that have corresponding queries in the query set are chosen to create a smaller corpus for evaluation purposes.</p> <p>Dataset: <code>mteb/GerDaLIRSmall</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found Citation <pre><code>@inproceedings{wrzalik-krechel-2021-gerdalir,\n  address = {Punta Cana, Dominican Republic},\n  author = {Wrzalik, Marco  and\nKrechel, Dirk},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2021},\n  month = nov,\n  pages = {123--128},\n  publisher = {Association for Computational Linguistics},\n  title = {{G}er{D}a{LIR}: A {G}erman Dataset for Legal Information Retrieval},\n  url = {https://aclanthology.org/2021.nllp-1.13},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#german1retrieval","title":"German1Retrieval","text":"<p>German dialogue retrieval dataset with business conversations and workplace communication scenarios.</p> <p>Dataset: <code>mteb-private/German1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Non-fiction, Written derived found"},{"location":"overview/available_tasks/retrieval/#germandpr","title":"GermanDPR","text":"<p>GermanDPR is a German Question Answering dataset for open-domain QA. It associates questions with a textual context containing the answer</p> <p>Dataset: <code>mteb/GermanDPR</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Non-fiction, Web, Written human-annotated found Citation <pre><code>@misc{m\u00f6ller2021germanquad,\n  archiveprefix = {arXiv},\n  author = {Timo M\u00f6ller and Julian Risch and Malte Pietsch},\n  eprint = {2104.12741},\n  primaryclass = {cs.CL},\n  title = {GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#germangovserviceretrieval","title":"GermanGovServiceRetrieval","text":"<p>LHM-Dienstleistungen-QA is a German question answering dataset for government services of the Munich city administration. It associates questions with a textual context containing the answer</p> <p>Dataset: <code>it-at-m/LHM-Dienstleistungen-QA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_5 deu Government, Written derived found Citation <pre><code>@software{lhm-dienstleistungen-qa,\n  author = {Schr\u00f6der, Leon Marius and\nGutknecht, Clemens and\nAlkiddeh, Oubada and\nSusanne Wei\u00df,\nLukas, Leon},\n  month = nov,\n  publisher = {it@M},\n  title = {LHM-Dienstleistungen-QA - german public domain question-answering dataset},\n  url = {https://huggingface.co/datasets/it-at-m/LHM-Dienstleistungen-QA},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#germanhealthcare1retrieval","title":"GermanHealthcare1Retrieval","text":"<p>German medical consultation retrieval dataset with patient questions and doctor responses about various health conditions.</p> <p>Dataset: <code>mteb-private/GermanHealthcare1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Medical, Written derived found"},{"location":"overview/available_tasks/retrieval/#germanlegal1retrieval","title":"GermanLegal1Retrieval","text":"<p>German educational regulation retrieval dataset with queries about university capacity calculations and academic administration.</p> <p>Dataset: <code>mteb-private/GermanLegal1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found"},{"location":"overview/available_tasks/retrieval/#germanquad-retrieval","title":"GermanQuAD-Retrieval","text":"<p>Context Retrieval for German Question Answering</p> <p>Dataset: <code>mteb/germanquad-retrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) mrr_at_5 deu Non-fiction, Web, Written human-annotated found Citation <pre><code>@misc{m\u00f6ller2021germanquad,\n  archiveprefix = {arXiv},\n  author = {Timo M\u00f6ller and Julian Risch and Malte Pietsch},\n  eprint = {2104.12741},\n  primaryclass = {cs.CL},\n  title = {GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#govreport","title":"GovReport","text":"<p>A dataset for evaluating the ability of information retrieval models to retrieve lengthy US government reports from their summaries.</p> <p>Dataset: <code>isaacus/mteb-GovReport</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Government, Legal expert-annotated found Citation <pre><code>@inproceedings{huang-etal-2021-efficient,\n  address = {Online},\n  author = {Huang, Luyang  and\nCao, Shuyang  and\nParulian, Nikolaus  and\nJi, Heng  and\nWang, Lu},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.112},\n  eprint = {2104.02112},\n  month = jun,\n  pages = {1419--1436},\n  publisher = {Association for Computational Linguistics},\n  title = {Efficient Attentions for Long Document Summarization},\n  url = {https://aclanthology.org/2021.naacl-main.112},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#greekcivicsqa","title":"GreekCivicsQA","text":"<p>This dataset was provided by AlloProf, an organisation in Quebec, Canada offering resources and a help forum curated by a large number of teachers to students on all subjects taught from in primary and secondary school</p> <p>Dataset: <code>ilsp/greek_civics_qa</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ell Academic, Written derived found"},{"location":"overview/available_tasks/retrieval/#greennodetablemarkdownretrieval","title":"GreenNodeTableMarkdownRetrieval","text":"<p>GreenNodeTable documents</p> <p>Dataset: <code>GreenNode/GreenNode-Table-Markdown-Retrieval-VN</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Financial, Non-fiction human-annotated found Citation <pre><code>@inproceedings{10.1007/978-981-95-1746-6_17,\n  abstract = {Information retrieval often comes in plain text, lacking semi-structured text such as HTML and markdown, retrieving data that contains rich format such as table became non-trivial. In this paper, we tackle this challenge by introducing a new dataset, GreenNode Table Retrieval VN (GN-TRVN), which is collected from a massive corpus, a wide range of topics, and a longer context compared to ViQuAD2.0. To evaluate the effectiveness of our proposed dataset, we introduce two versions, M3-GN-VN and M3-GN-VN-Mixed, by fine-tuning the M3-Embedding model on this dataset. Experimental results show that our models consistently outperform the baselines, including the base model, across most evaluation criteria on various datasets such as VieQuADRetrieval, ZacLegalTextRetrieval, and GN-TRVN. In general, we release a more comprehensive dataset and two model versions that improve response performance for Vietnamese Markdown Table Retrieval.},\n  address = {Singapore},\n  author = {Pham, Bao Loc\nand Hoang, Quoc Viet\nand Luu, Quy Tung\nand Vo, Trong Thu},\n  booktitle = {Proceedings of the Fifth International Conference on Intelligent Systems and Networks},\n  isbn = {978-981-95-1746-6},\n  pages = {153--163},\n  publisher = {Springer Nature Singapore},\n  title = {GN-TRVN: A Benchmark for\u00a0Vietnamese Table Markdown Retrieval Task},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hc3financeretrieval","title":"HC3FinanceRetrieval","text":"<p>A financial retrieval task based on HC3 Finance dataset containing human vs AI-generated financial text detection. Each query is a financial question or prompt (e.g., 'Explain the impact of interest rate changes on bond prices'), and the corpus contains both human-written and AI-generated financial responses. The task is to retrieve the most relevant and accurate financial content that addresses the query. Queries are financial questions while the corpus contains detailed financial explanations, analysis, and educational content covering various financial concepts and market dynamics.</p> <p>Dataset: <code>embedding-benchmark/HC3Finance</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Financial expert-annotated found Citation <pre><code>@article{guo2023hc3,\n  author = {Guo, Biyang and Zhang, Xin and Wang, Zhiyuan and Jiang, Mingyuan and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},\n  journal = {arXiv preprint arXiv:2301.07597},\n  title = {How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hagridretrieval","title":"HagridRetrieval","text":"<p>HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset)is a dataset for generative information-seeking scenarios. It consists of queriesalong with a set of manually labelled relevant passages</p> <p>Dataset: <code>mteb/HagridRetrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written expert-annotated found Citation <pre><code>@article{hagrid,\n  author = {Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},\n  journal = {arXiv:2307.16883},\n  title = {{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hellaswag","title":"HellaSwag","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on HellaSwag.</p> <p>Dataset: <code>mteb/HellaSwag</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n\n@article{zellers2019hellaswag,\n  author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1905.07830},\n  title = {Hellaswag: Can a machine really finish your sentence?},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa","title":"HotpotQA","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>mteb/hotpotqa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-fa","title":"HotpotQA-Fa","text":"<p>HotpotQA-Fa</p> <p>Dataset: <code>MCINext/hotpotqa-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-fahardnegatives","title":"HotpotQA-FaHardNegatives","text":"<p>HotpotQA-FaHardNegatives</p> <p>Dataset: <code>MCINext/HotpotQA_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-nl","title":"HotpotQA-NL","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strongsupervision for supporting facts to enable more explainable question answering systems. HotpotQA-NL is a Dutch translation. </p> <p>Dataset: <code>clips/beir-nl-hotpotqa</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Web, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-pl","title":"HotpotQA-PL","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>mteb/HotpotQA-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-plhardnegatives","title":"HotpotQA-PLHardNegatives","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/HotpotQA-PLHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqa-vn","title":"HotpotQA-VN","text":"<p>A translated dataset from HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/hotpotqa-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqahardnegatives","title":"HotpotQAHardNegatives","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/HotpotQA_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hotpotqahardnegativesv2","title":"HotpotQAHardNegatives.v2","text":"<p>HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/HotpotQA_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#humanevalretrieval","title":"HumanEvalRetrieval","text":"<p>A code retrieval task based on 164 Python programming problems from HumanEval. Each query is a natural language description of a programming task (e.g., 'Check if in given list of numbers, are any two numbers closer to each other than given threshold'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations with proper indentation and logic.</p> <p>Dataset: <code>embedding-benchmark/HumanEval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming derived found Citation <pre><code>@article{chen2021evaluating,\n  archiveprefix = {arXiv},\n  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Joshua and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},\n  eprint = {2107.03374},\n  primaryclass = {cs.LG},\n  title = {Evaluating Large Language Models Trained on Code},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#hunsum2abstractiveretrieval","title":"HunSum2AbstractiveRetrieval","text":"<p>HunSum-2-abstractive is a Hungarian dataset containing news articles along with lead, titles and metadata.</p> <p>Dataset: <code>SZTAKI-HLT/HunSum-2-abstractive</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 hun News, Written derived found Citation <pre><code>@misc{barta2024news,\n  archiveprefix = {arXiv},\n  author = {Botond Barta and Dorina Lakatos and Attila Nagy and Mil\u00e1n Konor Nyist and Judit \u00c1cs},\n  eprint = {2404.03555},\n  primaryclass = {cs.CL},\n  title = {From News to Summaries: Building a Hungarian Corpus for Extractive and Abstractive Summarization},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#indicqaretrieval","title":"IndicQARetrieval","text":"<p>IndicQA is a manually curated cloze-style reading comprehension dataset that can be used for evaluating question-answering models in 11 Indic languages. It is repurposed retrieving relevant context for each question.</p> <p>Dataset: <code>mteb/IndicQARetrieval</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 asm, ben, guj, hin, kan, ... (11) Web, Written human-annotated machine-translated and verified Citation <pre><code>@article{doddapaneni2022towards,\n  author = {Sumanth Doddapaneni and Rahul Aralikatte and Gowtham Ramesh and Shreyansh Goyal and Mitesh M. Khapra and Anoop Kunchukuttan and Pratyush Kumar},\n  doi = {10.18653/v1/2023.acl-long.693},\n  journal = {Annual Meeting of the Association for Computational Linguistics},\n  title = {Towards Leaving No Indic Language Behind: Building Monolingual Corpora, Benchmark and Models for Indic Languages},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jacwirretrieval","title":"JaCWIRRetrieval","text":"<p>JaCWIR is a small-scale Japanese information retrieval evaluation dataset consisting of 5000 question texts and approximately 500k web page titles and web page introductions or summaries (meta descriptions, etc.). The question texts are created based on one of the 500k web pages, and that data is used as a positive example for the question text.</p> <p>Dataset: <code>mteb/JaCWIRRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found Citation <pre><code>@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jacwirretrievallite","title":"JaCWIRRetrievalLite","text":"<p>JaCWIR (Japanese Casual Web IR) is a dataset consisting of questions and webpage meta descriptions collected from Hatena Bookmark. This is the lightweight version with a reduced corpus (302,638 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/JaCWIRRetrievalLite</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found Citation <pre><code>@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n\n@misc{yuichi-tateno-2024-jacwir,\n  author = {Yuichi Tateno},\n  title = {JaCWIR: Japanese Casual Web IR - \u65e5\u672c\u8a9e\u60c5\u5831\u691c\u7d22\u8a55\u4fa1\u306e\u305f\u3081\u306e\u5c0f\u898f\u6a21\u3067\u30ab\u30b8\u30e5\u30a2\u30eb\u306aWeb\u30bf\u30a4\u30c8\u30eb\u3068\u6982\u8981\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8},\n  url = {https://huggingface.co/datasets/hotchpotch/JaCWIR},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jagovfaqsretrieval","title":"JaGovFaqsRetrieval","text":"<p>JaGovFaqs is a dataset consisting of FAQs manually extracted from the website of Japanese bureaus. The dataset consists of 22k FAQs, where the queries (questions) and corpus (answers) have been shuffled, and the goal is to match the answer with the question.</p> <p>Dataset: <code>mteb/JaGovFaqsRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Web, Written derived found"},{"location":"overview/available_tasks/retrieval/#jaquadretrieval","title":"JaQuADRetrieval","text":"<p>Human-annotated question-answer pairs for Japanese wikipedia pages.</p> <p>Dataset: <code>mteb/JaQuADRetrieval</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@misc{so2022jaquad,\n  archiveprefix = {arXiv},\n  author = {ByungHoon So and Kyuhong Byun and Kyungwon Kang and Seongjin Cho},\n  eprint = {2202.01764},\n  primaryclass = {cs.CL},\n  title = {{JaQuAD: Japanese Question Answering Dataset for Machine Reading Comprehension}},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#japanesecode1retrieval","title":"JapaneseCode1Retrieval","text":"<p>Japanese code retrieval dataset. Japanese natural language queries paired with Python code snippets for cross-lingual code retrieval evaluation.</p> <p>Dataset: <code>mteb-private/JapaneseCode1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Programming, Written derived found"},{"location":"overview/available_tasks/retrieval/#japaneselegal1retrieval","title":"JapaneseLegal1Retrieval","text":"<p>Japanese legal regulation retrieval dataset with queries about government regulations, ministry ordinances, and administrative law.</p> <p>Dataset: <code>mteb-private/JapaneseLegal1Retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Legal, Written derived found"},{"location":"overview/available_tasks/retrieval/#jaqketretrieval","title":"JaqketRetrieval","text":"<p>JAQKET (JApanese Questions on Knowledge of EnTities) is a QA dataset that is created based on quiz questions.</p> <p>Dataset: <code>mteb/jaqket</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{Kurihara_nlp2020,\n  author = {\u9234\u6728\u6b63\u654f and \u9234\u6728\u6f64 and \u677e\u7530\u8015\u53f2 and \u2ec4\u7530\u4eac\u4ecb and \u4e95\u4e4b\u4e0a\u76f4\u4e5f},\n  booktitle = {\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a\u7b2c26\u56de\u5e74\u6b21\u5927\u4f1a},\n  note = {in Japanese},\n  title = {JAQKET: \u30af\u30a4\u30b9\u3099\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c6\u3099\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9},\n  url = {https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P2-24.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#jaqketretrievallite","title":"JaqketRetrievalLite","text":"<p>JAQKET (JApanese Questions on Knowledge of EnTities) is a QA dataset created based on quiz questions. This is the lightweight version with a reduced corpus (65,802 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/JaqketRetrievalLite</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n\n@inproceedings{Kurihara_nlp2020,\n  author = {\u9234\u6728\u6b63\u654f and \u9234\u6728\u6f64 and \u677e\u7530\u8015\u53f2 and \u2ec4\u7530\u4eac\u4ecb and \u4e95\u4e4b\u4e0a\u76f4\u4e5f},\n  booktitle = {\u8a00\u8a9e\u51e6\u7406\u5b66\u4f1a\u7b2c26\u56de\u5e74\u6b21\u5927\u4f1a},\n  note = {in Japanese},\n  title = {JAQKET: \u30af\u30a4\u30b9\u3099\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c6\u3099\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9},\n  url = {https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/P2-24.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ko-strategyqa","title":"Ko-StrategyQA","text":"<p>Ko-StrategyQA</p> <p>Dataset: <code>taeminlee/Ko-StrategyQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor Encyclopaedic, Written human-annotated machine-translated Citation <pre><code>@article{geva2021strategyqa,\n  author = {Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},\n  journal = {Transactions of the Association for Computational Linguistics (TACL)},\n  title = {{Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies}},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembnarrativeqaretrieval","title":"LEMBNarrativeQARetrieval","text":"<p>narrativeqa subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction, Non-fiction, Written derived found Citation <pre><code>@article{kocisky-etal-2018-narrativeqa,\n  address = {Cambridge, MA},\n  author = {Ko{\\v{c}}isk{\\'y}, Tom{\\'a}{\\v{s}}  and\nSchwarz, Jonathan  and\nBlunsom, Phil  and\nDyer, Chris  and\nHermann, Karl Moritz  and\nMelis, G{\\'a}bor  and\nGrefenstette, Edward},\n  doi = {10.1162/tacl_a_00023},\n  editor = {Lee, Lillian  and\nJohnson, Mark  and\nToutanova, Kristina  and\nRoark, Brian},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {317--328},\n  publisher = {MIT Press},\n  title = {The {N}arrative{QA} Reading Comprehension Challenge},\n  url = {https://aclanthology.org/Q18-1023},\n  volume = {6},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembneedleretrieval","title":"LEMBNeedleRetrieval","text":"<p>needle subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 eng Academic, Blog, Written derived found Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembpasskeyretrieval","title":"LEMBPasskeyRetrieval","text":"<p>passkey subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>mteb/LEMBPasskeyRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_1 eng Fiction, Written derived found Citation <pre><code>@article{zhu2024longembed,\n  author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},\n  journal = {arXiv preprint arXiv:2404.12096},\n  title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembqmsumretrieval","title":"LEMBQMSumRetrieval","text":"<p>qmsum subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Spoken, Written derived found Citation <pre><code>@inproceedings{zhong-etal-2021-qmsum,\n  address = {Online},\n  author = {Zhong, Ming  and\nYin, Da  and\nYu, Tao  and\nZaidi, Ahmad  and\nMutuma, Mutethia  and\nJha, Rahul  and\nAwadallah, Ahmed Hassan  and\nCelikyilmaz, Asli  and\nLiu, Yang  and\nQiu, Xipeng  and\nRadev, Dragomir},\n  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2021.naacl-main.472},\n  editor = {Toutanova, Kristina  and\nRumshisky, Anna  and\nZettlemoyer, Luke  and\nHakkani-Tur, Dilek  and\nBeltagy, Iz  and\nBethard, Steven  and\nCotterell, Ryan  and\nChakraborty, Tanmoy  and\nZhou, Yichao},\n  month = jun,\n  pages = {5905--5921},\n  publisher = {Association for Computational Linguistics},\n  title = {{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization},\n  url = {https://aclanthology.org/2021.naacl-main.472},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembsummscreenfdretrieval","title":"LEMBSummScreenFDRetrieval","text":"<p>summ_screen_fd subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Spoken, Written derived found Citation <pre><code>@inproceedings{chen-etal-2022-summscreen,\n  address = {Dublin, Ireland},\n  author = {Chen, Mingda  and\nChu, Zewei  and\nWiseman, Sam  and\nGimpel, Kevin},\n  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/2022.acl-long.589},\n  editor = {Muresan, Smaranda  and\nNakov, Preslav  and\nVillavicencio, Aline},\n  month = may,\n  pages = {8602--8615},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization},\n  url = {https://aclanthology.org/2022.acl-long.589},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lembwikimqaretrieval","title":"LEMBWikimQARetrieval","text":"<p>2wikimqa subset of dwzhu/LongEmbed dataset.</p> <p>Dataset: <code>dwzhu/LongEmbed</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{ho2020constructing,\n  author = {Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  pages = {6609--6625},\n  title = {Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#limitretrieval","title":"LIMITRetrieval","text":"<p>A simple retrieval task designed to test all combinations of top-2 documents. This version includes all 50k docs.</p> <p>Dataset: <code>orionweller/LIMIT</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_2 eng Fiction human-annotated created Citation <pre><code>@misc{weller2025theoreticallimit,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Michael Boratko and Iftekhar Naim and Jinhyuk Lee},\n  eprint = {2508.21038},\n  primaryclass = {cs.IR},\n  title = {On the Theoretical Limitations of Embedding-Based Retrieval},\n  url = {https://arxiv.org/abs/2508.21038},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#limitsmallretrieval","title":"LIMITSmallRetrieval","text":"<p>A simple retrieval task designed to test all combinations of top-2 documents. This version only includes the 46 documents that are relevant to the 1000 queries.</p> <p>Dataset: <code>orionweller/LIMIT-small</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_2 eng Fiction human-annotated created Citation <pre><code>@misc{weller2025theoreticallimit,\n  archiveprefix = {arXiv},\n  author = {Orion Weller and Michael Boratko and Iftekhar Naim and Jinhyuk Lee},\n  eprint = {2508.21038},\n  primaryclass = {cs.IR},\n  title = {On the Theoretical Limitations of Embedding-Based Retrieval},\n  url = {https://arxiv.org/abs/2508.21038},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lawirko","title":"LawIRKo","text":"<p>This dataset assesses a model's ability to retrieve relevant legal articles from queries referencing specific Korean laws and provisions. The corpus comprises official legal texts including statutes, acts, and regulations, with each document representing a single article. Queries are derived from law titles paired and article identifiers. For instance the law title might be \"\uac74\ucd95\ubc95\" (Building Act) and the article name \"\uae30\uc220\uc801 \uae30\uc900\" (Technical Standards), which would become \"\uac74\ucd95\ubc95\uc5d0 \uba85\uc2dc\ub41c \ubc95\ub960 \uc911\uc5d0 '\uae30\uc220\uc801 \uae30\uc900'\uc5d0 \ub300\ud574 \uc124\uba85\ud558\uace0 \uc788\ub294 \uc138\ubd80 \ud56d\ubaa9\uc740 \ubb34\uc5c7\uc785\ub2c8\uae4c?\" (\"Which specific articles in the Building Act explain the 'technical standards'?\").</p> <p>Dataset: <code>on-and-on/lawgov_ir-ko</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor Legal, Written derived found Citation <pre><code>@misc{law_ko_ir_khee,\n  author = {kang-hyeun Lee},\n  howpublished = {\\url{https://huggingface.co/datasets/on-and-on/lawgov_ir-ko}},\n  note = {A Benchmark Dataset for Korean Legal Information Retrieval and QA},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lecardv2","title":"LeCaRDv2","text":"<p>The task involves identifying and retrieving the case document that best matches or is most relevant to the scenario described in each of the provided queries.</p> <p>Dataset: <code>mteb/LeCaRDv2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 zho Legal, Written derived found Citation <pre><code>@misc{li2023lecardv2,\n  archiveprefix = {arXiv},\n  author = {Haitao Li and Yunqiu Shao and Yueyue Wu and Qingyao Ai and Yixiao Ma and Yiqun Liu},\n  eprint = {2310.17609},\n  primaryclass = {cs.CL},\n  title = {LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalbenchconsumercontractsqa","title":"LegalBenchConsumerContractsQA","text":"<p>The dataset includes questions and answers related to contracts.</p> <p>Dataset: <code>mteb/legalbench_consumer_contracts_qa</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalbenchcorporatelobbying","title":"LegalBenchCorporateLobbying","text":"<p>The dataset includes bill titles and bill summaries related to corporate lobbying.</p> <p>Dataset: <code>mteb/legalbench_corporate_lobbying</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@misc{guha2023legalbench,\n  archiveprefix = {arXiv},\n  author = {Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher R\u00e9 and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin Peters and Brandon Waldon and Daniel N. Rockmore and Diego Zambrano and Dmitry Talisman and Enam Hoque and Faiz Surani and Frank Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John Nay and Jonathan H. Choi and Kevin Tobia and Margaret Hagan and Megan Ma and Michael Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shang Gao and Spencer Williams and Sunny Gandhi and Tom Zur and Varun Iyer and Zehua Li},\n  eprint = {2308.11462},\n  primaryclass = {cs.CL},\n  title = {LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},\n  year = {2023},\n}\n\n@article{hendrycks2021cuad,\n  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},\n  journal = {arXiv preprint arXiv:2103.06268},\n  title = {Cuad: An expert-annotated nlp dataset for legal contract review},\n  year = {2021},\n}\n\n@article{holzenberger2021factoring,\n  author = {Holzenberger, Nils and Van Durme, Benjamin},\n  journal = {arXiv preprint arXiv:2105.07903},\n  title = {Factoring statutory reasoning as language understanding challenges},\n  year = {2021},\n}\n\n@article{koreeda2021contractnli,\n  author = {Koreeda, Yuta and Manning, Christopher D},\n  journal = {arXiv preprint arXiv:2110.01799},\n  title = {ContractNLI: A dataset for document-level natural language inference for contracts},\n  year = {2021},\n}\n\n@article{lippi2019claudette,\n  author = {Lippi, Marco and Pa{\\l}ka, Przemys{\\l}aw and Contissa, Giuseppe and Lagioia, Francesca and Micklitz, Hans-Wolfgang and Sartor, Giovanni and Torroni, Paolo},\n  journal = {Artificial Intelligence and Law},\n  pages = {117--139},\n  publisher = {Springer},\n  title = {CLAUDETTE: an automated detector of potentially unfair clauses in online terms of service},\n  volume = {27},\n  year = {2019},\n}\n\n@article{ravichander2019question,\n  author = {Ravichander, Abhilasha and Black, Alan W and Wilson, Shomir and Norton, Thomas and Sadeh, Norman},\n  journal = {arXiv preprint arXiv:1911.00841},\n  title = {Question answering for privacy policies: Combining computational and legal perspectives},\n  year = {2019},\n}\n\n@article{wang2023maud,\n  author = {Wang, Steven H and Scardigli, Antoine and Tang, Leonard and Chen, Wei and Levkin, Dimitry and Chen, Anya and Ball, Spencer and Woodside, Thomas and Zhang, Oliver and Hendrycks, Dan},\n  journal = {arXiv preprint arXiv:2301.00876},\n  title = {MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding},\n  year = {2023},\n}\n\n@inproceedings{wilson2016creation,\n  author = {Wilson, Shomir and Schaub, Florian and Dara, Aswarth Abhilash and Liu, Frederick and Cherivirala, Sushain and Leon, Pedro Giovanni and Andersen, Mads Schaarup and Zimmeck, Sebastian and Sathyendra, Kanthashree Mysore and Russell, N Cameron and others},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  pages = {1330--1340},\n  title = {The creation and analysis of a website privacy policy corpus},\n  year = {2016},\n}\n\n@inproceedings{zheng2021does,\n  author = {Zheng, Lucia and Guha, Neel and Anderson, Brandon R and Henderson, Peter and Ho, Daniel E},\n  booktitle = {Proceedings of the eighteenth international conference on artificial intelligence and law},\n  pages = {159--168},\n  title = {When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings},\n  year = {2021},\n}\n\n@article{zimmeck2019maps,\n  author = {Zimmeck, Sebastian and Story, Peter and Smullen, Daniel and Ravichander, Abhilasha and Wang, Ziqi and Reidenberg, Joel R and Russell, N Cameron and Sadeh, Norman},\n  journal = {Proc. Priv. Enhancing Tech.},\n  pages = {66},\n  title = {Maps: Scaling privacy compliance analysis to a million apps},\n  volume = {2019},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalqanlretrieval","title":"LegalQANLRetrieval","text":"<p>To this end, we create and publish a Dutch legal QA dataset, consisting of question-answer pairs with attributions to Dutch law articles.</p> <p>Dataset: <code>clips/mteb-nl-legalqa-pr</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Legal, Written expert-annotated found Citation <pre><code>@inproceedings{redelaar2024attributed,\n  author = {Redelaar, Felicia and Van Drie, Romy and Verberne, Suzan and De Boer, Maaike},\n  booktitle = {Proceedings of the natural legal language processing workshop 2024},\n  pages = {154--165},\n  title = {Attributed Question Answering for Preconditions in the Dutch Law},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalquad","title":"LegalQuAD","text":"<p>The dataset consists of questions and legal documents in German.</p> <p>Dataset: <code>mteb/LegalQuAD</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu Legal, Written derived found Citation <pre><code>@inproceedings{9723721,\n  author = {Hoppe, Christoph and Pelkmann, David and Migenda, Nico and H\u00f6tte, Daniel and Schenck, Wolfram},\n  booktitle = {2021 IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)},\n  doi = {10.1109/AIKE52691.2021.00011},\n  keywords = {Knowledge engineering;Law;Semantic search;Conferences;Bit error rate;NLP;knowledge extraction;question-answering;semantic search;document retrieval;German language},\n  number = {},\n  pages = {29-32},\n  title = {Towards Intelligent Legal Advisors for Document Retrieval and Question-Answering in German Legal Documents},\n  volume = {},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#legalsummarization","title":"LegalSummarization","text":"<p>The dataset consists of 439 pairs of contracts and their summarizations from https://tldrlegal.com and https://tosdr.org/.</p> <p>Dataset: <code>mteb/legal_summarization</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Legal, Written derived found Citation <pre><code>@inproceedings{manor-li-2019-plain,\n  address = {Minneapolis, Minnesota},\n  author = {Manor, Laura  and\nLi, Junyi Jessy},\n  booktitle = {Proceedings of the Natural Legal Language Processing Workshop 2019},\n  month = jun,\n  pages = {1--11},\n  publisher = {Association for Computational Linguistics},\n  title = {Plain {E}nglish Summarization of Contracts},\n  url = {https://www.aclweb.org/anthology/W19-2201},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#litsearchretrieval","title":"LitSearchRetrieval","text":"<p>The dataset contains the query set and retrieval corpus for the paper LitSearch: A Retrieval Benchmark for Scientific Literature Search. It introduces LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality.</p> <p>Dataset: <code>princeton-nlp/LitSearch</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written LM-generated found Citation <pre><code>@article{ajith2024litsearch,\n  author = {Ajith, Anirudh and Xia, Mengzhou and Chevalier, Alexis and Goyal, Tanya and Chen, Danqi and Gao, Tianyu},\n  title = {LitSearch: A Retrieval Benchmark for Scientific Literature Search},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#lotte","title":"LoTTE","text":"<p>LoTTE (Long-Tail Topic-stratified Evaluation for IR) is designed to evaluate retrieval models on underrepresented, long-tail topics. Unlike MSMARCO or BEIR, LoTTE features domain-specific queries and passages from StackExchange (covering writing, recreation, science, technology, and lifestyle), providing a challenging out-of-domain generalization benchmark.</p> <p>Dataset: <code>mteb/LoTTE</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) hit_rate_at_5 eng Academic, Social, Web derived found Citation <pre><code>@inproceedings{santhanam-etal-2022-colbertv2,\n  address = {Seattle, United States},\n  author = {Santhanam, Keshav  and\nKhattab, Omar  and\nSaad-Falcon, Jon  and\nPotts, Christopher  and\nZaharia, Matei},\n  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},\n  doi = {10.18653/v1/2022.naacl-main.272},\n  editor = {Carpuat, Marine  and\nde Marneffe, Marie-Catherine  and\nMeza Ruiz, Ivan Vladimir},\n  month = jul,\n  pages = {3715--3734},\n  publisher = {Association for Computational Linguistics},\n  title = {{C}ol{BERT}v2: Effective and Efficient Retrieval via Lightweight Late Interaction},\n  url = {https://aclanthology.org/2022.naacl-main.272/},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mbppretrieval","title":"MBPPRetrieval","text":"<p>A code retrieval task based on 378 Python programming problems from MBPP (Mostly Basic Python Programming). Each query is a natural language description of a programming task (e.g., 'Write a function to find the shared elements from the given two lists'), and the corpus contains Python code implementations. The task is to retrieve the correct code snippet that solves the described problem. Queries are problem descriptions while the corpus contains Python function implementations with proper syntax and logic.</p> <p>Dataset: <code>embedding-benchmark/MBPP</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, python Programming expert-annotated found Citation <pre><code>@article{austin2021program,\n  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},\n  journal = {arXiv preprint arXiv:2108.07732},\n  title = {Program Synthesis with Large Language Models},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miracljaretrievallite","title":"MIRACLJaRetrievalLite","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset. This is the lightweight Japanese version with a reduced corpus (105,064 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/MIRACLJaRetrievalLite</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David\nand Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  volume = {11},\n  year = {2023},\n}\n\n@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrieval","title":"MIRACLRetrieval","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages.</p> <p>Dataset: <code>mteb/MIRACLRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrievalhardnegatives","title":"MIRACLRetrievalHardNegatives","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MIRACLRetrievalHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#miraclretrievalhardnegativesv2","title":"MIRACLRetrievalHardNegatives.v2","text":"<p>MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual retrieval dataset that focuses on search across 18 different languages. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/MIRACLRetrievalHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, deu, eng, fas, ... (18) Encyclopaedic, Written expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00595,\n  author = {Zhang, Xinyu and Thakur, Nandan and Ogundepo, Odunayo and Kamalloo, Ehsan and Alfonso-Hermelo, David and Li, Xiaoguang and Liu, Qun and Rezagholizadeh, Mehdi and Lin, Jimmy},\n  doi = {10.1162/tacl_a_00595},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\_a\\_00595/2157340/tacl\\_a\\_00595.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {09},\n  pages = {1114-1131},\n  title = {{MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages}},\n  url = {https://doi.org/10.1162/tacl\\_a\\_00595},\n  volume = {11},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mkqaretrieval","title":"MKQARetrieval","text":"<p>Multilingual Knowledge Questions &amp; Answers (MKQA)contains 10,000 queries sampled from the Google Natural Questions dataset. For each query we collect new passage-independent answers. These queries and answers are then human translated into 25 Non-English languages.</p> <p>Dataset: <code>mteb/MKQARetrieval</code> \u2022 License: cc-by-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, dan, deu, eng, fin, ... (26) Written human-annotated found Citation <pre><code>@misc{mkqa,\n  author = {Shayne Longpre and Yi Lu and Joachim Daiber},\n  title = {MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering},\n  url = {https://arxiv.org/pdf/2007.15207.pdf},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mlqaretrieval","title":"MLQARetrieval","text":"<p>MLQA (MultiLingual Question Answering) is a benchmark dataset for evaluating cross-lingual question answering performance. MLQA consists of over 5K extractive QA instances (12K in English) in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA is highly parallel, with QA instances parallel between 4 different languages on average.</p> <p>Dataset: <code>mteb/MLQARetrieval</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, deu, eng, hin, spa, ... (7) Encyclopaedic, Written human-annotated found Citation <pre><code>@article{lewis2019mlqa,\n  author = {Lewis, Patrick and Oguz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},\n  eid = {arXiv: 1910.07475},\n  journal = {arXiv preprint arXiv:1910.07475},\n  title = {MLQA: Evaluating Cross-lingual Extractive Question Answering},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mlquestions","title":"MLQuestions","text":"<p>MLQuestions is a domain adaptation dataset for the machine learning domainIt consists of ML questions along with passages from Wikipedia machine learning pages (https://en.wikipedia.org/wiki/Category:Machine_learning)</p> <p>Dataset: <code>McGill-NLP/mlquestions</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Encyclopaedic, Written human-annotated found Citation <pre><code>@inproceedings{kulshreshtha-etal-2021-back,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Kulshreshtha, Devang  and\nBelfer, Robert  and\nSerban, Iulian Vlad  and\nReddy, Siva},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  month = nov,\n  pages = {7064--7078},\n  publisher = {Association for Computational Linguistics},\n  title = {Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval},\n  url = {https://aclanthology.org/2021.emnlp-main.566},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mmarcoretrieval","title":"MMarcoRetrieval","text":"<p>MMarcoRetrieval</p> <p>Dataset: <code>mteb/MMarcoRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Web, Written derived machine-translated Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco","title":"MSMARCO","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search</p> <p>Dataset: <code>mteb/msmarco</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-fa","title":"MSMARCO-Fa","text":"<p>MSMARCO-Fa</p> <p>Dataset: <code>MCINext/msmarco-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-fahardnegatives","title":"MSMARCO-FaHardNegatives","text":"<p>MSMARCO-FaHardNegatives</p> <p>Dataset: <code>MCINext/MSMARCO_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-pl","title":"MSMARCO-PL","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search</p> <p>Dataset: <code>mteb/MSMARCO-PL</code> \u2022 License: https://microsoft.github.io/msmarco/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-plhardnegatives","title":"MSMARCO-PLHardNegatives","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MSMARCO-PLHardNegatives</code> \u2022 License: https://microsoft.github.io/msmarco/ \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarco-vn","title":"MSMARCO-VN","text":"<p>A translated dataset from MS MARCO is a collection of datasets focused on deep learning in search The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/msmarco-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarcohardnegatives","title":"MSMARCOHardNegatives","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/MSMARCO_test_top_250_only_w_correct-v2</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#msmarcov2","title":"MSMARCOv2","text":"<p>MS MARCO is a collection of datasets focused on deep learning in search. This version is derived from BEIR</p> <p>Dataset: <code>mteb/msmarco-v2</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#medicalqaretrieval","title":"MedicalQARetrieval","text":"<p>The dataset consists 2048 medical question and answer pairs.</p> <p>Dataset: <code>mteb/medical_qa</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical, Written derived found Citation <pre><code>@article{BenAbacha-BMC-2019,\n  author = {Asma, Ben Abacha and Dina, Demner{-}Fushman},\n  journal = {{BMC} Bioinform.},\n  number = {1},\n  pages = {511:1--511:23},\n  title = {A Question-Entailment Approach to Question Answering},\n  url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3119-4},\n  volume = {20},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#medicalretrieval","title":"MedicalRetrieval","text":"<p>MedicalRetrieval</p> <p>Dataset: <code>mteb/MedicalRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Medical, Written human-annotated found Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mintakaretrieval","title":"MintakaRetrieval","text":"<p>We introduce Mintaka, a complex, natural, and multilingual dataset designed for experimenting with end-to-end question-answering models. Mintaka is composed of 20,000 question-answer pairs collected in English, annotated with Wikidata entities, and translated into Arabic, French, German, Hindi, Italian, Japanese, Portuguese, and Spanish for a total of 180,000 samples. Mintaka includes 8 types of complex questions, including superlative, intersection, and multi-hop questions, which were naturally elicited from crowd workers. </p> <p>Dataset: <code>mteb/MintakaRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, deu, fra, hin, ita, ... (8) Encyclopaedic, Written derived human-translated Citation <pre><code>@inproceedings{sen-etal-2022-mintaka,\n  address = {Gyeongju, Republic of Korea},\n  author = {Sen, Priyanka  and\nAji, Alham Fikri  and\nSaffari, Amir},\n  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},\n  month = oct,\n  pages = {1604--1619},\n  publisher = {International Committee on Computational Linguistics},\n  title = {Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering},\n  url = {https://aclanthology.org/2022.coling-1.138},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mrtidyretrieval","title":"MrTidyRetrieval","text":"<p>Mr. TyDi is a multi-lingual benchmark dataset built on TyDi, covering eleven typologically diverse languages. It is designed for monolingual retrieval, specifically to evaluate ranking with learned dense representations.</p> <p>Dataset: <code>mteb/mrtidy</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, ben, eng, fin, ind, ... (11) Encyclopaedic, Written human-annotated found Citation <pre><code>@article{mrtydi,\n  author = {Xinyu Zhang and Xueguang Ma and Peng Shi and Jimmy Lin},\n  journal = {arXiv:2108.08787},\n  title = {{Mr. TyDi}: A Multi-lingual Benchmark for Dense Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mrtydijaretrievallite","title":"MrTyDiJaRetrievalLite","text":"<p>Mr.TyDi is a multilingual benchmark dataset built on TyDi for document retrieval tasks. This is the lightweight Japanese version with a reduced corpus (93,382 documents) constructed using hard negatives from 5 high-performance models.</p> <p>Dataset: <code>mteb/MrTyDiJaRetrievalLite</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@misc{jmteb_lite,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan and Fukuchi, Akihiko and Shibata, Tomohide\nand Kawahara, Daisuke},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB-lite}},\n  title = {{J}{M}{T}{E}{B}-lite: {T}he {L}ightweight {V}ersion of {JMTEB}},\n  year = {2025},\n}\n\n@article{mrtydi,\n  author = {Xinyu Zhang and Xueguang Ma and Peng Shi and Jimmy Lin},\n  journal = {arXiv:2108.08787},\n  title = {{Mr. TyDi}: A Multi-lingual Benchmark for Dense Retrieval},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#multilongdocretrieval","title":"MultiLongDocRetrieval","text":"<p>Multi Long Doc Retrieval (MLDR) 'is curated by the multilingual articles from Wikipedia, Wudao and mC4 (see Table 7), and NarrativeQA (Koc\u02c7isky \u0301 et al., 2018; Gu \u0308nther et al., 2023), which is only for English.' (Chen et al., 2024). It is constructed by sampling lengthy articles from Wikipedia, Wudao and mC4 datasets and randomly choose paragraphs from them. Then we use GPT-3.5 to generate questions based on these paragraphs. The generated question and the sampled article constitute a new text pair to the dataset.</p> <p>Dataset: <code>mteb/MultiLongDocRetrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, cmn, deu, eng, fra, ... (13) Encyclopaedic, Fiction, Non-fiction, Web, Written LM-generated found Citation <pre><code>@misc{bge-m3,\n  archiveprefix = {arXiv},\n  author = {Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n  eprint = {2402.03216},\n  primaryclass = {cs.CL},\n  title = {BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus","title":"NFCorpus","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/nfcorpus</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written derived found Citation <pre><code>@inproceedings{boteva2016,\n  author = {Boteva, Vera and Gholipour, Demian and Sokolov, Artem and Riezler, Stefan},\n  city = {Padova},\n  country = {Italy},\n  journal = {Proceedings of the 38th European Conference on Information Retrieval},\n  journal-abbrev = {ECIR},\n  title = {A Full-Text Learning to Rank Dataset for Medical Information Retrieval},\n  url = {http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-fa","title":"NFCorpus-Fa","text":"<p>NFCorpus-Fa</p> <p>Dataset: <code>MCINext/nfcorpus-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-nl","title":"NFCorpus-NL","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. NFCorpus-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-nfcorpus</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-nlv2","title":"NFCorpus-NL.v2","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. NFCorpus-NL is a Dutch translation. This version adds a Dutch prompt to the dataset.</p> <p>Dataset: <code>clips/beir-nl-nfcorpus</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-pl","title":"NFCorpus-PL","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/NFCorpus-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Medical, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nfcorpus-vn","title":"NFCorpus-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nfcorpus-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsarticleretrieval","title":"NLPJournalAbsArticleRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsArticleRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsarticleretrievalv2","title":"NLPJournalAbsArticleRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding full article with the given abstract. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsArticleRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsintroretrieval","title":"NLPJournalAbsIntroRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V1 dataset (last update 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsIntroRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournalabsintroretrievalv2","title":"NLPJournalAbsIntroRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given abstract. This is the V2 dataset (last update 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalAbsIntroRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleabsretrieval","title":"NLPJournalTitleAbsRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleAbsRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleabsretrievalv2","title":"NLPJournalTitleAbsRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding abstract with the given title. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleAbsRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleintroretrieval","title":"NLPJournalTitleIntroRetrieval","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V1 dataset (last updated 2020-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleIntroRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nlpjournaltitleintroretrievalv2","title":"NLPJournalTitleIntroRetrieval.V2","text":"<p>This dataset was created from the Japanese NLP Journal LaTeX Corpus. The titles, abstracts and introductions of the academic papers were shuffled. The goal is to find the corresponding introduction with the given title. This is the V2 dataset (last updated 2025-06-15).</p> <p>Dataset: <code>mteb/NLPJournalTitleIntroRetrieval.V2</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 jpn Academic, Written derived found Citation <pre><code>@misc{jmteb,\n  author = {Li, Shengzhe and Ohagi, Masaya and Ri, Ryokan},\n  howpublished = {\\url{https://huggingface.co/datasets/sbintuitions/JMTEB}},\n  title = {{J}{M}{T}{E}{B}: {J}apanese {M}assive {T}ext {E}mbedding {B}enchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq","title":"NQ","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval</p> <p>Dataset: <code>mteb/nq</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-fa","title":"NQ-Fa","text":"<p>NQ-Fa</p> <p>Dataset: <code>MCINext/nq-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-fahardnegatives","title":"NQ-FaHardNegatives","text":"<p>NQ-FaHardNegatives</p> <p>Dataset: <code>MCINext/NQ_FA_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Encyclopaedic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-nl","title":"NQ-NL","text":"<p>NQ-NL is a translation of NQ</p> <p>Dataset: <code>clips/beir-nl-nq</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Encyclopaedic, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-pl","title":"NQ-PL","text":"<p>Natural Questions: A Benchmark for Question Answering Research</p> <p>Dataset: <code>mteb/NQ-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-plhardnegatives","title":"NQ-PLHardNegatives","text":"<p>Natural Questions: A Benchmark for Question Answering Research. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NQ-PLHardNegatives</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Encyclopaedic, Written human-annotated machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nq-vn","title":"NQ-VN","text":"<p>A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nq-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nqhardnegatives","title":"NQHardNegatives","text":"<p>NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NQ_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoarguanaretrieval","title":"NanoArguAnaRetrieval","text":"<p>NanoArguAna is a smaller subset of ArguAna, a dataset for argument retrieval in debate contexts.</p> <p>Dataset: <code>zeta-alpha-ai/NanoArguAna</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social, Web, Written derived found Citation <pre><code>@inproceedings{wachsmuth2018retrieval,\n  author = {Wachsmuth, Henning and Syed, Shahbaz and Stein, Benno},\n  booktitle = {ACL},\n  title = {Retrieval of the Best Counterargument without Prior Topic Knowledge},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoclimatefever-vn","title":"NanoClimateFEVER-VN","text":"<p>NanoClimateFEVERVN is a small version of A translated dataset from CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-climate-fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoclimatefeverretrieval","title":"NanoClimateFeverRetrieval","text":"<p>NanoClimateFever is a small version of the BEIR dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change.</p> <p>Dataset: <code>zeta-alpha-ai/NanoClimateFEVER</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, News, Non-fiction expert-annotated found Citation <pre><code>@misc{diggelmann2021climatefever,\n  archiveprefix = {arXiv},\n  author = {Thomas Diggelmann and Jordan Boyd-Graber and Jannis Bulian and Massimiliano Ciaramita and Markus Leippold},\n  eprint = {2012.00614},\n  primaryclass = {cs.CL},\n  title = {CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanodbpedia-vn","title":"NanoDBPedia-VN","text":"<p>NanoDBPediaVN is a small version of A translated dataset from DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-dbpedia-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanodbpediaretrieval","title":"NanoDBPediaRetrieval","text":"<p>NanoDBPediaRetrieval is a small version of the standard test collection for entity search over the DBpedia knowledge base.</p> <p>Dataset: <code>zeta-alpha-ai/NanoDBPedia</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic expert-annotated found Citation <pre><code>@article{lehmann2015dbpedia,\n  author = {Lehmann, Jens and et al.},\n  journal = {Semantic Web},\n  title = {DBpedia: A large-scale, multilingual knowledge base extracted from Wikipedia},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanofever-vn","title":"NanoFEVER-VN","text":"<p>NanoFEVERVN is a small version of A translated dataset from FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-fever-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanofeverretrieval","title":"NanoFEVERRetrieval","text":"<p>NanoFEVER is a smaller version of FEVER (Fact Extraction and VERification), which consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from.</p> <p>Dataset: <code>zeta-alpha-ai/NanoFEVER</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Encyclopaedic expert-annotated found Citation <pre><code>@inproceedings{thorne-etal-2018-fever,\n  address = {New Orleans, Louisiana},\n  author = {Thorne, James  and\nVlachos, Andreas  and\nChristodoulopoulos, Christos  and\nMittal, Arpit},\n  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},\n  doi = {10.18653/v1/N18-1074},\n  editor = {Walker, Marilyn  and\nJi, Heng  and\nStent, Amanda},\n  month = jun,\n  pages = {809--819},\n  publisher = {Association for Computational Linguistics},\n  title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},\n  url = {https://aclanthology.org/N18-1074},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanofiqa2018retrieval","title":"NanoFiQA2018Retrieval","text":"<p>NanoFiQA2018 is a smaller subset of the Financial Opinion Mining and Question Answering dataset.</p> <p>Dataset: <code>zeta-alpha-ai/NanoFiQA2018</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Social human-annotated found Citation <pre><code>@inproceedings{thakur2021beir,\n  author = {Nandan Thakur and Nils Reimers and Andreas R{\\\"u}ckl{\\'e} and Abhishek Srivastava and Iryna Gurevych},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},\n  title = {{BEIR}: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models},\n  url = {https://openreview.net/forum?id=wCu6T5xFjeJ},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanohotpotqa-vn","title":"NanoHotpotQA-VN","text":"<p>NanoHotpotQAVN is a small version of A translated dataset from HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-hotpotqa-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanohotpotqaretrieval","title":"NanoHotpotQARetrieval","text":"<p>NanoHotpotQARetrieval is a smaller subset of the HotpotQA dataset, which is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.</p> <p>Dataset: <code>zeta-alpha-ai/NanoHotpotQA</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web, Written human-annotated found Citation <pre><code>@inproceedings{yang-etal-2018-hotpotqa,\n  address = {Brussels, Belgium},\n  author = {Yang, Zhilin  and\nQi, Peng  and\nZhang, Saizheng  and\nBengio, Yoshua  and\nCohen, William  and\nSalakhutdinov, Ruslan  and\nManning, Christopher D.},\n  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D18-1259},\n  editor = {Riloff, Ellen  and\nChiang, David  and\nHockenmaier, Julia  and\nTsujii, Jun{'}ichi},\n  month = oct # {-} # nov,\n  pages = {2369--2380},\n  publisher = {Association for Computational Linguistics},\n  title = {{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},\n  url = {https://aclanthology.org/D18-1259},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanomsmarco-vn","title":"NanoMSMARCO-VN","text":"<p>NanoMSMARCOVN is a small version of A translated dataset from MS MARCO is a collection of datasets focused on deep learning in search The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-msmarco-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanomsmarcoretrieval","title":"NanoMSMARCORetrieval","text":"<p>NanoMSMARCORetrieval is a smaller subset of MS MARCO, a collection of datasets focused on deep learning in search.</p> <p>Dataset: <code>zeta-alpha-ai/NanoMSMARCO</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Web human-annotated found Citation <pre><code>@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanonfcorpusretrieval","title":"NanoNFCorpusRetrieval","text":"<p>NanoNFCorpus is a smaller subset of NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval.</p> <p>Dataset: <code>zeta-alpha-ai/NanoNFCorpus</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@inproceedings{boteva2016,\n  author = {Boteva, Vera and Gholipour, Demian and Sokolov, Artem and Riezler, Stefan},\n  city = {Padova},\n  country = {Italy},\n  journal = {Proceedings of the 38th European Conference on Information Retrieval},\n  journal-abbrev = {ECIR},\n  title = {A Full-Text Learning to Rank Dataset for Medical Information Retrieval},\n  url = {http://www.cl.uni-heidelberg.de/~riezler/publications/papers/ECIR2016.pdf},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanonq-vn","title":"NanoNQ-VN","text":"<p>NanoNQVN is a small version of A translated dataset from NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/nano-nq-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanonqretrieval","title":"NanoNQRetrieval","text":"<p>NanoNQ is a smaller subset of a dataset which contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.</p> <p>Dataset: <code>zeta-alpha-ai/NanoNQ</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Web human-annotated found Citation <pre><code>@article{47761,\n  author = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh\nand Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee\nand Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le\nand Slav Petrov},\n  journal = {Transactions of the Association of Computational\nLinguistics},\n  title = {Natural Questions: a Benchmark for Question Answering Research},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoquoraretrieval","title":"NanoQuoraRetrieval","text":"<p>NanoQuoraRetrieval is a smaller subset of the QuoraRetrieval dataset, which is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>zeta-alpha-ai/NanoQuoraRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Social human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoscidocsretrieval","title":"NanoSCIDOCSRetrieval","text":"<p>NanoFiQA2018 is a smaller subset of SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>zeta-alpha-ai/NanoSCIDOCS</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written expert-annotated found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanoscifactretrieval","title":"NanoSciFactRetrieval","text":"<p>NanoSciFact is a smaller subset of SciFact, which verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>zeta-alpha-ai/NanoSciFact</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#nanotouche2020retrieval","title":"NanoTouche2020Retrieval","text":"<p>NanoTouche2020 is a smaller subset of Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions.</p> <p>Dataset: <code>zeta-alpha-ai/NanoTouche2020</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@dataset{potthast_2022_6862281,\n  author = {Potthast, Martin and\nGienapp, Lukas and\nWachsmuth, Henning and\nHagen, Matthias and\nFr\u00f6be, Maik and\nBondarenko, Alexander and\nAjjour, Yamen and\nStein, Benno},\n  doi = {10.5281/zenodo.6862281},\n  month = jul,\n  publisher = {Zenodo},\n  title = {{Touch\u00e920-Argument-Retrieval-for-Controversial-\nQuestions}},\n  url = {https://doi.org/10.5281/zenodo.6862281},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#narrativeqaretrieval","title":"NarrativeQARetrieval","text":"<p>NarrativeQA is a dataset for the task of question answering on long narratives. It consists of realistic QA instances collected from literature (fiction and non-fiction) and movie scripts. </p> <p>Dataset: <code>deepmind/narrativeqa</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Fiction, Non-fiction, Written human-annotated found Citation <pre><code>@article{kocisky-etal-2018-narrativeqa,\n  address = {Cambridge, MA},\n  author = {Ko{\\v{c}}isk{\\'y}, Tom{\\'a}{\\v{s}}  and\nSchwarz, Jonathan  and\nBlunsom, Phil  and\nDyer, Chris  and\nHermann, Karl Moritz  and\nMelis, G{\\'a}bor  and\nGrefenstette, Edward},\n  doi = {10.1162/tacl_a_00023},\n  editor = {Lee, Lillian  and\nJohnson, Mark  and\nToutanova, Kristina  and\nRoark, Brian},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {317--328},\n  publisher = {MIT Press},\n  title = {The {N}arrative{QA} Reading Comprehension Challenge},\n  url = {https://aclanthology.org/Q18-1023},\n  volume = {6},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2022retrieval","title":"NeuCLIR2022Retrieval","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries.</p> <p>Dataset: <code>mteb/NeuCLIR2022Retrieval</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{lawrie2023overview,\n  author = {Lawrie, Dawn and MacAvaney, Sean and Mayfield, James and McNamee, Paul and Oard, Douglas W and Soldaini, Luca and Yang, Eugene},\n  journal = {arXiv preprint arXiv:2304.12367},\n  title = {Overview of the TREC 2022 NeuCLIR track},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2022retrievalhardnegatives","title":"NeuCLIR2022RetrievalHardNegatives","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NeuCLIR2022RetrievalHardNegatives</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@article{lawrie2023overview,\n  author = {Lawrie, Dawn and MacAvaney, Sean and Mayfield, James and McNamee, Paul and Oard, Douglas W and Soldaini, Luca and Yang, Eugene},\n  journal = {arXiv preprint arXiv:2304.12367},\n  title = {Overview of the TREC 2022 NeuCLIR track},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2023retrieval","title":"NeuCLIR2023Retrieval","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries.</p> <p>Dataset: <code>mteb/NeuCLIR2022Retrieval</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@misc{lawrie2024overview,\n  archiveprefix = {arXiv},\n  author = {Dawn Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},\n  eprint = {2404.08071},\n  primaryclass = {cs.IR},\n  title = {Overview of the TREC 2023 NeuCLIR Track},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#neuclir2023retrievalhardnegatives","title":"NeuCLIR2023RetrievalHardNegatives","text":"<p>The task involves identifying and retrieving the documents that are relevant to the queries. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/NeuCLIR2023RetrievalHardNegatives</code> \u2022 License: odc-by \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_20 fas, rus, zho News, Written expert-annotated found Citation <pre><code>@misc{lawrie2024overview,\n  archiveprefix = {arXiv},\n  author = {Dawn Lawrie and Sean MacAvaney and James Mayfield and Paul McNamee and Douglas W. Oard and Luca Soldaini and Eugene Yang},\n  eprint = {2404.08071},\n  primaryclass = {cs.IR},\n  title = {Overview of the TREC 2023 NeuCLIR Track},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#norquadretrieval","title":"NorQuadRetrieval","text":"<p>Human-created question for Norwegian wikipedia passages.</p> <p>Dataset: <code>mteb/norquad_retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{ivanova-etal-2023-norquad,\n  address = {T{\\'o}rshavn, Faroe Islands},\n  author = {Ivanova, Sardana  and\nAndreassen, Fredrik  and\nJentoft, Matias  and\nWold, Sondre  and\n{\\O}vrelid, Lilja},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Alum{\\\"a}e, Tanel  and\nFishel, Mark},\n  month = may,\n  pages = {159--168},\n  publisher = {University of Tartu Library},\n  title = {{N}or{Q}u{AD}: {N}orwegian Question Answering Dataset},\n  url = {https://aclanthology.org/2023.nodalida-1.17},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#opentenderretrieval","title":"OpenTenderRetrieval","text":"<p>This dataset contains Belgian and Dutch tender calls from OpenTender in Dutch</p> <p>Dataset: <code>clips/mteb-nl-opentender-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Government, Written derived found Citation <pre><code>@misc{banar2025mtebnle5nlembeddingbenchmark,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans},\n  eprint = {2509.12340},\n  primaryclass = {cs.CL},\n  title = {MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch},\n  url = {https://arxiv.org/abs/2509.12340},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#piqa","title":"PIQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on PIQA.</p> <p>Dataset: <code>mteb/PIQA</code> \u2022 License: afl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{bisk2020piqa,\n  author = {Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},\n  booktitle = {Proceedings of the AAAI conference on artificial intelligence},\n  number = {05},\n  pages = {7432--7439},\n  title = {Piqa: Reasoning about physical commonsense in natural language},\n  volume = {34},\n  year = {2020},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#puggretrieval","title":"PUGGRetrieval","text":"<p>Information Retrieval PUGG dataset for the Polish language.</p> <p>Dataset: <code>clarin-pl/PUGG_IR</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web human-annotated multiple Citation <pre><code>@inproceedings{sawczyn-etal-2024-developing,\n  address = {Bangkok, Thailand},\n  author = {Sawczyn, Albert  and\nViarenich, Katsiaryna  and\nWojtasik, Konrad  and\nDomoga{\\l}a, Aleksandra  and\nOleksy, Marcin  and\nPiasecki, Maciej  and\nKajdanowicz, Tomasz},\n  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},\n  doi = {10.18653/v1/2024.findings-acl.652},\n  editor = {Ku, Lun-Wei  and\nMartins, Andre  and\nSrikumar, Vivek},\n  month = aug,\n  pages = {10978--10996},\n  publisher = {Association for Computational Linguistics},\n  title = {Developing {PUGG} for {P}olish: A Modern Approach to {KBQA}, {MRC}, and {IR} Dataset Construction},\n  url = {https://aclanthology.org/2024.findings-acl.652/},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#persianwebdocumentretrieval","title":"PersianWebDocumentRetrieval","text":"<p>Persian dataset designed specifically for the task of text information retrieval through the web.</p> <p>Dataset: <code>MCINext/persian-web-document-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#publichealthqa","title":"PublicHealthQA","text":"<p>A multilingual dataset for public health question answering, based on FAQ sourced from CDC and WHO.</p> <p>Dataset: <code>xhluca/publichealth-qa</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, eng, fra, kor, rus, ... (8) Government, Medical, Web, Written derived found Citation <pre><code>@misc{xing_han_lu_2024,\n  author = { {Xing Han Lu} },\n  doi = { 10.57967/hf/2247 },\n  publisher = { Hugging Face },\n  title = { publichealth-qa (Revision 3b67b6b) },\n  url = { https://huggingface.co/datasets/xhluca/publichealth-qa },\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quail","title":"Quail","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on Quail.</p> <p>Dataset: <code>mteb/Quail</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{rogers2020getting,\n  author = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},\n  booktitle = {Proceedings of the AAAI conference on artificial intelligence},\n  number = {05},\n  pages = {8722--8731},\n  title = {Getting closer to AI complete question answering: A set of prerequisite real tasks},\n  volume = {34},\n  year = {2020},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-nl","title":"Quora-NL","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. QuoraRetrieval-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-quora</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-pl","title":"Quora-PL","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>mteb/Quora-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-plhardnegatives","title":"Quora-PLHardNegatives","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/Quora-PLHardNegatives</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Web, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quora-vn","title":"Quora-VN","text":"<p>A translated dataset from QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/quora-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Blog, Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval","title":"QuoraRetrieval","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions.</p> <p>Dataset: <code>mteb/quora</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval-fa","title":"QuoraRetrieval-Fa","text":"<p>QuoraRetrieval-Fa</p> <p>Dataset: <code>MCINext/quora-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrieval-fav2","title":"QuoraRetrieval-Fa.v2","text":"<p>QuoraRetrieval-Fa.v2</p> <p>Dataset: <code>MCINext/quora-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrievalhardnegatives","title":"QuoraRetrievalHardNegatives","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/QuoraRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#quoraretrievalhardnegativesv2","title":"QuoraRetrievalHardNegatives.v2","text":"<p>QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/QuoraRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Blog, Web, Written human-annotated found Citation <pre><code>@misc{quora-question-pairs,\n  author = {DataCanary, hilfialkaff, Lili Jiang, Meg Risdal, Nikhil Dandekar, tomtung},\n  publisher = {Kaggle},\n  title = {Quora Question Pairs},\n  url = {https://kaggle.com/competitions/quora-question-pairs},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medbioinformaticsretrieval","title":"R2MEDBioinformaticsRetrieval","text":"<p>Bioinformatics retrieval dataset.</p> <p>Dataset: <code>R2MED/Bioinformatics</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medbiologyretrieval","title":"R2MEDBiologyRetrieval","text":"<p>Biology retrieval dataset.</p> <p>Dataset: <code>R2MED/Biology</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2mediiyiclinicalretrieval","title":"R2MEDIIYiClinicalRetrieval","text":"<p>IIYi-Clinical retrieval dataset.</p> <p>Dataset: <code>R2MED/IIYi-Clinical</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedqadiagretrieval","title":"R2MEDMedQADiagRetrieval","text":"<p>MedQA-Diag retrieval dataset.</p> <p>Dataset: <code>R2MED/MedQA-Diag</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedxpertqaexamretrieval","title":"R2MEDMedXpertQAExamRetrieval","text":"<p>MedXpertQA-Exam retrieval dataset.</p> <p>Dataset: <code>R2MED/MedXpertQA-Exam</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medmedicalsciencesretrieval","title":"R2MEDMedicalSciencesRetrieval","text":"<p>Medical-Sciences retrieval dataset.</p> <p>Dataset: <code>R2MED/Medical-Sciences</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medpmcclinicalretrieval","title":"R2MEDPMCClinicalRetrieval","text":"<p>PMC-Clinical retrieval dataset.</p> <p>Dataset: <code>R2MED/PMC-Clinical</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#r2medpmctreatmentretrieval","title":"R2MEDPMCTreatmentRetrieval","text":"<p>PMC-Treatment retrieval dataset.</p> <p>Dataset: <code>R2MED/PMC-Treatment</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Medical LM-generated and reviewed found Citation <pre><code>@article{li2025r2med,\n  author = {Li, Lei and Zhou, Xiao and Liu, Zheng},\n  journal = {arXiv preprint arXiv:2505.14558},\n  title = {R2MED: A Benchmark for Reasoning-Driven Medical Retrieval},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rarbcode","title":"RARbCode","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on RAR-b code-pooled dataset.</p> <p>Dataset: <code>mteb/RARbCode</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@article{husain2019codesearchnet,\n  author = {Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},\n  journal = {arXiv preprint arXiv:1909.09436},\n  title = {{CodeSearchNet} challenge: Evaluating the state of semantic code search},\n  year = {2019},\n}\n\n@article{muennighoff2023octopack,\n  author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and Von Werra, Leandro and Longpre, Shayne},\n  journal = {arXiv preprint arXiv:2308.07124},\n  title = {Octopack: Instruction tuning code large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rarbmath","title":"RARbMath","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on RAR-b math-pooled dataset.</p> <p>Dataset: <code>mteb/RARbMath</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{cobbe2021training,\n  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},\n  journal = {arXiv preprint arXiv:2110.14168},\n  title = {Training verifiers to solve math word problems},\n  year = {2021},\n}\n\n@article{hendrycks2021measuring,\n  author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},\n  journal = {arXiv preprint arXiv:2103.03874},\n  title = {Measuring mathematical problem solving with the math dataset},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n\n@article{yu2023metamath,\n  author = {Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},\n  journal = {arXiv preprint arXiv:2309.12284},\n  title = {Metamath: Bootstrap your own mathematical questions for large language models},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrieval","title":"RiaNewsRetrieval","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset.</p> <p>Dataset: <code>ai-forever/ria-news-retrieval</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrievalhardnegatives","title":"RiaNewsRetrievalHardNegatives","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rianewsretrievalhardnegativesv2","title":"RiaNewsRetrievalHardNegatives.v2","text":"<p>News article retrieval by headline. Based on Rossiya Segodnya dataset. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.V2 uses a more appropriate prompt rather than the default prompt for retrieval. You can get more information on the effect of different prompt in the PR</p> <p>Dataset: <code>mteb/RiaNewsRetrieval_test_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-nd-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus News, Written derived found Citation <pre><code>@inproceedings{gavrilov2018self,\n  author = {Gavrilov, Daniil and  Kalaidin, Pavel and  Malykh, Valentin},\n  booktitle = {Proceedings of the 41st European Conference on Information Retrieval},\n  title = {Self-Attentive Model for Headline Generation},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#rubqretrieval","title":"RuBQRetrieval","text":"<p>Paragraph retrieval based on RuBQ 2.0. Retrieve paragraphs from Wikipedia that answer the question.</p> <p>Dataset: <code>ai-forever/rubq-retrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 rus Encyclopaedic, Written human-annotated created Citation <pre><code>@inproceedings{RuBQ2021,\n  author = {Ivan Rybin and Vladislav Korablinov and Pavel Efimov and Pavel Braslavski},\n  booktitle = {ESWC},\n  pages = {532--547},\n  title = {RuBQ 2.0: An Innovated Russian Question Answering Dataset},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ruscibenchciteretrieval","title":"RuSciBenchCiteRetrieval","text":"<p>This task is focused on Direct Citation Prediction for scientific papers from eLibrary, Russia's largest electronic library of scientific publications. Given a query paper (title and abstract), the goal is to retrieve papers that are directly cited by it from a larger corpus of papers. The dataset for this task consists of 3,000 query papers, 15,000 relevant (cited) papers, and 75,000 irrelevant papers. The task is available for both Russian and English scientific texts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_cite_retrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#ruscibenchcociteretrieval","title":"RuSciBenchCociteRetrieval","text":"<p>This task focuses on Co-citation Prediction for scientific papers from eLibrary, Russia's largest electronic library of scientific publications. Given a query paper (title and abstract), the goal is to retrieve other papers that are co-cited with it. Two papers are considered co-cited if they are both cited by at least 5 of the same other papers. Similar to the Direct Citation task, this task employs a retrieval setup: for a given query paper, all other papers in the corpus that are not co-cited with it are considered negative examples. The task is available for both Russian and English scientific texts.</p> <p>Dataset: <code>mlsa-iai-msu-lab/ru_sci_bench_cocite_retrieval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, rus Academic, Non-fiction, Written derived found Citation <pre><code>@article{vatolin2024ruscibench,\n  author = {Vatolin, A. and Gerasimenko, N. and Ianina, A. and Vorontsov, K.},\n  doi = {10.1134/S1064562424602191},\n  issn = {1531-8362},\n  journal = {Doklady Mathematics},\n  month = {12},\n  number = {1},\n  pages = {S251--S260},\n  title = {RuSciBench: Open Benchmark for Russian and English Scientific Document Representations},\n  url = {https://doi.org/10.1134/S1064562424602191},\n  volume = {110},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs","title":"SCIDOCS","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>mteb/scidocs</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-fa","title":"SCIDOCS-Fa","text":"<p>SCIDOCS-Fa</p> <p>Dataset: <code>MCINext/scidocs-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-fav2","title":"SCIDOCS-Fa.v2","text":"<p>SCIDOCS-Fa.v2</p> <p>Dataset: <code>MCINext/scidocs-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-nl","title":"SCIDOCS-NL","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. SciDocs-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-scidocs</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-nlv2","title":"SCIDOCS-NL.v2","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. SciDocs-NL is a Dutch translation. This version adds a Dutch prompt to the dataset.</p> <p>Dataset: <code>clips/beir-nl-scidocs</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Non-fiction, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-pl","title":"SCIDOCS-PL","text":"<p>SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.</p> <p>Dataset: <code>mteb/SCIDOCS-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scidocs-vn","title":"SCIDOCS-VN","text":"<p>A translated dataset from SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/scidocs-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Non-fiction, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#siqa","title":"SIQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on SIQA.</p> <p>Dataset: <code>mteb/SIQA</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{sap2019socialiqa,\n  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},\n  journal = {arXiv preprint arXiv:1904.09728},\n  title = {Socialiqa: Commonsense reasoning about social interactions},\n  year = {2019},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#skquadretrieval","title":"SKQuadRetrieval","text":"<p>Retrieval SK Quad evaluates Slovak search performance using questions and answers derived from the SK-QuAD dataset. It measures relevance with scores assigned to answers based on their relevancy to corresponding questions, which is vital for improving Slovak language search systems.</p> <p>Dataset: <code>TUKE-KEMT/retrieval-skquad</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 slk Encyclopaedic human-annotated found"},{"location":"overview/available_tasks/retrieval/#snlretrieval","title":"SNLRetrieval","text":"<p>Webscrabed articles and ingresses from the Norwegian lexicon 'Det Store Norske Leksikon'.</p> <p>Dataset: <code>adrlau/navjordj-SNL_summarization_copy</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nob Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@mastersthesis{navjord2023beyond,\n  author = {Navjord, J{\\o}rgen Johnsen and Korsvik, Jon-Mikkel Ryen},\n  school = {Norwegian University of Life Sciences, {\\AA}s},\n  title = {Beyond extractive: advancing abstractive automatic text summarization in Norwegian with transformers},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#squadkorv1retrieval","title":"SQuADKorV1Retrieval","text":"<p>Korean translation of SQuAD v1.0 dataset for retrieval task, based on Korean Wikipedia articles.</p> <p>Dataset: <code>yjoonjang/squad_kor_v1</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 kor Encyclopaedic, Written derived found Citation <pre><code>@inproceedings{rajpurkar-etal-2016-squad,\n  address = {Austin, Texas},\n  author = {Rajpurkar, Pranav  and\nZhang, Jian  and\nLopyrev, Konstantin  and\nLiang, Percy},\n  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/D16-1264},\n  editor = {Su, Jian  and\nDuh, Kevin  and\nCarreras, Xavier},\n  month = nov,\n  pages = {2383--2392},\n  publisher = {Association for Computational Linguistics},\n  title = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},\n  url = {https://aclanthology.org/D16-1264},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#sadeemquestionretrieval","title":"SadeemQuestionRetrieval","text":"<p>SadeemQuestion: A Benchmark Data Set for Community Question-Retrieval Research</p> <p>Dataset: <code>sadeem-ai/sadeem-ar-eval-retrieval-questions</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara Written, Written derived found Citation <pre><code>@inproceedings{sadeem-2024-ar-retrieval-questions,\n  author = {abubakr.soliman@sadeem.app},\n  title = {SadeemQuestionRetrieval: A New Benchmark for Arabic questions-based Articles Searching.},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact","title":"SciFact","text":"<p>SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>mteb/scifact</code> \u2022 License: cc-by-nc-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@inproceedings{specter2020cohan,\n  author = {Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},\n  booktitle = {ACL},\n  title = {SPECTER: Document-level Representation Learning using Citation-informed Transformers},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-fa","title":"SciFact-Fa","text":"<p>SciFact-Fa</p> <p>Dataset: <code>MCINext/scifact-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-fav2","title":"SciFact-Fa.v2","text":"<p>SciFact-Fa.v2</p> <p>Dataset: <code>MCINext/scifact-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Academic derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-nl","title":"SciFact-NL","text":"<p>SciFactNL verifies scientific claims in Dutch using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>clips/beir-nl-scifact</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-nlv2","title":"SciFact-NL.v2","text":"<p>SciFactNL verifies scientific claims in Dutch using evidence from the research literature containing scientific paper abstracts. This version adds a Dutch prompt to the dataset.</p> <p>Dataset: <code>clips/beir-nl-scifact</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-pl","title":"SciFact-PL","text":"<p>SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts.</p> <p>Dataset: <code>mteb/SciFact-PL</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Medical, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#scifact-vn","title":"SciFact-VN","text":"<p>A translated dataset from SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/scifact-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#slovaksumretrieval","title":"SlovakSumRetrieval","text":"<p>SlovakSum, a Slovak news summarization dataset consisting of over 200 thousand news articles with titles and short abstracts obtained from multiple Slovak newspapers. Originally intended as a summarization task, but since no human annotations were provided here reformulated to a retrieval task.</p> <p>Dataset: <code>NaiveNeuron/slovaksum</code> \u2022 License: openrail \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 slk News, Social, Web, Written derived found Citation <pre><code>@inproceedings{OndrejowaSlovakSum24,\n  author = {Ondrejov\u00e1, Vikt\u00f3ria and \u0160uppa, Marek},\n  booktitle = {Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation},\n  date = {2024},\n  title = {SlovakSum: A Large Scale Slovak Summarization Dataset},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spanishpassageretrievals2p","title":"SpanishPassageRetrievalS2P","text":"<p>Test collection for passage retrieval from health-related Web resources in Spanish.</p> <p>Dataset: <code>mteb/SpanishPassageRetrievalS2P</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 spa Medical, Web, Written human-annotated found Citation <pre><code>@inproceedings{10.1007/978-3-030-15719-7_19,\n  address = {Cham},\n  author = {Kamateri, Eleni\nand Tsikrika, Theodora\nand Symeonidis, Spyridon\nand Vrochidis, Stefanos\nand Minker, Wolfgang\nand Kompatsiaris, Yiannis},\n  booktitle = {Advances in Information Retrieval},\n  editor = {Azzopardi, Leif\nand Stein, Benno\nand Fuhr, Norbert\nand Mayr, Philipp\nand Hauff, Claudia\nand Hiemstra, Djoerd},\n  isbn = {978-3-030-15719-7},\n  pages = {148--154},\n  publisher = {Springer International Publishing},\n  title = {A Test Collection for Passage Retrieval Evaluation of Spanish Health-Related Resources},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spanishpassageretrievals2s","title":"SpanishPassageRetrievalS2S","text":"<p>Test collection for passage retrieval from health-related Web resources in Spanish.</p> <p>Dataset: <code>mteb/SpanishPassageRetrievalS2S</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 spa Medical, Web, Written human-annotated found Citation <pre><code>@inproceedings{10.1007/978-3-030-15719-7_19,\n  address = {Cham},\n  author = {Kamateri, Eleni\nand Tsikrika, Theodora\nand Symeonidis, Spyridon\nand Vrochidis, Stefanos\nand Minker, Wolfgang\nand Kompatsiaris, Yiannis},\n  booktitle = {Advances in Information Retrieval},\n  editor = {Azzopardi, Leif\nand Stein, Benno\nand Fuhr, Norbert\nand Mayr, Philipp\nand Hauff, Claudia\nand Hiemstra, Djoerd},\n  isbn = {978-3-030-15719-7},\n  pages = {148--154},\n  publisher = {Springer International Publishing},\n  title = {A Test Collection for Passage Retrieval Evaluation of Spanish Health-Related Resources},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#spartqa","title":"SpartQA","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on SpartQA.</p> <p>Dataset: <code>mteb/SpartQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{mirzaee2021spartqa,\n  author = {Mirzaee, Roshanak and Faghihi, Hossein Rajaby and Ning, Qiang and Kordjmashidi, Parisa},\n  journal = {arXiv preprint arXiv:2104.05832},\n  title = {Spartqa:: A textual question answering benchmark for spatial reasoning},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#stackoverflowqa","title":"StackOverflowQA","text":"<p>The dataset is a collection of natural language queries and their corresponding response which may include some text mixed with code snippets. The task is to retrieve the most relevant response for a given query.</p> <p>Dataset: <code>mteb/StackOverflowQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Programming, Written derived found Citation <pre><code>@misc{li2024coircomprehensivebenchmarkcode,\n  archiveprefix = {arXiv},\n  author = {Xiangyang Li and Kuicai Dong and Yi Quan Lee and Wei Xia and Yichun Yin and Hao Zhang and Yong Liu and Yasheng Wang and Ruiming Tang},\n  eprint = {2407.02883},\n  primaryclass = {cs.IR},\n  title = {CoIR: A Comprehensive Benchmark for Code Information Retrieval Models},\n  url = {https://arxiv.org/abs/2407.02883},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#statcandialoguedatasetretrieval","title":"StatcanDialogueDatasetRetrieval","text":"<p>A Dataset for Retrieving Data Tables through Conversations with Genuine Intents, available in English and French.</p> <p>Dataset: <code>mteb/StatcanDialogueDatasetRetrieval</code> \u2022 License: https://huggingface.co/datasets/McGill-NLP/statcan-dialogue-dataset-retrieval/blob/main/LICENSE.md \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) recall_at_10 eng, fra Government, Web, Written derived found Citation <pre><code>@inproceedings{lu-etal-2023-statcan,\n  address = {Dubrovnik, Croatia},\n  author = {Lu, Xing Han  and\nReddy, Siva  and\nde Vries, Harm},\n  booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},\n  month = may,\n  pages = {2799--2829},\n  publisher = {Association for Computational Linguistics},\n  title = {The {S}tat{C}an Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents},\n  url = {https://arxiv.org/abs/2304.01412},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#swefaqretrieval","title":"SweFaqRetrieval","text":"<p>A Swedish QA dataset derived from FAQ</p> <p>Dataset: <code>mteb/SweFaqRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 swe Government, Non-fiction, Written derived found Citation <pre><code>@inproceedings{berdivcevskis2023superlim,\n  author = {Berdi{\\v{c}}evskis, Aleksandrs and Bouma, Gerlof and Kurtz, Robin and Morger, Felix and {\\\"O}hman, Joey and Adesam, Yvonne and Borin, Lars and Dann{\\'e}lls, Dana and Forsberg, Markus and Isbister, Tim and others},\n  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},\n  pages = {8137--8153},\n  title = {Superlim: A Swedish language understanding evaluation benchmark},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#swednretrieval","title":"SwednRetrieval","text":"<p>The SWE-DN corpus is based on 1,963,576 news articles from the Swedish newspaper Dagens Nyheter (DN) during the years 2000--2020. The articles are filtered to resemble the CNN/DailyMail dataset both regarding textual structure</p> <p>Dataset: <code>mteb/SwednRetrieval</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 swe News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{monsen2021method,\n  author = {Monsen, Julius and J{\\\"o}nsson, Arne},\n  booktitle = {Proceedings of CLARIN Annual Conference},\n  title = {A method for building non-english corpora for abstractive text summarization},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synperchatbotragfaqretrieval","title":"SynPerChatbotRAGFAQRetrieval","text":"<p>Synthetic Persian Chatbot RAG FAQ Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-faq-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synperchatbotragtopicsretrieval","title":"SynPerChatbotRAGTopicsRetrieval","text":"<p>Synthetic Persian Chatbot RAG Topics Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-rag-topics-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synperchatbottopicsretrieval","title":"SynPerChatbotTopicsRetrieval","text":"<p>Synthetic Persian Chatbot Topics Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-chatbot-topics-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken LM-generated LM-generated and verified Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synperqaretrieval","title":"SynPerQARetrieval","text":"<p>Synthetic Persian QA Retrieval</p> <p>Dataset: <code>MCINext/synthetic-persian-qa-retrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Web LM-generated LM-generated and verified Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#syntecretrieval","title":"SyntecRetrieval","text":"<p>This dataset has been built from the Syntec Collective bargaining agreement.</p> <p>Dataset: <code>lyon-nlp/mteb-fr-retrieval-syntec-s2p</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fra Legal, Written human-annotated created Citation <pre><code>@misc{ciancone2024extending,\n  archiveprefix = {arXiv},\n  author = {Mathieu Ciancone and Imene Kerboua and Marion Schaeffer and Wissam Siblini},\n  eprint = {2405.20468},\n  primaryclass = {cs.CL},\n  title = {Extending the Massive Text Embedding Benchmark to French},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#synthetictext2sql","title":"SyntheticText2SQL","text":"<p>The dataset is a collection of natural language queries and their corresponding sql snippets. The task is to retrieve the most relevant code snippet for a given query.</p> <p>Dataset: <code>CoIR-Retrieval/synthetic-text2sql</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, sql Programming, Written derived found Citation <pre><code>@software{gretel-synthetic-text-to-sql-2024,\n  author = {Meyer, Yev and Emadi, Marjan and Nathawani, Dhruv and Ramaswamy, Lipika and Boyd, Kendrick and Van Segbroeck, Maarten and Grossman, Matthew and Mlocek, Piotr and Newberry, Drew},\n  month = {April},\n  title = {{Synthetic-Text-To-SQL}: A synthetic dataset for training language models to generate SQL queries from natural language prompts},\n  url = {https://huggingface.co/datasets/gretelai/synthetic-text-to-sql},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#t2retrieval","title":"T2Retrieval","text":"<p>T2Ranking: A large-scale Chinese Benchmark for Passage Ranking</p> <p>Dataset: <code>mteb/T2Retrieval</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Academic, Financial, Government, Medical, Non-fiction human-annotated found Citation <pre><code>@misc{xie2023t2ranking,\n  archiveprefix = {arXiv},\n  author = {Xiaohui Xie and Qian Dong and Bingning Wang and Feiyang Lv and Ting Yao and Weinan Gan and Zhijing Wu and Xiangsheng Li and Haitao Li and Yiqun Liu and Jin Ma},\n  eprint = {2304.03679},\n  primaryclass = {cs.IR},\n  title = {T2Ranking: A large-scale Chinese Benchmark for Passage Ranking},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid","title":"TRECCOVID","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic.</p> <p>Dataset: <code>mteb/trec-covid</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Medical, Written expert-annotated found Citation <pre><code>@misc{roberts2021searching,\n  archiveprefix = {arXiv},\n  author = {Kirk Roberts and Tasmeer Alam and Steven Bedrick and Dina Demner-Fushman and Kyle Lo and Ian Soboroff and Ellen Voorhees and Lucy Lu Wang and William R Hersh},\n  eprint = {2104.09632},\n  primaryclass = {cs.IR},\n  title = {Searching for Scientific Evidence in a Pandemic: An Overview of TREC-COVID},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-fa","title":"TRECCOVID-Fa","text":"<p>TRECCOVID-Fa</p> <p>Dataset: <code>MCINext/trec-covid-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-fav2","title":"TRECCOVID-Fa.v2","text":"<p>TRECCOVID-Fa.v2</p> <p>Dataset: <code>MCINext/trec-covid-fa-v2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Medical derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-nl","title":"TRECCOVID-NL","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic. TRECCOVID-NL is a Dutch translation. </p> <p>Dataset: <code>clips/beir-nl-trec-covid</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Medical, Written derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-pl","title":"TRECCOVID-PL","text":"<p>TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic.</p> <p>Dataset: <code>mteb/TRECCOVID-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic, Medical, Non-fiction, Written derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#treccovid-vn","title":"TRECCOVID-VN","text":"<p>A translated dataset from TRECCOVID is an ad-hoc search challenge based on the COVID-19 dataset containing scientific articles related to the COVID-19 pandemic. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/trec-covid-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic, Medical, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#trecdl2019","title":"TRECDL2019","text":"<p>TREC Deep Learning Track 2019 passage ranking task. The task involves retrieving relevant passages from the MS MARCO collection given web search queries. Queries have multi-graded relevance judgments.</p> <p>Dataset: <code>whybe-choi/trec-dl-2019</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@inproceedings{craswell2020overview,\n  author = {Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},\n  booktitle = {Proceedings of the 28th Text REtrieval Conference (TREC 2019)},\n  organization = {NIST},\n  title = {Overview of the TREC 2019 deep learning track},\n  year = {2020},\n}\n\n@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#trecdl2020","title":"TRECDL2020","text":"<p>TREC Deep Learning Track 2020 passage ranking task. The task involves retrieving relevant passages from the MS MARCO collection given web search queries. Queries have multi-graded relevance judgments.</p> <p>Dataset: <code>whybe-choi/trec-dl-2020</code> \u2022 License: msr-la-nc \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic, Blog, Encyclopaedic, Government, Medical, ... (10) derived found Citation <pre><code>@inproceedings{craswell2021overview,\n  author = {Craswell, Nick and Mitra, Bhaskar and Yilmaz, Emine and Campos, Daniel and Voorhees, Ellen M},\n  booktitle = {Proceedings of the 29th Text REtrieval Conference (TREC 2020)},\n  organization = {NIST},\n  title = {Overview of the TREC 2020 deep learning track},\n  year = {2021},\n}\n\n@article{DBLP:journals/corr/NguyenRSGTMD16,\n  archiveprefix = {arXiv},\n  author = {Tri Nguyen and\nMir Rosenberg and\nXia Song and\nJianfeng Gao and\nSaurabh Tiwary and\nRangan Majumder and\nLi Deng},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},\n  eprint = {1611.09268},\n  journal = {CoRR},\n  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},\n  title = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},\n  url = {http://arxiv.org/abs/1611.09268},\n  volume = {abs/1611.09268},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tv2nordretrieval","title":"TV2Nordretrieval","text":"<p>News Article and corresponding summaries extracted from the Danish newspaper TV2 Nord.</p> <p>Dataset: <code>alexandrainst/nordjylland-news-summarization</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan News, Non-fiction, Written derived found Citation <pre><code>@inproceedings{flansmose-mikkelsen-etal-2022-ddisco,\n  address = {Marseille, France},\n  author = {Flansmose Mikkelsen, Linea  and\nKinch, Oliver  and\nJess Pedersen, Anders  and\nLacroix, Oph{\\'e}lie},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2440--2445},\n  publisher = {European Language Resources Association},\n  title = {{DD}is{C}o: A Discourse Coherence Dataset for {D}anish},\n  url = {https://aclanthology.org/2022.lrec-1.260},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tvplretrieval","title":"TVPLRetrieval","text":"<p>A Vietnamese dataset for evaluating legal text retrieval. From Thu vien phap luat (TVPL) dataset: Optimizing Answer Generator in Vietnamese Legal Question Answering Systems Using Language Models.</p> <p>Dataset: <code>GreenNode/TVPL-Retrieval-VN</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Legal human-annotated found Citation <pre><code>@article{10.1145/3732938,\n  address = {New York, NY, USA},\n  author = {Le, Huong and Luu, Ngoc and Nguyen, Thanh and Dao, Tuan and Dinh, Sang},\n  doi = {10.1145/3732938},\n  issn = {2375-4699},\n  journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},\n  publisher = {Association for Computing Machinery},\n  title = {Optimizing Answer Generator in Vietnamese Legal Question Answering Systems Using Language Models},\n  url = {https://doi.org/10.1145/3732938},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl1","title":"TempReasonL1","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l1.</p> <p>Dataset: <code>mteb/TempReasonL1</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2context","title":"TempReasonL2Context","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-context.</p> <p>Dataset: <code>mteb/TempReasonL2Context</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2fact","title":"TempReasonL2Fact","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-fact.</p> <p>Dataset: <code>mteb/TempReasonL2Fact</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl2pure","title":"TempReasonL2Pure","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l2-pure.</p> <p>Dataset: <code>mteb/TempReasonL2Pure</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3context","title":"TempReasonL3Context","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-context.</p> <p>Dataset: <code>mteb/TempReasonL3Context</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3fact","title":"TempReasonL3Fact","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-fact.</p> <p>Dataset: <code>mteb/TempReasonL3Fact</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#tempreasonl3pure","title":"TempReasonL3Pure","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on TempReason l3-pure.</p> <p>Dataset: <code>mteb/TempReasonL3Pure</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{tan2023towards,\n  author = {Tan, Qingyu and Ng, Hwee Tou and Bing, Lidong},\n  journal = {arXiv preprint arXiv:2306.08952},\n  title = {Towards benchmarking and improving the temporal reasoning capability of large language models},\n  year = {2023},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#topiocqa","title":"TopiOCQA","text":"<p>TopiOCQA (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) is information-seeking conversational dataset with challenging topic switching phenomena. It consists of conversation histories along with manually labelled relevant/gold passage.</p> <p>Dataset: <code>mteb/TopiOCQA</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{adlakha2022topiocqa,\n  archiveprefix = {arXiv},\n  author = {Vaibhav Adlakha and Shehzaad Dhuliawala and Kaheer Suleman and Harm de Vries and Siva Reddy},\n  eprint = {2110.00768},\n  primaryclass = {cs.CL},\n  title = {TopiOCQA: Open-domain Conversational Question Answering with Topic Switching},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#topiocqahardnegatives","title":"TopiOCQAHardNegatives","text":"<p>TopiOCQA (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) is information-seeking conversational dataset with challenging topic switching phenomena. It consists of conversation histories along with manually labelled relevant/gold passage. The hard negative version has been created by pooling the 250 top documents per query from BM25, e5-multilingual-large and e5-mistral-instruct.</p> <p>Dataset: <code>mteb/TopiOCQA_validation_top_250_only_w_correct-v2</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written human-annotated found Citation <pre><code>@misc{adlakha2022topiocqa,\n  archiveprefix = {arXiv},\n  author = {Vaibhav Adlakha and Shehzaad Dhuliawala and Kaheer Suleman and Harm de Vries and Siva Reddy},\n  eprint = {2110.00768},\n  primaryclass = {cs.CL},\n  title = {TopiOCQA: Open-domain Conversational Question Answering with Topic Switching},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020","title":"Touche2020","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/touche2020</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@dataset{potthast_2022_6862281,\n  author = {Potthast, Martin and\nGienapp, Lukas and\nWachsmuth, Henning and\nHagen, Matthias and\nFr\u00f6be, Maik and\nBondarenko, Alexander and\nAjjour, Yamen and\nStein, Benno},\n  doi = {10.5281/zenodo.6862281},\n  month = jul,\n  publisher = {Zenodo},\n  title = {{Touch\u00e920-Argument-Retrieval-for-Controversial-\nQuestions}},\n  url = {https://doi.org/10.5281/zenodo.6862281},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-fa","title":"Touche2020-Fa","text":"<p>Touche2020-Fa</p> <p>Dataset: <code>MCINext/touche2020-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-fav2","title":"Touche2020-Fa.v2","text":"<p>Touche2020-Fa.v2</p> <p>Dataset: <code>MCINext/webis-touche2020-v3-fa</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 fas Spoken derived found Citation <pre><code>@article{zinvandi2025famteb,\n  author = {Zinvandi, Erfan and Alikhani, Morteza and Sarmadi, Mehran and Pourbahman, Zahra and Arvin, Sepehr and Kazemi, Reza and Amini, Arash},\n  journal = {arXiv preprint arXiv:2502.11571},\n  title = {Famteb: Massive text embedding benchmark in persian language},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-nl","title":"Touche2020-NL","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions. Touche2020-NL is a Dutch translation.</p> <p>Dataset: <code>clips/beir-nl-webis-touche2020</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Non-fiction derived machine-translated and verified Citation <pre><code>@misc{banar2024beirnlzeroshotinformationretrieval,\n  archiveprefix = {arXiv},\n  author = {Nikolay Banar and Ehsan Lotfi and Walter Daelemans},\n  eprint = {2412.08329},\n  primaryclass = {cs.CL},\n  title = {BEIR-NL: Zero-shot Information Retrieval Benchmark for the Dutch Language},\n  url = {https://arxiv.org/abs/2412.08329},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-pl","title":"Touche2020-PL","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/Touche2020-PL</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 pol Academic derived machine-translated Citation <pre><code>@misc{wojtasik2024beirpl,\n  archiveprefix = {arXiv},\n  author = {Konrad Wojtasik and Vadim Shishkin and Kacper Wo\u0142owiec and Arkadiusz Janz and Maciej Piasecki},\n  eprint = {2305.19840},\n  primaryclass = {cs.IR},\n  title = {BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020-vn","title":"Touche2020-VN","text":"<p>A translated dataset from Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/webis-touche2020-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Academic derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#touche2020retrievalv3","title":"Touche2020Retrieval.v3","text":"<p>Touch\u00e9 Task 1: Argument Retrieval for Controversial Questions</p> <p>Dataset: <code>mteb/webis-touche2020-v3</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Academic human-annotated found Citation <pre><code>@inproceedings{Thakur_etal_SIGIR2024,\n  address_ = {Washington, D.C.},\n  author = {Nandan Thakur and Luiz Bonifacio and Maik {Fr\\\"{o}be} and Alexander Bondarenko and Ehsan Kamalloo and Martin Potthast and Matthias Hagen and Jimmy Lin},\n  booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n  title = {Systematic Evaluation of Neural Retrieval Models on the {Touch\\'{e}} 2020 Argument Retrieval Subset of {BEIR}},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#turhistquadretrieval","title":"TurHistQuadRetrieval","text":"<p>Question Answering dataset on Ottoman History in Turkish</p> <p>Dataset: <code>asparius/TurHistQuAD</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 tur Academic, Encyclopaedic, Non-fiction, Written derived found Citation <pre><code>@inproceedings{9559013,\n  author = {Soygazi, Fatih and \u00c7ift\u00e7i, Okan and K\u00f6k, U\u011furcan and Cengiz, Soner},\n  booktitle = {2021 6th International Conference on Computer Science and Engineering (UBMK)},\n  doi = {10.1109/UBMK52708.2021.9559013},\n  keywords = {Computer science;Computational modeling;Neural networks;Knowledge discovery;Information retrieval;Natural language processing;History;question answering;information retrieval;natural language understanding;deep learning;contextualized word embeddings},\n  number = {},\n  pages = {215-220},\n  title = {THQuAD: Turkish Historic Question Answering Dataset for Reading Comprehension},\n  volume = {},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#twitterhjerneretrieval","title":"TwitterHjerneRetrieval","text":"<p>Danish question asked on Twitter with the Hashtag #Twitterhjerne ('Twitter brain') and their corresponding answer.</p> <p>Dataset: <code>mteb/TwitterHjerneRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 dan Social, Written derived found Citation <pre><code>@article{holm2024gllms,\n  author = {Holm, Soren Vejlgaard},\n  title = {Are GLLMs Danoliterate? Benchmarking Generative NLP in Danish},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#vabbretrieval","title":"VABBRetrieval","text":"<p>This dataset contains the fourteenth edition of the Flemish Academic Bibliography for the Social Sciences and Humanities (VABB-SHW), a database of academic publications from the social sciences and humanities authored by researchers affiliated to Flemish universities (more information). Publications in the database are used as one of the parameters of the Flemish performance-based research funding system</p> <p>Dataset: <code>clips/mteb-nl-vabb-ret</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Academic, Written derived found Citation <pre><code>@dataset{aspeslagh2024vabb,\n  author = {Aspeslagh, Pieter and Guns, Raf and Engels, Tim C. E.},\n  doi = {10.5281/zenodo.14214806},\n  publisher = {Zenodo},\n  title = {VABB-SHW: Dataset of Flemish Academic Bibliography for the Social Sciences and Humanities (edition 14)},\n  url = {https://doi.org/10.5281/zenodo.14214806},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#vdrmultilingualretrieval","title":"VDRMultilingualRetrieval","text":"<p>Multilingual Visual Document retrieval Dataset covering 5 languages: Italian, Spanish, English, French and German</p> <p>Dataset: <code>llamaindex/vdr-multilingual-test</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) ndcg_at_5 deu, eng, fra, ita, spa Web LM-generated found Citation <pre><code>@misc{llamaindex2024vdrmultilingual,\n  author = {LlamaIndex},\n  howpublished = {https://huggingface.co/datasets/llamaindex/vdr-multilingual-test},\n  title = {Visual Document Retrieval Goes Multilingual},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#videoretrieval","title":"VideoRetrieval","text":"<p>VideoRetrieval</p> <p>Dataset: <code>mteb/VideoRetrieval</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 cmn Entertainment, Written human-annotated found Citation <pre><code>@misc{long2022multicprmultidomainchinese,\n  archiveprefix = {arXiv},\n  author = {Dingkun Long and Qiong Gao and Kuan Zou and Guangwei Xu and Pengjun Xie and Ruijie Guo and Jian Xu and Guanjun Jiang and Luxi Xing and Ping Yang},\n  eprint = {2203.03367},\n  primaryclass = {cs.IR},\n  title = {Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval},\n  url = {https://arxiv.org/abs/2203.03367},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#viequadretrieval","title":"VieQuADRetrieval","text":"<p>A Vietnamese dataset for evaluating Machine Reading Comprehension from Wikipedia articles.</p> <p>Dataset: <code>taidng/UIT-ViQuAD2.0</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Encyclopaedic, Non-fiction, Written human-annotated found Citation <pre><code>@inproceedings{nguyen-etal-2020-vietnamese,\n  address = {Barcelona, Spain (Online)},\n  author = {Nguyen, Kiet  and\nNguyen, Vu  and\nNguyen, Anh  and\nNguyen, Ngan},\n  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\n  doi = {10.18653/v1/2020.coling-main.233},\n  editor = {Scott, Donia  and\nBel, Nuria  and\nZong, Chengqing},\n  month = dec,\n  pages = {2595--2605},\n  publisher = {International Committee on Computational Linguistics},\n  title = {A Vietnamese Dataset for Evaluating Machine Reading Comprehension},\n  url = {https://aclanthology.org/2020.coling-main.233},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretarxivqa","title":"VisRAGRetArxivQA","text":"<p>evaluate vision-based retrieval and generation on scientific figures and their surrounding context to preserve complex layouts and mathematical notations.</p> <p>Dataset: <code>mteb/VisRAGRetArxivQA</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Academic, Non-fiction derived found Citation <pre><code>@misc{li2024multimodalarxivdatasetimproving,\n  archiveprefix = {arXiv},\n  author = {Lei Li and Yuqi Wang and Runxin Xu and Peiyi Wang and Xiachong Feng and Lingpeng Kong and Qi Liu},\n  eprint = {2403.00231},\n  primaryclass = {cs.CV},\n  title = {Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models},\n  url = {https://arxiv.org/abs/2403.00231},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretchartqa","title":"VisRAGRetChartQA","text":"<p>Assess end-to-end vision-based RAG performance on real-world charts requiring complex logical and visual reasoning from retrieved images.</p> <p>Dataset: <code>mteb/VisRAGRetChartQA</code> \u2022 License: gpl-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Non-fiction, Web derived found Citation <pre><code>@misc{masry2022chartqabenchmarkquestionanswering,\n  archiveprefix = {arXiv},\n  author = {Ahmed Masry and Do Xuan Long and Jia Qing Tan and Shafiq Joty and Enamul Hoque},\n  eprint = {2203.10244},\n  primaryclass = {cs.CL},\n  title = {ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning},\n  url = {https://arxiv.org/abs/2203.10244},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretinfovqa","title":"VisRAGRetInfoVQA","text":"<p>Evaluate the retrieval and understanding of complex infographics where layout and graphical elements are essential for cross-modal question answering.</p> <p>Dataset: <code>mteb/VisRAGRetInfoVQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Web derived found Citation <pre><code>@misc{mathew2021infographicvqa,\n  archiveprefix = {arXiv},\n  author = {Minesh Mathew and Viraj Bagal and Rub\u00e8n P\u00e9rez Tito and Dimosthenis Karatzas and Ernest Valveny and C. V Jawahar},\n  eprint = {2104.12756},\n  primaryclass = {cs.CV},\n  title = {InfographicVQA},\n  url = {https://arxiv.org/abs/2104.12756},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretmpdocvqa","title":"VisRAGRetMPDocVQA","text":"<p>Benchmark the ability to retrieve specific relevant pages from multi-page documents and generate answers based on visual evidence.</p> <p>Dataset: <code>mteb/VisRAGRetMPDocVQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Non-fiction, Web derived found Citation <pre><code>@misc{tito2023hierarchicalmultimodaltransformersmultipage,\n  archiveprefix = {arXiv},\n  author = {Rub\u00e8n Tito and Dimosthenis Karatzas and Ernest Valveny},\n  eprint = {2212.05935},\n  primaryclass = {cs.CV},\n  title = {Hierarchical multimodal transformers for Multi-Page DocVQA},\n  url = {https://arxiv.org/abs/2212.05935},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretplotqa","title":"VisRAGRetPlotQA","text":"<p>Execute vision-based retrieval and numerical reasoning over scientific plots to answer questions without relying on structured data parsing.</p> <p>Dataset: <code>mteb/VisRAGRetPlotQA</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Non-fiction, Web derived found Citation <pre><code>@misc{methani2020plotqareasoningscientificplots,\n  archiveprefix = {arXiv},\n  author = {Nitesh Methani and Pritha Ganguly and Mitesh M. Khapra and Pratyush Kumar},\n  eprint = {1909.00997},\n  primaryclass = {cs.CV},\n  title = {PlotQA: Reasoning over Scientific Plots},\n  url = {https://arxiv.org/abs/1909.00997},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#visragretslidevqa","title":"VisRAGRetSlideVQA","text":"<p>Retrieve and reason across multiple slide images within a deck to answer multi-hop questions in a vision-centric retrieval-augmented generation pipeline.</p> <p>Dataset: <code>mteb/VisRAGRetSlideVQA</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to image (t2i) recall_at_10 eng Web derived found Citation <pre><code>@misc{tanaka2023slidevqadatasetdocumentvisual,\n  archiveprefix = {arXiv},\n  author = {Ryota Tanaka and Kyosuke Nishida and Kosuke Nishida and Taku Hasegawa and Itsumi Saito and Kuniko Saito},\n  eprint = {2301.04883},\n  primaryclass = {cs.CL},\n  title = {SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images},\n  url = {https://arxiv.org/abs/2301.04883},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#webfaqretrieval","title":"WebFAQRetrieval","text":"<p>WebFAQ is a broad-coverage corpus of natural question-answer pairs in 75 languages, gathered from FAQ pages on the web.</p> <p>Dataset: <code>mteb/WebFAQRetrieval</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, aze, ben, bul, cat, ... (51) Web, Written derived found Citation <pre><code>@misc{dinzinger2025webfaq,\n  archiveprefix = {arXiv},\n  author = {Michael Dinzinger and Laura Caspari and Kanishka Ghosh Dastidar and Jelena Mitrovi\u0107 and Michael Granitzer},\n  eprint = {2502.20936},\n  primaryclass = {cs.CL},\n  title = {WebFAQ: A Multilingual Collection of Natural Q&amp;amp;A Datasets for Dense Retrieval},\n  url = {https://arxiv.org/abs/2502.20936},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#wikisqlretrieval","title":"WikiSQLRetrieval","text":"<p>A code retrieval task based on WikiSQL dataset with natural language questions and corresponding SQL queries. Each query is a natural language question (e.g., 'What is the name of the team that has scored the most goals?'), and the corpus contains SQL query implementations. The task is to retrieve the correct SQL query that answers the natural language question. Queries are natural language questions while the corpus contains SQL SELECT statements with proper syntax and logic for querying database tables.</p> <p>Dataset: <code>embedding-benchmark/WikiSQL_mteb</code> \u2022 License: bsd-3-clause \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng, sql Programming expert-annotated found Citation <pre><code>@article{zhong2017seq2sql,\n  archiveprefix = {arXiv},\n  author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},\n  eprint = {1709.00103},\n  primaryclass = {cs.CL},\n  title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#wikipediaretrievalmultilingual","title":"WikipediaRetrievalMultilingual","text":"<p>The dataset is derived from Cohere's wikipedia-2023-11 dataset and contains synthetically generated queries.</p> <p>Dataset: <code>mteb/WikipediaRetrievalMultilingual</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ben, bul, ces, dan, deu, ... (16) Encyclopaedic, Written LM-generated and reviewed LM-generated and verified"},{"location":"overview/available_tasks/retrieval/#winogrande","title":"WinoGrande","text":"<p>Measuring the ability to retrieve the groundtruth answers to reasoning task queries on winogrande.</p> <p>Dataset: <code>mteb/WinoGrande</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 eng Encyclopaedic, Written derived found Citation <pre><code>@article{sakaguchi2021winogrande,\n  author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},\n  journal = {Communications of the ACM},\n  number = {9},\n  pages = {99--106},\n  publisher = {ACM New York, NY, USA},\n  title = {Winogrande: An adversarial winograd schema challenge at scale},\n  volume = {64},\n  year = {2021},\n}\n\n@article{xiao2024rar,\n  author = {Xiao, Chenghao and Hudson, G Thomas and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2404.06347},\n  title = {RAR-b: Reasoning as Retrieval Benchmark},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xmarket","title":"XMarket","text":"<p>XMarket</p> <p>Dataset: <code>mteb/XMarket</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 deu, eng, spa Reviews, Written derived found Citation <pre><code>@inproceedings{Bonab_2021,\n  author = {Bonab, Hamed and Aliannejadi, Mohammad and Vardasbi, Ali and Kanoulas, Evangelos and Allan, James},\n  booktitle = {Proceedings of the 30th ACM International Conference on Information &amp;amp; Knowledge Management},\n  collection = {CIKM \u201921},\n  doi = {10.1145/3459637.3482493},\n  month = oct,\n  publisher = {ACM},\n  series = {CIKM \u201921},\n  title = {Cross-Market Product Recommendation},\n  url = {http://dx.doi.org/10.1145/3459637.3482493},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xpqaretrieval","title":"XPQARetrieval","text":"<p>XPQARetrieval</p> <p>Dataset: <code>mteb/XPQARetrieval</code> \u2022 License: cdla-sharing-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 ara, cmn, deu, eng, fra, ... (13) Reviews, Written human-annotated found Citation <pre><code>@inproceedings{shen2023xpqa,\n  author = {Shen, Xiaoyu and Asai, Akari and Byrne, Bill and De Gispert, Adria},\n  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)},\n  pages = {103--115},\n  title = {xPQA: Cross-Lingual Product Question Answering in 12 Languages},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#xquadretrieval","title":"XQuADRetrieval","text":"<p>XQuAD is a benchmark dataset for evaluating cross-lingual question answering performance. It is repurposed retrieving relevant context for each question.</p> <p>Dataset: <code>google/xquad</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 arb, deu, ell, eng, hin, ... (12) Web, Written human-annotated created Citation <pre><code>@article{Artetxe:etal:2019,\n  archiveprefix = {arXiv},\n  author = {Mikel Artetxe and Sebastian Ruder and Dani Yogatama},\n  eprint = {1910.11856},\n  journal = {CoRR},\n  title = {On the cross-lingual transferability of monolingual representations},\n  volume = {abs/1910.11856},\n  year = {2019},\n}\n\n@inproceedings{dumitrescu2021liro,\n  author = {Stefan Daniel Dumitrescu and Petru Rebeja and Beata Lorincz and Mihaela Gaman and Andrei Avram and Mihai Ilie and Andrei Pruteanu and Adriana Stan and Lorena Rosia and Cristina Iacobescu and Luciana Morogan and George Dima and Gabriel Marchidan and Traian Rebedea and Madalina Chitez and Dani Yogatama and Sebastian Ruder and Radu Tudor Ionescu and Razvan Pascanu and Viorica Patraucean},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\n  title = {LiRo: Benchmark and leaderboard for Romanian language tasks},\n  url = {https://openreview.net/forum?id=JH61CD7afTv},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#zaclegaltextretrieval","title":"ZacLegalTextRetrieval","text":"<p>Zalo Legal Text documents</p> <p>Dataset: <code>GreenNode/zalo-ai-legal-text-retrieval-vn</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 vie Legal human-annotated found Citation <pre><code>@inproceedings{10.1007/978-981-95-1746-6_17,\n  address = {Singapore},\n  author = {Pham, Bao Loc\nand Hoang, Quoc Viet\nand Luu, Quy Tung\nand Vo, Trong Thu},\n  booktitle = {Proceedings of the Fifth International Conference on Intelligent Systems and Networks},\n  isbn = {978-981-95-1746-6},\n  pages = {153--163},\n  publisher = {Springer Nature Singapore},\n  title = {GN-TRVN: A Benchmark for\u00a0Vietnamese Table Markdown Retrieval Task},\n  year = {2026},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#bbsardnlretrieval","title":"bBSARDNLRetrieval","text":"<p>Building on the Belgian Statutory Article Retrieval Dataset (BSARD) in French, we introduce the bilingual version of this dataset, bBSARD. The dataset contains parallel Belgian statutory articles in both French and Dutch, along with legal questions from BSARD and their Dutch translation.</p> <p>Dataset: <code>clips/mteb-nl-bbsard</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Legal, Written expert-annotated found Citation <pre><code>@article{lotfi2025bilingual,\n  author = {Lotfi, Ehsan and Banar, Nikolay and Yuzbashyan, Nerses and Daelemans, Walter},\n  journal = {COLING 2025},\n  pages = {10},\n  title = {Bilingual BSARD: Extending Statutory Article Retrieval to Dutch},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/retrieval/#mmarco-nl","title":"mMARCO-NL","text":"<p>mMARCO is a multi-lingual (translated) collection of datasets focused on deep learning in search</p> <p>Dataset: <code>clips/beir-nl-mmarco</code> \u2022 License: apache-2.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) ndcg_at_10 nld Web, Written derived machine-translated and verified Citation <pre><code>@article{DBLP:journals/corr/abs-2108-13897,\n  author = {Luiz Bonifacio and\nIsrael Campiotti and\nRoberto de Alencar Lotufo and\nRodrigo Frassetto Nogueira},\n  bibsource = {dblp computer science bibliography, https://dblp.org},\n  biburl = {https://dblp.org/rec/journals/corr/abs-2108-13897.bib},\n  eprint = {2108.13897},\n  eprinttype = {arXiv},\n  journal = {CoRR},\n  timestamp = {Mon, 20 Mar 2023 15:35:34 +0100},\n  title = {mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset},\n  url = {https://arxiv.org/abs/2108.13897},\n  volume = {abs/2108.13897},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/","title":"STS","text":"<ul> <li>Number of tasks: 48</li> </ul>"},{"location":"overview/available_tasks/sts/#afqmc","title":"AFQMC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/AFQMC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Web, Written human-annotated found Citation <pre><code>@inproceedings{raghu-etal-2021-end,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Raghu, Dinesh  and\nAgarwal, Shantanu  and\nJoshi, Sachindra  and\n{Mausam}},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.357},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {4348--4366},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs},\n  url = {https://aclanthology.org/2021.emnlp-main.357},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#atec","title":"ATEC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/ATEC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Web, Written human-annotated found Citation <pre><code>@inproceedings{raghu-etal-2021-end,\n  address = {Online and Punta Cana, Dominican Republic},\n  author = {Raghu, Dinesh  and\nAgarwal, Shantanu  and\nJoshi, Sachindra  and\n{Mausam}},\n  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},\n  doi = {10.18653/v1/2021.emnlp-main.357},\n  editor = {Moens, Marie-Francine  and\nHuang, Xuanjing  and\nSpecia, Lucia  and\nYih, Scott Wen-tau},\n  month = nov,\n  pages = {4348--4366},\n  publisher = {Association for Computational Linguistics},\n  title = {End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs},\n  url = {https://aclanthology.org/2021.emnlp-main.357},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#assin2sts","title":"Assin2STS","text":"<p>Semantic Textual Similarity part of the ASSIN 2, an evaluation shared task collocated with STIL 2019.</p> <p>Dataset: <code>nilc-nlp/assin2</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman por Written human-annotated found Citation <pre><code>@inproceedings{real2020assin,\n  author = {Real, Livy and Fonseca, Erick and Oliveira, Hugo Goncalo},\n  booktitle = {International Conference on Computational Processing of the Portuguese Language},\n  organization = {Springer},\n  pages = {406--412},\n  title = {The assin 2 shared task: a quick overview},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#biosses","title":"BIOSSES","text":"<p>Biomedical Semantic Similarity Estimation.</p> <p>Dataset: <code>mteb/biosses-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Medical derived found Citation <pre><code>@article{10.1093/bioinformatics/btx238,\n  author = {So\u011fanc\u0131o\u011flu, Gizem and \u00d6zt\u00fcrk, Hakime and \u00d6zg\u00fcr, Arzucan},\n  doi = {10.1093/bioinformatics/btx238},\n  eprint = {https://academic.oup.com/bioinformatics/article-pdf/33/14/i49/50315066/bioinformatics\\_33\\_14\\_i49.pdf},\n  issn = {1367-4803},\n  journal = {Bioinformatics},\n  month = {07},\n  number = {14},\n  pages = {i49-i58},\n  title = {{BIOSSES: a semantic sentence similarity estimation system for the biomedical domain}},\n  url = {https://doi.org/10.1093/bioinformatics/btx238},\n  volume = {33},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#biosses-vn","title":"BIOSSES-VN","text":"<p>A translated dataset from Biomedical Semantic Similarity Estimation. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/biosses-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Medical derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#bq","title":"BQ","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/BQ</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Web, Written human-annotated found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#cdsc-r","title":"CDSC-R","text":"<p>Compositional Distributional Semantics Corpus for textual relatedness.</p> <p>Dataset: <code>PL-MTEB/cdscr-sts</code> \u2022 License: cc-by-nc-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman pol Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{wroblewska-krasnowska-kieras-2017-polish,\n  address = {Vancouver, Canada},\n  author = {Wr{\\'o}blewska, Alina  and\nKrasnowska-Kiera{\\'s}, Katarzyna},\n  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  doi = {10.18653/v1/P17-1073},\n  editor = {Barzilay, Regina  and\nKan, Min-Yen},\n  month = jul,\n  pages = {784--792},\n  publisher = {Association for Computational Linguistics},\n  title = {{P}olish evaluation dataset for compositional distributional semantics models},\n  url = {https://aclanthology.org/P17-1073},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#faroesests","title":"FaroeseSTS","text":"<p>Semantic Text Similarity (STS) corpus for Faroese.</p> <p>Dataset: <code>mteb/FaroeseSTS</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fao News, Web, Written human-annotated found Citation <pre><code>@inproceedings{snaebjarnarson-etal-2023-transfer,\n  address = {T\u00f3rshavn, Faroe Islands},\n  author = {Sn\u00e6bjarnarson, V\u00e9steinn  and\nSimonsen, Annika  and\nGlava\u0161, Goran  and\nVuli\u0107, Ivan},\n  booktitle = {Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  month = {may 22--24},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{T}ransfer to a Low-Resource Language via Close Relatives: The Case Study on Faroese},\n  year = {2023},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#farsick","title":"Farsick","text":"<p>A Persian Semantic Textual Similarity And Natural Language Inference Dataset</p> <p>Dataset: <code>MCINext/farsick-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/sts/#finparasts","title":"FinParaSTS","text":"<p>Finnish paraphrase-based semantic similarity corpus</p> <p>Dataset: <code>mteb/FinParaSTS</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fin News, Subtitles, Written expert-annotated found Citation <pre><code>@inproceedings{kanerva-etal-2021-finnish,\n  address = {Reykjavik, Iceland (Online)},\n  author = {Kanerva, Jenna  and\nGinter, Filip  and\nChang, Li-Hsin  and\nRastas, Iiro  and\nSkantsi, Valtteri  and\nKilpel{\\\"a}inen, Jemina  and\nKupari, Hanna-Mari  and\nSaarni, Jenna  and\nSev{\\'o}n, Maija  and\nTarkka, Otto},\n  booktitle = {Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)},\n  editor = {Dobnik, Simon  and\n{\\\\O}vrelid, Lilja},\n  month = may # { 31--2 } # jun,\n  pages = {288--298},\n  publisher = {Link{\\\"o}ping University Electronic Press, Sweden},\n  title = {{F}innish Paraphrase Corpus},\n  url = {https://aclanthology.org/2021.nodalida-main.29},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#germanstsbenchmark","title":"GermanSTSBenchmark","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset translated into German. Translations were originally done by T-Systems on site services GmbH.</p> <p>Dataset: <code>mteb/GermanSTSBenchmark</code> \u2022 License: cc-by-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman deu News, Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humesick-r","title":"HUMESICK-R","text":"<p>Human evaluation subset of Semantic Textual Similarity SICK-R dataset</p> <p>Dataset: <code>mteb/mteb-human-sickr-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Web, Written human-annotated created Citation <pre><code>@inproceedings{marelli-etal-2014-sick,\n  address = {Reykjavik, Iceland},\n  author = {Marelli, Marco  and\nMenini, Stefano  and\nBaroni, Marco  and\nBentivogli, Luisa  and\nBernardi, Raffaella  and\nZamparelli, Roberto},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {216--223},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {A {SICK} cure for the evaluation of compositional distributional semantic models},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humests12","title":"HUMESTS12","text":"<p>Human evaluation subset of SemEval-2012 Task 6.</p> <p>Dataset: <code>mteb/mteb-human-sts12-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Encyclopaedic, News, Written human-annotated created Citation <pre><code>@inproceedings{10.5555/2387636.2387697,\n  address = {USA},\n  author = {Agirre, Eneko and Diab, Mona and Cer, Daniel and Gonzalez-Agirre, Aitor},\n  booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},\n  location = {Montr\\'{e}al, Canada},\n  numpages = {9},\n  pages = {385\u2013393},\n  publisher = {Association for Computational Linguistics},\n  series = {SemEval '12},\n  title = {SemEval-2012 task 6: a pilot on semantic textual similarity},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humests22","title":"HUMESTS22","text":"<p>Human evaluation subset of SemEval 2022 Task 8: Multilingual News Article Similarity</p> <p>Dataset: <code>mteb/mteb-human-sts22-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, eng, fra, rus News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#humestsbenchmark","title":"HUMESTSBenchmark","text":"<p>Human evaluation subset of Semantic Textual Similarity Benchmark (STSbenchmark) dataset.</p> <p>Dataset: <code>mteb/mteb-human-stsbenchmark-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#indiccrosslingualsts","title":"IndicCrosslingualSTS","text":"<p>This is a Semantic Textual Similarity testset between English and 12 high-resource Indic languages.</p> <p>Dataset: <code>mteb/IndicCrosslingualSTS</code> \u2022 License: cc0-1.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman asm, ben, eng, guj, hin, ... (13) Government, News, Non-fiction, Spoken, Spoken, ... (7) expert-annotated created Citation <pre><code>@article{10.1162/tacl_a_00452,\n  author = {Ramesh, Gowtham and Doddapaneni, Sumanth and Bheemaraj, Aravinth and Jobanputra, Mayank and AK, Raghavan and Sharma, Ajitesh and Sahoo, Sujit and Diddee, Harshita and J, Mahalakshmi and Kakwani, Divyanshu and Kumar, Navneet and Pradeep, Aswin and Nagaraj, Srihari and Deepak, Kumar and Raghavan, Vivek and Kunchukuttan, Anoop and Kumar, Pratyush and Khapra, Mitesh Shantadevi},\n  doi = {10.1162/tacl_a_00452},\n  eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\\\\_a\\\\_00452/1987010/tacl\\\\_a\\\\_00452.pdf},\n  issn = {2307-387X},\n  journal = {Transactions of the Association for Computational Linguistics},\n  month = {02},\n  pages = {145-162},\n  title = {{Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages}},\n  url = {https://doi.org/10.1162/tacl\\\\_a\\\\_00452},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#jsick","title":"JSICK","text":"<p>JSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.</p> <p>Dataset: <code>mteb/JSICK</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman jpn Web, Written human-annotated found Citation <pre><code>@article{yanaka2022compositional,\n  author = {Yanaka, Hitomi and Mineshima, Koji},\n  journal = {Transactions of the Association for Computational Linguistics},\n  pages = {1266--1284},\n  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~\u2026},\n  title = {Compositional Evaluation on Japanese Textual Entailment and Similarity},\n  volume = {10},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#jsts","title":"JSTS","text":"<p>Japanese Semantic Textual Similarity Benchmark dataset construct from YJ Image Captions Dataset (Miyazaki and Shimizu, 2016) and annotated by crowdsource annotators.</p> <p>Dataset: <code>mteb/JSTS</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman jpn Web, Written human-annotated found Citation <pre><code>@inproceedings{kurihara-etal-2022-jglue,\n  address = {Marseille, France},\n  author = {Kurihara, Kentaro  and\nKawahara, Daisuke  and\nShibata, Tomohide},\n  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, H{\\'e}l{\\`e}ne  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = jun,\n  pages = {2957--2966},\n  publisher = {European Language Resources Association},\n  title = {{JGLUE}: {J}apanese General Language Understanding Evaluation},\n  url = {https://aclanthology.org/2022.lrec-1.317},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#klue-sts","title":"KLUE-STS","text":"<p>Human-annotated STS dataset of Korean reviews, news, and spoken word sets. Part of the Korean Language Understanding Evaluation (KLUE).</p> <p>Dataset: <code>klue/klue</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman kor News, Reviews, Spoken, Spoken, Written human-annotated found Citation <pre><code>@misc{park2021klue,\n  archiveprefix = {arXiv},\n  author = {Sungjoon Park and Jihyung Moon and Sungdong Kim and Won Ik Cho and Jiyoon Han and Jangwon Park and Chisung Song and Junseong Kim and Yongsook Song and Taehwan Oh and Joohong Lee and Juhyun Oh and Sungwon Lyu and Younghoon Jeong and Inkwon Lee and Sangwoo Seo and Dongjun Lee and Hyunwoo Kim and Myeonghwa Lee and Seongbo Jang and Seungwon Do and Sunkyoung Kim and Kyungtae Lim and Jongwon Lee and Kyumin Park and Jamin Shin and Seonghyun Kim and Lucy Park and Alice Oh and Jungwoo Ha and Kyunghyun Cho},\n  eprint = {2105.09680},\n  primaryclass = {cs.CL},\n  title = {KLUE: Korean Language Understanding Evaluation},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#korsts","title":"KorSTS","text":"<p>Benchmark dataset for STS in Korean. Created by machine translation and human post editing of the STS-B dataset.</p> <p>Dataset: <code>dkoterwa/kor-sts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman kor News, Web human-annotated machine-translated and localized Citation <pre><code>@article{ham2020kornli,\n  author = {Ham, Jiyeon and Choe, Yo Joong and Park, Kyubyong and Choi, Ilji and Soh, Hyungjoon},\n  journal = {arXiv preprint arXiv:2004.03289},\n  title = {KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#lcqmc","title":"LCQMC","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>C-MTEB/LCQMC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Web, Written human-annotated found Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#pawsx","title":"PAWSX","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>mteb/PAWSX</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Encyclopaedic, Web, Written human-annotated human-translated Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#qbqtc","title":"QBQTC","text":"<p>A Chinese question bank question title similarity dataset</p> <p>Dataset: <code>C-MTEB/QBQTC</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn Web, Written human-annotated found Citation <pre><code>@misc{clue2020qbqtc,\n  author = {CLUE},\n  title = {QBQTC: Question Bank Question Title Corpus},\n  url = {https://github.com/CLUEbenchmark/QBQTC},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#query2query","title":"Query2Query","text":"<p>Query to Query Datasets.</p> <p>Dataset: <code>MCINext/query-to-query-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas not specified derived found Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/sts/#ruparaphrasersts","title":"RUParaPhraserSTS","text":"<p>ParaPhraser is a news headlines corpus with precise, near and non-paraphrases.</p> <p>Dataset: <code>merionum/ru_paraphraser</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman rus News, Written human-annotated found Citation <pre><code>@inproceedings{gudkov-etal-2020-automatically,\n  address = {Online},\n  author = {Gudkov, Vadim  and\nMitrofanova, Olga  and\nFilippskikh, Elizaveta},\n  booktitle = {Proceedings of the Fourth Workshop on Neural Generation and Translation},\n  doi = {10.18653/v1/2020.ngt-1.6},\n  month = jul,\n  pages = {54--59},\n  publisher = {Association for Computational Linguistics},\n  title = {Automatically Ranked {R}ussian Paraphrase Corpus for Text Generation},\n  url = {https://aclanthology.org/2020.ngt-1.6},\n  year = {2020},\n}\n\n@inproceedings{pivovarova2017paraphraser,\n  author = {Pivovarova, Lidia and Pronoza, Ekaterina and Yagunova, Elena and Pronoza, Anton},\n  booktitle = {Conference on artificial intelligence and natural language},\n  organization = {Springer},\n  pages = {211--225},\n  title = {ParaPhraser: Russian paraphrase corpus and shared task},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#ronsts","title":"RonSTS","text":"<p>High-quality Romanian translation of STSBenchmark.</p> <p>Dataset: <code>mteb/RonSTS</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ron News, Social, Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{dumitrescu2021liro,\n  author = {Dumitrescu, Stefan Daniel and Rebeja, Petru and Lorincz, Beata and Gaman, Mihaela and Avram, Andrei and Ilie, Mihai and Pruteanu, Andrei and Stan, Adriana and Rosia, Lorena and Iacobescu, Cristina and others},\n  booktitle = {Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\n  title = {LiRo: Benchmark and leaderboard for Romanian language tasks},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#rustsbenchmarksts","title":"RuSTSBenchmarkSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset translated into Russian and verified. The dataset was checked with RuCOLA model to ensure that the translation is good and filtered.</p> <p>Dataset: <code>ai-forever/ru-stsbenchmark-sts</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman rus News, Social, Web, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-br-sts","title":"SICK-BR-STS","text":"<p>SICK-BR is a Portuguese inference corpus, human translated from SICK</p> <p>Dataset: <code>eduagarcia/sick-br</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman por Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{real18,\n  author = {Real, Livy\nand Rodrigues, Ana\nand Vieira e Silva, Andressa\nand Albiero, Beatriz\nand Thalenberg, Bruna\nand Guide, Bruno\nand Silva, Cindy\nand de Oliveira Lima, Guilherme\nand Camara, Igor C. S.\nand Stanojevi{\\'{c}}, Milo{\\v{s}}\nand Souza, Rodrigo\nand de Paiva, Valeria},\n  booktitle = {{Computational Processing of the Portuguese Language. PROPOR 2018.}},\n  doi = {10.1007/978-3-319-99722-3_31},\n  isbn = {978-3-319-99722-3},\n  title = {{SICK-BR: A Portuguese Corpus for Inference}},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-nl-sts","title":"SICK-NL-STS","text":"<p>SICK-NL (read: signal), a dataset targeting Natural Language Inference in Dutch. SICK-NL is obtained by translating the SICK dataset of (Marelli et al., 2014) from English into Dutch.</p> <p>Dataset: <code>clips/mteb-nl-sick-sts-pr</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman nld News, Social, Spoken, Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{wijnholds2021sick,\n  author = {Wijnholds, Gijs and Moortgat, Michael},\n  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},\n  pages = {1474--1479},\n  title = {SICK-NL: A Dataset for Dutch Natural Language Inference},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r","title":"SICK-R","text":"<p>Semantic Textual Similarity SICK-R dataset</p> <p>Dataset: <code>mteb/sickr-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Web, Written human-annotated created Citation <pre><code>@inproceedings{marelli-etal-2014-sick,\n  address = {Reykjavik, Iceland},\n  author = {Marelli, Marco  and\nMenini, Stefano  and\nBaroni, Marco  and\nBentivogli, Luisa  and\nBernardi, Raffaella  and\nZamparelli, Roberto},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  editor = {Calzolari, Nicoletta  and\nChoukri, Khalid  and\nDeclerck, Thierry  and\nLoftsson, Hrafn  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  month = may,\n  pages = {216--223},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {A {SICK} cure for the evaluation of compositional distributional semantic models},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r-pl","title":"SICK-R-PL","text":"<p>Polish version of SICK dataset for textual relatedness.</p> <p>Dataset: <code>PL-MTEB/sickr-pl-sts</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman pol Web, Written human-annotated human-translated and localized Citation <pre><code>@inproceedings{dadas-etal-2020-evaluation,\n  address = {Marseille, France},\n  author = {Dadas, Slawomir  and\nPerelkiewicz, Michal  and\nPoswiata, Rafal},\n  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},\n  editor = {Calzolari, Nicoletta  and\nB{\\'e}chet, Fr{\\'e}d{\\'e}ric  and\nBlache, Philippe  and\nChoukri, Khalid  and\nCieri, Christopher  and\nDeclerck, Thierry  and\nGoggi, Sara  and\nIsahara, Hitoshi  and\nMaegaard, Bente  and\nMariani, Joseph  and\nMazo, Helene  and\nMoreno, Asuncion  and\nOdijk, Jan  and\nPiperidis, Stelios},\n  isbn = {979-10-95546-34-4},\n  language = {English},\n  month = may,\n  pages = {1674--1680},\n  publisher = {European Language Resources Association},\n  title = {Evaluation of Sentence Representations in {P}olish},\n  url = {https://aclanthology.org/2020.lrec-1.207},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sick-r-vn","title":"SICK-R-VN","text":"<p>A translated dataset from Semantic Textual Similarity SICK-R dataset as described here: The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/sickr-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Web, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sickfr","title":"SICKFr","text":"<p>SICK dataset french version</p> <p>Dataset: <code>Lajavaness/SICK-fr</code> \u2022 License: cc-by-nc-sa-3.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{marelli-etal-2014-sick,\n  address = {Reykjavik, Iceland},\n  author = {Marelli, Marco  and\nMenini, Stefano  and\nBaroni, Marco  and\nBentivogli, Luisa  and\nBernardi, Raffaella  and\nZamparelli, Roberto},\n  booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},\n  month = may,\n  pages = {216--223},\n  publisher = {European Language Resources Association (ELRA)},\n  title = {A {SICK} cure for the evaluation of compositional distributional semantic models},\n  url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts12","title":"STS12","text":"<p>SemEval-2012 Task 6.</p> <p>Dataset: <code>mteb/sts12-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Encyclopaedic, News, Written human-annotated created Citation <pre><code>@inproceedings{10.5555/2387636.2387697,\n  address = {USA},\n  author = {Agirre, Eneko and Diab, Mona and Cer, Daniel and Gonzalez-Agirre, Aitor},\n  booktitle = {Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation},\n  location = {Montr\\'{e}al, Canada},\n  numpages = {9},\n  pages = {385\u2013393},\n  publisher = {Association for Computational Linguistics},\n  series = {SemEval '12},\n  title = {SemEval-2012 task 6: a pilot on semantic textual similarity},\n  year = {2012},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts13","title":"STS13","text":"<p>SemEval STS 2013 dataset.</p> <p>Dataset: <code>mteb/sts13-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Non-fiction, Web, Written human-annotated created Citation <pre><code>@inproceedings{Agirre2013SEM2S,\n  author = {Eneko Agirre and Daniel Matthew Cer and Mona T. Diab and Aitor Gonzalez-Agirre and Weiwei Guo},\n  booktitle = {International Workshop on Semantic Evaluation},\n  title = {*SEM 2013 shared task: Semantic Textual Similarity},\n  url = {https://api.semanticscholar.org/CorpusID:10241043},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts14","title":"STS14","text":"<p>SemEval STS 2014 dataset. Currently only the English dataset</p> <p>Dataset: <code>mteb/sts14-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, Spoken, Web derived created Citation <pre><code>@inproceedings{bandhakavi-etal-2014-generating,\n  address = {Dublin, Ireland},\n  author = {Bandhakavi, Anil  and\nWiratunga, Nirmalie  and\nP, Deepak  and\nMassie, Stewart},\n  booktitle = {Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014)},\n  doi = {10.3115/v1/S14-1002},\n  editor = {Bos, Johan  and\nFrank, Anette  and\nNavigli, Roberto},\n  month = aug,\n  pages = {12--21},\n  publisher = {Association for Computational Linguistics and Dublin City University},\n  title = {Generating a Word-Emotion Lexicon from {\\#}Emotional Tweets},\n  url = {https://aclanthology.org/S14-1002},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts15","title":"STS15","text":"<p>SemEval STS 2015 dataset</p> <p>Dataset: <code>mteb/sts15-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Spoken, Web, Written human-annotated created Citation <pre><code>@inproceedings{bicici-2015-rtm,\n  address = {Denver, Colorado},\n  author = {Bi{\\c{c}}ici, Ergun},\n  booktitle = {Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015)},\n  doi = {10.18653/v1/S15-2010},\n  editor = {Nakov, Preslav  and\nZesch, Torsten  and\nCer, Daniel  and\nJurgens, David},\n  month = jun,\n  pages = {56--63},\n  publisher = {Association for Computational Linguistics},\n  title = {{RTM}-{DCU}: Predicting Semantic Similarity with Referential Translation Machines},\n  url = {https://aclanthology.org/S15-2010},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts16","title":"STS16","text":"<p>SemEval-2016 Task 4</p> <p>Dataset: <code>mteb/sts16-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, Spoken, Web human-annotated created Citation <pre><code>@inproceedings{nakov-etal-2016-semeval,\n  address = {San Diego, California},\n  author = {Nakov, Preslav  and\nRitter, Alan  and\nRosenthal, Sara  and\nSebastiani, Fabrizio  and\nStoyanov, Veselin},\n  booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)},\n  doi = {10.18653/v1/S16-1001},\n  editor = {Bethard, Steven  and\nCarpuat, Marine  and\nCer, Daniel  and\nJurgens, David  and\nNakov, Preslav  and\nZesch, Torsten},\n  month = jun,\n  pages = {1--18},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2016 Task 4: Sentiment Analysis in {T}witter},\n  url = {https://aclanthology.org/S16-1001},\n  year = {2016},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts17","title":"STS17","text":"<p>Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</p> <p>Dataset: <code>mteb/sts17-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, deu, eng, fra, ita, ... (9) News, Web, Written human-annotated created Citation <pre><code>@inproceedings{cer-etal-2017-semeval,\n  address = {Vancouver, Canada},\n  author = {Cer, Daniel  and\nDiab, Mona  and\nAgirre, Eneko  and\nLopez-Gazpio, I{\\\\~n}igo  and\nSpecia, Lucia},\n  booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)},\n  doi = {10.18653/v1/S17-2001},\n  editor = {Bethard, Steven  and\nCarpuat, Marine  and\nApidianaki, Marianna  and\nMohammad, Saif M.  and\nCer, Daniel  and\nJurgens, David},\n  month = aug,\n  pages = {1--14},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},\n  url = {https://aclanthology.org/S17-2001},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts22","title":"STS22","text":"<p>SemEval 2022 Task 8: Multilingual News Article Similarity</p> <p>Dataset: <code>mteb/sts22-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, cmn, deu, eng, fra, ... (10) News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#sts22v2","title":"STS22.v2","text":"<p>SemEval 2022 Task 8: Multilingual News Article Similarity. Version 2 filters updated on STS22 by removing pairs where one of entries contain empty sentences.</p> <p>Dataset: <code>mteb/sts22-crosslingual-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman ara, cmn, deu, eng, fra, ... (10) News, Written human-annotated found Citation <pre><code>@inproceedings{chen-etal-2022-semeval,\n  address = {Seattle, United States},\n  author = {Chen, Xi  and\nZeynali, Ali  and\nCamargo, Chico  and\nFl{\\\"o}ck, Fabian  and\nGaffney, Devin  and\nGrabowicz, Przemyslaw  and\nHale, Scott  and\nJurgens, David  and\nSamory, Mattia},\n  booktitle = {Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)},\n  doi = {10.18653/v1/2022.semeval-1.155},\n  editor = {Emerson, Guy  and\nSchluter, Natalie  and\nStanovsky, Gabriel  and\nKumar, Ritesh  and\nPalmer, Alexis  and\nSchneider, Nathan  and\nSingh, Siddharth  and\nRatan, Shyam},\n  month = jul,\n  pages = {1094--1106},\n  publisher = {Association for Computational Linguistics},\n  title = {{S}em{E}val-2022 Task 8: Multilingual news article similarity},\n  url = {https://aclanthology.org/2022.semeval-1.155},\n  year = {2022},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsb","title":"STSB","text":"<p>A Chinese dataset for textual relatedness</p> <p>Dataset: <code>mteb/STSB</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn News, Web, Written human-annotated machine-translated Citation <pre><code>@misc{xiao2024cpackpackagedresourcesadvance,\n  archiveprefix = {arXiv},\n  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie},\n  eprint = {2309.07597},\n  primaryclass = {cs.CL},\n  title = {C-Pack: Packaged Resources To Advance General Chinese Embedding},\n  url = {https://arxiv.org/abs/2309.07597},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmark","title":"STSBenchmark","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset.</p> <p>Dataset: <code>mteb/stsbenchmark-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng Blog, News, Written human-annotated machine-translated and verified Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmark-vn","title":"STSBenchmark-VN","text":"<p>A translated dataset from Semantic Textual Similarity Benchmark (STSbenchmark) dataset. The process of creating the VN-MTEB (Vietnamese Massive Text Embedding Benchmark) from English samples involves a new automated system: - The system uses large language models (LLMs), specifically Coherence's Aya model, for translation. - Applies advanced embedding models to filter the translations. - Use LLM-as-a-judge to scoring the quality of the samples base on multiple criteria.</p> <p>Dataset: <code>GreenNode/stsbenchmark-sts-vn</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to category (t2c) cosine_spearman vie Blog, News, Written derived machine-translated and LM verified Citation <pre><code>@misc{pham2025vnmtebvietnamesemassivetext,\n  archiveprefix = {arXiv},\n  author = {Loc Pham and Tung Luu and Thu Vo and Minh Nguyen and Viet Hoang},\n  eprint = {2507.21500},\n  primaryclass = {cs.CL},\n  title = {VN-MTEB: Vietnamese Massive Text Embedding Benchmark},\n  url = {https://arxiv.org/abs/2507.21500},\n  year = {2025},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stsbenchmarkmultilingualsts","title":"STSBenchmarkMultilingualSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset, but translated using DeepL API.</p> <p>Dataset: <code>mteb/stsb_multi_mt</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman cmn, deu, eng, fra, ita, ... (10) News, Social, Spoken, Web, Written human-annotated machine-translated Citation <pre><code>@inproceedings{huggingface:dataset:stsb_multi_mt,\n  author = {Philip May},\n  title = {Machine translated multilingual STS benchmark dataset.},\n  url = {https://github.com/PhilipMay/stsb-multi-mt},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#stses","title":"STSES","text":"<p>Spanish test sets from SemEval-2014 (Agirre et al., 2014) and SemEval-2015 (Agirre et al., 2015)</p> <p>Dataset: <code>mteb/STSES</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman spa News, Web, Written human-annotated found Citation <pre><code>@inproceedings{agirre2014semeval,\n  author = {Agirre, Eneko and Banea, Carmen and Cardie, Claire and Cer, Daniel M and Diab, Mona T and Gonzalez-Agirre, Aitor and Guo, Weiwei and Mihalcea, Rada and Rigau, German and Wiebe, Janyce},\n  booktitle = {SemEval@ COLING},\n  pages = {81--91},\n  title = {SemEval-2014 Task 10: Multilingual Semantic Textual Similarity.},\n  year = {2014},\n}\n\n@inproceedings{agirre2015semeval,\n  author = {Agirre, Eneko and Banea, Carmen and Cardie, Claire and Cer, Daniel and Diab, Mona and Gonzalez-Agirre, Aitor and Guo, Weiwei and Lopez-Gazpio, Inigo and Maritxalar, Montse and Mihalcea, Rada and others},\n  booktitle = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},\n  pages = {252--263},\n  title = {Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#semrel24sts","title":"SemRel24STS","text":"<p>SemRel2024 is a collection of Semantic Textual Relatedness (STR) datasets for 14 languages, including African and Asian languages. The datasets are composed of sentence pairs, each assigned a relatedness score between 0 (completely) unrelated and 1 (maximally related) with a large range of expected relatedness values.</p> <p>Dataset: <code>mteb/SemRel24STS</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman afr, amh, arb, arq, ary, ... (12) Spoken, Written human-annotated created Citation <pre><code>@misc{ousidhoum2024semrel2024,\n  archiveprefix = {arXiv},\n  author = {Nedjma Ousidhoum and Shamsuddeen Hassan Muhammad and Mohamed Abdalla and Idris Abdulmumin and Ibrahim Said Ahmad and\nSanchit Ahuja and Alham Fikri Aji and Vladimir Araujo and Abinew Ali Ayele and Pavan Baswani and Meriem Beloucif and\nChris Biemann and Sofia Bourhim and Christine De Kock and Genet Shanko Dekebo and\nOumaima Hourrane and Gopichand Kanumolu and Lokesh Madasu and Samuel Rutunda and Manish Shrivastava and\nThamar Solorio and Nirmal Surange and Hailegnaw Getaneh Tilaye and Krishnapriya Vishnubhotla and Genta Winata and\nSeid Muhie Yimam and Saif M. Mohammad},\n  eprint = {2402.08638},\n  primaryclass = {cs.CL},\n  title = {SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14 Languages},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/sts/#synpersts","title":"SynPerSTS","text":"<p>Synthetic Persian Semantic Textual Similarity Dataset</p> <p>Dataset: <code>MCINext/synthetic-persian-sts</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fas Blog, News, Religious, Web LM-generated LM-generated and verified Citation <pre><code>\n</code></pre>"},{"location":"overview/available_tasks/summarization/","title":"Summarization","text":"<ul> <li>Number of tasks: 4</li> </ul>"},{"location":"overview/available_tasks/summarization/#summeval","title":"SummEval","text":"<p>News Article Summary Semantic Similarity Estimation.</p> <p>Dataset: <code>mteb/summeval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Written human-annotated created Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalfr","title":"SummEvalFr","text":"<p>News Article Summary Semantic Similarity Estimation translated from english to french with DeepL.</p> <p>Dataset: <code>lyon-nlp/summarization-summeval-fr-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra News, Written human-annotated machine-translated Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalfrsummarizationv2","title":"SummEvalFrSummarization.v2","text":"<p>News Article Summary Semantic Similarity Estimation translated from english to french with DeepL. This version fixes a bug in the evaluation script that caused the main score to be computed incorrectly.</p> <p>Dataset: <code>lyon-nlp/summarization-summeval-fr-p2p</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman fra News, Written human-annotated machine-translated Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/summarization/#summevalsummarizationv2","title":"SummEvalSummarization.v2","text":"<p>News Article Summary Semantic Similarity Estimation. This version fixes a bug in the evaluation script that caused the main score to be computed incorrectly.</p> <p>Dataset: <code>mteb/summeval</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation text to text (t2t) cosine_spearman eng News, Written human-annotated created Citation <pre><code>@article{fabbri2020summeval,\n  author = {Fabbri, Alexander R and Kry{\\'s}ci{\\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},\n  journal = {arXiv preprint arXiv:2007.12626},\n  title = {SummEval: Re-evaluating Summarization Evaluation},\n  year = {2020},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/","title":"VisionCentricQA","text":"<ul> <li>Number of tasks: 6</li> </ul>"},{"location":"overview/available_tasks/visioncentricqa/#blinkit2imultichoice","title":"BLINKIT2IMultiChoice","text":"<p>Retrieve images based on images and specific retrieval instructions.</p> <p>Dataset: <code>mteb/blink-it2i-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to image (it2i) accuracy eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#blinkit2tmultichoice","title":"BLINKIT2TMultiChoice","text":"<p>Retrieve the correct text answer based on images and specific retrieval instructions.</p> <p>Dataset: <code>mteb/blink-it2t-multi</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Encyclopaedic derived found Citation <pre><code>@article{fu2024blink,\n  author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A and Ma, Wei-Chiu and Krishna, Ranjay},\n  journal = {arXiv preprint arXiv:2404.12390},\n  title = {Blink: Multimodal large language models can see but not perceive},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchcount","title":"CVBenchCount","text":"<p>count the number of objects in the image.</p> <p>Dataset: <code>mteb/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchdepth","title":"CVBenchDepth","text":"<p>judge the depth of the objects in the image with similarity matching.</p> <p>Dataset: <code>mteb/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchdistance","title":"CVBenchDistance","text":"<p>judge the distance of the objects in the image with similarity matching.</p> <p>Dataset: <code>mteb/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visioncentricqa/#cvbenchrelation","title":"CVBenchRelation","text":"<p>decide the relation of the objects in the image.</p> <p>Dataset: <code>mteb/CV-Bench</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image, text to text (it2t) accuracy eng Academic derived found Citation <pre><code>@article{tong2024cambrian,\n  author = {Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng and Iyer, Adithya and Pan, Xichen and others},\n  journal = {arXiv preprint arXiv:2406.16860},\n  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/","title":"VisualSTS(eng)","text":"<ul> <li>Number of tasks: 7</li> </ul>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts12visualsts","title":"STS12VisualSTS","text":"<p>SemEval-2012 Task 6.then rendered into images.</p> <p>Dataset: <code>mteb/rendered-sts12</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Encyclopaedic, News, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts13visualsts","title":"STS13VisualSTS","text":"<p>SemEval STS 2013 dataset.then rendered into images.</p> <p>Dataset: <code>mteb/rendered-sts13</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng News, Non-fiction, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts14visualsts","title":"STS14VisualSTS","text":"<p>SemEval STS 2014 dataset. Currently only the English dataset.rendered into images.</p> <p>Dataset: <code>mteb/rendered-sts14</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, Spoken, Web derived rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts15visualsts","title":"STS15VisualSTS","text":"<p>SemEval STS 2015 datasetrendered into images.</p> <p>Dataset: <code>mteb/rendered-sts15</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, News, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#sts16visualsts","title":"STS16VisualSTS","text":"<p>SemEval STS 2016 datasetrendered into images.</p> <p>Dataset: <code>mteb/rendered-sts16</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng Blog, Spoken, Web human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28eng%29/#visualsts-b-eng","title":"VisualSTS-b-Eng","text":"<p>STSBenchmarkMultilingualVisualSTS English only.</p> <p>License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages STSBenchmarkMultilingualVisualSTS VisualSTS(multi) image cmn, deu, eng, fra, ita, ... (10)"},{"location":"overview/available_tasks/visualsts%28eng%29/#visualsts17eng","title":"VisualSTS17Eng","text":"<p>STS17MultilingualVisualSTS English only.</p> <p>License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman eng News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages STS17MultilingualVisualSTS VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9)"},{"location":"overview/available_tasks/visualsts%28multi%29/","title":"VisualSTS(multi)","text":"<ul> <li>Number of tasks: 4</li> </ul>"},{"location":"overview/available_tasks/visualsts%28multi%29/#sts17multilingualvisualsts","title":"STS17MultilingualVisualSTS","text":"<p>Semantic Textual Similarity 17 (STS-17) dataset, rendered into images.</p> <p>Dataset: <code>Pixel-Linguist/rendered-sts17</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman ara, deu, eng, fra, ita, ... (9) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28multi%29/#stsbenchmarkmultilingualvisualsts","title":"STSBenchmarkMultilingualVisualSTS","text":"<p>Semantic Textual Similarity Benchmark (STSbenchmark) dataset, translated into target languages using DeepL API,then rendered into images.built upon multi-sts created by Philip May</p> <p>Dataset: <code>Pixel-Linguist/rendered-stsb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman cmn, deu, eng, fra, ita, ... (10) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/visualsts%28multi%29/#visualsts-b-multilingual","title":"VisualSTS-b-Multilingual","text":"<p>STSBenchmarkMultilingualVisualSTS multilingual.</p> <p>License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman cmn, deu, fra, ita, nld, ... (9) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages STSBenchmarkMultilingualVisualSTS VisualSTS(multi) image cmn, deu, eng, fra, ita, ... (10)"},{"location":"overview/available_tasks/visualsts%28multi%29/#visualsts17multilingual","title":"VisualSTS17Multilingual","text":"<p>STS17MultilingualVisualSTS multilingual.</p> <p>License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to image (i2i) cosine_spearman ara, deu, eng, fra, ita, ... (9) News, Social, Spoken, Web, Written human-annotated rendered Citation <pre><code>@article{xiao2024pixel,\n  author = {Xiao, Chenghao and Huang, Zhuoxu and Chen, Danlu and Hudson, G Thomas and Li, Yizhi and Duan, Haoran and Lin, Chenghua and Fu, Jie and Han, Jungong and Moubayed, Noura Al},\n  journal = {arXiv preprint arXiv:2402.08183},\n  title = {Pixel Sentence Representation Learning},\n  year = {2024},\n}\n</code></pre> Tasks name type modalities languages STS17MultilingualVisualSTS VisualSTS(multi) image ara, deu, eng, fra, ita, ... (9)"},{"location":"overview/available_tasks/zeroshotclassification/","title":"ZeroShotClassification","text":"<ul> <li>Number of tasks: 24</li> </ul>"},{"location":"overview/available_tasks/zeroshotclassification/#birdsnapzeroshot","title":"BirdsnapZeroShot","text":"<p>Classifying bird images from 500 species. </p> <p>Dataset: <code>mteb/birdsnap</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{Berg_2014_CVPR,\n  author = {Berg, Thomas and Liu, Jiongxin and Woo Lee, Seung and Alexander, Michelle L. and Jacobs, David W. and Belhumeur, Peter N.},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  title = {Birdsnap: Large-scale Fine-grained Visual Categorization of Birds},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#cifar100zeroshot","title":"CIFAR100ZeroShot","text":"<p>Classifying images from 100 classes.</p> <p>Dataset: <code>mteb/cifar100</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#cifar10zeroshot","title":"CIFAR10ZeroShot","text":"<p>Classifying images from 10 classes.</p> <p>Dataset: <code>mteb/cifar10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@techreport{Krizhevsky09learningmultiple,\n  author = {Alex Krizhevsky},\n  institution = {},\n  title = {Learning multiple layers of features from tiny images},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#clevrcountzeroshot","title":"CLEVRCountZeroShot","text":"<p>CLEVR count objects task.</p> <p>Dataset: <code>mteb/wds_vtab-clevr_count_all</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Constructed human-annotated created Citation <pre><code>@inproceedings{Johnson_2017_CVPR,\n  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#clevrzeroshot","title":"CLEVRZeroShot","text":"<p>CLEVR closest object distance identification task.</p> <p>Dataset: <code>mteb/wds_vtab-clevr_closest_object_distance</code> \u2022 License: cc-by-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Constructed human-annotated created Citation <pre><code>@inproceedings{Johnson_2017_CVPR,\n  author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C. and Girshick, Ross},\n  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {July},\n  title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#caltech101zeroshot","title":"Caltech101ZeroShot","text":"<p>Classifying images of 101 widely varied objects.</p> <p>Dataset: <code>mteb/Caltech101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{1384978,\n  author = {Li Fei-Fei and Fergus, R. and Perona, P.},\n  booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n  doi = {10.1109/CVPR.2004.383},\n  keywords = {Bayesian methods;Testing;Humans;Maximum likelihood estimation;Assembly;Shape;Machine vision;Image recognition;Parameter estimation;Image databases},\n  number = {},\n  pages = {178-178},\n  title = {Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},\n  volume = {},\n  year = {2004},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#country211zeroshot","title":"Country211ZeroShot","text":"<p>Classifying images of 211 countries.</p> <p>Dataset: <code>mteb/wds_country211</code> \u2022 License: cc-by-sa-4.0 \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@article{radford2021learning,\n  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},\n  journal = {arXiv preprint arXiv:2103.00020},\n  title = {Learning Transferable Visual Models From Natural Language Supervision},\n  year = {2021},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#dtdzeroshot","title":"DTDZeroShot","text":"<p>Describable Textures Dataset in 47 categories.</p> <p>Dataset: <code>mteb/dtd</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{cimpoi14describing,\n  author = {M. Cimpoi and S. Maji and I. Kokkinos and S. Mohamed and and A. Vedaldi},\n  booktitle = {Proceedings of the {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},\n  title = {Describing Textures in the Wild},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#eurosatzeroshot","title":"EuroSATZeroShot","text":"<p>Classifying satellite images.</p> <p>Dataset: <code>mteb/eurosat-rgb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{8736785,\n  author = {Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n  doi = {10.1109/JSTARS.2019.2918242},\n  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n  keywords = {Satellites;Earth;Remote sensing;Machine learning;Spatial resolution;Feature extraction;Benchmark testing;Dataset;deep convolutional neural network;deep learning;earth observation;land cover classification;land use classification;machine learning;remote sensing;satellite image classification;satellite images},\n  number = {7},\n  pages = {2217-2226},\n  title = {EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},\n  volume = {12},\n  year = {2019},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#fer2013zeroshot","title":"FER2013ZeroShot","text":"<p>Classifying facial emotions.</p> <p>Dataset: <code>mteb/wds_fer2013</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{goodfellow2015explainingharnessingadversarialexamples,\n  archiveprefix = {arXiv},\n  author = {Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},\n  eprint = {1412.6572},\n  primaryclass = {stat.ML},\n  title = {Explaining and Harnessing Adversarial Examples},\n  url = {https://arxiv.org/abs/1412.6572},\n  year = {2015},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#fgvcaircraftzeroshot","title":"FGVCAircraftZeroShot","text":"<p>Classifying aircraft images from 41 manufacturers and 102 variants.</p> <p>Dataset: <code>mteb/FGVCAircraftZeroShot</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#food101zeroshot","title":"Food101ZeroShot","text":"<p>Classifying food.</p> <p>Dataset: <code>mteb/food101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Web derived created Citation <pre><code>@inproceedings{bossard14,\n  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n  booktitle = {European Conference on Computer Vision},\n  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n  year = {2014},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#gtsrbzeroshot","title":"GTSRBZeroShot","text":"<p>The German Traffic Sign Recognition Benchmark (GTSRB) is a multi-class classification dataset for traffic signs. It consists of dataset of more than 50,000 traffic sign images. The dataset comprises 43 classes with unbalanced class frequencies.</p> <p>Dataset: <code>mteb/wds_gtsrb</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@inproceedings{6033395,\n  author = {Stallkamp, Johannes and Schlipsing, Marc and Salmen, Jan and Igel, Christian},\n  booktitle = {The 2011 International Joint Conference on Neural Networks},\n  doi = {10.1109/IJCNN.2011.6033395},\n  keywords = {Humans;Training;Image color analysis;Benchmark testing;Lead;Histograms;Image resolution},\n  number = {},\n  pages = {1453-1460},\n  title = {The German Traffic Sign Recognition Benchmark: A multi-class classification competition},\n  volume = {},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#imagenet1kzeroshot","title":"Imagenet1kZeroShot","text":"<p>ImageNet, a large-scale ontology of images built upon the backbone of the WordNet structure.</p> <p>Dataset: <code>mteb/wds_imagenet1k</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene human-annotated created Citation <pre><code>@inproceedings{deng2009imagenet,\n  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},\n  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2009.5206848},\n  keywords = {Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},\n  number = {},\n  pages = {248-255},\n  title = {ImageNet: A large-scale hierarchical image database},\n  volume = {},\n  year = {2009},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#mnistzeroshot","title":"MNISTZeroShot","text":"<p>Classifying handwritten digits.</p> <p>Dataset: <code>mteb/mnist</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{lecun2010mnist,\n  author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},\n  journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n  title = {MNIST handwritten digit database},\n  volume = {2},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#oxfordpetszeroshot","title":"OxfordPetsZeroShot","text":"<p>Classifying animal images.</p> <p>Dataset: <code>mteb/OxfordPets</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@misc{maji2013finegrainedvisualclassificationaircraft,\n  archiveprefix = {arXiv},\n  author = {Subhransu Maji and Esa Rahtu and Juho Kannala and Matthew Blaschko and Andrea Vedaldi},\n  eprint = {1306.5151},\n  primaryclass = {cs.CV},\n  title = {Fine-Grained Visual Classification of Aircraft},\n  url = {https://arxiv.org/abs/1306.5151},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#patchcamelyonzeroshot","title":"PatchCamelyonZeroShot","text":"<p>Histopathology diagnosis classification dataset.</p> <p>Dataset: <code>mteb/wds_vtab-pcam</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Medical derived created Citation <pre><code>@inproceedings{10.1007/978-3-030-00934-2_24,\n  address = {Cham},\n  author = {Veeling, Bastiaan S.\nand Linmans, Jasper\nand Winkens, Jim\nand Cohen, Taco\nand Welling, Max},\n  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2018},\n  editor = {Frangi, Alejandro F.\nand Schnabel, Julia A.\nand Davatzikos, Christos\nand Alberola-L{\\'o}pez, Carlos\nand Fichtinger, Gabor},\n  isbn = {978-3-030-00934-2},\n  pages = {210--218},\n  publisher = {Springer International Publishing},\n  title = {Rotation Equivariant CNNs for Digital Pathology},\n  year = {2018},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#resisc45zeroshot","title":"RESISC45ZeroShot","text":"<p>Remote Sensing Image Scene Classification by Northwestern Polytechnical University (NWPU).</p> <p>Dataset: <code>mteb/resisc45</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@article{7891544,\n  author = {Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},\n  doi = {10.1109/JPROC.2017.2675998},\n  journal = {Proceedings of the IEEE},\n  keywords = {Remote sensing;Benchmark testing;Spatial resolution;Social network services;Satellites;Image analysis;Machine learning;Unsupervised learning;Classification;Benchmark data set;deep learning;handcrafted features;remote sensing image;scene classification;unsupervised feature learning},\n  number = {10},\n  pages = {1865-1883},\n  title = {Remote Sensing Image Scene Classification: Benchmark and State of the Art},\n  volume = {105},\n  year = {2017},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#renderedsst2","title":"RenderedSST2","text":"<p>RenderedSST2.</p> <p>Dataset: <code>mteb/wds_renderedsst2</code> \u2022 License: mit \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Reviews human-annotated created"},{"location":"overview/available_tasks/zeroshotclassification/#stl10zeroshot","title":"STL10ZeroShot","text":"<p>Classifying 96x96 images from 10 classes.</p> <p>Dataset: <code>mteb/stl10</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{pmlr-v15-coates11a,\n  address = {Fort Lauderdale, FL, USA},\n  author = {Coates, Adam and Ng, Andrew and Lee, Honglak},\n  booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},\n  editor = {Gordon, Geoffrey and Dunson, David and Dud\u00edk, Miroslav},\n  month = {11--13 Apr},\n  pages = {215--223},\n  pdf = {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},\n  publisher = {PMLR},\n  series = {Proceedings of Machine Learning Research},\n  title = {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},\n  url = {https://proceedings.mlr.press/v15/coates11a.html},\n  volume = {15},\n  year = {2011},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#sun397zeroshot","title":"SUN397ZeroShot","text":"<p>Large scale scene recognition in 397 categories.</p> <p>Dataset: <code>mteb/sun397</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Encyclopaedic derived created Citation <pre><code>@inproceedings{5539970,\n  author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista A. and Oliva, Aude and Torralba, Antonio},\n  booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n  doi = {10.1109/CVPR.2010.5539970},\n  number = {},\n  pages = {3485-3492},\n  title = {SUN database: Large-scale scene recognition from abbey to zoo},\n  volume = {},\n  year = {2010},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#scimmir","title":"SciMMIR","text":"<p>SciMMIR.</p> <p>Dataset: <code>mteb/SciMMIR</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Academic human-annotated created Citation <pre><code>@misc{wu2024scimmirbenchmarkingscientificmultimodal,\n  archiveprefix = {arXiv},\n  author = {Siwei Wu and Yizhi Li and Kang Zhu and Ge Zhang and Yiming Liang and Kaijing Ma and Chenghao Xiao and Haoran Zhang and Bohao Yang and Wenhu Chen and Wenhao Huang and Noura Al Moubayed and Jie Fu and Chenghua Lin},\n  eprint = {2401.13478},\n  primaryclass = {cs.IR},\n  title = {SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval},\n  url = {https://arxiv.org/abs/2401.13478},\n  year = {2024},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#stanfordcarszeroshot","title":"StanfordCarsZeroShot","text":"<p>Classifying car images from 96 makes.</p> <p>Dataset: <code>mteb/StanfordCars</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@inproceedings{Krause2013CollectingAL,\n  author = {Jonathan Krause and Jia Deng and Michael Stark and Li Fei-Fei},\n  title = {Collecting a Large-scale Dataset of Fine-grained Cars},\n  url = {https://api.semanticscholar.org/CorpusID:16632981},\n  year = {2013},\n}\n</code></pre>"},{"location":"overview/available_tasks/zeroshotclassification/#ucf101zeroshot","title":"UCF101ZeroShot","text":"<p>UCF101 is an action recognition data set of realistic action videos collected from YouTube, having 101 action categories. This version of the dataset does not contain images but images saved frame by frame. Train and test splits are generated based on the authors' first version train/test list.</p> <p>Dataset: <code>mteb/ucf101</code> \u2022 License: not specified \u2022 Learn more \u2192</p> Task category Score Languages Domains Annotations Creators Sample Creation image to text (i2t) accuracy eng Scene derived created Citation <pre><code>@misc{soomro2012ucf101dataset101human,\n  archiveprefix = {arXiv},\n  author = {Khurram Soomro and Amir Roshan Zamir and Mubarak Shah},\n  eprint = {1212.0402},\n  primaryclass = {cs.CV},\n  title = {UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild},\n  url = {https://arxiv.org/abs/1212.0402},\n  year = {2012},\n}\n</code></pre>"},{"location":"usage/cli/","title":"Command Line Interface","text":"<p>This described the is the command line interface for <code>mteb</code>.</p> <p><code>mteb</code> is a toolkit for evaluating the quality of embedding models on various benchmarks. It supports the following commands:</p> <ul> <li><code>mteb run</code>: Runs a model on a set of tasks</li> <li><code>mteb available_tasks</code>: Lists the available tasks within MTEB</li> <li><code>mteb available_benchmarks</code>: Lists the available benchmarks</li> <li><code>mteb create_meta</code>: Creates the metadata for a model card from a folder of results</li> <li><code>mteb leaderboard</code>: Runs the MTEB leaderboard locally</li> </ul> <p>In the following we outline some sample use cases, but if you want to learn more about the arguments for each command you can run:</p> <pre><code>mteb {command} --help\n</code></pre>"},{"location":"usage/cli/#running-models-on-tasks","title":"Running Models on Tasks","text":"<p>To run a model on a set of tasks, use the <code>mteb run</code> command. For example:</p> <pre><code>mteb run -m sentence-transformers/average_word_embeddings_komninos \\\n         -t Banking77Classification EmotionClassification \\\n         --output-folder mteb_output\n</code></pre> <p>This will create a folder <code>mteb_output/{model_name}/{model_revision}</code> containing the results of the model on the specified tasks supplied as a json file; <code>{task_name}.json</code>.</p>"},{"location":"usage/cli/#listing-available-tasks","title":"Listing Available Tasks","text":"<p>To list the available tasks within MTEB, use the <code>mteb available-tasks</code> command. For example:</p> <pre><code>mteb available-tasks # list _all_ available tasks\n</code></pre> <p>You can also use the multiple arguments for filtering: <pre><code>mteb available-tasks --task-types Retrieval --languages eng # list all English (eng) retrieval tasks\n</code></pre></p>"},{"location":"usage/cli/#listing-available-benchmarks","title":"Listing Available Benchmarks","text":"<p>To list the available benchmarks within MTEB:</p> <pre><code>mteb available-benchmarks # list all available benchmarks\n</code></pre>"},{"location":"usage/cli/#creating-model-metadata","title":"Creating Model Metadata","text":"<p>Once a model is run you can create the metadata for a model card from a folder of results, use the <code>mteb create-meta</code> command. For example:</p> <pre><code>mteb create-meta --results-folder mteb_output/sentence-transformers__average_word_embeddings_komninos/{revision} \\\n                 --output-path model_card.md\n</code></pre> <p>This will create a model card at <code>model_card.md</code> containing the metadata for the model on MTEB within the YAML frontmatter. This will make the model discoverable on the MTEB leaderboard.</p> <p>An example frontmatter for a model card is shown below:</p> <pre><code>---\n\n## Running the Leaderboard\n\nTo run the MTEB leaderboard locally, use the `mteb leaderboard` command. For example:\n\n```bash\nmteb leaderboard\n</code></pre> <p>You can specify a custom cache path and other options:</p> <pre><code>mteb leaderboard --cache-path results --port 8080 --share\n</code></pre> <p>Available options: - <code>--cache-path PATH</code>: Custom path for model results cache - <code>--host HOST</code>: Host to run the server on (default: 0.0.0.0) - <code>--port PORT</code>: Port to run the server on (default: 7860) - <code>--share</code>: Create a public URL for the leaderboard - <code>--rebuild</code>: Force rebuild from full results repository, bypassing cached JSON</p> <p>For more details on running the leaderboard, see the leaderboard documentation. tags: - mteb model-index: - name: SGPT-5.8B-weightedmean-msmarco-specb-bitfit   results:   - task:       type: classification     dataset:       type: mteb/banking77       name: MTEB Banking77       config: default       split: test       revision: 44fa15921b4c889113cc5df03dd4901b49161ab7     metrics:     - type: accuracy       value: 84.49350649350649</p> <p>```</p>"},{"location":"usage/defining_the_model/","title":"Defining the Model","text":""},{"location":"usage/defining_the_model/#using-a-pre-defined-model","title":"Using a pre-defined Model","text":"<p>MTEB comes with an implementation of many popular models and APIs. These can be loaded using <code>mteb.get_model_meta</code> or <code>mteb.get_model</code>:</p> <pre><code>model_name = \"intfloat/multilingual-e5-small\"\nmeta = mteb.get_model_meta(model_name) # (1)\nmodel = meta.load_model()\n# or directly using\nmodel = mteb.get_model(model_name)\n</code></pre> <ol> <li>Using <code>mteb.get_model_meta</code> allows us to work with the model without loading it. E.g. you can pass it to <code>mteb.evaluate</code>, which only loads the model if the results doesn't already exist.</li> </ol> <p>You can get an overview of the models available in <code>mteb</code> as follows:</p> <pre><code>model_metas = mteb.get_model_metas()\n\n# You can e.g. use the model metas to find all openai models\nopenai_models = [meta for meta in model_metas if \"openai\" in meta.name]\n</code></pre> <p>Tip</p> <p>Some models require additional dependencies to run on MTEB. An example of such a model is the OpenAI APIs. These dependencies can be installed using <code>pip install mteb[openai]</code> or <code>uv add \"mteb[openai]\"</code></p>"},{"location":"usage/defining_the_model/#using-a-sentence-transformer-model","title":"Using a Sentence Transformer Model","text":"<p>MTEB is made to be compatible with sentence transformers and thus you can readily evaluate any model that can be loaded via. sentence transformers. on <code>MTEB</code>:</p> SentenceTransformersCrossEncoder <pre><code>import mteb\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformers(\"sentence-transformers/LaBSE\")\n\n# select the desired tasks and evaluate\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <pre><code>import mteb\nfrom sentence_transformers import CrossEncoder\nmodel = CrossEncoder(\"sentence-transformers/LaBSE\")\n\n# select a reranking task and evaluate\ntasks = mteb.get_tasks(tasks=[\"AskUbuntuDupQuestions\"])\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>Note</p> <p>We do recommend using <code>mteb.get_model</code> which will by default load the model using the implementation in <code>mteb</code> if there is one, otherwise it will use <code>SentenceTransformers</code> or <code>CrossEncoder</code> from sentence transformers if appropriate. The <code>mteb</code> implementations typically differ due to models requiring specific prompts or similar hyperparameters, and not specifying these may reduce performance (e.g. the multilingual e5 models require specific prompts).</p>"},{"location":"usage/defining_the_model/#using-a-custom-model","title":"Using a Custom Model","text":"<p>It is also possible to implement your own custom model in MTEB as long as it adheres to the EncoderProtocol.</p> <p>This entails implementing an <code>encode</code> function taking as input a list of sentences, and returning a list of embeddings (embeddings can be <code>np.array</code>, <code>torch.tensor</code>, etc.).</p> <pre><code>import mteb\nfrom mteb.types import PromptType, Array\nimport numpy as np\n\n\nclass CustomModel:\n    def encode(\n        self,\n        inputs: DataLoader[BatchedInput],\n        task_metadata: TaskMetadata,\n        hf_split: str,\n        hf_subset: str,\n        prompt_type: PromptType | None = None,\n        **kwargs,\n    ) -&gt; Array:\n        \"\"\"Encodes the given sentences using the encoder.\n\n        Args:\n            inputs: The inputs to encode.\n            task_metadata: The name of the task.\n            hf_subset: The subset of the dataset.\n            hf_split: The split of the dataset.\n            prompt_type: The prompt type to use.\n            **kwargs: Additional arguments to pass to the encoder.\n\n        Returns:\n            The encoded sentences.\n        \"\"\"\n        pass\n\n\n# evaluating the model:\nmodel = CustomModel()\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nmodel = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>If you want to submit your implementation to be included in the leaderboard see the section on submitting a model.</p>"},{"location":"usage/get_started/","title":"Get Started","text":"<p>This usage documentation first introduces a simple example of how to evaluate a model using <code>mteb</code>. Then introduces model detailed section of defining model, selecting tasks and running the evaluation. Each section contains subsections pertaining to these.</p>"},{"location":"usage/get_started/#evaluating-a-model","title":"Evaluating a Model","text":"<p>Evaluating a model on MTEB follows a three step approach, 1) defining model, 2) selecting the tasks and 3) running the evaluation</p> <pre><code>import mteb\n\n# Specify the model that we want to evaluate\nmodel = ...\n\n# specify what you want to evaluate it on\ntasks = mteb.get_tasks(tasks=[\"{task1}\", \"{task2}\"])\n\n# run the evaluation\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>For instance if we want to run <code>\"sentence-transformers/all-MiniLM-L6-v2\"</code> on <code>\"Banking77Classification\"</code> we can do this using the following code:</p> <pre><code>model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# load the model using MTEB\nmodel = mteb.get_model(model_name) # (1)\n\n# select the desired tasks and evaluate\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <ol> <li>Will default to <code>SentenceTransformers(model_name)</code> or <code>CrossEncoder(model_name)</code> if not implemented in MTEB</li> </ol> <p>Note</p> <p>While <code>mteb.evaluate</code> supports <code>SentenceTransformers</code> we do recommend that the user use <code>mteb.get_model</code> to fetch the model as this prioritizes the implementation in <code>mteb</code>, which might not match 1-1 its <code>SentenceTransformers</code> implementation. For leaderboards results we see the <code>mteb</code> implementation as the reference implementation.</p>"},{"location":"usage/get_started/#evaluating-on-different-modalities","title":"Evaluating on Different Modalities","text":"<p>MTEB is not only text evaluating, but also allow you to evaluate image and image-text embeddings.</p> <p>Note</p> <p>Running MTEB on images requires you to install the optional dependencies using <code>pip install mteb[image]</code> or <code>uv add \"mteb[image]\"</code></p> <p>To evaluate image embeddings you can follow the same approach for any other task in <code>mteb</code>. Simply ensuring that the task contains the modality \"image\":</p> <pre><code>tasks = mteb.get_tasks(modalities=[\"image\"]) # Only select tasks with image modalities\ntask = task[0]\n\nprint(task.metadata.modalites)\n# ['text', 'image']\n</code></pre> <p>However, we recommend starting with one of the predefined benchmarks:</p> <pre><code>import mteb\nbenchmark = mteb.get_benchmark(\"MIEB(eng)\")\nmodel = mteb.get_model(\"openai/clip-vit-base-patch16\") # example model\n\nresults = mteb.evaluate(model, tasks=benchmark)\n</code></pre> <p>You can also specify exclusive modality filtering to only get tasks with exactly the requested modalities (default behavior with <code>exclusive_modality_filter=False</code>):</p> <pre><code># Get tasks with image modality, this will also include tasks having both text and image modalities\ntasks = mteb.get_tasks(modalities=[\"image\"], exclusive_modality_filter=False)\n\n# Get tasks that have ONLY image modality\ntasks = mteb.get_tasks(modalities=[\"image\"], exclusive_modality_filter=True)\n</code></pre>"},{"location":"usage/leaderboard/","title":"Running the Leaderboard","text":"<p>This section contains information on how to interact with the leaderboard including running it locally, analysing the results, annotating contamination and more.</p>"},{"location":"usage/leaderboard/#running-the-leaderboard-locally","title":"Running the Leaderboard Locally","text":"<p>It is possible to completely deploy the leaderboard locally or self-host it. This can e.g. be relevant for companies that might want to integrate build their own benchmarks or integrate custom tasks into existing benchmarks.</p> <p>The leaderboard can be run in two ways:</p>"},{"location":"usage/leaderboard/#using-the-cli-command","title":"Using the CLI Command","text":"<p>The easiest way to run the leaderboard is using the MTEB CLI:</p> <pre><code>mteb leaderboard\n</code></pre> <p>You can also specify a custom cache path for model results:</p> <pre><code>mteb leaderboard --cache-path results\n</code></pre> <p>Additional options: - <code>--host HOST</code>: Specify the host to run the server on (default: 0.0.0.0) - <code>--port PORT</code>: Specify the port to run the server on (default: 7860) - <code>--share</code>: Create a public URL for the leaderboard - <code>--rebuild</code>: Force rebuild from full results repository, bypassing cached JSON. This is useful when you suspect the cache is stale or corrupted.</p> <p>Example with all options: <pre><code>mteb leaderboard --cache-path results --port 8080 --share --rebuild\n</code></pre></p>"},{"location":"usage/leaderboard/#cache-directory-structure","title":"Cache Directory Structure","text":"<p>The leaderboard stores data in the cache directory (default: <code>~/.cache/mteb</code> or set via <code>MTEB_CACHE</code> environment variable):</p> <ul> <li><code>results/</code> - Your locally run evaluation results</li> <li><code>remote/</code> - Cloned results from the results repository</li> <li><code>leaderboard/</code> - Pre-computed leaderboard cache for fast loading</li> </ul>"},{"location":"usage/leaderboard/#using-make-command","title":"Using Make Command","text":"<p>Alternatively, you can use the Makefile: <pre><code>make run-leaderboard\n</code></pre></p> <p>The leaderboard requires gradio install, which can be installed using <code>pip install mteb[leaderboard]</code> or <code>uv add \"mteb[leaderboard]\"</code> and requires python &gt;3.10.</p>"},{"location":"usage/leaderboard/#annotate-contamination","title":"Annotate Contamination","text":"<p>have your found contamination in the training data of a model? Please let us know, either by opening an issue or ideally by submitting a PR annotating the training datasets of the model:</p> <pre><code>model_w_contamination = ModelMeta(\n    name = \"model-with-contamination\"\n    ...\n    training_datasets = {\"ArguAna\", ...} # name of dataset within MTEB\n    ...\n)\n</code></pre>"},{"location":"usage/loading_results/","title":"Loading and working with results","text":"<p>To make the results more easily accessible, we have designed functionality for retrieving results from both online and the local cache. Generally you access this functionality using the <code>ResultCache</code> object.</p> <p>For instance, if you are selecting the best model for semantic text similarity (STS) you could fetch the relevant tasks and create a dataframe of the results using the following code:</p> <pre><code>import mteb\n\ntasks = mteb.get_tasks(tasks=[\"STS12\"])\nmodel_names = [\"intfloat/multilingual-e5-large\"]\n\ncache = mteb.ResultCache(\"~/.cache/mteb\")\nresults = cache.load_results(models=model_names, tasks=tasks)\n</code></pre> <p>From this you will get a <code>BenchmarkResults</code> object: <pre><code>results\n# BenchmarkResults(model_results=[...](#1))\ntype(results)\n# mteb.load_results.benchmark_results.BenchmarkResults\n</code></pre> Which you can then convert to a dataframe:</p> <pre><code>df = results.to_dataframe()\n</code></pre>"},{"location":"usage/loading_results/#working-with-public-results","title":"Working with public results","text":"<p>All previously submitted results are available results repository.</p> <p>You can download this using:</p> <pre><code>import mteb\n\ncache = mteb.ResultCache()\ncache.download_from_remote() # download results from the remote repository\n</code></pre> <p>From here, you can work with the cache as usual. For instance, if you are selecting the best model for your French and English retrieval task on legal documents, you could fetch the relevant tasks and create a dataframe of the results using the following code:</p> <pre><code>import mteb\n\n# select your tasks\ntasks = mteb.get_tasks(task_types=[\"Retrieval\"], languages=[\"eng\", \"fra\"], domains=[\"Legal\"])\n\nmodel_names = [\n    \"GritLM/GritLM-7B\",\n    \"intfloat/multilingual-e5-large\",\n]\n\n\ncache = mteb.ResultCache()\ncache.download_from_remote() # download results from the remote repository. Might take a while the first time.\n\nresults = cache.load_results(\n    models=model_names,\n    tasks=tasks,\n    include_remote=True, # default\n)\n</code></pre>"},{"location":"usage/loading_results/#working-with-benchmarkresults","title":"Working with <code>BenchmarkResults</code>","text":"<p>The result object is a convenient object in <code>mteb</code> for working with dataframes and allows you to quick examine your results.</p> <p></p> <p>The object contain a lot of convenience functions for inspecting and examining the results: <pre><code>print(results.model_names)\n# ['GritLM/GritLM-7B', 'intfloat/multilingual-e5-large']\n\ntask_names = results.task_names\nprint(task_names)\n# ['SpartQA', 'PlscClusteringP2P.v2', 'StackOverflowQA', 'JSICK', ...\n</code></pre></p>"},{"location":"usage/loading_results/#getting-benchmark-results","title":"Getting Benchmark Results","text":"<p>If you loaded results for a specific benchmark, you can get the aggregated benchmark scores for each model using the <code>get_benchmark_result()</code> method:</p> <pre><code>import mteb\n\n# Load results for a specific benchmark\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\ncache = mteb.ResultCache()\ncache.download_from_remote()  # download results from the remote repository\nresults = cache.load_results(\n    models=[\"intfloat/e5-small\", \"intfloat/multilingual-e5-small\"],\n    tasks=benchmark,\n)\n\nbenchmark_scores_df = results.get_benchmark_result()\nprint(benchmark_scores_df)\n#    Rank (Borda)                                              Model  Zero-shot  Memory Usage (MB)  Number of Parameters (B)  Embedding Dimensions  Max Tokens  ...  Classification  Clustering  Pair Classification  Reranking  Retrieval       STS  Summarization\n# 0             1  [e5-small](https://huggingface.co/intfloat/e5-...        100                127                     0.033                   384       512.0  ...        0.599545    0.422085             0.850895   0.444613   0.450684  0.790284       0.310609\n# 1             2  [multilingual-e5-small](https://huggingface.co...         95                449                     0.118                   384       512.0  ...        0.673919    0.413591             0.840878   0.431942   0.464342  0.800185       0.292190\n</code></pre>"},{"location":"usage/loading_results/#filtering-results","title":"Filtering Results","text":"<p>There is also utility function that allows you to select certain models or tasks: <pre><code># select only gritLM\nresults = results.select_models([\"GritLM/GritLM-7B\"])\n\n# select only retrieval tasks\ntasks = mteb.get_tasks(tasks=task_names)\nretrieval_tasks = [task for task in tasks if task.metadata.type == \"Retrieval\"]\n\nresults = results.select_tasks(retrieval_tasks)\n</code></pre></p>"},{"location":"usage/loading_results/#creating-a-dataframe","title":"Creating a Dataframe","text":"<pre><code>df = results.to_dataframe()\n\nprint(df)\n# model_name                        task_name  GritLM/GritLM-7B\n# 0                              AILAStatutes          0.418000\n# 1                                   ArguAna          0.631710\n# 2                         BelebeleRetrieval          0.717035\n# 3                            CovidRetrieval          0.734010\n# 4                           HagridRetrieval          0.986730\n# 5                      LEMBPasskeyRetrieval          0.382500\n# 6               LegalBenchCorporateLobbying          0.949990\n# 7              MIRACLRetrievalHardNegatives          0.516793\n# 8                             MLQARetrieval          0.727420\n# 9                                   SCIDOCS          0.244090\n# 10                                  SpartQA          0.093550\n# 11                          StackOverflowQA          0.933670\n# 12          StatcanDialogueDatasetRetrieval          0.457587\n# 13                                TRECCOVID          0.743130\n# 14                             TempReasonL1          0.071640\n# 15                   TwitterHjerneRetrieval          0.432660\n# 16           WikipediaRetrievalMultilingual          0.917722\n# 17                               WinoGrande          0.536970\n</code></pre> <p>By default this will give you the results in a <code>\"wide\"</code> format. However, you can just as well get them in a long format:</p> <pre><code>long_format_df = results.to_dataframe(format=\"long\")\n\nprint(long_format_df.head(5))\n#          model_name          task_name     score\n# 0  GritLM/GritLM-7B       AILAStatutes  0.418000\n# 1  GritLM/GritLM-7B            ArguAna  0.631710\n# 2  GritLM/GritLM-7B  BelebeleRetrieval  0.717035\n# 3  GritLM/GritLM-7B     CovidRetrieval  0.734010\n# 4  GritLM/GritLM-7B    HagridRetrieval  0.986730\n</code></pre>"},{"location":"usage/loading_results/#adding-metadata-to-table","title":"Adding metadata to table","text":"<p>One might want to add some more metadata to the table. This is luckily quite easy using:</p> <pre><code>import pandas as pd\n\ntask_df = tasks.to_dataframe(properties=[\"name\", \"type\", \"domains\"])\ntask_df = task_df.rename(columns={\"name\": \"task_name\"})\n\ndf_with_meta = pd.merge(task_df, df)\n\nprint(df_with_meta.head(5))\n#            task_name       type                   domains  GritLM/GritLM-7B\n# 0            SpartQA  Retrieval  [Encyclopaedic, Written]          0.093550\n# 1    StackOverflowQA  Retrieval    [Programming, Written]          0.933670\n# 2  BelebeleRetrieval  Retrieval      [Web, News, Written]          0.717035\n# 3            ArguAna  Retrieval        [Medical, Written]          0.631710\n# 4       TempReasonL1  Retrieval  [Encyclopaedic, Written]          0.071640\n</code></pre>"},{"location":"usage/running_the_evaluation/","title":"Running the Evaluation","text":"<p>This section contains documentation related to the runtime of the evaluation. How to pass arguments to the encoder, saving outputs and similar.</p>"},{"location":"usage/running_the_evaluation/#simple-example","title":"Simple Example","text":"<p>Evaluating models in <code>mteb</code> typically takes the simple form:</p> PythonCLI <pre><code>import mteb\n\nmodel = mteb.get_model(\"sentence-transformers/static-similarity-mrl-multilingual-v1\")\ntasks = mteb.get_task([\"MultiHateClassification\"], languages = [\"ita\", \"dan\"])\n\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <pre><code>mteb run -t MultiHateClassification \\\n  -m sentence-transformers/static-similarity-mrl-multilingual-v1 \\\n  -l ita dan\n</code></pre> <p>Compatibility with SentenceTransformers</p> <p><code>mteb</code> is designed to be compatible with <code>sentence-transformers</code> so you can also directly pass a <code>SentenceTransformer</code> or <code>CrossEncoder</code> to <code>mteb.evaluate</code>.</p>"},{"location":"usage/running_the_evaluation/#specifying-the-cache","title":"Specifying the cache","text":"<p>By default <code>mteb</code> with save the results in cache folder located at <code>~/.cache/mteb</code>, however if you want to save the results in a specific folder you can specify it as follows:</p> <pre><code>cache = mteb.ResultCache(cache_path=\"~/.cache/mteb\")\nresults = mteb.evaluate(model, tasks=tasks, cache=cache)\n</code></pre> <p>If you don't wish to run model which results already exist on the leaderboard you can download these simply by running:</p> <pre><code>cache.download_from_remote()\nresults = mteb.evaluate(model, tasks=tasks, cache=cache)\n</code></pre>"},{"location":"usage/running_the_evaluation/#tracking-carbon-emissions","title":"Tracking Carbon Emissions","text":"<p><code>mteb</code> allows for easy tracking of carbon emission eq. using <code>codecarbon</code>. You simply need to install <code>mteb[codecarbon]</code> and enable co2 tracking:</p> PythonCLI <pre><code>results = mteb.evaluate(tasks=tasks, co2_tracker=True)\n</code></pre> <pre><code>mteb run -t NanoArguAnaRetrieval \\\n  -m sentence-transformers/static-similarity-mrl-multilingual-v1 \\\n  --co2-tracker\n</code></pre>"},{"location":"usage/running_the_evaluation/#passing-in-encode-arguments","title":"Passing in <code>encode</code> arguments","text":"<p>To pass in arguments to the model's <code>encode</code> function, you can use the encode keyword arguments (<code>encode_kwargs</code>):</p> <pre><code>mteb.evaluate(model, tasks, encode_kwargs={\"batch_size\": 32})\n</code></pre>"},{"location":"usage/running_the_evaluation/#running-sentencetransformer-model-with-prompts","title":"Running SentenceTransformer model with prompts","text":"<p>Prompts can be passed to the SentenceTransformer model using the <code>prompts</code> parameter. The following code shows how to use prompts with SentenceTransformer:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer(\n    \"intfloat/multilingual-e5-small\", \n  prompts={\"query\": \"Query:\", \"document\": \"Passage:\"}\n)\nresults = mteb.evaluate(model, tasks=tasks)\n</code></pre> <p>In prompts the key can be:</p> <ol> <li>Prompt types (<code>passage</code>, <code>query</code>) - they will be used in reranking and retrieval tasks</li> <li>Task type - these prompts will be used in all tasks of the given type</li> <li><code>BitextMining</code></li> <li><code>Classification</code></li> <li><code>MultilabelClassification</code></li> <li><code>Clustering</code></li> <li><code>PairClassification</code></li> <li><code>Reranking</code></li> <li><code>Retrieval</code></li> <li><code>STS</code></li> <li><code>Summarization</code></li> <li><code>InstructionRetrieval</code></li> <li>Pair of task type and prompt type like <code>Retrieval-query</code> - these prompts will be used in all Retrieval tasks</li> <li>Task name - these prompts will be used in the specific task</li> <li>Pair of task name and prompt type like <code>NFCorpus-query</code> - these prompts will be used in the specific task</li> </ol>"},{"location":"usage/running_the_evaluation/#saving-predictions","title":"Saving predictions","text":"<p>To save the predictions from a task simply set the <code>prediction_folder</code>:</p> PythonCLI <pre><code># ...\nresults = mteb.evaluate(\n    model,\n    task,\n    prediction_folder=\"model_predictions\",\n)\n</code></pre> <pre><code>mteb run -t NanoArguAnaRetrieval \\\n  -m sentence-transformers/static-similarity-mrl-multilingual-v1 \\\n  --prediction-folder predictions\n</code></pre> <p>The file will now be saved to <code>\"{prediction_folder}/{task_name}_predictions.json\"</code> and contain the rankings for each query along with the model name and revision of the model that produced the result.</p>"},{"location":"usage/running_the_evaluation/#speeding-up-evaluations","title":"Speeding up Evaluations","text":"<p>Evaluation in MTEB consists of three main components: 1) downloading the dataset, 2) encoding of the samples, and 3) the evaluation. Typically, the most notable bottlenecks are either in the encoding step or in the download step. Where this bottleneck is depends on the tasks that you evaluate on and the models that you evaluate.</p> <p>If you find that any of our design decisions prevents you from running the evaluation efficiently, do feel free to create an issue.</p> <p>Info</p> <p>In version <code>2.1.4</code> an issue was found that caused the GPU and CPU to idle during evaluation of retrieval/reranking tasks. We suggest upgrading to <code>mteb&gt;=2.2.0</code>.</p>"},{"location":"usage/running_the_evaluation/#speeding-up-the-model","title":"Speeding up the Model","text":"<p>MTEB is an evaluation framework, and therefore, we try to avoid doing inference optimizations. However, here are a few tricks to speed up inference.</p> <p>First of all, it is possible to pass directly to the model loader: <pre><code>import mteb\n\nmeta = mteb.get_model_meta(\"intfloat/multilingual-e5-small\")\n\nkwargs = dict(\n    device=\"cuda\", # use a gpu\n    model_kwargs={\"torch_dtype\": \"float16\"}, # use low-precision\n)\n\nmodel = meta.load_model(**kwargs) # passed to model loader, e.g. SentenceTransformer\n</code></pre></p> <p>This e.g., allows you to use all the inference optimization tricks from sentence-transformers. However, in general, you can always pass <code>device</code>.</p> <p>If you can't find the required keyword argument, a solution might be to extract the model: <pre><code>model = meta.load_model(**kwargs)\n# extract model\nsentence_trf_model = model.model\n\n# optimizations:\nsentence_trf_model.half() # half precision\n</code></pre></p> <p>A last option is to make a custom implementation of the model. This way you have full flexibility of how the model handles the input.</p>"},{"location":"usage/running_the_evaluation/#speeding-download","title":"Speeding Download","text":"<p>The simplest way to speed up downloads is by using Huggingface's <code>xet</code>. You can use this simply using:</p> <pre><code># pip\npip install mteb[xet]\n\n# or uv\nuv add \"mteb[xet]\"\n</code></pre> <p>For one of the larger datasets, <code>MrTidyRetrieval</code> (~15 GB), we have seen speed-ups from ~40 minutes to ~30 minutes while using <code>xet</code>.</p>"},{"location":"usage/selecting_tasks/","title":"Selecting Tasks or Benchmarks","text":"<p>This section describes how to select benchmarks and tasks to evaluate, including selecting specific subsets or splits to run.</p>"},{"location":"usage/selecting_tasks/#selecting-a-benchmark","title":"Selecting a Benchmark","text":"<p><code>mteb</code> comes with a set of predefined benchmarks. These can be fetched using <code>get_benchmark</code> or <code>get_benchmarks</code> and run in a similar fashion to other sets of tasks. For instance to select the English benchmark that forms the English leaderboard:</p> <pre><code>import mteb\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nmodel = ...\nresults = mteb.evaluate(model, tasks=benchmark)\n</code></pre> <p>The benchmark specifies not only a list of tasks, but also what splits and language to run on.</p>"},{"location":"usage/selecting_tasks/#filtering-benchmark-tasks","title":"Filtering Benchmark Tasks","text":"<p>You can filter benchmarks to evaluate your model on specific subsets of tasks. Use the tabs below to explore different filtering approaches:</p> By Task TypeBy LanguageBy DomainCustom Filters <p>Filter a benchmark to only include specific task types. This is useful when you want to evaluate your model on a subset of tasks:</p> <pre><code>import mteb\n\n# Get the full English benchmark\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\n\n# Filter to only retrieval tasks\nretrieval_tasks = mteb.filter_tasks(benchmark, task_types=[\"Retrieval\"])\nprint(f\"Found {len(retrieval_tasks)} retrieval tasks\")\n\n# Run evaluation on only retrieval tasks\nmodel = mteb.get_model(...)\nresults = mteb.evaluate(model, tasks=retrieval_tasks)\n</code></pre> <p>You can filter by any task type:</p> <ul> <li><code>\"Retrieval\"</code> - Information retrieval tasks</li> <li><code>\"Classification\"</code> - Text classification tasks</li> <li><code>\"Clustering\"</code> - Document clustering tasks</li> <li><code>\"STS\"</code> - Semantic textual similarity tasks</li> <li><code>\"PairClassification\"</code> - Pair classification tasks</li> <li><code>\"Reranking\"</code> - Reranking tasks</li> <li><code>\"Summarization\"</code> - Text summarization tasks</li> <li><code>\"InstructionRetrieval\"</code> - Instruction-based retrieval tasks</li> </ul> <p>For multiple task types:</p> <pre><code># Get retrieval and reranking tasks from a benchmark\nfiltered_tasks = [\n    task for task in benchmark.tasks\n    if task.metadata.type in [\"Retrieval\", \"Reranking\"]\n]\n</code></pre> <p>Filter tasks by language using ISO 639-3 language codes:</p> <pre><code>import mteb\n\n# Get all English retrieval tasks\neng_retrieval_tasks = mteb.get_tasks(\n    task_types=[\"Retrieval\"],\n    languages=[\"eng\"]\n)\n\n# Get tasks in multiple languages\nmultilingual_tasks = mteb.get_tasks(\n    languages=[\"eng\", \"fra\", \"deu\", \"spa\"]\n)\n\n# Get retrieval tasks from the English benchmark\neng_benchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nbenchmark_task_names = [task.metadata.name for task in eng_benchmark.tasks]\n\nretrieval_from_benchmark = mteb.get_tasks(\n    task_types=[\"Retrieval\"],\n    tasks=benchmark_task_names  # Only tasks from the benchmark\n)\n\nprint(f\"Found {len(retrieval_from_benchmark)} retrieval tasks in MTEB(eng, v2)\")\n</code></pre> <p>For multilingual/cross-lingual tasks:</p> <pre><code># Specify which languages to load\ntasks = [\n    mteb.get_task(\"AmazonReviewsClassification\", languages=[\"eng\", \"fra\"]),\n    mteb.get_task(\"BUCCBitextMining\", languages=[\"deu\"]), # all subsets containing \"deu\"\n]\n\n# Filter tasks supporting multiple languages\nmultilingual_retrieval = mteb.get_tasks(\n    task_types=[\"Retrieval\"],\n    modalities=[\"text\"]\n)\nmultilingual_retrieval = [\n    task for task in multilingual_retrieval\n    if len(task.metadata.languages) &gt; 1\n]\n</code></pre> <p>Filter tasks by their domain to focus on specific areas:</p> <pre><code>import mteb\n\n# Get tasks in specific domains\nlegal_tasks = mteb.get_tasks(domains=[\"Legal\"])\n\n# Get English retrieval tasks in scientific domains\nspecialized_tasks = mteb.get_tasks(\n    task_types=[\"Retrieval\", \"InstructionRetrieval\"],\n    languages=[\"eng\"],\n    domains=[\"Scientific\", \"Medical\", \"Legal\"]\n)\n\n# Filter benchmark tasks by domain\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\nscientific_tasks = [\n    task for task in benchmark.tasks\n    if \"Scientific\" in task.metadata.domains\n]\n</code></pre> <p>Combine multiple criteria for advanced filtering:</p> <pre><code>import mteb\n\n# Complex filter: English classification in legal domain\nfiltered = mteb.get_tasks(\n    task_types=[\"Classification\"],\n    languages=[\"eng\"],\n    domains=[\"Legal\"],\n    modalities=[\"text\"]\n)\n\n# Filter by custom logic\nbenchmark = mteb.get_benchmark(\"MTEB(eng, v2)\")\n\n# Get short retrieval tasks (&lt; 10k documents)\nshort_retrieval = [\n    task for task in benchmark.tasks\n    if task.metadata.type == \"Retrieval\"\n    and hasattr(task, 'metadata_dict')\n    and task.metadata_dict.get('n_documents', float('inf')) &lt; 10000\n]\n\n# Filter by task name patterns\nnews_tasks = [\n    task for task in benchmark.tasks\n    if \"news\" in task.metadata.name.lower()\n    or \"News\" in task.metadata.domains\n]\n\n# Combine filters with set operations\nretrieval_set = set(mteb.get_tasks(task_types=[\"Retrieval\"]))\nenglish_set = set(mteb.get_tasks(languages=[\"eng\"]))\neng_retrieval = list(retrieval_set &amp; english_set)\n</code></pre> <p>Note</p> <p>Generally we use the naming scheme for benchmarks <code>MTEB(*)</code>, where the \"*\" denotes the target of the benchmark. In the case of a language, we use the three-letter language code. For large groups of languages, we use the group notation, e.g., <code>MTEB(Scandinavian, v1)</code> for Scandinavian languages. External benchmarks implemented in MTEB like <code>CoIR</code><sup>1</sup> use their original name.</p> <p>To get an overview of all available benchmarks, simply run:</p> <pre><code>import mteb\nbenchmarks = mteb.get_benchmarks()\n</code></pre> <p>When using a benchmark from MTEB please cite <code>mteb</code> along with the citations of the benchmark which you can access using <code>benchmark.citation</code>.</p>"},{"location":"usage/selecting_tasks/#selecting-a-task","title":"Selecting a Task","text":"<p><code>mteb</code> comes with the utility function <code>get_task</code> and <code>get_tasks</code> for fetching and analysing the tasks of interest.</p> <p>This can be done in multiple ways, e.g.:</p> <ul> <li>by the task name</li> <li>by their type (e.g. \"Clustering\" or \"Classification\")</li> <li>by their languages (specified as a three letter code)</li> <li>by their domains</li> <li>by their modalities</li> <li>and many more</li> </ul> <pre><code># by name\ntasks = mteb.get_tasks(tasks=[\"Banking77Classification\"])\n# by type\ntasks = mteb.get_tasks(task_types=[\"Clustering\", \"Retrieval\"]) # (1)\n# by language\ntasks = mteb.get_tasks(languages=[\"eng\", \"deu\"]) # (2)\n# by domain\ntasks = get_tasks(domains=[\"Legal\"])\n# by modality\ntasks = mteb.get_tasks(modalities=[\"text\", \"image\"]) # (3)\n# or using multiple\ntasks = get_tasks(languages=[\"eng\", \"deu\"], script=[\"Latn\"], domains=[\"Legal\"])\n</code></pre> <ol> <li> <p>Only select clustering and retrieval tasks</p> </li> <li> <p>Only select datasets which contain \"eng\" or \"deu\" (iso 639-3 codes)</p> </li> <li> <p>Only select tasks with text or image modalities</p> </li> </ol>"},{"location":"usage/selecting_tasks/#selecting-evaluation-split-or-subsets","title":"Selecting Evaluation Split or Subsets","text":"<p>A task in <code>mteb</code> mirrors the structure of a dataset on Huggingface. It includes a splits (i.e. \"test\") and a subset.</p> <pre><code># selecting an evaluation split\ntask = mteb.get_task(\"Banking77Classification\", eval_splits=[\"test\"])\n# selecting a Huggingface subset\ntask = mteb.get_task(\"AmazonReviewsClassification\", hf_subsets=[\"en\", \"fr\"])\n</code></pre> <p>What is a subset?</p> <p>A subset on a Huggingface dataset is what you specify after the dataset name, e.g. <code>datasets.load_dataset(\"nyu-mll/glue\", \"cola\")</code>. Often the subset does not need to be defined and is left as \"default\". The subset is however useful, especially for multilingual datasets to specify the desired language or language pair e.g. in <code>mteb/bucc-bitext-mining</code> we might want to evaluate only on the French-English subset <code>\"fr-en\"</code>.</p>"},{"location":"usage/selecting_tasks/#using-a-custom-task","title":"Using a Custom Task","text":"<p>To evaluate on a custom task, you can run the following code on your custom task. See how to add a new task, for how to create a new task in MTEB.</p> <pre><code>import mteb\nfrom mteb.abstasks.retrieval import AbsTaskRetrieval\nfrom mteb.abstasks.task_metadata import TaskMetadata\n\n\nclass MyCustomTask(AbsTaskRetrieval):\n    metadata = TaskMetadata(...)\n\nmodel = mteb.get_model(...)\nresults = mteb.evaluate(model, tasks=[MyCustomTask()])\n</code></pre> <ol> <li> <p>Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Yichun Yin, Hao Zhang, Yong Liu, Yasheng Wang, and Ruiming Tang. Coir: a comprehensive benchmark for code information retrieval models. 2024. URL: https://arxiv.org/abs/2407.02883, arXiv:2407.02883.\u00a0\u21a9</p> </li> </ol>"}]}