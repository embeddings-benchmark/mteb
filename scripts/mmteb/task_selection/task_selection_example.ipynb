{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Selection: An Example\n",
    "\n",
    "This is an example of how to perform task selection using MTEB when creating a benchmark. The goal here is to subsample a potentially large number of tasks down to the one with the most information. We do this as a feature selection approach, where we remove a task if its performance if predictable by the performance of other tasks. See the paper for more information.\n",
    "\n",
    "For this example we will be using Danish (dan) as it has relatively few tasks, but there is not reason to limit to only Danish tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au561649/.virtualenvs/mteb/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mteb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "We will start out by loading in the relevant data for the model and tasks of interests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting revision for sentence-transformers/all-MiniLM-L12-v2\n",
      "Getting revision for sentence-transformers/all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "def get_models():\n",
    "    model_names = [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"sentence-transformers/LaBSE\",\n",
    "        \"intfloat/multilingual-e5-large-instruct\",\n",
    "        \"intfloat/e5-mistral-7b-instruct\",\n",
    "        \"GritLM/GritLM-7B\",\n",
    "        \"GritLM/GritLM-8x7B\",\n",
    "        \"intfloat/multilingual-e5-small\",\n",
    "        \"intfloat/multilingual-e5-base\",\n",
    "        \"intfloat/multilingual-e5-large\",\n",
    "    ]\n",
    "    models: list[mteb.ModelMeta] = [mteb.get_model_meta(name) for name in model_names]\n",
    "\n",
    "    # get missing revisions - Assuming we are using the latest revision\n",
    "    for model in models:\n",
    "        if model.revision is None:\n",
    "            print(f\"Getting revision for {model.name}\")\n",
    "            encoder = model.load_model()\n",
    "            model.revision = encoder.model_card_data.base_model_revision  # type: ignore\n",
    "\n",
    "    return models\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "danish_tasks = mteb.get_tasks(languages=[\"dan\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BornholmBitextMining\n",
      "BibleNLPBitextMining\n",
      "FloresBitextMining\n",
      "NTREXBitextMining\n",
      "Tatoeba\n",
      "AngryTweetsClassification\n",
      "DanishPoliticalCommentsClassification\n",
      "DKHateClassification\n",
      "LccSentimentClassification\n",
      "MassiveIntentClassification\n",
      "MassiveScenarioClassification\n",
      "NordicLangClassification\n",
      "ScalaClassification\n",
      "SIB200Classification\n",
      "DanFeverRetrieval\n",
      "TV2Nordretrieval\n",
      "TwitterHjerneRetrieval\n",
      "BelebeleRetrieval\n",
      "WikipediaRetrievalMultilingual\n",
      "MultiEURLEXMultilabelClassification\n",
      "WikipediaRerankingMultilingual\n",
      "SIB200ClusteringS2S\n",
      "WikiClusteringP2P.v2\n"
     ]
    }
   ],
   "source": [
    "# just to see what tasks we are working with\n",
    "for task in danish_tasks:\n",
    "    print(task.metadata.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mteb_dataset_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Github/mteb/mteb/load_results/mteb_results.py:278\u001b[0m, in \u001b[0;36mMTEBResults.from_disk\u001b[0;34m(cls, path, load_historic_data)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.virtualenvs/mteb/lib/python3.8/site-packages/pydantic/main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for MTEBResults\ndataset_revision\n  Field required [type=missing, input_value={'name': 'sentence-transf...rmers'], 'loader': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\ntask_name\n  Field required [type=missing, input_value={'name': 'sentence-transf...rmers'], 'loader': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\nmteb_version\n  Field required [type=missing, input_value={'name': 'sentence-transf...rmers'], 'loader': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\nscores\n  Field required [type=missing, input_value={'name': 'sentence-transf...rmers'], 'loader': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\nevaluation_time\n  Field required [type=missing, input_value={'name': 'sentence-transf...rmers'], 'loader': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load results from mteb/results repository\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mteb_results \u001b[38;5;241m=\u001b[39m \u001b[43mmteb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# note that this will produce a bunch of warnings, this is mainly due to the function loading ALL results, including historic mteb\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/mteb/mteb/load_results/load_results.py:160\u001b[0m, in \u001b[0;36mload_results\u001b[0;34m(results_repo, download_latest, models, require_model_meta)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         task_json_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    158\u001b[0m             f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m revision_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_meta.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m!=\u001b[39m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    159\u001b[0m         ]\n\u001b[0;32m--> 160\u001b[0m         results[model_name][revision] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    161\u001b[0m             mteb\u001b[38;5;241m.\u001b[39mMTEBResults\u001b[38;5;241m.\u001b[39mfrom_disk(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m task_json_files\n\u001b[1;32m    162\u001b[0m         ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(results)\n",
      "File \u001b[0;32m~/Github/mteb/mteb/load_results/load_results.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         task_json_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    158\u001b[0m             f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m revision_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_meta.json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m!=\u001b[39m f\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    159\u001b[0m         ]\n\u001b[1;32m    160\u001b[0m         results[model_name][revision] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 161\u001b[0m             \u001b[43mmteb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMTEBResults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m task_json_files\n\u001b[1;32m    162\u001b[0m         ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(results)\n",
      "File \u001b[0;32m~/Github/mteb/mteb/load_results/mteb_results.py:285\u001b[0m, in \u001b[0;36mMTEBResults.from_disk\u001b[0;34m(cls, path, load_historic_data)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    282\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load MTEBResults from disk, got error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Attempting to load from disk using format from before v1.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 285\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_before_v1_11_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m pre_v_12_48 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmteb_version\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m Version(\n\u001b[1;32m    288\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmteb_version\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    289\u001b[0m ) \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.12.48\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_v_12_48:\n",
      "File \u001b[0;32m~/Github/mteb/mteb/load_results/mteb_results.py:328\u001b[0m, in \u001b[0;36mMTEBResults._convert_from_before_v1_11_0\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    323\u001b[0m scores \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata}\n\u001b[1;32m    325\u001b[0m dataset_revision \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mpop(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset revision not available\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m )\n\u001b[0;32m--> 328\u001b[0m task_name \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmteb_dataset_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m mteb_version \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmteb_version\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmteb version not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# calculate evaluation time across all splits (move to top level)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mteb_dataset_name'"
     ]
    }
   ],
   "source": [
    "# load results from mteb/results repository\n",
    "mteb_results = mteb.load_results(models=models) \n",
    "\n",
    "# note that this will produce a bunch of warnings, this is mainly due to the function loading ALL results, including historic mteb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and preprocessing result files\n",
    "\n",
    "As multiple result files contain performance metrics on other splits that the ones specified by the task we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mteb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
