
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../image_text/">
      
      
        <link rel="next" href="../../../api/">
      
      
      <link rel="icon" href="../../../images/logos/mteb_logo/dots-icon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Text Model - Massive Text Embedding Benchmark</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#text-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Massive Text Embedding Benchmark" class="md-header__button md-logo" aria-label="Massive Text Embedding Benchmark" data-md-component="logo">
      
  <img src="../../../images/logos/mteb_logo/dots-icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Massive Text Embedding Benchmark
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Text Model
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/embeddings-benchmark/mteb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../.." class="md-tabs__link">
          
  
  
    
  
  Get Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../contributing/adding_a_model/" class="md-tabs__link">
          
  
  
    
  
  Contributing

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  Overview

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../api/" class="md-tabs__link">
          
  
  
    
  
  API

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://huggingface.co/spaces/mteb/leaderboard" class="md-tabs__link">
        
  
  
    
  
  Leaderboard

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Massive Text Embedding Benchmark" class="md-nav__button md-logo" aria-label="Massive Text Embedding Benchmark" data-md-component="logo">
      
  <img src="../../../images/logos/mteb_logo/dots-icon.png" alt="logo">

    </a>
    Massive Text Embedding Benchmark
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/embeddings-benchmark/mteb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../.." class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Get Started
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_1" id="__nav_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Get Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../whats_new/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    New in v2.0 🎉
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Installation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4" >
        
          
          <label class="md-nav__link" for="__nav_1_4" id="__nav_1_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Usage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4">
            <span class="md-nav__icon md-icon"></span>
            Usage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/get_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Get Started
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/defining_the_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Defining the Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/selecting_tasks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Selecting Tasks
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/running_the_evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Running the Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/loading_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Loading Results
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/cli/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Command Line Interface
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/leaderboard/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Running the Leaderboard
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5" >
        
          
          <label class="md-nav__link" for="__nav_1_5" id="__nav_1_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Advanced Usage
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5">
            <span class="md-nav__icon md-icon"></span>
            Advanced Usage
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/two_stage_reranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Two stage reranking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/cache_embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cache embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Contributing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adding a Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adding a Task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Adding a Benchmark
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Guidelines
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Overview
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Overview
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Benchmarks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Benchmarks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_benchmarks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Available Benchmarks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            Tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_1" id="__nav_3_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Available Tasks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_1">
            <span class="md-nav__icon md-icon"></span>
            Available Tasks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/any2anymultilingualretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Any2AnyMultilingualRetrieval
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/any2anyretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Any2AnyRetrieval
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/bitextmining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    BitextMining
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/classification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Classification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/clustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Clustering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/compositionality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Compositionality
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/documentunderstanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DocumentUnderstanding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/imageclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ImageClassification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/imageclustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ImageClustering
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/instructionreranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructionReranking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/instructionretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    InstructionRetrieval
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/multilabelclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MultilabelClassification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/pairclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    PairClassification
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Regression
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/reranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reranking
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Retrieval
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/sts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    STS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/summarization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Summarization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visioncentricqa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VisionCentricQA
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visualsts%28eng%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VisualSTS(eng)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visualsts%28multi%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    VisualSTS(multi)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/zeroshotclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ZeroShotClassification
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4_1" id="__nav_3_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Available Models
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4_1">
            <span class="md-nav__icon md-icon"></span>
            Available Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image_text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image-text Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Text Model
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Text Model
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      Non-instruction Model
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../api/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Evaluation
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmark
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/task/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Task
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Results
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Additional Types
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://huggingface.co/spaces/mteb/leaderboard" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Leaderboard
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      Instruction Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      Non-instruction Model
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/embeddings-benchmark/mteb/blob/main/docs/overview/available_models/text.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/embeddings-benchmark/mteb/raw/main/docs/overview/available_models/text.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="text-model">Text Model<a class="headerlink" href="#text-model" title="Permanent link">&para;</a></h1>
<!-- This document is auto-generated. Changes will be overwritten. Please change the generating script. -->

<ul>
<li><strong>Number of models:</strong> 206</li>
</ul>
<h2 id="instruction-model">Instruction Model<a class="headerlink" href="#instruction-model" title="Permanent link">&para;</a></h2>
<h4 id="alibaba-nlpgte-qwen15-7b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct"><code>Alibaba-NLP/gte-Qwen1.5-7B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen15-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.7B</td>
<td>28.8 GB</td>
<td>2024-04-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="alibaba-nlpgte-qwen2-15b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct"><code>Alibaba-NLP/gte-Qwen2-1.5B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen2-15b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>8960</td>
<td>1.8B</td>
<td>6.6 GB</td>
<td>2024-07-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="alibaba-nlpgte-qwen2-7b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct"><code>Alibaba-NLP/gte-Qwen2-7B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen2-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.6B</td>
<td>28.4 GB</td>
<td>2024-06-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-en"><a href="https://huggingface.co/BAAI/bge-base-en"><code>BAAI/bge-base-en</code></a><a class="headerlink" href="#baaibge-base-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-base-en-v15"><a href="https://huggingface.co/BAAI/bge-base-en-v1.5"><code>BAAI/bge-base-en-v1.5</code></a><a class="headerlink" href="#baaibge-base-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-09-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge_embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-zh"><a href="https://huggingface.co/BAAI/bge-base-zh"><code>BAAI/bge-base-zh</code></a><a class="headerlink" href="#baaibge-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-08-05</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-base-zh-v15"><a href="https://huggingface.co/BAAI/bge-base-zh-v1.5"><code>BAAI/bge-base-zh-v1.5</code></a><a class="headerlink" href="#baaibge-base-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>416.0 MB</td>
<td>2023-09-11</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-large-en"><a href="https://huggingface.co/BAAI/bge-large-en"><code>BAAI/bge-large-en</code></a><a class="headerlink" href="#baaibge-large-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-large-en-v15"><a href="https://huggingface.co/BAAI/bge-large-en-v1.5"><code>BAAI/bge-large-en-v1.5</code></a><a class="headerlink" href="#baaibge-large-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-09-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge_embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-large-zh"><a href="https://huggingface.co/BAAI/bge-large-zh"><code>BAAI/bge-large-zh</code></a><a class="headerlink" href="#baaibge-large-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-08-02</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-large-zh-v15"><a href="https://huggingface.co/BAAI/bge-large-zh-v1.5"><code>BAAI/bge-large-zh-v1.5</code></a><a class="headerlink" href="#baaibge-large-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-09-12</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-small-en"><a href="https://huggingface.co/BAAI/bge-small-en"><code>BAAI/bge-small-en</code></a><a class="headerlink" href="#baaibge-small-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-small-en-v15"><a href="https://huggingface.co/BAAI/bge-small-en-v1.5"><code>BAAI/bge-small-en-v1.5</code></a><a class="headerlink" href="#baaibge-small-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-09-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge_embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-small-zh"><a href="https://huggingface.co/BAAI/bge-small-zh"><code>BAAI/bge-small-zh</code></a><a class="headerlink" href="#baaibge-small-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-08-05</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-small-zh-v15"><a href="https://huggingface.co/BAAI/bge-small-zh-v1.5"><code>BAAI/bge-small-zh-v1.5</code></a><a class="headerlink" href="#baaibge-small-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>91.0 MB</td>
<td>2023-09-12</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="bmretrieverbmretriever-1b"><a href="https://huggingface.co/BMRetriever/BMRetriever-1B"><code>BMRetriever/BMRetriever-1B</code></a><a class="headerlink" href="#bmretrieverbmretriever-1b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>908.8M</td>
<td>3.4 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-2b"><a href="https://huggingface.co/BMRetriever/BMRetriever-2B"><code>BMRetriever/BMRetriever-2B</code></a><a class="headerlink" href="#bmretrieverbmretriever-2b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.5B</td>
<td>9.3 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-410m"><a href="https://huggingface.co/BMRetriever/BMRetriever-410M"><code>BMRetriever/BMRetriever-410M</code></a><a class="headerlink" href="#bmretrieverbmretriever-410m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>353.8M</td>
<td>1.3 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-7b"><a href="https://huggingface.co/BMRetriever/BMRetriever-7B"><code>BMRetriever/BMRetriever-7B</code></a><a class="headerlink" href="#bmretrieverbmretriever-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="beastyze5-r-mistral-7b"><a href="https://huggingface.co/BeastyZ/e5-R-mistral-7b"><code>BeastyZ/e5-R-mistral-7b</code></a><a class="headerlink" href="#beastyze5-r-mistral-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>27.0 GB</td>
<td>2024-06-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bytedance-seedseed15-embedding"><a href="https://seed1-5-embedding.github.io/"><code>ByteDance-Seed/Seed1.5-Embedding</code></a><a class="headerlink" href="#bytedance-seedseed15-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-04-25</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="bytedanceseed16-embedding"><a href="https://seed1-6-embedding.github.io/"><code>Bytedance/Seed1.6-embedding</code></a><a class="headerlink" href="#bytedanceseed16-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-06-18</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-english-light-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-english-light-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-english-light-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-english-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-english-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-english-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-multilingual-light-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-multilingual-light-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-multilingual-light-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-multilingual-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-multilingual-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-multilingual-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="geogpt-research-projectgeoembedding"><a href="https://huggingface.co/GeoGPT-Research-Project/GeoEmbedding"><code>GeoGPT-Research-Project/GeoEmbedding</code></a><a class="headerlink" href="#geogpt-research-projectgeoembedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>27.0 GB</td>
<td>2025-04-22</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="gritlmgritlm-7b"><a href="https://huggingface.co/GritLM/GritLM-7B"><code>GritLM/GritLM-7B</code></a><a class="headerlink" href="#gritlmgritlm-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>13.5 GB</td>
<td>2024-02-15</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">muennighoff2024generative</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Generative Representational Instruction Tuning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.09906}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="gritlmgritlm-8x7b"><a href="https://huggingface.co/GritLM/GritLM-8x7B"><code>GritLM/GritLM-8x7B</code></a><a class="headerlink" href="#gritlmgritlm-8x7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>32768</td>
<td>57.9B</td>
<td>87.0 GB</td>
<td>2024-02-15</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">muennighoff2024generative</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Generative Representational Instruction Tuning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.09906}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v1"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-10-23</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025kalm</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v15"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-12-26</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025kalm</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v2"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>942.0 MB</td>
<td>2025-06-25</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025kalm</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25"><a href="https://huggingface.co/KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5"><code>KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5</code></a><a class="headerlink" href="#kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2025-09-30</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="kingsoft-llmqzhou-embedding"><a href="https://huggingface.co/Kingsoft-LLM/QZhou-Embedding"><code>Kingsoft-LLM/QZhou-Embedding</code></a><a class="headerlink" href="#kingsoft-llmqzhou-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3584</td>
<td>7.1B</td>
<td>28.4 GB</td>
<td>2025-08-24</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">yu2025qzhouembeddingtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{QZhou-Embedding Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.21632}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.21632}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kingsoft-llmqzhou-embedding-zh"><a href="http://huggingface.co/Kingsoft-LLM/QZhou-Embedding-Zh"><code>Kingsoft-LLM/QZhou-Embedding-Zh</code></a><a class="headerlink" href="#kingsoft-llmqzhou-embedding-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1792</td>
<td>7.6B</td>
<td>28.7 GB</td>
<td>2025-09-28</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">yu2025qzhouembeddingtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{QZhou-Embedding Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.21632}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.21632}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="linq-ai-researchlinq-embed-mistral"><a href="https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral"><code>Linq-AI-Research/Linq-Embed-Mistral</code></a><a class="headerlink" href="#linq-ai-researchlinq-embed-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-05-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">LinqAIResearch2024</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Linq-Embed-Mistral:Elevating Text Retrieval with Improved GPT Data Through Task-Specific Control and Quality Refinement}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Junseong Kim and Seolhwa Lee and Jihoon Kwon and Sangmo Gu and Yejin Kim and Minkyung Cho and Jy-yong Sohn and Chanyeol Choi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="p">=</span><span class="s">{Linq AI Research Blog}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://getlinq.com/blog/linq-embed-mistral/}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised"><code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised"><code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised"><code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-sheared-llama-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised"><code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-sheared-llama-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mongodbmdbr-leaf-ir"><a href="https://huggingface.co/MongoDB/mdbr-leaf-ir"><code>MongoDB/mdbr-leaf-ir</code></a><a class="headerlink" href="#mongodbmdbr-leaf-ir" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>22.9M</td>
<td>86.0 MB</td>
<td>2025-08-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">mdbr_leaf</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Robin Vujanic and Thomas Rueckstiess}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.12539}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.12539}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mongodbmdbr-leaf-mt"><a href="https://huggingface.co/MongoDB/mdbr-leaf-mt"><code>MongoDB/mdbr-leaf-mt</code></a><a class="headerlink" href="#mongodbmdbr-leaf-mt" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>23.0M</td>
<td>86.0 MB</td>
<td>2025-08-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">mdbr_leaf</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Robin Vujanic and Thomas Rueckstiess}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.12539}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.12539}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchjasper_en_vision_language_v1"><a href="https://huggingface.co/infgrad/jasper_en_vision_language_v1"><code>NovaSearch/jasper_en_vision_language_v1</code></a><a class="headerlink" href="#novasearchjasper_en_vision_language_v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>8960</td>
<td>2.0B</td>
<td>3.7 GB</td>
<td>2024-12-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchstella_en_15b_v5"><a href="https://huggingface.co/NovaSearch/stella_en_1.5B_v5"><code>NovaSearch/stella_en_1.5B_v5</code></a><a class="headerlink" href="#novasearchstella_en_15b_v5" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>8960</td>
<td>1.5B</td>
<td>5.7 GB</td>
<td>2024-07-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchstella_en_400m_v5"><a href="https://huggingface.co/NovaSearch/stella_en_400M_v5"><code>NovaSearch/stella_en_400M_v5</code></a><a class="headerlink" href="#novasearchstella_en_400m_v5" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2024-07-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-06b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"><code>Qwen/Qwen3-Embedding-0.6B</code></a><a class="headerlink" href="#qwenqwen3-embedding-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1024</td>
<td>595.8M</td>
<td>2.2 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-4b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-4B"><code>Qwen/Qwen3-Embedding-4B</code></a><a class="headerlink" href="#qwenqwen3-embedding-4b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2560</td>
<td>4.0B</td>
<td>15.0 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-8b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-8B"><code>Qwen/Qwen3-Embedding-8B</code></a><a class="headerlink" href="#qwenqwen3-embedding-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.6B</td>
<td>28.2 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="reasonirreasonir-8b"><a href="https://huggingface.co/ReasonIR/ReasonIR-8B"><code>ReasonIR/ReasonIR-8B</code></a><a class="headerlink" href="#reasonirreasonir-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>4096</td>
<td>7.5B</td>
<td>not specified</td>
<td>2025-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">shao2025reasonir</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{ReasonIR: Training Retrievers for Reasoning Tasks}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rulin Shao and Rui Qiao and Varsha Kishore and Niklas Muennighoff and Xi Victoria Lin and Daniela Rus and Bryan Kian Hsiang Low and Sewon Min and Wen-tau Yih and Pang Wei Koh and Luke Zettlemoyer}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2504.20595}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2504.20595}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sailesh97hinvec"><a href="https://huggingface.co/Sailesh97/Hinvec"><code>Sailesh97/Hinvec</code></a><a class="headerlink" href="#sailesh97hinvec" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>939.6M</td>
<td>3.6 GB</td>
<td>2025-06-19</td>
<td>eng-Latn, hin-Deva</td>
</tr>
</tbody>
</table>
<h4 id="salesforcesfr-embedding-2_r"><a href="https://huggingface.co/Salesforce/SFR-Embedding-2_R"><code>Salesforce/SFR-Embedding-2_R</code></a><a class="headerlink" href="#salesforcesfr-embedding-2_r" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-06-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">SFR-embedding-2</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/Salesforce/SFR-Embedding-2_R}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="salesforcesfr-embedding-code-2b_r"><a href="https://huggingface.co/Salesforce/SFR-Embedding-Code-2B_R"><code>Salesforce/SFR-Embedding-Code-2B_R</code></a><a class="headerlink" href="#salesforcesfr-embedding-code-2b_r" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2304</td>
<td>2.6B</td>
<td>4.9 GB</td>
<td>2025-01-17</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024codexembed</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ye and Meng, Rui and Jot, Shafiq and Savarese, Silvio and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2411.12644}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="salesforcesfr-embedding-mistral"><a href="https://huggingface.co/Salesforce/SFR-Embedding-Mistral"><code>Salesforce/SFR-Embedding-Mistral</code></a><a class="headerlink" href="#salesforcesfr-embedding-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-01-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">SFRAIResearch2024</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="p">=</span><span class="s">{Salesforce AI Research Blog}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.salesforce.com/blog/sfr-embedding/}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samilpwc-axnode-genaipwc-embedding_expr"><a href="https://huggingface.co/SamilPwC-AXNode-GenAI/PwC-Embedding_expr"><code>SamilPwC-AXNode-GenAI/PwC-Embedding_expr</code></a><a class="headerlink" href="#samilpwc-axnode-genaipwc-embedding_expr" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>560.0M</td>
<td>2.1 GB</td>
<td>2025-08-12</td>
<td>kor-Hang</td>
</tr>
</tbody>
</table>
<h4 id="snowflakesnowflake-arctic-embed-l"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l"><code>Snowflake/snowflake-arctic-embed-l</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-l" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-l-v20"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0"><code>Snowflake/snowflake-arctic-embed-l-v2.0</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-l-v20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-12-04</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2024arctic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Arctic-Embed 2.0: Multilingual Retrieval Without Compromise}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.04506}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m"><code>Snowflake/snowflake-arctic-embed-m</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>415.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-long"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long"><code>Snowflake/snowflake-arctic-embed-m-long</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-long" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>137.0M</td>
<td>522.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-v15"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v1.5"><code>Snowflake/snowflake-arctic-embed-m-v1.5</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>415.0 MB</td>
<td>2024-07-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-v20"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0"><code>Snowflake/snowflake-arctic-embed-m-v2.0</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-v20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>305.0M</td>
<td>1.1 GB</td>
<td>2024-12-04</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2024arctic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Arctic-Embed 2.0: Multilingual Retrieval Without Compromise}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.04506}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-s"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-s"><code>Snowflake/snowflake-arctic-embed-s</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-s" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>32.2M</td>
<td>127.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-xs"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-xs"><code>Snowflake/snowflake-arctic-embed-xs</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-xs" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.6M</td>
<td>86.0 MB</td>
<td>2024-07-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tencentbacconan-embedding-v2"><a href="https://huggingface.co/TencentBAC/Conan-embedding-v2"><code>TencentBAC/Conan-embedding-v2</code></a><a class="headerlink" href="#tencentbacconan-embedding-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-04-10</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="vplabssearchmap_preview"><a href="https://huggingface.co/VPLabs/SearchMap_Preview"><code>VPLabs/SearchMap_Preview</code></a><a class="headerlink" href="#vplabssearchmap_preview" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2025-03-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vectorpath2025searchmap</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{SearchMap: Conversational E-commerce Search Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{VectorPath Research Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{HuggingFace Model Hub}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="whereisaiuae-large-v1"><a href="https://huggingface.co/WhereIsAI/UAE-Large-V1"><code>WhereIsAI/UAE-Large-V1</code></a><a class="headerlink" href="#whereisaiuae-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-12-04</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023angle</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{AnglE-optimized Text Embeddings}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Xianming and Li, Jing}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2309.12871}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ai-foreverfrida"><a href="https://huggingface.co/ai-forever/FRIDA"><code>ai-forever/FRIDA</code></a><a class="headerlink" href="#ai-foreverfrida" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>823.0M</td>
<td>3.1 GB</td>
<td>2024-12-29</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="ai-foreverru-en-rosberta"><a href="https://huggingface.co/ai-forever/ru-en-RoSBERTa"><code>ai-forever/ru-en-RoSBERTa</code></a><a class="headerlink" href="#ai-foreverru-en-rosberta" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>404.0M</td>
<td>1.5 GB</td>
<td>2024-07-29</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">snegirev2024russianfocusedembeddersexplorationrumteb</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{The Russian-focused embedders&#39; exploration: ruMTEB benchmark and Russian embedding model design}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2408.12503}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2408.12503}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ai-sagegiga-embeddings-instruct"><a href="https://huggingface.co/ai-sage/Giga-Embeddings-instruct"><code>ai-sage/Giga-Embeddings-instruct</code></a><a class="headerlink" href="#ai-sagegiga-embeddings-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>2048</td>
<td>3.2B</td>
<td>12.6 GB</td>
<td>2025-09-23</td>
<td>eng-Latn, rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="annamodelslgai-embedding-preview"><a href="https://huggingface.co/annamodels/LGAI-Embedding-Preview"><code>annamodels/LGAI-Embedding-Preview</code></a><a class="headerlink" href="#annamodelslgai-embedding-preview" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-06-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">choi2025lgaiembeddingpreviewtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LGAI-EMBEDDING-Preview Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jooyoung Choi and Hyun Kim and Hansol Jang and Changwook Jun and Kyunghoon Bae and Hyewon Choi and Stanley Jungkyu Choi and Honglak Lee and Chulmin Yun}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2506.07438}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2506.07438}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bedrockcohere-embed-english-v3"><a href="https://cohere.com/blog/introducing-embed-v3"><code>bedrock/cohere-embed-english-v3</code></a><a class="headerlink" href="#bedrockcohere-embed-english-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bedrockcohere-embed-multilingual-v3"><a href="https://cohere.com/blog/introducing-embed-v3"><code>bedrock/cohere-embed-multilingual-v3</code></a><a class="headerlink" href="#bedrockcohere-embed-multilingual-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="castorinirepllama-v1-7b-lora-passage"><a href="https://huggingface.co/samaya-ai/castorini/repllama-v1-7b-lora-passage"><code>castorini/repllama-v1-7b-lora-passage</code></a><a class="headerlink" href="#castorinirepllama-v1-7b-lora-passage" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0M</td>
<td>27.0 MB</td>
<td>2023-10-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rankllama</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fine-Tuning LLaMA for Multi-Stage Text Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv:2310.08319}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codefuse-aif2llm-06b"><a href="https://huggingface.co/codefuse-ai/F2LLM-0.6B"><code>codefuse-ai/F2LLM-0.6B</code></a><a class="headerlink" href="#codefuse-aif2llm-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>595.8M</td>
<td>1.1 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="codefuse-aif2llm-17b"><a href="https://huggingface.co/codefuse-ai/F2LLM-1.7B"><code>codefuse-ai/F2LLM-1.7B</code></a><a class="headerlink" href="#codefuse-aif2llm-17b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2560</td>
<td>1.7B</td>
<td>3.2 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="codefuse-aif2llm-4b"><a href="https://huggingface.co/codefuse-ai/F2LLM-4B"><code>codefuse-ai/F2LLM-4B</code></a><a class="headerlink" href="#codefuse-aif2llm-4b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="deepvkuser-base"><a href="https://huggingface.co/deepvk/USER-base"><code>deepvk/USER-base</code></a><a class="headerlink" href="#deepvkuser-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>427.0M</td>
<td>473.0 MB</td>
<td>2024-06-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deepvk2024user</span><span class="p">,</span>
<span class="w">        </span><span class="na">title</span><span class="p">=</span><span class="s">{USER: Universal Sentence Encoder for Russian}</span><span class="p">,</span>
<span class="w">        </span><span class="na">author</span><span class="p">=</span><span class="s">{Malashenko, Boris and  Zemerov, Anton and Spirin, Egor}</span><span class="p">,</span>
<span class="w">        </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/datasets/deepvk/USER-base}</span><span class="p">,</span>
<span class="w">        </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span>
<span class="w">        </span><span class="nv">year</span><span class="err">={2024</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">}</span>
</code></pre></div>
</details>
<h4 id="deepvkuser2-base"><a href="https://huggingface.co/collections/deepvk/user2-6802650d7210f222ec60e05f"><code>deepvk/USER2-base</code></a><a class="headerlink" href="#deepvkuser2-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2025-04-19</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="deepvkuser2-small"><a href="https://huggingface.co/collections/deepvk/user2-6802650d7210f222ec60e05f"><code>deepvk/USER2-small</code></a><a class="headerlink" href="#deepvkuser2-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>384</td>
<td>34.4M</td>
<td>131.0 MB</td>
<td>2025-04-19</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="fyaronskiyenglish_code_retriever"><a href="https://huggingface.co/fyaronskiy/english_code_retriever"><code>fyaronskiy/english_code_retriever</code></a><a class="headerlink" href="#fyaronskiyenglish_code_retriever" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2025-07-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googleembeddinggemma-300m"><a href="https://ai.google.dev/gemma/docs/embeddinggemma/model_card"><code>google/embeddinggemma-300m</code></a><a class="headerlink" href="#googleembeddinggemma-300m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> gemma</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>307.6M</td>
<td>578.0 MB</td>
<td>2025-09-04</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<h4 id="googlegemini-embedding-001"><a href="https://ai.google.dev/gemini-api/docs/embeddings"><code>google/gemini-embedding-001</code></a><a class="headerlink" href="#googlegemini-embedding-001" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>3072</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-03-07</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<h4 id="googletext-embedding-004"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-embedding-004</code></a><a class="headerlink" href="#googletext-embedding-004" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googletext-embedding-005"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-embedding-005</code></a><a class="headerlink" href="#googletext-embedding-005" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-11-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googletext-multilingual-embedding-002"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-multilingual-embedding-002</code></a><a class="headerlink" href="#googletext-multilingual-embedding-002" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-14</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<h4 id="inflyinf-retriever-v1"><a href="https://huggingface.co/infly/inf-retriever-v1"><code>infly/inf-retriever-v1</code></a><a class="headerlink" href="#inflyinf-retriever-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-12-24</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">infly-ai_2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{inf-retriever-v1 (Revision 5f469d7)}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/infly/inf-retriever-v1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.57967/hf/4262}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="inflyinf-retriever-v1-15b"><a href="https://huggingface.co/infly/inf-retriever-v1-1.5b"><code>infly/inf-retriever-v1-1.5b</code></a><a class="headerlink" href="#inflyinf-retriever-v1-15b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>1.5B</td>
<td>2.9 GB</td>
<td>2025-02-08</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">infly-ai_2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{inf-retriever-v1 (Revision 5f469d7)}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/infly/inf-retriever-v1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.57967/hf/4262}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-base"><a href="https://huggingface.co/intfloat/e5-base"><code>intfloat/e5-base</code></a><a class="headerlink" href="#intfloate5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2022-12-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-base-v2"><a href="https://huggingface.co/intfloat/e5-base-v2"><code>intfloat/e5-base-v2</code></a><a class="headerlink" href="#intfloate5-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-large"><a href="https://huggingface.co/intfloat/e5-large"><code>intfloat/e5-large</code></a><a class="headerlink" href="#intfloate5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2022-12-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-large-v2"><a href="https://huggingface.co/intfloat/e5-large-v2"><code>intfloat/e5-large-v2</code></a><a class="headerlink" href="#intfloate5-large-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-mistral-7b-instruct"><a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct"><code>intfloat/e5-mistral-7b-instruct</code></a><a class="headerlink" href="#intfloate5-mistral-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-02-08</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023improving</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Improving Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2401.00368}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-small"><a href="https://huggingface.co/intfloat/e5-small"><code>intfloat/e5-small</code></a><a class="headerlink" href="#intfloate5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.0M</td>
<td>127.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-small-v2"><a href="https://huggingface.co/intfloat/e5-small-v2"><code>intfloat/e5-small-v2</code></a><a class="headerlink" href="#intfloate5-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.0M</td>
<td>127.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-base"><a href="https://huggingface.co/intfloat/multilingual-e5-base"><code>intfloat/multilingual-e5-base</code></a><a class="headerlink" href="#intfloatmultilingual-e5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-large"><a href="https://huggingface.co/intfloat/multilingual-e5-large"><code>intfloat/multilingual-e5-large</code></a><a class="headerlink" href="#intfloatmultilingual-e5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>560.0M</td>
<td>2.1 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-large-instruct"><a href="https://huggingface.co/intfloat/multilingual-e5-large-instruct"><code>intfloat/multilingual-e5-large-instruct</code></a><a class="headerlink" href="#intfloatmultilingual-e5-large-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>560.0M</td>
<td>1.0 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-small"><a href="https://huggingface.co/intfloat/multilingual-e5-small"><code>intfloat/multilingual-e5-small</code></a><a class="headerlink" href="#intfloatmultilingual-e5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>118.0M</td>
<td>449.0 MB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v3"><a href="https://huggingface.co/jinaai/jina-embeddings-v3"><code>jinaai/jina-embeddings-v3</code></a><a class="headerlink" href="#jinaaijina-embeddings-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>572.0M</td>
<td>1.1 GB</td>
<td>2024-09-18</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">sturua2024jinaembeddingsv3multilingualembeddingstask</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{jina-embeddings-v3: Multilingual Embeddings With Task LoRA}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Günther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.10173}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.10173}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jxmcde-small-v1"><a href="https://huggingface.co/jxm/cde-small-v1"><code>jxm/cde-small-v1</code></a><a class="headerlink" href="#jxmcde-small-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>281.0M</td>
<td>1.0 GB</td>
<td>2024-09-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="jxmcde-small-v2"><a href="https://huggingface.co/jxm/cde-small-v1"><code>jxm/cde-small-v2</code></a><a class="headerlink" href="#jxmcde-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>306.0M</td>
<td>1.1 GB</td>
<td>2025-01-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="llamaindexvdr-2b-multi-v1"><a href="https://huggingface.co/llamaindex/vdr-2b-multi-v1"><code>llamaindex/vdr-2b-multi-v1</code></a><a class="headerlink" href="#llamaindexvdr-2b-multi-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>2.0B</td>
<td>4.1 GB</td>
<td>2024-01-08</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<h4 id="manveertambercadet-embed-base-v1"><a href="https://huggingface.co/manveertamber/cadet-embed-base-v1"><code>manveertamber/cadet-embed-base-v1</code></a><a class="headerlink" href="#manveertambercadet-embed-base-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2025-05-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="mixedbread-aimxbai-embed-2d-large-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1"><code>mixedbread-ai/mxbai-embed-2d-large-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-2d-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.0M</td>
<td>not specified</td>
<td>2024-03-04</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="mixedbread-aimxbai-embed-large-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1"><code>mixedbread-ai/mxbai-embed-large-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>639.0 MB</td>
<td>2024-03-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@online</span><span class="p">{</span><span class="nl">emb2024mxbai</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Open Source Strikes Bread - New Fluffy Embeddings Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-embed-large-v1}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023angle</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{AnglE-optimized Text Embeddings}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Xianming and Li, Jing}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2309.12871}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-embed-xsmall-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-xsmall-v1"><code>mixedbread-ai/mxbai-embed-xsmall-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-xsmall-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>24.1M</td>
<td>not specified</td>
<td>2024-08-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-aimodernbert-embed-base"><a href="https://huggingface.co/nomic-ai/modernbert-embed-base"><code>nomic-ai/modernbert-embed-base</code></a><a class="headerlink" href="#nomic-aimodernbert-embed-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2024-12-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-ainomic-embed-text-v1"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1"><code>nomic-ai/nomic-embed-text-v1</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>not specified</td>
<td>522.0 MB</td>
<td>2024-01-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2024nomic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Nomic Embed: Training a Reproducible Long Context Text Embedder}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.01613}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-ainomic-embed-text-v1-ablated"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1-ablated"><code>nomic-ai/nomic-embed-text-v1-ablated</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1-ablated" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-ainomic-embed-text-v1-unsupervised"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised"><code>nomic-ai/nomic-embed-text-v1-unsupervised</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1-unsupervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-ainomic-embed-text-v15"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5"><code>nomic-ai/nomic-embed-text-v1.5</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>137.0M</td>
<td>522.0 MB</td>
<td>2024-02-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2024nomic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Nomic Embed: Training a Reproducible Long Context Text Embedder}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.01613}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidianv-embed-v1"><a href="https://huggingface.co/nvidia/NV-Embed-v1"><code>nvidia/NV-Embed-v1</code></a><a class="headerlink" href="#nvidianv-embed-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.8B</td>
<td>29.2 GB</td>
<td>2024-09-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">moreira2025nvretrieverimprovingtextembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{NV-Retriever: Improving text embedding models with effective hard-negative mining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.15831}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.15831}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidianv-embed-v2"><a href="https://huggingface.co/nvidia/NV-Embed-v2"><code>nvidia/NV-Embed-v2</code></a><a class="headerlink" href="#nvidianv-embed-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.8B</td>
<td>14.6 GB</td>
<td>2024-09-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">moreira2025nvretrieverimprovingtextembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{NV-Retriever: Improving text embedding models with effective hard-negative mining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.15831}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.15831}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidiallama-embed-nemotron-8b"><a href="https://huggingface.co/nvidia/llama-embed-nemotron-8b"><code>nvidia/llama-embed-nemotron-8b</code></a><a class="headerlink" href="#nvidiallama-embed-nemotron-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/nvidia/llama-embed-nemotron-8b/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2025-10-23</td>
<td>afr-Latn, amh-Ethi, ara-Arab, arq-Arab, ary-Arab, ... (66)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">moreira2025nvretrieverimprovingtextembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{NV-Retriever: Improving text embedding models with effective hard-negative mining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.15831}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.15831}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v1"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v1</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>133.0M</td>
<td>507.0 MB</td>
<td>2024-03-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>67.0M</td>
<td>267.0 MB</td>
<td>2024-07-17</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>22.7M</td>
<td>86.0 MB</td>
<td>2024-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>67.0M</td>
<td>267.0 MB</td>
<td>2025-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>30522</td>
<td>137.4M</td>
<td>549.0 MB</td>
<td>2025-06-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="samaya-airepllama-reproduced"><a href="https://huggingface.co/samaya-ai/RepLLaMA-reproduced"><code>samaya-ai/RepLLaMA-reproduced</code></a><a class="headerlink" href="#samaya-airepllama-reproduced" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0M</td>
<td>27.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rankllama</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fine-Tuning LLaMA for Multi-Stage Text Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv:2310.08319}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama2-7b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama2-7b-v1"><code>samaya-ai/promptriever-llama2-7b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama2-7b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>27.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama31-8b-instruct-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama3.1-8b-instruct-v1"><code>samaya-ai/promptriever-llama3.1-8b-instruct-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama31-8b-instruct-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>8.0B</td>
<td>31.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama31-8b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama3.1-8b-v1"><code>samaya-ai/promptriever-llama3.1-8b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama31-8b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>8.0B</td>
<td>31.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-mistral-v01-7b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-mistral-v0.1-7b-v1"><code>samaya-ai/promptriever-mistral-v0.1-7b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-mistral-v01-7b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>27.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sergeyzhberta"><a href="https://huggingface.co/sergeyzh/BERTA"><code>sergeyzh/BERTA</code></a><a class="headerlink" href="#sergeyzhberta" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2025-03-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhrubert-mini-frida"><a href="https://huggingface.co/sergeyzh/rubert-mini-frida"><code>sergeyzh/rubert-mini-frida</code></a><a class="headerlink" href="#sergeyzhrubert-mini-frida" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>32.3M</td>
<td>123.0 MB</td>
<td>2025-03-02</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="tencentyoutu-embedding"><a href="https://huggingface.co/tencent/Youtu-Embedding"><code>tencent/Youtu-Embedding</code></a><a class="headerlink" href="#tencentyoutu-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.7B</td>
<td>not specified</td>
<td>2025-09-28</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025codiemb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.11442}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.11442}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="voyageaivoyage-2"><a href="https://blog.voyageai.com/2023/10/29/voyage-embeddings/"><code>voyageai/voyage-2</code></a><a class="headerlink" href="#voyageaivoyage-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3"><a href="https://blog.voyageai.com/2024/09/18/voyage-3/"><code>voyageai/voyage-3</code></a><a class="headerlink" href="#voyageaivoyage-3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-09-18</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-large"><a href="https://blog.voyageai.com/2025/01/07/voyage-3-large/"><code>voyageai/voyage-3-large</code></a><a class="headerlink" href="#voyageaivoyage-3-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-07</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-lite"><a href="https://blog.voyageai.com/2024/09/18/voyage-3/"><code>voyageai/voyage-3-lite</code></a><a class="headerlink" href="#voyageaivoyage-3-lite" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-09-18</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-m-exp"><a href="https://huggingface.co/voyageai/voyage-3-m-exp"><code>voyageai/voyage-3-m-exp</code></a><a class="headerlink" href="#voyageaivoyage-3-m-exp" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>2048</td>
<td>6.9B</td>
<td>not specified</td>
<td>2025-01-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5</code></a><a class="headerlink" href="#voyageaivoyage-35" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35-output_dtypebinary"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5 (output_dtype=binary)</code></a><a class="headerlink" href="#voyageaivoyage-35-output_dtypebinary" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35-output_dtypeint8"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5 (output_dtype=int8)</code></a><a class="headerlink" href="#voyageaivoyage-35-output_dtypeint8" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-code-2"><a href="https://blog.voyageai.com/2024/01/23/voyage-code-2-elevate-your-code-retrieval/"><code>voyageai/voyage-code-2</code></a><a class="headerlink" href="#voyageaivoyage-code-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-code-3"><a href="https://blog.voyageai.com/2024/12/04/voyage-code-3/"><code>voyageai/voyage-code-3</code></a><a class="headerlink" href="#voyageaivoyage-code-3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-12-04</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-finance-2"><a href="https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/"><code>voyageai/voyage-finance-2</code></a><a class="headerlink" href="#voyageaivoyage-finance-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-large-2"><a href="https://blog.voyageai.com/2023/10/29/voyage-embeddings/"><code>voyageai/voyage-large-2</code></a><a class="headerlink" href="#voyageaivoyage-large-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-large-2-instruct"><a href="https://blog.voyageai.com/2024/05/05/voyage-large-2-instruct-instruction-tuned-and-rank-1-on-mteb/"><code>voyageai/voyage-large-2-instruct</code></a><a class="headerlink" href="#voyageaivoyage-large-2-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-05</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-law-2"><a href="https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/"><code>voyageai/voyage-law-2</code></a><a class="headerlink" href="#voyageaivoyage-law-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-multilingual-2"><a href="https://blog.voyageai.com/2024/06/10/voyage-multilingual-2-multilingual-embedding-model/"><code>voyageai/voyage-multilingual-2</code></a><a class="headerlink" href="#voyageaivoyage-multilingual-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-06-10</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="yibinleilens-d4000"><a href="https://huggingface.co/yibinlei/LENS-d4000"><code>yibinlei/LENS-d4000</code></a><a class="headerlink" href="#yibinleilens-d4000" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4000</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-01-17</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">lei2025lens</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Enhancing Lexicon-Based Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.09749}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="yibinleilens-d8000"><a href="https://huggingface.co/yibinlei/LENS-d8000"><code>yibinlei/LENS-d8000</code></a><a class="headerlink" href="#yibinleilens-d8000" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>8000</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-01-17</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">lei2025lens</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Enhancing Lexicon-Based Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.09749}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="zeta-alpha-aizeta-alpha-e5-mistral"><a href="https://huggingface.co/zeta-alpha-ai/Zeta-Alpha-E5-Mistral"><code>zeta-alpha-ai/Zeta-Alpha-E5-Mistral</code></a><a class="headerlink" href="#zeta-alpha-aizeta-alpha-e5-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h2 id="non-instruction-model">Non-instruction Model<a class="headerlink" href="#non-instruction-model" title="Permanent link">&para;</a></h2>
<h4 id="aiteamvnvietnamese_embedding"><a href="https://huggingface.co/AITeamVN/Vietnamese_Embedding"><code>AITeamVN/Vietnamese_Embedding</code></a><a class="headerlink" href="#aiteamvnvietnamese_embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-03-17</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="alibaba-nlpgte-base-en-v15"><a href="https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5"><code>Alibaba-NLP/gte-base-en-v1.5</code></a><a class="headerlink" href="#alibaba-nlpgte-base-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>137.0M</td>
<td>not specified</td>
<td>2024-06-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="alibaba-nlpgte-modernbert-base"><a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base"><code>Alibaba-NLP/gte-modernbert-base</code></a><a class="headerlink" href="#alibaba-nlpgte-modernbert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>284.0 MB</td>
<td>2025-01-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="alibaba-nlpgte-multilingual-base"><a href="https://huggingface.co/Alibaba-NLP/gte-multilingual-base"><code>Alibaba-NLP/gte-multilingual-base</code></a><a class="headerlink" href="#alibaba-nlpgte-multilingual-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>305.0M</td>
<td>582.0 MB</td>
<td>2024-07-20</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-en-icl"><a href="https://huggingface.co/BAAI/bge-en-icl"><code>BAAI/bge-en-icl</code></a><a class="headerlink" href="#baaibge-en-icl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-07-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2024makingtextembeddersfewshot</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Making Text Embedders Few-Shot Learners}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.15700}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.15700}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-m3"><a href="https://huggingface.co/BAAI/bge-m3"><code>BAAI/bge-m3</code></a><a class="headerlink" href="#baaibge-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-06-28</td>
<td>afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-m3-unsupervised"><a href="https://huggingface.co/BAAI/bge-m3-unsupervised"><code>BAAI/bge-m3-unsupervised</code></a><a class="headerlink" href="#baaibge-m3-unsupervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-01-30</td>
<td>afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-multilingual-gemma2"><a href="https://huggingface.co/BAAI/bge-multilingual-gemma2"><code>BAAI/bge-multilingual-gemma2</code></a><a class="headerlink" href="#baaibge-multilingual-gemma2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://ai.google.dev/gemma/terms</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3584</td>
<td>9.2B</td>
<td>34.4 GB</td>
<td>2024-07-25</td>
<td>eng-Latn, fra-Latn, jpn-Jpan, jpn-Latn, kor-Hang, ... (7)</td>
</tr>
</tbody>
</table>
<h4 id="baaibge-reranker-v2-m3">BAAI/bge-reranker-v2-m3<a class="headerlink" href="#baaibge-reranker-v2-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2.1 GB</td>
<td>2024-06-24</td>
<td>ara-Arab, ben-Beng, dan-Latn, deu-Latn, eng-Latn, ... (32)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2023making</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Making Large Language Models A Better Foundation For Dense Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2312.15503}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">chen2024bge</span><span class="p">,</span>
<span class="w">          </span><span class="na">title</span><span class="p">=</span><span class="s">{BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}</span><span class="p">,</span>
<span class="w">          </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">          </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">          </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.03216}</span><span class="p">,</span>
<span class="w">          </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">          </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bytedancelistconranker"><a href="https://huggingface.co/ByteDance/ListConRanker"><code>ByteDance/ListConRanker</code></a><a class="headerlink" href="#bytedancelistconranker" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>401.0M</td>
<td>1.2 GB</td>
<td>2024-12-11</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025listconranker</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{ListConRanker: A Contrastive Text Reranker with Listwise Encoding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Liu, Junlong and Ma, Yue and Zhao, Ruihui and Zheng, Junhao and Ma, Qianli and Kang, Yangyang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.07111}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="classicalyinka"><a href="https://huggingface.co/Classical/Yinka"><code>Classical/Yinka</code></a><a class="headerlink" href="#classicalyinka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-01-09</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dmetasouldmeta-embedding-zh-small"><a href="https://huggingface.co/DMetaSoul/Dmeta-embedding-zh-small/"><code>DMetaSoul/Dmeta-embedding-zh-small</code></a><a class="headerlink" href="#dmetasouldmeta-embedding-zh-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0K</td>
<td>768</td>
<td>74.2M</td>
<td>283.0 MB</td>
<td>2024-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dmetasoulsbert-chinese-general-v1"><a href="https://huggingface.co/DMetaSoul/sbert-chinese-general-v1"><code>DMetaSoul/sbert-chinese-general-v1</code></a><a class="headerlink" href="#dmetasoulsbert-chinese-general-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>128</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="deeppavlovdistilrubert-small-cased-conversational"><a href="https://huggingface.co/DeepPavlov/distilrubert-small-cased-conversational"><code>DeepPavlov/distilrubert-small-cased-conversational</code></a><a class="headerlink" href="#deeppavlovdistilrubert-small-cased-conversational" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>107.0M</td>
<td>408.0 MB</td>
<td>2022-06-28</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2205.02340</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2205.02340}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2205.02340}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Kolesnikova, Alina and Kuratov, Yuri and Konovalov, Vasily and Burtsev, Mikhail}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Knowledge Distillation of Russian Language Models with Reduction of Vocabulary}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv.org perpetual, non-exclusive license}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deeppavlovrubert-base-cased"><a href="https://huggingface.co/DeepPavlov/rubert-base-cased"><code>DeepPavlov/rubert-base-cased</code></a><a class="headerlink" href="#deeppavlovrubert-base-cased" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>1.3B</td>
<td>4.8 GB</td>
<td>2020-03-04</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kuratov2019adaptationdeepbidirectionalmultilingual</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yuri Kuratov and Mikhail Arkhipov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{1905.07213}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/1905.07213}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deeppavlovrubert-base-cased-sentence"><a href="https://huggingface.co/DeepPavlov/rubert-base-cased-sentence"><code>DeepPavlov/rubert-base-cased-sentence</code></a><a class="headerlink" href="#deeppavlovrubert-base-cased-sentence" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>107.0M</td>
<td>408.0 MB</td>
<td>2020-03-04</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="gameselosts-multilingual-mpnet-base-v2"><a href="https://huggingface.co/Gameselo/STS-multilingual-mpnet-base-v2"><code>Gameselo/STS-multilingual-mpnet-base-v2</code></a><a class="headerlink" href="#gameselosts-multilingual-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-07</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="greennodegreennode-embedding-large-vn-mixed-v1"><a href="https://huggingface.co/GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1"><code>GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1</code></a><a class="headerlink" href="#greennodegreennode-embedding-large-vn-mixed-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="greennodegreennode-embedding-large-vn-v1"><a href="https://huggingface.co/GreenNode/GreenNode-Embedding-Large-VN-V1"><code>GreenNode/GreenNode-Embedding-Large-VN-V1</code></a><a class="headerlink" href="#greennodegreennode-embedding-large-vn-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-v1"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-v1"><code>HIT-TMG/KaLM-embedding-multilingual-mini-v1</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-08-27</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">hu2025kalm</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hu, Xinshuo and Shan, Zifei and Zhao, Xinping and Sun, Zetian and Liu, Zhenyu and Li, Dongfang and Ye, Shaolin and Wei, Xinyuan and Chen, Qian and Hu, Baotian and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="haon-chenspeed-embedding-7b-instruct"><a href="https://huggingface.co/Haon-Chen/speed-embedding-7b-instruct"><code>Haon-Chen/speed-embedding-7b-instruct</code></a><a class="headerlink" href="#haon-chenspeed-embedding-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>not specified</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-10-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="hooshvarelabbert-base-parsbert-uncased"><a href="https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased"><code>HooshvareLab/bert-base-parsbert-uncased</code></a><a class="headerlink" href="#hooshvarelabbert-base-parsbert-uncased" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2021-05-19</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">ParsBERT</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{ParsBERT: Transformer-based Model for Persian Language Understanding}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{ArXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2020}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="p">=</span><span class="s">{abs/2005.12515}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hum-workslodestone-base-4096-v1"><a href="https://huggingface.co/Hum-Works/lodestone-base-4096-v1"><code>Hum-Works/lodestone-base-4096-v1</code></a><a class="headerlink" href="#hum-workslodestone-base-4096-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-08-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="human">Human<a class="headerlink" href="#human" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>ara-Arab, dan-Latn, eng-Latn, nob-Latn, rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="jaumegemma-2b-embeddings"><a href="https://huggingface.co/Jaume/gemma-2b-embeddings"><code>Jaume/gemma-2b-embeddings</code></a><a class="headerlink" href="#jaumegemma-2b-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.5B</td>
<td>9.3 GB</td>
<td>2024-06-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="lajavanessbilingual-embedding-base"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-base"><code>Lajavaness/bilingual-embedding-base</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="lajavanessbilingual-embedding-large"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-large"><code>Lajavaness/bilingual-embedding-large</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2024-06-24</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="lajavanessbilingual-embedding-small"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-small"><code>Lajavaness/bilingual-embedding-small</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2024-07-17</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="mcinexthakim"><a href="https://huggingface.co/MCINext/Hakim"><code>MCINext/Hakim</code></a><a class="headerlink" href="#mcinexthakim" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcinexthakim-small"><a href="https://huggingface.co/MCINext/Hakim-small"><code>MCINext/Hakim-small</code></a><a class="headerlink" href="#mcinexthakim-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>38.7M</td>
<td>148.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcinexthakim-unsup"><a href="https://huggingface.co/MCINext/Hakim-unsup"><code>MCINext/Hakim-unsup</code></a><a class="headerlink" href="#mcinexthakim-unsup" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mihaiiibulbasaur"><a href="https://huggingface.co/Mihaiii/Bulbasaur"><code>Mihaiii/Bulbasaur</code></a><a class="headerlink" href="#mihaiiibulbasaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiiivysaur"><a href="https://huggingface.co/Mihaiii/Ivysaur"><code>Mihaiii/Ivysaur</code></a><a class="headerlink" href="#mihaiiiivysaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-04-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiisquirtle"><a href="https://huggingface.co/Mihaiii/Squirtle"><code>Mihaiii/Squirtle</code></a><a class="headerlink" href="#mihaiiisquirtle" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>15.6M</td>
<td>60.0 MB</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiivenusaur"><a href="https://huggingface.co/Mihaiii/Venusaur"><code>Mihaiii/Venusaur</code></a><a class="headerlink" href="#mihaiiivenusaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>15.6M</td>
<td>60.0 MB</td>
<td>2024-04-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiiwartortle"><a href="https://huggingface.co/Mihaiii/Wartortle"><code>Mihaiii/Wartortle</code></a><a class="headerlink" href="#mihaiiiwartortle" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiigte-micro"><a href="https://huggingface.co/Mihaiii/gte-micro"><code>Mihaiii/gte-micro</code></a><a class="headerlink" href="#mihaiiigte-micro" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiigte-micro-v4"><a href="https://huggingface.co/Mihaiii/gte-micro-v4"><code>Mihaiii/gte-micro-v4</code></a><a class="headerlink" href="#mihaiiigte-micro-v4" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>19.2M</td>
<td>73.0 MB</td>
<td>2024-04-22</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="nbailabnb-sbert-base"><a href="https://huggingface.co/NbAiLab/nb-sbert-base"><code>NbAiLab/nb-sbert-base</code></a><a class="headerlink" href="#nbailabnb-sbert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>75</td>
<td>4096</td>
<td>1.8B</td>
<td>197.0 MB</td>
<td>2022-11-23</td>
<td>dan-Latn, nno-Latn, nob-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-100k"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-100K"><code>NeuML/pubmedbert-base-embeddings-100K</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-100k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>100.0K</td>
<td>0.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-1m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-1M"><code>NeuML/pubmedbert-base-embeddings-1M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-1m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>1.0M</td>
<td>2.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-2m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-2M"><code>NeuML/pubmedbert-base-embeddings-2M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-2m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>1.9M</td>
<td>7.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-500k"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-500K"><code>NeuML/pubmedbert-base-embeddings-500K</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-500k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>500.0K</td>
<td>2.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-8m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-8M"><code>NeuML/pubmedbert-base-embeddings-8M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-8m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.8M</td>
<td>30.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>135.2M</td>
<td>516.0 MB</td>
<td>2024-06-16</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet"><code>Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2024-06-25</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-triplet-matryoshka-v2"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2"><code>Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-triplet-matryoshka-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>768</td>
<td>768</td>
<td>135.0M</td>
<td>516.0 MB</td>
<td>2024-07-28</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-14</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-labse-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-labse-Matryoshka"><code>Omartificial-Intelligence-Space/Arabic-labse-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-labse-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>470.9M</td>
<td>1.8 GB</td>
<td>2024-06-16</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet"><code>Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2024-06-15</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2024-06-17</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-aiops-moa-conan-embedding-v1"><a href="https://huggingface.co/OpenSearch-AI/Ops-MoA-Conan-embedding-v1"><code>OpenSearch-AI/Ops-MoA-Conan-embedding-v1</code></a><a class="headerlink" href="#opensearch-aiops-moa-conan-embedding-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>343.0M</td>
<td>2.0 GB</td>
<td>2025-03-26</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-aiops-moa-yuan-embedding-10"><a href="https://huggingface.co/OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0"><code>OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0</code></a><a class="headerlink" href="#opensearch-aiops-moa-yuan-embedding-10" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>343.0M</td>
<td>2.0 GB</td>
<td>2025-03-26</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="ordalietechsolon-embeddings-large-01"><a href="https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1"><code>OrdalieTech/Solon-embeddings-large-0.1</code></a><a class="headerlink" href="#ordalietechsolon-embeddings-large-01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2023-12-09</td>
<td>fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="ordalietechsolon-embeddings-mini-beta-11"><a href="https://huggingface.co/OrdalieTech/Solon-embeddings-mini-beta-1.1"><code>OrdalieTech/Solon-embeddings-mini-beta-1.1</code></a><a class="headerlink" href="#ordalietechsolon-embeddings-mini-beta-11" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>210.0M</td>
<td>808.0 MB</td>
<td>2025-01-01</td>
<td>fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="orlikbkartonbert-use-base-v1"><a href="https://huggingface.co/OrlikB/KartonBERT-USE-base-v1"><code>OrlikB/KartonBERT-USE-base-v1</code></a><a class="headerlink" href="#orlikbkartonbert-use-base-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> gpl-3.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>103.7M</td>
<td>396.0 MB</td>
<td>2024-09-30</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="orlikbst-polish-kartonberta-base-alpha-v1"><a href="https://huggingface.co/OrlikB/st-polish-kartonberta-base-alpha-v1"><code>OrlikB/st-polish-kartonberta-base-alpha-v1</code></a><a class="headerlink" href="#orlikbst-polish-kartonberta-base-alpha-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> lgpl</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-12</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="partaitooka-sbert"><a href="https://huggingface.co/PartAI/Tooka-SBERT"><code>PartAI/Tooka-SBERT</code></a><a class="headerlink" href="#partaitooka-sbert" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>353.0M</td>
<td>1.3 GB</td>
<td>2024-12-07</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="partaitooka-sbert-v2-large"><a href="https://huggingface.co/PartAI/Tooka-SBERT-V2-Large"><code>PartAI/Tooka-SBERT-V2-Large</code></a><a class="headerlink" href="#partaitooka-sbert-v2-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>353.0M</td>
<td>1.3 GB</td>
<td>2025-05-01</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="partaitooka-sbert-v2-small"><a href="https://huggingface.co/PartAI/Tooka-SBERT-V2-Small"><code>PartAI/Tooka-SBERT-V2-Small</code></a><a class="headerlink" href="#partaitooka-sbert-v2-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>122.9M</td>
<td>496.0 MB</td>
<td>2025-05-01</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="partaitookabert-base"><a href="https://huggingface.co/PartAI/TookaBERT-Base"><code>PartAI/TookaBERT-Base</code></a><a class="headerlink" href="#partaitookabert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>122.9M</td>
<td>469.0 MB</td>
<td>2024-12-08</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="qodoqodo-embed-1-15b"><a href="https://huggingface.co/Qodo/Qodo-Embed-1-1.5B"><code>Qodo/Qodo-Embed-1-1.5B</code></a><a class="headerlink" href="#qodoqodo-embed-1-15b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>1.8B</td>
<td>6.6 GB</td>
<td>2025-02-19</td>
<td>c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)</td>
</tr>
</tbody>
</table>
<h4 id="qodoqodo-embed-1-7b"><a href="https://huggingface.co/Qodo/Qodo-Embed-1-7B"><code>Qodo/Qodo-Embed-1-7B</code></a><a class="headerlink" href="#qodoqodo-embed-1-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.6B</td>
<td>28.4 GB</td>
<td>2025-02-24</td>
<td>c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)</td>
</tr>
</tbody>
</table>
<h4 id="shuu12121codesearch-modernbert-crow-plus"><a href="https://huggingface.co/Shuu12121/CodeSearch-ModernBERT-Crow-Plus"><code>Shuu12121/CodeSearch-ModernBERT-Crow-Plus</code></a><a class="headerlink" href="#shuu12121codesearch-modernbert-crow-plus" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0K</td>
<td>768</td>
<td>151.7M</td>
<td>607.0 MB</td>
<td>2025-04-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="tencentbacconan-embedding-v1"><a href="https://huggingface.co/Classical/Yinka"><code>TencentBAC/Conan-embedding-v1</code></a><a class="headerlink" href="#tencentbacconan-embedding-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-08-22</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="vovanphucsup-simcse-vietnamese-phobert-base"><a href="https://huggingface.co/VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"><code>VoVanPhuc/sup-SimCSE-VietNamese-phobert-base</code></a><a class="headerlink" href="#vovanphucsup-simcse-vietnamese-phobert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>768</td>
<td>135.0M</td>
<td>517.0 MB</td>
<td>2021-05-26</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="aari1995german_semantic_sts_v2"><a href="https://huggingface.co/aari1995/German_Semantic_STS_V2"><code>aari1995/German_Semantic_STS_V2</code></a><a class="headerlink" href="#aari1995german_semantic_sts_v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.7M</td>
<td>1.3 GB</td>
<td>2022-11-17</td>
<td>deu-Latn</td>
</tr>
</tbody>
</table>
<h4 id="abhinandmedembed-small-v01"><a href="https://huggingface.co/abhinand/MedEmbed-small-v0.1"><code>abhinand/MedEmbed-small-v0.1</code></a><a class="headerlink" href="#abhinandmedembed-small-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-10-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="ai-foreversbert_large_mt_nlu_ru"><a href="https://huggingface.co/ai-forever/sbert_large_mt_nlu_ru"><code>ai-forever/sbert_large_mt_nlu_ru</code></a><a class="headerlink" href="#ai-foreversbert_large_mt_nlu_ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>427.0M</td>
<td>1.6 GB</td>
<td>2021-05-18</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="ai-foreversbert_large_nlu_ru"><a href="https://huggingface.co/ai-forever/sbert_large_nlu_ru"><code>ai-forever/sbert_large_nlu_ru</code></a><a class="headerlink" href="#ai-foreversbert_large_nlu_ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>427.0M</td>
<td>1.6 GB</td>
<td>2020-11-20</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="amazontitan-text-embeddings-v2"><a href="https://huggingface.co/amazon/Titan-text-embeddings-v2"><code>amazon/Titan-text-embeddings-v2</code></a><a class="headerlink" href="#amazontitan-text-embeddings-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://aws.amazon.com/service-terms/</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="avsolatoriogist-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-Embedding-v0"><code>avsolatorio/GIST-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2024-01-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="avsolatoriogist-all-minilm-l6-v2"><a href="https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2"><code>avsolatorio/GIST-all-MiniLM-L6-v2</code></a><a class="headerlink" href="#avsolatoriogist-all-minilm-l6-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-02-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="avsolatoriogist-large-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-large-Embedding-v0"><code>avsolatorio/GIST-large-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-large-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>1.2 GB</td>
<td>2024-02-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="avsolatoriogist-small-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-small-Embedding-v0"><code>avsolatorio/GIST-small-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-small-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-02-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="avsolatorionoinstruct-small-embedding-v0"><a href="https://huggingface.co/avsolatorio/NoInstruct-small-Embedding-v0"><code>avsolatorio/NoInstruct-small-Embedding-v0</code></a><a class="headerlink" href="#avsolatorionoinstruct-small-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-05-01</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bedrockamazon-titan-embed-text-v1"><a href="https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/"><code>bedrock/amazon-titan-embed-text-v1</code></a><a class="headerlink" href="#bedrockamazon-titan-embed-text-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-09-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="bedrockamazon-titan-embed-text-v2"><a href="https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-titan-text-embeddings-v2-amazon-bedrock/"><code>bedrock/amazon-titan-embed-text-v2</code></a><a class="headerlink" href="#bedrockamazon-titan-embed-text-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="bigsciencesgpt-bloom-7b1-msmarco"><a href="https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco"><code>bigscience/sgpt-bloom-7b1-msmarco</code></a><a class="headerlink" href="#bigsciencesgpt-bloom-7b1-msmarco" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>4096</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-08-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="bkai-foundation-modelsvietnamese-bi-encoder"><a href="https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder"><code>bkai-foundation-models/vietnamese-bi-encoder</code></a><a class="headerlink" href="#bkai-foundation-modelsvietnamese-bi-encoder" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>768</td>
<td>135.0M</td>
<td>515.0 MB</td>
<td>2023-09-09</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bm25s"><a href="https://github.com/xhluca/bm25s"><code>bm25s</code></a><a class="headerlink" href="#bm25s" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-07-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bm25s</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BM25S: Orders of magnitude faster lexical search via eager sparse scoring}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xing Han Lù}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.03618}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.03618}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="brahmairesearchslx-v01"><a href="https://huggingface.co/brahmairesearch/slx-v0.1"><code>brahmairesearch/slx-v0.1</code></a><a class="headerlink" href="#brahmairesearchslx-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-08-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="castorinimonobert-large-msmarco">castorini/monobert-large-msmarco<a class="headerlink" href="#castorinimonobert-large-msmarco" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2020-05-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="castorinimonot5-3b-msmarco-10k">castorini/monot5-3b-msmarco-10k<a class="headerlink" href="#castorinimonot5-3b-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-base-msmarco-10k">castorini/monot5-base-msmarco-10k<a class="headerlink" href="#castorinimonot5-base-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-large-msmarco-10k">castorini/monot5-large-msmarco-10k<a class="headerlink" href="#castorinimonot5-large-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-small-msmarco-10k">castorini/monot5-small-msmarco-10k<a class="headerlink" href="#castorinimonot5-small-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codesagecodesage-base-v2"><a href="https://huggingface.co/codesage/codesage-base-v2"><code>codesage/codesage-base-v2</code></a><a class="headerlink" href="#codesagecodesage-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>356.0M</td>
<td>1.3 GB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<h4 id="codesagecodesage-large-v2"><a href="https://huggingface.co/codesage/codesage-large-v2"><code>codesage/codesage-large-v2</code></a><a class="headerlink" href="#codesagecodesage-large-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>4.8 GB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<h4 id="codesagecodesage-small-v2"><a href="https://huggingface.co/codesage/codesage-small-v2"><code>codesage/codesage-small-v2</code></a><a class="headerlink" href="#codesagecodesage-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>130.0M</td>
<td>496.0 MB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<h4 id="cointegratedlabse-en-ru"><a href="https://huggingface.co/cointegrated/LaBSE-en-ru"><code>cointegrated/LaBSE-en-ru</code></a><a class="headerlink" href="#cointegratedlabse-en-ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>129.0M</td>
<td>492.0 MB</td>
<td>2021-06-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="cointegratedrubert-tiny"><a href="https://huggingface.co/cointegrated/rubert-tiny"><code>cointegrated/rubert-tiny</code></a><a class="headerlink" href="#cointegratedrubert-tiny" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>312</td>
<td>11.9M</td>
<td>45.0 MB</td>
<td>2021-05-24</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="cointegratedrubert-tiny2"><a href="https://huggingface.co/cointegrated/rubert-tiny2"><code>cointegrated/rubert-tiny2</code></a><a class="headerlink" href="#cointegratedrubert-tiny2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>29.4M</td>
<td>112.0 MB</td>
<td>2021-10-28</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="colbert-ircolbertv20"><a href="https://huggingface.co/colbert-ir/colbertv2.0"><code>colbert-ir/colbertv2.0</code></a><a class="headerlink" href="#colbert-ircolbertv20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>180</td>
<td>not specified</td>
<td>110.0M</td>
<td>418.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="consciousaicai-lunaris-text-embeddings"><a href="https://huggingface.co/consciousAI/cai-lunaris-text-embeddings"><code>consciousAI/cai-lunaris-text-embeddings</code></a><a class="headerlink" href="#consciousaicai-lunaris-text-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-06-22</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="consciousaicai-stellaris-text-embeddings"><a href="https://huggingface.co/consciousAI/cai-stellaris-text-embeddings"><code>consciousAI/cai-stellaris-text-embeddings</code></a><a class="headerlink" href="#consciousaicai-stellaris-text-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-06-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="deepfileembedder-100p"><a href="https://huggingface.co/deepfile/embedder-100p"><code>deepfile/embedder-100p</code></a><a class="headerlink" href="#deepfileembedder-100p" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>not specified</td>
<td>1.0 GB</td>
<td>2023-07-24</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="deepvkuser-bge-m3"><a href="https://huggingface.co/deepvk/USER-base"><code>deepvk/USER-bge-m3</code></a><a class="headerlink" href="#deepvkuser-bge-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>359.0M</td>
<td>1.3 GB</td>
<td>2024-07-05</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="deepvkdeberta-v1-base"><a href="https://huggingface.co/deepvk/deberta-v1-base"><code>deepvk/deberta-v1-base</code></a><a class="headerlink" href="#deepvkdeberta-v1-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.0M</td>
<td>473.0 MB</td>
<td>2023-02-07</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="dunzhangstella-large-zh-v3-1792d"><a href="https://huggingface.co/dunzhang/stella-large-zh-v3-1792d"><code>dunzhang/stella-large-zh-v3-1792d</code></a><a class="headerlink" href="#dunzhangstella-large-zh-v3-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-02-17</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dunzhangstella-mrl-large-zh-v35-1792d"><a href="https://huggingface.co/dunzhang/stella-large-zh-v3-1792d"><code>dunzhang/stella-mrl-large-zh-v3.5-1792d</code></a><a class="headerlink" href="#dunzhangstella-mrl-large-zh-v35-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-02-27</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dwzhue5-base-4k"><a href="https://huggingface.co/dwzhu/e5-base-4k"><code>dwzhu/e5-base-4k</code></a><a class="headerlink" href="#dwzhue5-base-4k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="facebooksonar"><a href="https://ai.meta.com/research/publications/sonar-sentence-level-multimodal-and-language-agnostic-representations/"><code>facebook/SONAR</code></a><a class="headerlink" href="#facebooksonar" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2021-05-21</td>
<td>ace-Arab, ace-Latn, acm-Arab, acq-Arab, aeb-Arab, ... (204)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Duquenne:2023:sonar_arxiv</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Paul-Ambroise Duquenne and Holger Schwenk and Benoit Sagot}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{{SONAR:} Sentence-Level Multimodal and Language-Agnostic Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2308.11466}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="facebookcontriever-msmarco"><a href="https://huggingface.co/facebook/contriever-msmarco"><code>facebook/contriever-msmarco</code></a><a class="headerlink" href="#facebookcontriever-msmarco" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>150.0M</td>
<td>572.0 MB</td>
<td>2022-06-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">izacard2021contriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Unsupervised Dense Information Retrieval with Contrastive Learning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2112.09118}</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2112.09118}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="fangxqxyz-embedding"><a href="https://huggingface.co/fangxq/XYZ-embedding"><code>fangxq/XYZ-embedding</code></a><a class="headerlink" href="#fangxqxyz-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-09-13</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="googleflan-t5-base">google/flan-t5-base<a class="headerlink" href="#googleflan-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>944.0 MB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-large">google/flan-t5-large<a class="headerlink" href="#googleflan-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2.9 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-xl">google/flan-t5-xl<a class="headerlink" href="#googleflan-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>10.6 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-xxl">google/flan-t5-xxl<a class="headerlink" href="#googleflan-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>42.0 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hiieuhalong_embedding"><a href="https://huggingface.co/hiieu/halong_embedding"><code>hiieu/halong_embedding</code></a><a class="headerlink" href="#hiieuhalong_embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-07-06</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="iampandazpoint_large_embedding_zh"><a href="https://huggingface.co/iampanda/zpoint_large_embedding_zh"><code>iampanda/zpoint_large_embedding_zh</code></a><a class="headerlink" href="#iampandazpoint_large_embedding_zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-06-04</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="ibm-granitegranite-embedding-107m-multilingual"><a href="https://huggingface.co/ibm-granite/granite-embedding-107m-multilingual"><code>ibm-granite/granite-embedding-107m-multilingual</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-107m-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>107.0M</td>
<td>204.0 MB</td>
<td>2024-12-18</td>
<td>ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-125m-english"><a href="https://huggingface.co/ibm-granite/granite-embedding-125m-english"><code>ibm-granite/granite-embedding-125m-english</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-125m-english" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>125.0M</td>
<td>238.0 MB</td>
<td>2024-12-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-278m-multilingual"><a href="https://huggingface.co/ibm-granite/granite-embedding-278m-multilingual"><code>ibm-granite/granite-embedding-278m-multilingual</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-278m-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>530.0 MB</td>
<td>2024-12-18</td>
<td>ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-30m-english"><a href="https://huggingface.co/ibm-granite/granite-embedding-30m-english"><code>ibm-granite/granite-embedding-30m-english</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-30m-english" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>30.0M</td>
<td>58.0 MB</td>
<td>2024-12-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-english-r2"><a href="https://huggingface.co/ibm-granite/granite-embedding-english-r2"><code>ibm-granite/granite-embedding-english-r2</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-english-r2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>284.0 MB</td>
<td>2025-08-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-small-english-r2"><a href="https://huggingface.co/ibm-granite/granite-embedding-small-english-r2"><code>ibm-granite/granite-embedding-small-english-r2</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-small-english-r2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>384</td>
<td>47.0M</td>
<td>91.0 MB</td>
<td>2025-08-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="infgradstella-base-en-v2"><a href="https://huggingface.co/infgrad/stella-base-en-v2"><code>infgrad/stella-base-en-v2</code></a><a class="headerlink" href="#infgradstella-base-en-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-19</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="infgradstella-base-zh-v3-1792d"><a href="https://huggingface.co/infgrad/stella-base-zh-v3-1792d"><code>infgrad/stella-base-zh-v3-1792d</code></a><a class="headerlink" href="#infgradstella-base-zh-v3-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-02-17</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="izhxudever-bloom-1b1"><a href="https://huggingface.co/izhx/udever-bloom-1b1"><code>izhx/udever-bloom-1b1</code></a><a class="headerlink" href="#izhxudever-bloom-1b1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<h4 id="izhxudever-bloom-3b"><a href="https://huggingface.co/izhx/udever-bloom-3b"><code>izhx/udever-bloom-3b</code></a><a class="headerlink" href="#izhxudever-bloom-3b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<h4 id="izhxudever-bloom-560m"><a href="https://huggingface.co/izhx/udever-bloom-560m"><code>izhx/udever-bloom-560m</code></a><a class="headerlink" href="#izhxudever-bloom-560m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<h4 id="izhxudever-bloom-7b1"><a href="https://huggingface.co/izhx/udever-bloom-7b1"><code>izhx/udever-bloom-7b1</code></a><a class="headerlink" href="#izhxudever-bloom-7b1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<h4 id="jhu-clspfollowir-7b">jhu-clsp/FollowIR-7B<a class="headerlink" href="#jhu-clspfollowir-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>13.5 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">weller2024followir</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2403.15246}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-colbert-v2"><a href="https://huggingface.co/jinaai/jina-colbert-v2"><code>jinaai/jina-colbert-v2</code></a><a class="headerlink" href="#jinaaijina-colbert-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>not specified</td>
<td>559.0M</td>
<td>1.0 GB</td>
<td>2024-08-16</td>
<td>ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (22)</td>
</tr>
</tbody>
</table>
<h4 id="jinaaijina-embedding-b-en-v1"><a href="https://huggingface.co/jinaai/jina-embedding-b-en-v1"><code>jinaai/jina-embedding-b-en-v1</code></a><a class="headerlink" href="#jinaaijina-embedding-b-en-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>110.0M</td>
<td>420.0 MB</td>
<td>2023-07-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">günther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Günther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.11224}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embedding-s-en-v1"><a href="https://huggingface.co/jinaai/jina-embedding-s-en-v1"><code>jinaai/jina-embedding-s-en-v1</code></a><a class="headerlink" href="#jinaaijina-embedding-s-en-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>35.0M</td>
<td>134.0 MB</td>
<td>2023-07-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">günther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Günther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.11224}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v2-base-en"><a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en"><code>jinaai/jina-embeddings-v2-base-en</code></a><a class="headerlink" href="#jinaaijina-embeddings-v2-base-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>137.0M</td>
<td>262.0 MB</td>
<td>2023-09-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">günther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Günther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.19923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v2-small-en"><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en"><code>jinaai/jina-embeddings-v2-small-en</code></a><a class="headerlink" href="#jinaaijina-embeddings-v2-small-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>32.7M</td>
<td>62.0 MB</td>
<td>2023-09-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">günther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Günther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.19923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-reranker-v2-base-multilingual">jinaai/jina-reranker-v2-base-multilingual<a class="headerlink" href="#jinaaijina-reranker-v2-base-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>531.0 MB</td>
<td>2024-09-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="keeeeenwmicrollama-text-embedding"><a href="https://huggingface.co/keeeeenw/MicroLlama-text-embedding"><code>keeeeenw/MicroLlama-text-embedding</code></a><a class="headerlink" href="#keeeeenwmicrollama-text-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>272.0M</td>
<td>1.0 GB</td>
<td>2024-11-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="lier007xiaobu-embedding"><a href="https://huggingface.co/lier007/xiaobu-embedding"><code>lier007/xiaobu-embedding</code></a><a class="headerlink" href="#lier007xiaobu-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-01-09</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="lier007xiaobu-embedding-v2"><a href="https://huggingface.co/lier007/xiaobu-embedding-v2"><code>lier007/xiaobu-embedding-v2</code></a><a class="headerlink" href="#lier007xiaobu-embedding-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-06-30</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="lightonaigte-moderncolbert-v1"><a href="https://huggingface.co/lightonai/GTE-ModernColBERT-v1"><code>lightonai/GTE-ModernColBERT-v1</code></a><a class="headerlink" href="#lightonaigte-moderncolbert-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>not specified</td>
<td>149.0M</td>
<td>not specified</td>
<td>2025-04-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="llmrailsember-v1"><a href="https://huggingface.co/llmrails/ember-v1"><code>llmrails/ember-v1</code></a><a class="headerlink" href="#llmrailsember-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-10-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="m3hrdadfibert-zwnj-wnli-mean-tokens"><a href="https://huggingface.co/m3hrdadfi/bert-zwnj-wnli-mean-tokens"><code>m3hrdadfi/bert-zwnj-wnli-mean-tokens</code></a><a class="headerlink" href="#m3hrdadfibert-zwnj-wnli-mean-tokens" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>118.3M</td>
<td>451.0 MB</td>
<td>2021-06-28</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="m3hrdadfiroberta-zwnj-wnli-mean-tokens"><a href="https://huggingface.co/m3hrdadfi/roberta-zwnj-wnli-mean-tokens"><code>m3hrdadfi/roberta-zwnj-wnli-mean-tokens</code></a><a class="headerlink" href="#m3hrdadfiroberta-zwnj-wnli-mean-tokens" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>118.3M</td>
<td>451.0 MB</td>
<td>2021-06-28</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="malenia1ternary-weight-embedding"><a href="https://huggingface.co/malenia1/ternary-weight-embedding"><code>malenia1/ternary-weight-embedding</code></a><a class="headerlink" href="#malenia1ternary-weight-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>98.7M</td>
<td>158.0 MB</td>
<td>2024-10-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manubge-m3-custom-fr"><a href="https://huggingface.co/manu/bge-m3-custom-fr"><code>manu/bge-m3-custom-fr</code></a><a class="headerlink" href="#manubge-m3-custom-fr" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v02"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.2"><code>manu/sentence_croissant_alpha_v0.2</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v02" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-03-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v03"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.3"><code>manu/sentence_croissant_alpha_v0.3</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v03" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-04-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v04"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.4"><code>manu/sentence_croissant_alpha_v0.4</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v04" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-04-27</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="meta-llamallama-2-7b-chat-hf">meta-llama/Llama-2-7b-chat-hf<a class="headerlink" href="#meta-llamallama-2-7b-chat-hf" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">touvron2023llama2openfoundation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Llama 2: Open Foundation and Fine-Tuned Chat Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.09288}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2307.09288}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="meta-llamallama-2-7b-hf">meta-llama/Llama-2-7b-hf<a class="headerlink" href="#meta-llamallama-2-7b-hf" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">touvron2023llama2openfoundation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Llama 2: Open Foundation and Fine-Tuned Chat Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.09288}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2307.09288}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_glove"><a href="https://huggingface.co/minishlab/M2V_base_glove"><code>minishlab/M2V_base_glove</code></a><a class="headerlink" href="#minishlabm2v_base_glove" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>102.0M</td>
<td>391.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_glove_subword"><a href="https://huggingface.co/minishlab/M2V_base_glove_subword"><code>minishlab/M2V_base_glove_subword</code></a><a class="headerlink" href="#minishlabm2v_base_glove_subword" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>103.0M</td>
<td>391.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_output"><a href="https://huggingface.co/minishlab/M2V_base_output"><code>minishlab/M2V_base_output</code></a><a class="headerlink" href="#minishlabm2v_base_output" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.6M</td>
<td>29.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_multilingual_output"><a href="https://huggingface.co/minishlab/M2V_multilingual_output"><code>minishlab/M2V_multilingual_output</code></a><a class="headerlink" href="#minishlabm2v_multilingual_output" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-2m"><a href="https://huggingface.co/minishlab/potion-base-2M"><code>minishlab/potion-base-2M</code></a><a class="headerlink" href="#minishlabpotion-base-2m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>2.0M</td>
<td>7.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-4m"><a href="https://huggingface.co/minishlab/potion-base-4M"><code>minishlab/potion-base-4M</code></a><a class="headerlink" href="#minishlabpotion-base-4m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>128</td>
<td>3.8M</td>
<td>14.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-8m"><a href="https://huggingface.co/minishlab/potion-base-8M"><code>minishlab/potion-base-8M</code></a><a class="headerlink" href="#minishlabpotion-base-8m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.6M</td>
<td>29.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-multilingual-128m"><a href="https://huggingface.co/minishlab/potion-multilingual-128M"><code>minishlab/potion-multilingual-128M</code></a><a class="headerlink" href="#minishlabpotion-multilingual-128m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2025-05-23</td>
<td>afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (101)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mistralaimistral-7b-instruct-v02">mistralai/Mistral-7B-Instruct-v0.2<a class="headerlink" href="#mistralaimistral-7b-instruct-v02" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-12-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">jiang2023mistral7b</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Mistral 7B}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.06825}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2310.06825}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-base"><a href="https://huggingface.co/moka-ai/m3e-base"><code>moka-ai/m3e-base</code></a><a class="headerlink" href="#moka-aim3e-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>102.0M</td>
<td>390.0 MB</td>
<td>2023-06-06</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-large"><a href="https://huggingface.co/moka-ai/m3e-large"><code>moka-ai/m3e-large</code></a><a class="headerlink" href="#moka-aim3e-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-06-21</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-small"><a href="https://huggingface.co/moka-ai/m3e-small"><code>moka-ai/m3e-small</code></a><a class="headerlink" href="#moka-aim3e-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-06-02</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="myrkursentence-transformer-parsbert-fa"><a href="https://huggingface.co/myrkur/sentence-transformer-parsbert-fa"><code>myrkur/sentence-transformer-parsbert-fa</code></a><a class="headerlink" href="#myrkursentence-transformer-parsbert-fa" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2024-12-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omarelshehyarabic-english-sts-matryoshka"><a href="https://huggingface.co/omarelshehy/arabic-english-sts-matryoshka"><code>omarelshehy/arabic-english-sts-matryoshka</code></a><a class="headerlink" href="#omarelshehyarabic-english-sts-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2024-10-13</td>
<td>ara-Arab, eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-large"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-large</code></a><a class="headerlink" href="#openaitext-embedding-3-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3072</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-large-embed_dim512"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-large (embed_dim=512)</code></a><a class="headerlink" href="#openaitext-embedding-3-large-embed_dim512" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-small"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-small</code></a><a class="headerlink" href="#openaitext-embedding-3-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-small-embed_dim512"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-small (embed_dim=512)</code></a><a class="headerlink" href="#openaitext-embedding-3-small-embed_dim512" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-ada-002"><a href="https://openai.com/index/new-and-improved-embedding-model/"><code>openai/text-embedding-ada-002</code></a><a class="headerlink" href="#openaitext-embedding-ada-002" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-12-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openbmbminicpm-embedding"><a href="https://huggingface.co/openbmb/MiniCPM-Embedding"><code>openbmb/MiniCPM-Embedding</code></a><a class="headerlink" href="#openbmbminicpm-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>2304</td>
<td>2.7B</td>
<td>5.1 GB</td>
<td>2024-09-04</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="panalexeuxlm-roberta-ua-distilled"><a href="https://github.com/panalexeu/xlm-roberta-ua-distilled/tree/main"><code>panalexeu/xlm-roberta-ua-distilled</code></a><a class="headerlink" href="#panalexeuxlm-roberta-ua-distilled" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2025-04-15</td>
<td>eng-Latn, ukr-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="prdevmini-gte"><a href="https://huggingface.co/prdev/mini-gte"><code>prdev/mini-gte</code></a><a class="headerlink" href="#prdevmini-gte" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>66.3M</td>
<td>253.0 MB</td>
<td>2025-01-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="richinfoairitrieve_zh_v1"><a href="https://huggingface.co/richinfoai/ritrieve_zh_v1"><code>richinfoai/ritrieve_zh_v1</code></a><a class="headerlink" href="#richinfoairitrieve_zh_v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2025-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="sbunlpfabert"><a href="https://huggingface.co/sbunlp/fabert"><code>sbunlp/fabert</code></a><a class="headerlink" href="#sbunlpfabert" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2024-10-07</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="sdadasmmlw-e5-base"><a href="https://huggingface.co/sdadas/mmlw-e5-base"><code>sdadas/mmlw-e5-base</code></a><a class="headerlink" href="#sdadasmmlw-e5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sdadasmmlw-e5-large"><a href="https://huggingface.co/sdadas/mmlw-e5-large"><code>sdadas/mmlw-e5-large</code></a><a class="headerlink" href="#sdadasmmlw-e5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sdadasmmlw-e5-small"><a href="https://huggingface.co/sdadas/mmlw-e5-small"><code>sdadas/mmlw-e5-small</code></a><a class="headerlink" href="#sdadasmmlw-e5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sdadasmmlw-roberta-base"><a href="https://huggingface.co/sdadas/mmlw-roberta-base"><code>sdadas/mmlw-roberta-base</code></a><a class="headerlink" href="#sdadasmmlw-roberta-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sdadasmmlw-roberta-large"><a href="https://huggingface.co/sdadas/mmlw-roberta-large"><code>sdadas/mmlw-roberta-large</code></a><a class="headerlink" href="#sdadasmmlw-roberta-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sensenovapiccolo-base-zh"><a href="https://huggingface.co/sensenova/piccolo-base-zh"><code>sensenova/piccolo-base-zh</code></a><a class="headerlink" href="#sensenovapiccolo-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-09-04</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="sensenovapiccolo-large-zh-v2"><a href="https://huggingface.co/sensenova/piccolo-large-zh-v2"><code>sensenova/piccolo-large-zh-v2</code></a><a class="headerlink" href="#sensenovapiccolo-large-zh-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-22</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformerslabse"><a href="https://huggingface.co/sentence-transformers/LaBSE"><code>sentence-transformers/LaBSE</code></a><a class="headerlink" href="#sentence-transformerslabse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>471.0M</td>
<td>1.8 GB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">feng2022languageagnosticbertsentenceembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Language-agnostic BERT Sentence Embedding}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2007.01852}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2007.01852}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-minilm-l12-v2"><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"><code>sentence-transformers/all-MiniLM-L12-v2</code></a><a class="headerlink" href="#sentence-transformersall-minilm-l12-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-minilm-l6-v2"><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code>sentence-transformers/all-MiniLM-L6-v2</code></a><a class="headerlink" href="#sentence-transformersall-minilm-l6-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-mpnet-base-v2"><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2"><code>sentence-transformers/all-mpnet-base-v2</code></a><a class="headerlink" href="#sentence-transformersall-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>384</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersgtr-t5-base"><a href="https://huggingface.co/sentence-transformers/gtr-t5-base"><code>sentence-transformers/gtr-t5-base</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>110.0M</td>
<td>209.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersgtr-t5-large"><a href="https://huggingface.co/sentence-transformers/gtr-t5-large"><code>sentence-transformers/gtr-t5-large</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.0M</td>
<td>639.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersgtr-t5-xl"><a href="https://huggingface.co/sentence-transformers/gtr-t5-xl"><code>sentence-transformers/gtr-t5-xl</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>1.2B</td>
<td>2.3 GB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersgtr-t5-xxl"><a href="https://huggingface.co/sentence-transformers/gtr-t5-xxl"><code>sentence-transformers/gtr-t5-xxl</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>4.9B</td>
<td>9.1 GB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersmulti-qa-minilm-l6-cos-v1"><a href="https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1"><code>sentence-transformers/multi-qa-MiniLM-L6-cos-v1</code></a><a class="headerlink" href="#sentence-transformersmulti-qa-minilm-l6-cos-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersparaphrase-multilingual-minilm-l12-v2"><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"><code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code></a><a class="headerlink" href="#sentence-transformersparaphrase-multilingual-minilm-l12-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>118.0M</td>
<td>449.0 MB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersparaphrase-multilingual-mpnet-base-v2"><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2"><code>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</code></a><a class="headerlink" href="#sentence-transformersparaphrase-multilingual-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerssentence-t5-base"><a href="https://huggingface.co/sentence-transformers/sentence-t5-base"><code>sentence-transformers/sentence-t5-base</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>110.0M</td>
<td>209.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformerssentence-t5-large"><a href="https://huggingface.co/sentence-transformers/sentence-t5-large"><code>sentence-transformers/sentence-t5-large</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.0M</td>
<td>639.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformerssentence-t5-xl"><a href="https://huggingface.co/sentence-transformers/sentence-t5-xl"><code>sentence-transformers/sentence-t5-xl</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>3.0B</td>
<td>2.3 GB</td>
<td>2024-03-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformerssentence-t5-xxl"><a href="https://huggingface.co/sentence-transformers/sentence-t5-xxl"><code>sentence-transformers/sentence-t5-xxl</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>11.0B</td>
<td>9.1 GB</td>
<td>2024-03-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersstatic-similarity-mrl-multilingual-v1"><a href="https://huggingface.co/sentence-transformers/static-similarity-mrl-multilingual-v1"><code>sentence-transformers/static-similarity-mrl-multilingual-v1</code></a><a class="headerlink" href="#sentence-transformersstatic-similarity-mrl-multilingual-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>1024</td>
<td>108.4M</td>
<td>413.0 MB</td>
<td>2025-01-15</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (49)</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhlabse-ru-turbo"><a href="https://huggingface.co/sergeyzh/LaBSE-ru-turbo"><code>sergeyzh/LaBSE-ru-turbo</code></a><a class="headerlink" href="#sergeyzhlabse-ru-turbo" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>129.0M</td>
<td>490.0 MB</td>
<td>2024-06-27</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhrubert-tiny-turbo"><a href="https://huggingface.co/sergeyzh/rubert-tiny-turbo"><code>sergeyzh/rubert-tiny-turbo</code></a><a class="headerlink" href="#sergeyzhrubert-tiny-turbo" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>29.2M</td>
<td>111.0 MB</td>
<td>2024-06-21</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="shibing624text2vec-base-chinese"><a href="https://huggingface.co/shibing624/text2vec-base-chinese"><code>shibing624/text2vec-base-chinese</code></a><a class="headerlink" href="#shibing624text2vec-base-chinese" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>102.0M</td>
<td>390.0 MB</td>
<td>2022-01-23</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="shibing624text2vec-base-chinese-paraphrase"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase"><code>shibing624/text2vec-base-chinese-paraphrase</code></a><a class="headerlink" href="#shibing624text2vec-base-chinese-paraphrase" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>118.0M</td>
<td>450.0 MB</td>
<td>2023-06-19</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="shibing624text2vec-base-multilingual"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase"><code>shibing624/text2vec-base-multilingual</code></a><a class="headerlink" href="#shibing624text2vec-base-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2023-06-22</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, nld-Latn, ... (10)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="silma-aisilma-embeddding-matryoshka-v01"><a href="https://huggingface.co/silma-ai/silma-embeddding-matryoshka-v0.1"><code>silma-ai/silma-embeddding-matryoshka-v0.1</code></a><a class="headerlink" href="#silma-aisilma-embeddding-matryoshka-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>135.2M</td>
<td>516.0 MB</td>
<td>2024-10-12</td>
<td>ara-Arab, eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-base"><a href="https://huggingface.co/thenlper/gte-base"><code>thenlper/gte-base</code></a><a class="headerlink" href="#thenlpergte-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>209.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-base-zh"><a href="https://huggingface.co/thenlper/gte-base-zh"><code>thenlper/gte-base-zh</code></a><a class="headerlink" href="#thenlpergte-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>102.0M</td>
<td>195.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-large"><a href="https://huggingface.co/thenlper/gte-large"><code>thenlper/gte-large</code></a><a class="headerlink" href="#thenlpergte-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>639.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-large-zh"><a href="https://huggingface.co/thenlper/gte-large-zh"><code>thenlper/gte-large-zh</code></a><a class="headerlink" href="#thenlpergte-large-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>326.0M</td>
<td>621.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-small"><a href="https://huggingface.co/thenlper/gte-small"><code>thenlper/gte-small</code></a><a class="headerlink" href="#thenlpergte-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>64.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="thenlpergte-small-zh"><a href="https://huggingface.co/thenlper/gte-small-zh"><code>thenlper/gte-small-zh</code></a><a class="headerlink" href="#thenlpergte-small-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>30.3M</td>
<td>58.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="unicamp-dlmt5-13b-mmarco-100k">unicamp-dl/mt5-13b-mmarco-100k<a class="headerlink" href="#unicamp-dlmt5-13b-mmarco-100k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-11-04</td>
<td>afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103)</td>
</tr>
</tbody>
</table>
<h4 id="unicamp-dlmt5-base-mmarco-v2">unicamp-dl/mt5-base-mmarco-v2<a class="headerlink" href="#unicamp-dlmt5-base-mmarco-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-01-05</td>
<td>afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bonifacio2021mmarco</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Luiz Henrique Bonifacio and Vitor Jeronymo and Hugo Queiroz Abonizio and Israel Campiotti and Marzieh Fadaee and  and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2108.13897}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="w601sxsb1ade-embed"><a href="https://huggingface.co/w601sxs/b1ade-embed"><code>w601sxs/b1ade-embed</code></a><a class="headerlink" href="#w601sxsb1ade-embed" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2025-03-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">bigscience_workshop_2022</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{ {Shreyas Subramanian} }</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{ {b1ade series of models} }</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2024</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ https://huggingface.co/w601sxs/b1ade-embed }</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{ Hugging Face }</span>
<span class="p">}</span>
</code></pre></div>
</details>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../image_text/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Image-text Model">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Image-text Model
              </div>
            </div>
          </a>
        
        
          
          <a href="../../../api/" class="md-footer__link md-footer__link--next" aria-label="Next: Overview">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Overview
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      This text is freely available under a Creative Commons Attribution 4.0 license
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/embeddings-benchmark/mteb" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tracking", "navigation.instant", "navigation.tabs", "navigation.sections", "navigation.top", "search.suggest", "search.highlight", "content.tabs.link", "content.code.annotation", "content.code.copy", "content.action.edit", "content.action.view", "content.code.annotate", "content.tooltips", "navigation.footer", "navigation.indexes", "toc.follow"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>