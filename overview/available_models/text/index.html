
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../image_text/">
      
      
        <link rel="next" href="../../../api/">
      
      
        
      
      
      <link rel="icon" href="../../../images/logos/mteb_logo/dots-icon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Text Model - Massive Text Embedding Benchmark</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#text-model" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Massive Text Embedding Benchmark" class="md-header__button md-logo" aria-label="Massive Text Embedding Benchmark" data-md-component="logo">
      
  <img src="../../../images/logos/mteb_logo/dots-icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Massive Text Embedding Benchmark
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Text Model
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m14.3 16-.7-2h-3.2l-.7 2H7.8L11 7h2l3.2 9zM20 8.69V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12zm-9.15 3.96h2.3L12 9z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="light-blue"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to system preference"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to system preference" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/embeddings-benchmark/mteb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../.." class="md-tabs__link">
          
  
  
    
  
  Get Started

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../contributing/adding_a_model/" class="md-tabs__link">
          
  
  
    
  
  Contributing

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  Overview

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../api/" class="md-tabs__link">
          
  
  
    
  
  API

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="https://huggingface.co/spaces/mteb/leaderboard" class="md-tabs__link">
        
  
  
    
  
  Leaderboard

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Massive Text Embedding Benchmark" class="md-nav__button md-logo" aria-label="Massive Text Embedding Benchmark" data-md-component="logo">
      
  <img src="../../../images/logos/mteb_logo/dots-icon.png" alt="logo">

    </a>
    Massive Text Embedding Benchmark
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/embeddings-benchmark/mteb" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../.." class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Get Started
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_1" id="__nav_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Get Started
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../installation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Installation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../whats_new/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    What's New
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_4" >
        
          
          <label class="md-nav__link" for="__nav_1_4" id="__nav_1_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Usage
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Usage
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/get_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Get Started
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/defining_the_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Defining the Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/selecting_tasks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Selecting Tasks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/running_the_evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Running the Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/loading_results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Loading Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/cli/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Command Line Interface
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../usage/leaderboard/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Running the Leaderboard
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_5" >
        
          
          <label class="md-nav__link" for="__nav_1_5" id="__nav_1_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Advanced Usage
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Advanced Usage
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/two_stage_reranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Two stage reranking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/cache_embeddings/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cache embeddings
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/retrieval_backend/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Retrieval backend
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../advanced_usage/vllm_wrapper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    vLLM Wrapper
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Contributing
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding a Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_dataset/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding a Task
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../contributing/adding_a_benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Adding a Benchmark
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing to mteb
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Overview
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Benchmarks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Benchmarks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_benchmarks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Available Benchmarks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Tasks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Tasks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_3_1" id="__nav_3_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Available Tasks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Available Tasks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/any2anymultilingualretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Any2AnyMultilingualRetrieval
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/any2anyretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Any2AnyRetrieval
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audioclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audioclustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioClustering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audiomultilabelclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioMultilabelClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audiopairclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioPairClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audioreranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioReranking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/audiozeroshotclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AudioZeroshotClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/bitextmining/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    BitextMining
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/classification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Classification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/clustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Clustering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/compositionality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Compositionality
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/documentunderstanding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DocumentUnderstanding
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/imageclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ImageClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/imageclustering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ImageClustering
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/instructionreranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    InstructionReranking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/instructionretrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    InstructionRetrieval
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/multilabelclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MultilabelClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/pairclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PairClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/regression/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Regression
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/reranking/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reranking
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Retrieval
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/sts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    STS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/summarization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Summarization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visioncentricqa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VisionCentricQA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visualsts%28eng%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VisualSTS(eng)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/visualsts%28multi%29/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    VisualSTS(multi)
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../available_tasks/zeroshotclassification/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ZeroShotClassification
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Models
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Models
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_4_1" id="__nav_3_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Available Models
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_4_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_4_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Available Models
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Audio Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_image_text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Audio-image-text Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../audio_text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Audio-text Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../image_text/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image-text Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Text Model
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Text Model
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Instruction Model
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-instruction Model
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../api/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    API
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    API
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Evaluation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/benchmark/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Benchmark
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/task/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Task
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/model/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Model
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/results/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Results
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../api/types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Additional Types
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://huggingface.co/spaces/mteb/leaderboard" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Leaderboard
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Instruction Model
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-instruction-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Non-instruction Model
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/embeddings-benchmark/mteb/blob/main/docs/overview/available_models/text.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/embeddings-benchmark/mteb/raw/main/docs/overview/available_models/text.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="text-model">Text Model<a class="headerlink" href="#text-model" title="Permanent link">&para;</a></h1>
<!-- This document is auto-generated. Changes will be overwritten. Please change the generating script. -->

<ul>
<li><strong>Number of models:</strong> 240</li>
</ul>
<h2 id="instruction-model">Instruction Model<a class="headerlink" href="#instruction-model" title="Permanent link">&para;</a></h2>
<h4 id="alibaba-nlpgte-qwen15-7b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen1.5-7B-instruct"><code>Alibaba-NLP/gte-Qwen1.5-7B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen15-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>28.8 GB</td>
<td>2024-04-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="alibaba-nlpgte-qwen2-15b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct"><code>Alibaba-NLP/gte-Qwen2-1.5B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen2-15b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>8960</td>
<td>1.5B</td>
<td>6.6 GB</td>
<td>2024-07-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="alibaba-nlpgte-qwen2-7b-instruct"><a href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct"><code>Alibaba-NLP/gte-Qwen2-7B-instruct</code></a><a class="headerlink" href="#alibaba-nlpgte-qwen2-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.1B</td>
<td>28.4 GB</td>
<td>2024-06-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-en"><a href="https://huggingface.co/BAAI/bge-base-en"><code>BAAI/bge-base-en</code></a><a class="headerlink" href="#baaibge-base-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-en-v15"><a href="https://huggingface.co/BAAI/bge-base-en-v1.5"><code>BAAI/bge-base-en-v1.5</code></a><a class="headerlink" href="#baaibge-base-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-09-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-zh"><a href="https://huggingface.co/BAAI/bge-base-zh"><code>BAAI/bge-base-zh</code></a><a class="headerlink" href="#baaibge-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>390.0 MB</td>
<td>2023-08-05</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-base-zh-v15"><a href="https://huggingface.co/BAAI/bge-base-zh-v1.5"><code>BAAI/bge-base-zh-v1.5</code></a><a class="headerlink" href="#baaibge-base-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>416.0 MB</td>
<td>2023-09-11</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-large-en"><a href="https://huggingface.co/BAAI/bge-large-en"><code>BAAI/bge-large-en</code></a><a class="headerlink" href="#baaibge-large-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-large-en-v15"><a href="https://huggingface.co/BAAI/bge-large-en-v1.5"><code>BAAI/bge-large-en-v1.5</code></a><a class="headerlink" href="#baaibge-large-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-09-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-large-zh"><a href="https://huggingface.co/BAAI/bge-large-zh"><code>BAAI/bge-large-zh</code></a><a class="headerlink" href="#baaibge-large-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-08-02</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-large-zh-v15"><a href="https://huggingface.co/BAAI/bge-large-zh-v1.5"><code>BAAI/bge-large-zh-v1.5</code></a><a class="headerlink" href="#baaibge-large-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-09-12</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-small-en"><a href="https://huggingface.co/BAAI/bge-small-en"><code>BAAI/bge-small-en</code></a><a class="headerlink" href="#baaibge-small-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-08-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-small-en-v15"><a href="https://huggingface.co/BAAI/bge-small-en-v1.5"><code>BAAI/bge-small-en-v1.5</code></a><a class="headerlink" href="#baaibge-small-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-09-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-small-zh"><a href="https://huggingface.co/BAAI/bge-small-zh"><code>BAAI/bge-small-zh</code></a><a class="headerlink" href="#baaibge-small-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2023-08-05</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-small-zh-v15"><a href="https://huggingface.co/BAAI/bge-small-zh-v1.5"><code>BAAI/bge-small-zh-v1.5</code></a><a class="headerlink" href="#baaibge-small-zh-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>33.4M</td>
<td>91.0 MB</td>
<td>2023-09-12</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-1b"><a href="https://huggingface.co/BMRetriever/BMRetriever-1B"><code>BMRetriever/BMRetriever-1B</code></a><a class="headerlink" href="#bmretrieverbmretriever-1b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>908.8M</td>
<td>3.4 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-2b"><a href="https://huggingface.co/BMRetriever/BMRetriever-2B"><code>BMRetriever/BMRetriever-2B</code></a><a class="headerlink" href="#bmretrieverbmretriever-2b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.5B</td>
<td>9.3 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-410m"><a href="https://huggingface.co/BMRetriever/BMRetriever-410M"><code>BMRetriever/BMRetriever-410M</code></a><a class="headerlink" href="#bmretrieverbmretriever-410m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>353.8M</td>
<td>1.3 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bmretrieverbmretriever-7b"><a href="https://huggingface.co/BMRetriever/BMRetriever-7B"><code>BMRetriever/BMRetriever-7B</code></a><a class="headerlink" href="#bmretrieverbmretriever-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-bmretriever</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{BMR}etriever: Tuning Large Language Models as Better Biomedical Text Retrievers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Xu, Ran and Shi, Wenqi and Yu, Yue and Zhuang, Yuchen and Zhu, Yanqiao and Wang, May Dongmei and Ho, Joyce C. and Zhang, Chao and Yang, Carl&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;November&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;22234--22254&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.emnlp-main.1241/&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="beastyze5-r-mistral-7b"><a href="https://huggingface.co/BeastyZ/e5-R-mistral-7b"><code>BeastyZ/e5-R-mistral-7b</code></a><a class="headerlink" href="#beastyze5-r-mistral-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>27.0 GB</td>
<td>2024-06-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bytedance-seedseed15-embedding"><a href="https://seed1-5-embedding.github.io/"><code>ByteDance-Seed/Seed1.5-Embedding</code></a><a class="headerlink" href="#bytedance-seedseed15-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-04-25</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="bytedanceseed16-embedding"><a href="https://seed1-6-embedding.github.io/"><code>Bytedance/Seed1.6-embedding</code></a><a class="headerlink" href="#bytedanceseed16-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-06-18</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="bytedanceseed16-embedding-1215"><a href="https://console.volcengine.com/ark/region:ark+cn-beijing/model/detail?Id=doubao-embedding-vision"><code>Bytedance/Seed1.6-embedding-1215</code></a><a class="headerlink" href="#bytedanceseed16-embedding-1215" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-12-15</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-english-light-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-english-light-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-english-light-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-english-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-english-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-english-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-multilingual-light-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-multilingual-light-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-multilingual-light-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="coherecohere-embed-multilingual-v30"><a href="https://cohere.com/blog/introducing-embed-v3"><code>Cohere/Cohere-embed-multilingual-v3.0</code></a><a class="headerlink" href="#coherecohere-embed-multilingual-v30" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="geogpt-research-projectgeoembedding"><a href="https://huggingface.co/GeoGPT-Research-Project/GeoEmbedding"><code>GeoGPT-Research-Project/GeoEmbedding</code></a><a class="headerlink" href="#geogpt-research-projectgeoembedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>27.0 GB</td>
<td>2025-04-22</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="gritlmgritlm-7b"><a href="https://huggingface.co/GritLM/GritLM-7B"><code>GritLM/GritLM-7B</code></a><a class="headerlink" href="#gritlmgritlm-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>13.5 GB</td>
<td>2024-02-15</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">muennighoff2024generative</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Generative Representational Instruction Tuning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.09906}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="gritlmgritlm-8x7b"><a href="https://huggingface.co/GritLM/GritLM-8x7B"><code>GritLM/GritLM-8x7B</code></a><a class="headerlink" href="#gritlmgritlm-8x7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>32768</td>
<td>57.9B</td>
<td>87.0 GB</td>
<td>2024-02-15</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">muennighoff2024generative</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Generative Representational Instruction Tuning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.09906}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v1"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-10-23</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v15"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-12-26</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-instruct-v2"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2"><code>HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v2</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-instruct-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>942.0 MB</td>
<td>2025-06-25</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ict-time-and-queritboom_4b_v1"><a href="https://huggingface.co/ICT-TIME-and-Querit/BOOM_4B_v1"><code>ICT-TIME-and-Querit/BOOM_4B_v1</code></a><a class="headerlink" href="#ict-time-and-queritboom_4b_v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2026-01-31</td>
<td>ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (18)</td>
</tr>
</tbody>
</table>
<h4 id="ieityuanyuan-embedding-20-en"><a href="https://huggingface.co/IEITYuan/Yuan-embedding-2.0-en"><code>IEITYuan/Yuan-embedding-2.0-en</code></a><a class="headerlink" href="#ieityuanyuan-embedding-20-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>595.8M</td>
<td>2.2 GB</td>
<td>2025-11-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25"><a href="https://huggingface.co/KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5"><code>KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5</code></a><a class="headerlink" href="#kalm-embeddingkalm-embedding-multilingual-mini-instruct-v25" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2025-09-30</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhao2025kalmembeddingv2</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinping Zhao and Xinshuo Hu and Zifei Shan and Shouzheng Huang and Yao Zhou and Xin Zhang and Zetian Sun and Zhenyu Liu and Dongfang Li and Xinyuan Wei and Youcheng Pan and Yang Xiang and Meishan Zhang and Haofen Wang and Jun Yu and Baotian Hu and Min Zhang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2506.20923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2506.20923}</span><span class="p">,</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kingsoft-llmqzhou-embedding"><a href="https://huggingface.co/Kingsoft-LLM/QZhou-Embedding"><code>Kingsoft-LLM/QZhou-Embedding</code></a><a class="headerlink" href="#kingsoft-llmqzhou-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3584</td>
<td>7.1B</td>
<td>14.1 GB</td>
<td>2025-08-24</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">yu2025qzhouembeddingtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{QZhou-Embedding Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.21632}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.21632}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kingsoft-llmqzhou-embedding-zh"><a href="http://huggingface.co/Kingsoft-LLM/QZhou-Embedding-Zh"><code>Kingsoft-LLM/QZhou-Embedding-Zh</code></a><a class="headerlink" href="#kingsoft-llmqzhou-embedding-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1792</td>
<td>7.6B</td>
<td>28.7 GB</td>
<td>2025-09-28</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">yu2025qzhouembeddingtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{QZhou-Embedding Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Peng Yu and En Xu and Bin Chen and Haibiao Chen and Yinfei Xu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.21632}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.21632}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="linq-ai-researchlinq-embed-mistral"><a href="https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral"><code>Linq-AI-Research/Linq-Embed-Mistral</code></a><a class="headerlink" href="#linq-ai-researchlinq-embed-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-05-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">LinqAIResearch2024</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Linq-Embed-Mistral:Elevating Text Retrieval with Improved GPT Data Through Task-Specific Control and Quality Refinement}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Junseong Kim and Seolhwa Lee and Jihoon Kwon and Sangmo Gu and Yejin Kim and Minkyung Cho and Jy-yong Sohn and Chanyeol Choi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="p">=</span><span class="s">{Linq AI Research Blog}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://getlinq.com/blog/linq-embed-mistral/}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="maniaclabsminiac-embed"><a href="https://huggingface.co/ManiacLabs/miniac-embed"><code>ManiacLabs/miniac-embed</code></a><a class="headerlink" href="#maniaclabsminiac-embed" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2026-02-19</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised"><code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>6.6B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-llama-2-7b-chat-hf-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>6.6B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised"><code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-meta-llama-3-8b-instruct-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised"><code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-mistral-7b-instruct-v2-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.2B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-sheared-llama-mntp-supervised"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised"><code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-supervised</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-sheared-llama-mntp-supervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>1.3B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse"><a href="https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse"><code>McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse</code></a><a class="headerlink" href="#mcgill-nlpllm2vec-sheared-llama-mntp-unsup-simcse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>1.3B</td>
<td>26.5 GB</td>
<td>2024-04-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">behnamghader2024llm2veclargelanguagemodels</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2404.05961}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2404.05961}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mongodbmdbr-leaf-ir"><a href="https://huggingface.co/MongoDB/mdbr-leaf-ir"><code>MongoDB/mdbr-leaf-ir</code></a><a class="headerlink" href="#mongodbmdbr-leaf-ir" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>22.7M</td>
<td>86.0 MB</td>
<td>2025-08-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">mdbr_leaf</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Robin Vujanic and Thomas Rueckstiess}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.12539}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.12539}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mongodbmdbr-leaf-mt"><a href="https://huggingface.co/MongoDB/mdbr-leaf-mt"><code>MongoDB/mdbr-leaf-mt</code></a><a class="headerlink" href="#mongodbmdbr-leaf-mt" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>22.7M</td>
<td>86.0 MB</td>
<td>2025-08-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">mdbr_leaf</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{LEAF: Knowledge Distillation of Text Embedding Models with Teacher-Aligned Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Robin Vujanic and Thomas Rueckstiess}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.12539}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.12539}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchjasper_en_vision_language_v1"><a href="https://huggingface.co/infgrad/jasper_en_vision_language_v1"><code>NovaSearch/jasper_en_vision_language_v1</code></a><a class="headerlink" href="#novasearchjasper_en_vision_language_v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>8960</td>
<td>1.6B</td>
<td>3.7 GB</td>
<td>2024-12-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchstella_en_15b_v5"><a href="https://huggingface.co/NovaSearch/stella_en_1.5B_v5"><code>NovaSearch/stella_en_1.5B_v5</code></a><a class="headerlink" href="#novasearchstella_en_15b_v5" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>8960</td>
<td>1.5B</td>
<td>5.7 GB</td>
<td>2024-07-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="novasearchstella_en_400m_v5"><a href="https://huggingface.co/NovaSearch/stella_en_400M_v5"><code>NovaSearch/stella_en_400M_v5</code></a><a class="headerlink" href="#novasearchstella_en_400m_v5" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2024-07-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jasperstelladistillationsota</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper and Stella: distillation of SOTA embedding models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Jiacheng Li and Ziyang Zeng and Fulong Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.19048}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.19048}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-06b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-0.6B"><code>Qwen/Qwen3-Embedding-0.6B</code></a><a class="headerlink" href="#qwenqwen3-embedding-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1024</td>
<td>595.8M</td>
<td>1.1 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-4b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-4B"><code>Qwen/Qwen3-Embedding-4B</code></a><a class="headerlink" href="#qwenqwen3-embedding-4b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="qwenqwen3-embedding-8b"><a href="https://huggingface.co/Qwen/Qwen3-Embedding-8B"><code>Qwen/Qwen3-Embedding-8B</code></a><a class="headerlink" href="#qwenqwen3-embedding-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.6B</td>
<td>14.1 GB</td>
<td>2025-06-05</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">qwen3embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2506.05176}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="reasonirreasonir-8b"><a href="https://huggingface.co/ReasonIR/ReasonIR-8B"><code>ReasonIR/ReasonIR-8B</code></a><a class="headerlink" href="#reasonirreasonir-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>4096</td>
<td>7.5B</td>
<td>not specified</td>
<td>2025-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">shao2025reasonir</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{ReasonIR: Training Retrievers for Reasoning Tasks}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rulin Shao and Rui Qiao and Varsha Kishore and Niklas Muennighoff and Xi Victoria Lin and Daniela Rus and Bryan Kian Hsiang Low and Sewon Min and Wen-tau Yih and Pang Wei Koh and Luke Zettlemoyer}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2504.20595}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2504.20595}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sailesh97hinvec"><a href="https://huggingface.co/Sailesh97/Hinvec"><code>Sailesh97/Hinvec</code></a><a class="headerlink" href="#sailesh97hinvec" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>939.6M</td>
<td>3.6 GB</td>
<td>2025-06-19</td>
<td>eng-Latn, hin-Deva</td>
</tr>
</tbody>
</table>
<h4 id="salesforcesfr-embedding-2_r"><a href="https://huggingface.co/Salesforce/SFR-Embedding-2_R"><code>Salesforce/SFR-Embedding-2_R</code></a><a class="headerlink" href="#salesforcesfr-embedding-2_r" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-06-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">SFR-embedding-2</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rui Meng*, Ye Liu*, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/Salesforce/SFR-Embedding-2_R}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="salesforcesfr-embedding-code-2b_r"><a href="https://huggingface.co/Salesforce/SFR-Embedding-Code-2B_R"><code>Salesforce/SFR-Embedding-Code-2B_R</code></a><a class="headerlink" href="#salesforcesfr-embedding-code-2b_r" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2304</td>
<td>2.6B</td>
<td>4.9 GB</td>
<td>2025-01-17</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2024codexembed</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{CodeXEmbed: A Generalist Embedding Model Family for Multiligual and Multi-task Code Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Liu, Ye and Meng, Rui and Jot, Shafiq and Savarese, Silvio and Xiong, Caiming and Zhou, Yingbo and Yavuz, Semih}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2411.12644}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="salesforcesfr-embedding-mistral"><a href="https://huggingface.co/Salesforce/SFR-Embedding-Mistral"><code>Salesforce/SFR-Embedding-Mistral</code></a><a class="headerlink" href="#salesforcesfr-embedding-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-01-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">SFRAIResearch2024</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, Semih Yavuz}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="p">=</span><span class="s">{Salesforce AI Research Blog}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.salesforce.com/blog/sfr-embedding/}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samilpwc-axnode-genaipwc-embedding_expr"><a href="https://huggingface.co/SamilPwC-AXNode-GenAI/PwC-Embedding_expr"><code>SamilPwC-AXNode-GenAI/PwC-Embedding_expr</code></a><a class="headerlink" href="#samilpwc-axnode-genaipwc-embedding_expr" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2025-08-12</td>
<td>kor-Hang</td>
</tr>
</tbody>
</table>
<h4 id="snowflakesnowflake-arctic-embed-l"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l"><code>Snowflake/snowflake-arctic-embed-l</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-l" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>1.2 GB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-l-v20"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0"><code>Snowflake/snowflake-arctic-embed-l-v2.0</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-l-v20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-12-04</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2024arctic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Arctic-Embed 2.0: Multilingual Retrieval Without Compromise}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.04506}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m"><code>Snowflake/snowflake-arctic-embed-m</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>415.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-long"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long"><code>Snowflake/snowflake-arctic-embed-m-long</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-long" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>137.0M</td>
<td>522.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-v15"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v1.5"><code>Snowflake/snowflake-arctic-embed-m-v1.5</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>415.0 MB</td>
<td>2024-07-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-m-v20"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-m-v2.0"><code>Snowflake/snowflake-arctic-embed-m-v2.0</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-m-v20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>305.0M</td>
<td>1.1 GB</td>
<td>2024-12-04</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">yu2024arctic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Arctic-Embed 2.0: Multilingual Retrieval Without Compromise}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.04506}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-s"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-s"><code>Snowflake/snowflake-arctic-embed-s</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-s" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-04-12</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="snowflakesnowflake-arctic-embed-xs"><a href="https://huggingface.co/Snowflake/snowflake-arctic-embed-xs"><code>Snowflake/snowflake-arctic-embed-xs</code></a><a class="headerlink" href="#snowflakesnowflake-arctic-embed-xs" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>86.0 MB</td>
<td>2024-07-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">merrick2024embedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Embedding And Clustering Your Data Can Improve Contrastive Pretraining}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Merrick, Luke}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.18887}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.18887}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tarka-airtarka-embedding-150m-v1"><a href="https://huggingface.co/Tarka-AIR/Tarka-Embedding-150M-V1"><code>Tarka-AIR/Tarka-Embedding-150M-V1</code></a><a class="headerlink" href="#tarka-airtarka-embedding-150m-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> gemma</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>155.7M</td>
<td>576.0 MB</td>
<td>2025-11-04</td>
<td>arb-Arab, deu-Latn, eng-Latn, fra-Latn, jpn-Jpan, ... (8)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">tarka_ai_research_2025</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{ Tarka AI Research }</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{ Tarka-Embedding-150M-V1 (Revision c5f4f43) }</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ https://huggingface.co/Tarka-AIR/Tarka-Embedding-150M-V1 }</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ 10.57967/hf/6875 }</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{ Hugging Face }</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tarka-airtarka-embedding-350m-v1"><a href="https://huggingface.co/Tarka-AIR/Tarka-Embedding-350M-V1"><code>Tarka-AIR/Tarka-Embedding-350M-V1</code></a><a class="headerlink" href="#tarka-airtarka-embedding-350m-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>128.0K</td>
<td>1024</td>
<td>354.5M</td>
<td>676.0 MB</td>
<td>2025-11-11</td>
<td>arb-Arab, deu-Latn, eng-Latn, fra-Latn, jpn-Jpan, ... (8)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">tarka_ai_research_2025</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{ Tarka AI Research }</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{ Tarka-Embedding-350M-V1 (Revision f4b5de8) }</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ https://huggingface.co/Tarka-AIR/Tarka-Embedding-350M-V1 }</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ 10.57967/hf/6979 }</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{ Hugging Face }</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tencentbacconan-embedding-v2"><a href="https://huggingface.co/TencentBAC/Conan-embedding-v2"><code>TencentBAC/Conan-embedding-v2</code></a><a class="headerlink" href="#tencentbacconan-embedding-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-04-10</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="vplabssearchmap_preview"><a href="https://huggingface.co/VPLabs/SearchMap_Preview"><code>VPLabs/SearchMap_Preview</code></a><a class="headerlink" href="#vplabssearchmap_preview" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2025-03-05</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vectorpath2025searchmap</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{SearchMap: Conversational E-commerce Search Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{VectorPath Research Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{HuggingFace Model Hub}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="whereisaiuae-large-v1"><a href="https://huggingface.co/WhereIsAI/UAE-Large-V1"><code>WhereIsAI/UAE-Large-V1</code></a><a class="headerlink" href="#whereisaiuae-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>1.2 GB</td>
<td>2023-12-04</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023angle</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{AnglE-optimized Text Embeddings}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Xianming and Li, Jing}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2309.12871}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ai-foreverfrida"><a href="https://huggingface.co/ai-forever/FRIDA"><code>ai-forever/FRIDA</code></a><a class="headerlink" href="#ai-foreverfrida" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>823.0M</td>
<td>3.1 GB</td>
<td>2024-12-29</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="ai-foreverru-en-rosberta"><a href="https://huggingface.co/ai-forever/ru-en-RoSBERTa"><code>ai-forever/ru-en-RoSBERTa</code></a><a class="headerlink" href="#ai-foreverru-en-rosberta" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>404.0M</td>
<td>1.5 GB</td>
<td>2024-07-29</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">snegirev2024russianfocusedembeddersexplorationrumteb</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{The Russian-focused embedders&#39; exploration: ruMTEB benchmark and Russian embedding model design}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Artem Snegirev and Maria Tikhonova and Anna Maksimova and Alena Fenogenova and Alexander Abramov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2408.12503}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2408.12503}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ai-sagegiga-embeddings-instruct"><a href="https://huggingface.co/ai-sage/Giga-Embeddings-instruct"><code>ai-sage/Giga-Embeddings-instruct</code></a><a class="headerlink" href="#ai-sagegiga-embeddings-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>2048</td>
<td>3.2B</td>
<td>12.6 GB</td>
<td>2025-09-23</td>
<td>eng-Latn, rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="annamodelslgai-embedding-preview"><a href="https://huggingface.co/annamodels/LGAI-Embedding-Preview"><code>annamodels/LGAI-Embedding-Preview</code></a><a class="headerlink" href="#annamodelslgai-embedding-preview" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-06-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">choi2025lgaiembeddingpreviewtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{LGAI-EMBEDDING-Preview Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jooyoung Choi and Hyun Kim and Hansol Jang and Changwook Jun and Kyunghoon Bae and Hyewon Choi and Stanley Jungkyu Choi and Honglak Lee and Chulmin Yun}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2506.07438}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2506.07438}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bedrockcohere-embed-english-v3"><a href="https://cohere.com/blog/introducing-embed-v3"><code>bedrock/cohere-embed-english-v3</code></a><a class="headerlink" href="#bedrockcohere-embed-english-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="bedrockcohere-embed-multilingual-v3"><a href="https://cohere.com/blog/introducing-embed-v3"><code>bedrock/cohere-embed-multilingual-v3</code></a><a class="headerlink" href="#bedrockcohere-embed-multilingual-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-11-02</td>
<td>afr-Latn, amh-Ethi, ara-Arab, asm-Beng, aze-Latn, ... (111)</td>
</tr>
</tbody>
</table>
<h4 id="bflhcmod-embedding"><a href="https://huggingface.co/bflhc/MoD-Embedding"><code>bflhc/MoD-Embedding</code></a><a class="headerlink" href="#bflhcmod-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2025-12-14</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">mod-embedding-2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{MoD-Embedding: A Fine-tuned Multilingual Text Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{MoD Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/bflhc/MoD-Embedding}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bflhcocten-embedding-06b"><a href="https://huggingface.co/bflhc/Octen-Embedding-0.6B"><code>bflhc/Octen-Embedding-0.6B</code></a><a class="headerlink" href="#bflhcocten-embedding-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1024</td>
<td>595.8M</td>
<td>1.1 GB</td>
<td>2026-01-10</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">octen-embedding-2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Octen Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bflhcocten-embedding-4b"><a href="https://huggingface.co/bflhc/Octen-Embedding-4B"><code>bflhc/Octen-Embedding-4B</code></a><a class="headerlink" href="#bflhcocten-embedding-4b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2025-12-30</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">octen-embedding-2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Octen Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bflhcocten-embedding-8b"><a href="https://huggingface.co/bflhc/Octen-Embedding-8B"><code>bflhc/Octen-Embedding-8B</code></a><a class="headerlink" href="#bflhcocten-embedding-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.6B</td>
<td>14.1 GB</td>
<td>2025-12-23</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">octen-embedding-2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Octen-Embedding-8B: A Fine-tuned Multilingual Text Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Octen Team}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/bflhc/bflhc/Octen-Embedding-8B}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinirepllama-v1-7b-lora-passage"><a href="https://huggingface.co/castorini/repllama-v1-7b-lora-passage"><code>castorini/repllama-v1-7b-lora-passage</code></a><a class="headerlink" href="#castorinirepllama-v1-7b-lora-passage" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>27.0 MB</td>
<td>2023-10-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rankllama</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fine-Tuning LLaMA for Multi-Stage Text Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv:2310.08319}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-base"><a href="https://huggingface.co/cl-nagoya/ruri-base"><code>cl-nagoya/ruri-base</code></a><a class="headerlink" href="#cl-nagoyaruri-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>111.2M</td>
<td>212.0 MB</td>
<td>2024-08-28</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-base-v2"><a href="https://huggingface.co/cl-nagoya/ruri-base-v2"><code>cl-nagoya/ruri-base-v2</code></a><a class="headerlink" href="#cl-nagoyaruri-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>111.2M</td>
<td>424.0 MB</td>
<td>2024-12-05</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-large"><a href="https://huggingface.co/cl-nagoya/ruri-large"><code>cl-nagoya/ruri-large</code></a><a class="headerlink" href="#cl-nagoyaruri-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>337.4M</td>
<td>644.0 MB</td>
<td>2024-08-28</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-large-v2"><a href="https://huggingface.co/cl-nagoya/ruri-large-v2"><code>cl-nagoya/ruri-large-v2</code></a><a class="headerlink" href="#cl-nagoyaruri-large-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>337.4M</td>
<td>1.3 GB</td>
<td>2024-12-06</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-small"><a href="https://huggingface.co/cl-nagoya/ruri-small"><code>cl-nagoya/ruri-small</code></a><a class="headerlink" href="#cl-nagoyaruri-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>68.1M</td>
<td>130.0 MB</td>
<td>2024-08-28</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-small-v2"><a href="https://huggingface.co/cl-nagoya/ruri-small-v2"><code>cl-nagoya/ruri-small-v2</code></a><a class="headerlink" href="#cl-nagoyaruri-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>68.1M</td>
<td>260.0 MB</td>
<td>2024-12-05</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-v3-130m"><a href="https://huggingface.co/cl-nagoya/ruri-v3-130m"><code>cl-nagoya/ruri-v3-130m</code></a><a class="headerlink" href="#cl-nagoyaruri-v3-130m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>132.1M</td>
<td>504.0 MB</td>
<td>2025-04-09</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-v3-30m"><a href="https://huggingface.co/cl-nagoya/ruri-v3-30m"><code>cl-nagoya/ruri-v3-30m</code></a><a class="headerlink" href="#cl-nagoyaruri-v3-30m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>256</td>
<td>36.7M</td>
<td>140.0 MB</td>
<td>2025-04-07</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-v3-310m"><a href="https://huggingface.co/cl-nagoya/ruri-v3-310m"><code>cl-nagoya/ruri-v3-310m</code></a><a class="headerlink" href="#cl-nagoyaruri-v3-310m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>314.6M</td>
<td>1.2 GB</td>
<td>2025-04-09</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cl-nagoyaruri-v3-70m"><a href="https://huggingface.co/cl-nagoya/ruri-v3-70m"><code>cl-nagoya/ruri-v3-70m</code></a><a class="headerlink" href="#cl-nagoyaruri-v3-70m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>256</td>
<td>70.0M</td>
<td>140.0 MB</td>
<td>2025-04-09</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Ruri</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{Ruri: Japanese General Text Embeddings}}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Hayato Tsukagoshi and Ryohei Sasano}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.07737}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.07737}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="clipse5-base-trm-nl"><a href="https://huggingface.co/clips/e5-base-trm-nl"><code>clips/e5-base-trm-nl</code></a><a class="headerlink" href="#clipse5-base-trm-nl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>124.4M</td>
<td>237.0 MB</td>
<td>2025-09-23</td>
<td>nld-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">banar2025mtebnle5nlembeddingbenchmark</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="clipse5-large-trm-nl"><a href="https://huggingface.co/clips/e5-large-trm-nl"><code>clips/e5-large-trm-nl</code></a><a class="headerlink" href="#clipse5-large-trm-nl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>355.0M</td>
<td>1.3 GB</td>
<td>2025-09-23</td>
<td>nld-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">banar2025mtebnle5nlembeddingbenchmark</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="clipse5-small-trm-nl"><a href="https://huggingface.co/clips/e5-small-trm-nl"><code>clips/e5-small-trm-nl</code></a><a class="headerlink" href="#clipse5-small-trm-nl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>40.8M</td>
<td>78.0 MB</td>
<td>2025-09-23</td>
<td>nld-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">banar2025mtebnle5nlembeddingbenchmark</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Nikolay Banar and Ehsan Lotfi and Jens Van Nooten and Cristina Arhiliuc and Marija Kliocaite and Walter Daelemans}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2509.12340}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codefuse-aic2llm-05b"><a href="https://huggingface.co/codefuse-ai/C2LLM-0.5B"><code>codefuse-ai/C2LLM-0.5B</code></a><a class="headerlink" href="#codefuse-aic2llm-05b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>896</td>
<td>497.3M</td>
<td>948.0 MB</td>
<td>2025-12-22</td>
<td>eng-Latn, go-Code, java-Code, javascript-Code, php-Code, ... (8)</td>
</tr>
</tbody>
</table>
<h4 id="codefuse-aic2llm-7b"><a href="https://huggingface.co/codefuse-ai/C2LLM-7B"><code>codefuse-ai/C2LLM-7B</code></a><a class="headerlink" href="#codefuse-aic2llm-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.7B</td>
<td>14.3 GB</td>
<td>2025-12-22</td>
<td>eng-Latn, go-Code, java-Code, javascript-Code, php-Code, ... (8)</td>
</tr>
</tbody>
</table>
<h4 id="codefuse-aif2llm-06b"><a href="https://huggingface.co/codefuse-ai/F2LLM-0.6B"><code>codefuse-ai/F2LLM-0.6B</code></a><a class="headerlink" href="#codefuse-aif2llm-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>596.0M</td>
<td>1.1 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">2025F2LLM</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="p">=</span><span class="s">{abs/2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://doi.org/10.48550/arXiv.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="p">=</span><span class="s">{10.48550/ARXIV.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprinttype</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2510.02294}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codefuse-aif2llm-17b"><a href="https://huggingface.co/codefuse-ai/F2LLM-1.7B"><code>codefuse-ai/F2LLM-1.7B</code></a><a class="headerlink" href="#codefuse-aif2llm-17b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2560</td>
<td>1.7B</td>
<td>3.2 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">2025F2LLM</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="p">=</span><span class="s">{abs/2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://doi.org/10.48550/arXiv.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="p">=</span><span class="s">{10.48550/ARXIV.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprinttype</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2510.02294}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codefuse-aif2llm-4b"><a href="https://huggingface.co/codefuse-ai/F2LLM-4B"><code>codefuse-ai/F2LLM-4B</code></a><a class="headerlink" href="#codefuse-aif2llm-4b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2560</td>
<td>4.0B</td>
<td>7.5 GB</td>
<td>2025-09-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">2025F2LLM</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Ziyin Zhang and Zihan Liao and Hang Yu and Peng Di and Rui Wang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="p">=</span><span class="s">{abs/2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://doi.org/10.48550/arXiv.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="p">=</span><span class="s">{10.48550/ARXIV.2510.02294}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprinttype</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2510.02294}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deepvkuser-base"><a href="https://huggingface.co/deepvk/USER-base"><code>deepvk/USER-base</code></a><a class="headerlink" href="#deepvkuser-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>427.0M</td>
<td>473.0 MB</td>
<td>2024-06-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deepvk2024user</span><span class="p">,</span>
<span class="w">        </span><span class="na">title</span><span class="p">=</span><span class="s">{USER: Universal Sentence Encoder for Russian}</span><span class="p">,</span>
<span class="w">        </span><span class="na">author</span><span class="p">=</span><span class="s">{Malashenko, Boris and  Zemerov, Anton and Spirin, Egor}</span><span class="p">,</span>
<span class="w">        </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/datasets/deepvk/USER-base}</span><span class="p">,</span>
<span class="w">        </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span>
<span class="w">        </span><span class="nv">year</span><span class="err">={2024</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">}</span>
</code></pre></div>
</details>
<h4 id="deepvkuser2-base"><a href="https://huggingface.co/collections/deepvk/user2-6802650d7210f222ec60e05f"><code>deepvk/USER2-base</code></a><a class="headerlink" href="#deepvkuser2-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2025-04-19</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deepvk2025user</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{USER2}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Malashenko, Boris and Spirin, Egor and Sokolov Andrey}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/deepvk/USER2-base}</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deepvkuser2-small"><a href="https://huggingface.co/collections/deepvk/user2-6802650d7210f222ec60e05f"><code>deepvk/USER2-small</code></a><a class="headerlink" href="#deepvkuser2-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>384</td>
<td>34.4M</td>
<td>131.0 MB</td>
<td>2025-04-19</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deepvk2025user</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{USER2}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Malashenko, Boris and Spirin, Egor and Sokolov Andrey}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/deepvk/USER2-small}</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="emillykkejensenembeddinggemma-scandi-300m"><a href="https://huggingface.co/emillykkejensen/EmbeddingGemma-Scandi-300m"><code>emillykkejensen/EmbeddingGemma-Scandi-300m</code></a><a class="headerlink" href="#emillykkejensenembeddinggemma-scandi-300m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>307.6M</td>
<td>578.0 MB</td>
<td>2025-10-17</td>
<td>dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="emillykkejensenqwen3-embedding-scandi-06b"><a href="https://huggingface.co/emillykkejensen/Qwen3-Embedding-Scandi-0.6B"><code>emillykkejensen/Qwen3-Embedding-Scandi-0.6B</code></a><a class="headerlink" href="#emillykkejensenqwen3-embedding-scandi-06b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1024</td>
<td>595.8M</td>
<td>2.2 GB</td>
<td>2025-10-17</td>
<td>dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<h4 id="emillykkejensenmmbertscandi-base-embedding"><a href="https://huggingface.co/emillykkejensen/Qwen3-Embedding-Scandi-0.6B"><code>emillykkejensen/mmBERTscandi-base-embedding</code></a><a class="headerlink" href="#emillykkejensenmmbertscandi-base-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>306.9M</td>
<td>1.1 GB</td>
<td>2025-10-17</td>
<td>dan-Latn, nno-Latn, nob-Latn, nor-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="fyaronskiyenglish_code_retriever"><a href="https://huggingface.co/fyaronskiy/english_code_retriever"><code>fyaronskiy/english_code_retriever</code></a><a class="headerlink" href="#fyaronskiyenglish_code_retriever" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2025-07-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googleembeddinggemma-300m"><a href="https://ai.google.dev/gemma/docs/embeddinggemma/model_card"><code>google/embeddinggemma-300m</code></a><a class="headerlink" href="#googleembeddinggemma-300m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> gemma</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>307.6M</td>
<td>1.1 GB</td>
<td>2025-09-04</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vera2025embeddinggemmapowerfullightweighttext</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{EmbeddingGemma: Powerful and Lightweight Text Representations}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Henrique Schechter Vera and Sahil Dua and Biao Zhang and Daniel Salz and Ryan Mullins and Sindhu Raghuram Panyam and Sara Smoot and Iftekhar Naim and Joe Zou and Feiyang Chen and Daniel Cer and Alice Lisak and Min Choi and Lucas Gonzalez and Omar Sanseviero and Glenn Cameron and Ian Ballantyne and Kat Black and Kaifeng Chen and Weiyi Wang and Zhe Li and Gus Martins and Jinhyuk Lee and Mark Sherwood and Juyeong Ji and Renjie Wu and Jingxiao Zheng and Jyotinder Singh and Abheesht Sharma and Divyashree Sreepathihalli and Aashi Jain and Adham Elarabawy and AJ Co and Andreas Doumanoglou and Babak Samari and Ben Hora and Brian Potetz and Dahun Kim and Enrique Alfonseca and Fedor Moiseev and Feng Han and Frank Palma Gomez and Gustavo Hernndez brego and Hesen Zhang and Hui Hui and Jay Han and Karan Gill and Ke Chen and Koert Chen and Madhuri Shanbhogue and Michael Boratko and Paul Suganthan and Sai Meher Karthik Duddu and Sandeep Mariserla and Setareh Ariafar and Shanfeng Zhang and Shijie Zhang and Simon Baumgartner and Sonam Goenka and Steve Qiu and Tanmaya Dabral and Trevor Walker and Vikram Rao and Waleed Khawaja and Wenlei Zhou and Xiaoqi Ren and Ye Xia and Yichang Chen and Yi-Ting Chen and Zhe Dong and Zhongli Ding and Francesco Visin and Gal Liu and Jiageng Zhang and Kathleen Kenealy and Michelle Casbon and Ravin Kumar and Thomas Mesnard and Zach Gleicher and Cormac Brick and Olivier Lacombe and Adam Roberts and Qin Yin and Yunhsuan Sung and Raphael Hoffmann and Tris Warkentin and Armand Joulin and Tom Duerig and Mojtaba Seyedhosseini}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.20354}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.20354}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googlegemini-embedding-001"><a href="https://ai.google.dev/gemini-api/docs/embeddings"><code>google/gemini-embedding-001</code></a><a class="headerlink" href="#googlegemini-embedding-001" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>3072</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-03-07</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<h4 id="googletext-embedding-004"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-embedding-004</code></a><a class="headerlink" href="#googletext-embedding-004" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googletext-embedding-005"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-embedding-005</code></a><a class="headerlink" href="#googletext-embedding-005" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-11-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googletext-multilingual-embedding-002"><a href="https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings"><code>google/text-multilingual-embedding-002</code></a><a class="headerlink" href="#googletext-multilingual-embedding-002" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-14</td>
<td>arb-Arab, ben-Beng, deu-Latn, eng-Latn, fin-Latn, ... (19)</td>
</tr>
</tbody>
</table>
<h4 id="infgradjasper-token-compression-600m"><a href="https://huggingface.co/infgrad/Jasper-Token-Compression-600M"><code>infgrad/Jasper-Token-Compression-600M</code></a><a class="headerlink" href="#infgradjasper-token-compression-600m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>2048</td>
<td>607.3M</td>
<td>2.2 GB</td>
<td>2025-11-14</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025jaspertokencompression600mtechnicalreport</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jasper-Token-Compression-600M Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Dun Zhang and Ziyang Zeng and Yudong Zhou and Shuyang Lu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2511.14405}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2511.14405}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="inflyinf-retriever-v1"><a href="https://huggingface.co/infly/inf-retriever-v1"><code>infly/inf-retriever-v1</code></a><a class="headerlink" href="#inflyinf-retriever-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-12-24</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">infly-ai_2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{inf-retriever-v1 (Revision 5f469d7)}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/infly/inf-retriever-v1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.57967/hf/4262}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="inflyinf-retriever-v1-15b"><a href="https://huggingface.co/infly/inf-retriever-v1-1.5b"><code>infly/inf-retriever-v1-1.5b</code></a><a class="headerlink" href="#inflyinf-retriever-v1-15b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>1.5B</td>
<td>2.9 GB</td>
<td>2025-02-08</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">infly-ai_2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{Junhan Yang and Jiahe Wan and Yichen Yao and Wei Chu and Yinghui Xu and Yuan Qi}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{inf-retriever-v1 (Revision 5f469d7)}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2025</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/infly/inf-retriever-v1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.57967/hf/4262}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-base"><a href="https://huggingface.co/intfloat/e5-base"><code>intfloat/e5-base</code></a><a class="headerlink" href="#intfloate5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2022-12-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-base-v2"><a href="https://huggingface.co/intfloat/e5-base-v2"><code>intfloat/e5-base-v2</code></a><a class="headerlink" href="#intfloate5-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-large"><a href="https://huggingface.co/intfloat/e5-large"><code>intfloat/e5-large</code></a><a class="headerlink" href="#intfloate5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2022-12-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-large-v2"><a href="https://huggingface.co/intfloat/e5-large-v2"><code>intfloat/e5-large-v2</code></a><a class="headerlink" href="#intfloate5-large-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-mistral-7b-instruct"><a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct"><code>intfloat/e5-mistral-7b-instruct</code></a><a class="headerlink" href="#intfloate5-mistral-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-02-08</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023improving</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Improving Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2401.00368}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-small"><a href="https://huggingface.co/intfloat/e5-small"><code>intfloat/e5-small</code></a><a class="headerlink" href="#intfloate5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.0M</td>
<td>127.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloate5-small-v2"><a href="https://huggingface.co/intfloat/e5-small-v2"><code>intfloat/e5-small-v2</code></a><a class="headerlink" href="#intfloate5-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.0M</td>
<td>127.0 MB</td>
<td>2024-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2022text</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Text Embeddings by Weakly-Supervised Contrastive Pre-training}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2212.03533}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-base"><a href="https://huggingface.co/intfloat/multilingual-e5-base"><code>intfloat/multilingual-e5-base</code></a><a class="headerlink" href="#intfloatmultilingual-e5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-large"><a href="https://huggingface.co/intfloat/multilingual-e5-large"><code>intfloat/multilingual-e5-large</code></a><a class="headerlink" href="#intfloatmultilingual-e5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>560.0M</td>
<td>2.1 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-large-instruct"><a href="https://huggingface.co/intfloat/multilingual-e5-large-instruct"><code>intfloat/multilingual-e5-large-instruct</code></a><a class="headerlink" href="#intfloatmultilingual-e5-large-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>560.0M</td>
<td>1.0 GB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="intfloatmultilingual-e5-small"><a href="https://huggingface.co/intfloat/multilingual-e5-small"><code>intfloat/multilingual-e5-small</code></a><a class="headerlink" href="#intfloatmultilingual-e5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>118.0M</td>
<td>449.0 MB</td>
<td>2024-02-08</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2024multilingual</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Multilingual E5 Text Embeddings: A Technical Report}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.05672}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v3"><a href="https://huggingface.co/jinaai/jina-embeddings-v3"><code>jinaai/jina-embeddings-v3</code></a><a class="headerlink" href="#jinaaijina-embeddings-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>572.3M</td>
<td>1.1 GB</td>
<td>2024-09-18</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">sturua2024jinaembeddingsv3multilingualembeddingstask</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{jina-embeddings-v3: Multilingual Embeddings With Task LoRA}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Saba Sturua and Isabelle Mohr and Mohammad Kalim Akram and Michael Gnther and Bo Wang and Markus Krimmel and Feng Wang and Georgios Mastrapas and Andreas Koukounas and Andreas Koukounas and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.10173}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.10173}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v5-text-nano"><a href="https://huggingface.co/jinaai/jina-embeddings-v5-text-nano"><code>jinaai/jina-embeddings-v5-text-nano</code></a><a class="headerlink" href="#jinaaijina-embeddings-v5-text-nano" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>211.8M</td>
<td>404.0 MB</td>
<td>2026-02-17</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">akram2026jinaembeddingsv5texttasktargetedembeddingdistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{jina-embeddings-v5-text: Task-Targeted Embedding Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Mohammad Kalim Akram and Saba Sturua and Nastia Havriushenko and Quentin Herreros and Michael Gnther and Maximilian Werk and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2602.15547}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2602.15547}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v5-text-small"><a href="https://huggingface.co/jinaai/jina-embeddings-v5-text-small"><code>jinaai/jina-embeddings-v5-text-small</code></a><a class="headerlink" href="#jinaaijina-embeddings-v5-text-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1024</td>
<td>596.0M</td>
<td>1.1 GB</td>
<td>2026-02-17</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">akram2026jinaembeddingsv5texttasktargetedembeddingdistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{jina-embeddings-v5-text: Task-Targeted Embedding Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Mohammad Kalim Akram and Saba Sturua and Nastia Havriushenko and Quentin Herreros and Michael Gnther and Maximilian Werk and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2602.15547}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2602.15547}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jxmcde-small-v1"><a href="https://huggingface.co/jxm/cde-small-v1"><code>jxm/cde-small-v1</code></a><a class="headerlink" href="#jxmcde-small-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>281.0M</td>
<td>1.0 GB</td>
<td>2024-09-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">morris2024contextualdocumentembeddings</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{Contextual Document Embeddings}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{John X. Morris and Alexander M. Rush}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2410.02525}</span><span class="p">,</span>
<span class="w">    </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2410.02525}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jxmcde-small-v2"><a href="https://huggingface.co/jxm/cde-small-v1"><code>jxm/cde-small-v2</code></a><a class="headerlink" href="#jxmcde-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>306.0M</td>
<td>1.1 GB</td>
<td>2025-01-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">morris2024contextualdocumentembeddings</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{Contextual Document Embeddings}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{John X. Morris and Alexander M. Rush}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2410.02525}</span><span class="p">,</span>
<span class="w">    </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2410.02525}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="llamaindexvdr-2b-multi-v1"><a href="https://huggingface.co/llamaindex/vdr-2b-multi-v1"><code>llamaindex/vdr-2b-multi-v1</code></a><a class="headerlink" href="#llamaindexvdr-2b-multi-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>2.2B</td>
<td>4.1 GB</td>
<td>2024-01-08</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, spa-Latn</td>
</tr>
</tbody>
</table>
<h4 id="manveertambercadet-embed-base-v1"><a href="https://huggingface.co/manveertamber/cadet-embed-base-v1"><code>manveertamber/cadet-embed-base-v1</code></a><a class="headerlink" href="#manveertambercadet-embed-base-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.0M</td>
<td>418.0 MB</td>
<td>2025-05-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">tamber2025conventionalcontrastivelearningfalls</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{Conventional Contrastive Learning Often Falls Short: Improving Dense Retrieval with Cross-Encoder Listwise Distillation and Synthetic Data}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Manveer Singh Tamber and Suleman Kazi and Vivek Sourabh and Jimmy Lin}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv:2505.19274}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-embed-2d-large-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1"><code>mixedbread-ai/mxbai-embed-2d-large-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-2d-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.1M</td>
<td>not specified</td>
<td>2024-03-04</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="mixedbread-aimxbai-embed-large-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1"><code>mixedbread-ai/mxbai-embed-large-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>639.0 MB</td>
<td>2024-03-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@online</span><span class="p">{</span><span class="nl">emb2024mxbai</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Open Source Strikes Bread - New Fluffy Embeddings Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sean Lee and Aamir Shakir and Darius Koenig and Julius Lipp}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-embed-large-v1}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023angle</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{AnglE-optimized Text Embeddings}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Xianming and Li, Jing}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2309.12871}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-embed-xsmall-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-embed-xsmall-v1"><code>mixedbread-ai/mxbai-embed-xsmall-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-embed-xsmall-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>24.1M</td>
<td>not specified</td>
<td>2024-08-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@online</span><span class="p">{</span><span class="nl">xsmall2024mxbai</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Every Byte Matters: Introducing mxbai-embed-xsmall-v1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sean Lee and Julius Lipp and Rui Huang and Darius Koenig}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-embed-xsmall-v1}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-aimodernbert-embed-base"><a href="https://huggingface.co/nomic-ai/modernbert-embed-base"><code>nomic-ai/modernbert-embed-base</code></a><a class="headerlink" href="#nomic-aimodernbert-embed-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>568.0 MB</td>
<td>2024-12-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2024nomic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Nomic Embed: Training a Reproducible Long Context Text Embedder}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.01613}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-ainomic-embed-code"><a href="https://huggingface.co/nomic-ai/nomic-embed-code"><code>nomic-ai/nomic-embed-code</code></a><a class="headerlink" href="#nomic-ainomic-embed-code" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.1B</td>
<td>26.3 GB</td>
<td>2025-03-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">suresh2025cornstackhighqualitycontrastivedata</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Tarun Suresh and Revanth Gangi Reddy and Yifei Xu and Zach Nussbaum and Andriy Mulyar and Brandon Duderstadt and Heng Ji}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.01007}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.01007}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-ainomic-embed-text-v1"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1"><code>nomic-ai/nomic-embed-text-v1</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>136.7M</td>
<td>522.0 MB</td>
<td>2024-01-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2024nomic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Nomic Embed: Training a Reproducible Long Context Text Embedder}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.01613}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-ainomic-embed-text-v1-ablated"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1-ablated"><code>nomic-ai/nomic-embed-text-v1-ablated</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1-ablated" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>136.7M</td>
<td>not specified</td>
<td>2024-01-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-ainomic-embed-text-v1-unsupervised"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised"><code>nomic-ai/nomic-embed-text-v1-unsupervised</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v1-unsupervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>136.7M</td>
<td>not specified</td>
<td>2024-01-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nomic-ainomic-embed-text-v15"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v1.5"><code>nomic-ai/nomic-embed-text-v1.5</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>136.7M</td>
<td>522.0 MB</td>
<td>2024-02-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2024nomic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Nomic Embed: Training a Reproducible Long Context Text Embedder}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.01613}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nomic-ainomic-embed-text-v2-moe"><a href="https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe"><code>nomic-ai/nomic-embed-text-v2-moe</code></a><a class="headerlink" href="#nomic-ainomic-embed-text-v2-moe" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>475.3M</td>
<td>1.8 GB</td>
<td>2025-02-07</td>
<td>amh-Ethi, arb-Arab, bel-Cyrl, ben-Beng, bul-Cyrl, ... (98)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nussbaum2025trainingsparsemixtureexperts</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Training Sparse Mixture Of Experts Text Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Zach Nussbaum and Brandon Duderstadt}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2502.07972}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2502.07972}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidianv-embed-v1"><a href="https://huggingface.co/nvidia/NV-Embed-v1"><code>nvidia/NV-Embed-v1</code></a><a class="headerlink" href="#nvidianv-embed-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.8B</td>
<td>14.6 GB</td>
<td>2024-09-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">lee2025nvembedimprovedtechniquestraining</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2405.17428}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2405.17428}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidianv-embed-v2"><a href="https://huggingface.co/nvidia/NV-Embed-v2"><code>nvidia/NV-Embed-v2</code></a><a class="headerlink" href="#nvidianv-embed-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.8B</td>
<td>14.6 GB</td>
<td>2024-09-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">lee2025nvembedimprovedtechniquestraining</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2405.17428}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2405.17428}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nvidiallama-embed-nemotron-8b"><a href="https://huggingface.co/nvidia/llama-embed-nemotron-8b"><code>nvidia/llama-embed-nemotron-8b</code></a><a class="headerlink" href="#nvidiallama-embed-nemotron-8b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/nvidia/llama-embed-nemotron-8b/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.5B</td>
<td>28.0 GB</td>
<td>2025-10-23</td>
<td>afr-Latn, amh-Ethi, ara-Arab, arq-Arab, ary-Arab, ... (66)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">babakhin2025llamaembednemotron8buniversaltextembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yauhen Babakhin and Radek Osmulski and Ronay Ak and Gabriel Moreira and Mengyao Xu and Benedikt Schifferer and Bo Liu and Even Oldridge}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2511.07025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2511.07025}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v1"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v1"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v1</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>133.0M</td>
<td>507.0 MB</td>
<td>2024-03-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-distill" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>66.4M</td>
<td>267.0 MB</td>
<td>2024-07-17</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v2-mini</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v2-mini" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>22.7M</td>
<td>86.0 MB</td>
<td>2024-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-distill</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-distill" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>30522</td>
<td>66.4M</td>
<td>267.0 MB</td>
<td>2025-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte"><a href="https://huggingface.co/opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte"><code>opensearch-project/opensearch-neural-sparse-encoding-doc-v3-gte</code></a><a class="headerlink" href="#opensearch-projectopensearch-neural-sparse-encoding-doc-v3-gte" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>30522</td>
<td>136.8M</td>
<td>549.0 MB</td>
<td>2025-06-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="samaya-airepllama-reproduced"><a href="https://huggingface.co/samaya-ai/RepLLaMA-reproduced"><code>samaya-ai/RepLLaMA-reproduced</code></a><a class="headerlink" href="#samaya-airepllama-reproduced" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>27.0 MB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">rankllama</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fine-Tuning LLaMA for Multi-Stage Text Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv:2310.08319}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama2-7b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama2-7b-v1"><code>samaya-ai/promptriever-llama2-7b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama2-7b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>26.1 GB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama31-8b-instruct-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama3.1-8b-instruct-v1"><code>samaya-ai/promptriever-llama3.1-8b-instruct-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama31-8b-instruct-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>8.0B</td>
<td>29.8 GB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-llama31-8b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-llama3.1-8b-v1"><code>samaya-ai/promptriever-llama3.1-8b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-llama31-8b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>4096</td>
<td>8.0B</td>
<td>29.8 GB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="samaya-aipromptriever-mistral-v01-7b-v1"><a href="https://huggingface.co/samaya-ai/promptriever-mistral-v0.1-7b-v1"><code>samaya-ai/promptriever-mistral-v0.1-7b-v1</code></a><a class="headerlink" href="#samaya-aipromptriever-mistral-v01-7b-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>4096</td>
<td>7.0B</td>
<td>26.1 GB</td>
<td>2024-09-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">weller2024promptriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Van Durme and Dawn Lawrie and Ashwin Paranjape and Yuhao Zhang and Jack Hessel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.11136}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.11136}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sbintuitionssarashina-embedding-v2-1b"><a href="https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b"><code>sbintuitions/sarashina-embedding-v2-1b</code></a><a class="headerlink" href="#sbintuitionssarashina-embedding-v2-1b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/sbintuitions/sarashina-embedding-v2-1b/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1792</td>
<td>1.2B</td>
<td>4.6 GB</td>
<td>2025-07-30</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhberta"><a href="https://huggingface.co/sergeyzh/BERTA"><code>sergeyzh/BERTA</code></a><a class="headerlink" href="#sergeyzhberta" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2025-03-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhrubert-mini-frida"><a href="https://huggingface.co/sergeyzh/rubert-mini-frida"><code>sergeyzh/rubert-mini-frida</code></a><a class="headerlink" href="#sergeyzhrubert-mini-frida" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>32.3M</td>
<td>123.0 MB</td>
<td>2025-03-02</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="telepixpixie-rune-v10"><a href="https://huggingface.co/telepix/PIXIE-Rune-v1.0"><code>telepix/PIXIE-Rune-v1.0</code></a><a class="headerlink" href="#telepixpixie-rune-v10" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>6.1K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2026-01-15</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (74)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">TelePIX-PIXIE-Rune-v1.0</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{PIXIE-Rune-v1.0}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{TelePIX AI Research Team and Bongmin Kim}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="s">{2026}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face model card}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/telepix/PIXIE-Rune-v1.0}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">yu2024arctic</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Arctic-Embed 2.0: Multilingual Retrieval Without Compromise}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yu, Puxuan and Merrick, Luke and Nuti, Gaurav and Campos, Daniel}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2412.04506}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2412.04506}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tencentkalm-embedding-gemma3-12b-2511"><a href="https://kalm-embedding.github.io/"><code>tencent/KaLM-Embedding-Gemma3-12B-2511</code></a><a class="headerlink" href="#tencentkalm-embedding-gemma3-12b-2511" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3840</td>
<td>11.8B</td>
<td>43.8 GB</td>
<td>2025-11-06</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhao2025kalmembeddingv2</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinping Zhao and Xinshuo Hu and Zifei Shan and Shouzheng Huang and Yao Zhou and Xin Zhang and Zetian Sun and Zhenyu Liu and Dongfang Li and Xinyuan Wei and Youcheng Pan and Yang Xiang and Meishan Zhang and Haofen Wang and Jun Yu and Baotian Hu and Min Zhang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2506.20923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2506.20923}</span><span class="p">,</span>
<span class="p">}</span>

<span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="tencentyoutu-embedding"><a href="https://huggingface.co/tencent/Youtu-Embedding"><code>tencent/Youtu-Embedding</code></a><a class="headerlink" href="#tencentyoutu-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.7B</td>
<td>not specified</td>
<td>2025-09-28</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2025codiemb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Bowen and Song, Zixin and Chen, Chunquan and Zhang, Qian-Wen and Yin, Di and Sun, Xing}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2508.11442}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2508.11442}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="voyageaivoyage-2"><a href="https://blog.voyageai.com/2023/10/29/voyage-embeddings/"><code>voyageai/voyage-2</code></a><a class="headerlink" href="#voyageaivoyage-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3"><a href="https://blog.voyageai.com/2024/09/18/voyage-3/"><code>voyageai/voyage-3</code></a><a class="headerlink" href="#voyageaivoyage-3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-09-18</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-large"><a href="https://blog.voyageai.com/2025/01/07/voyage-3-large/"><code>voyageai/voyage-3-large</code></a><a class="headerlink" href="#voyageaivoyage-3-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-07</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-lite"><a href="https://blog.voyageai.com/2024/09/18/voyage-3/"><code>voyageai/voyage-3-lite</code></a><a class="headerlink" href="#voyageaivoyage-3-lite" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-09-18</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-3-m-exp"><a href="https://huggingface.co/voyageai/voyage-3-m-exp"><code>voyageai/voyage-3-m-exp</code></a><a class="headerlink" href="#voyageaivoyage-3-m-exp" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>2048</td>
<td>6.9B</td>
<td>not specified</td>
<td>2025-01-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5</code></a><a class="headerlink" href="#voyageaivoyage-35" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35-output_dtypebinary"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5 (output_dtype=binary)</code></a><a class="headerlink" href="#voyageaivoyage-35-output_dtypebinary" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-35-output_dtypeint8"><a href="https://blog.voyageai.com/2025/05/20/voyage-3-5/"><code>voyageai/voyage-3.5 (output_dtype=int8)</code></a><a class="headerlink" href="#voyageaivoyage-35-output_dtypeint8" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2025-01-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-4"><a href="https://blog.voyageai.com/2026/01/15/voyage-4/"><code>voyageai/voyage-4</code></a><a class="headerlink" href="#voyageaivoyage-4" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2026-01-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-4-large"><a href="https://blog.voyageai.com/2026/01/15/voyage-4/"><code>voyageai/voyage-4-large</code></a><a class="headerlink" href="#voyageaivoyage-4-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2026-01-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-4-large-embed_dim2048"><a href="https://blog.voyageai.com/2026/01/15/voyage-4/"><code>voyageai/voyage-4-large (embed_dim=2048)</code></a><a class="headerlink" href="#voyageaivoyage-4-large-embed_dim2048" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>2048</td>
<td>not specified</td>
<td>not specified</td>
<td>2026-01-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-4-lite"><a href="https://blog.voyageai.com/2026/01/15/voyage-4/"><code>voyageai/voyage-4-lite</code></a><a class="headerlink" href="#voyageaivoyage-4-lite" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2026-01-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-4-nano"><a href="https://huggingface.co/voyageai/voyage-4-nano"><code>voyageai/voyage-4-nano</code></a><a class="headerlink" href="#voyageaivoyage-4-nano" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>2048</td>
<td>346.5M</td>
<td>661.0 MB</td>
<td>2026-01-15</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-code-2"><a href="https://blog.voyageai.com/2024/01/23/voyage-code-2-elevate-your-code-retrieval/"><code>voyageai/voyage-code-2</code></a><a class="headerlink" href="#voyageaivoyage-code-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-code-3"><a href="https://blog.voyageai.com/2024/12/04/voyage-code-3/"><code>voyageai/voyage-code-3</code></a><a class="headerlink" href="#voyageaivoyage-code-3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-12-04</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-finance-2"><a href="https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/"><code>voyageai/voyage-finance-2</code></a><a class="headerlink" href="#voyageaivoyage-finance-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-large-2"><a href="https://blog.voyageai.com/2023/10/29/voyage-embeddings/"><code>voyageai/voyage-large-2</code></a><a class="headerlink" href="#voyageaivoyage-large-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-large-2-instruct"><a href="https://blog.voyageai.com/2024/05/05/voyage-large-2-instruct-instruction-tuned-and-rank-1-on-mteb/"><code>voyageai/voyage-large-2-instruct</code></a><a class="headerlink" href="#voyageaivoyage-large-2-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-05-05</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-law-2"><a href="https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/"><code>voyageai/voyage-law-2</code></a><a class="headerlink" href="#voyageaivoyage-law-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>16.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="voyageaivoyage-multilingual-2"><a href="https://blog.voyageai.com/2024/06/10/voyage-multilingual-2-multilingual-embedding-model/"><code>voyageai/voyage-multilingual-2</code></a><a class="headerlink" href="#voyageaivoyage-multilingual-2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.0K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-06-10</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="yibinleilens-d4000"><a href="https://huggingface.co/yibinlei/LENS-d4000"><code>yibinlei/LENS-d4000</code></a><a class="headerlink" href="#yibinleilens-d4000" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4000</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-01-17</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">lei2025lens</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Enhancing Lexicon-Based Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.09749}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="yibinleilens-d8000"><a href="https://huggingface.co/yibinlei/LENS-d8000"><code>yibinlei/LENS-d8000</code></a><a class="headerlink" href="#yibinleilens-d8000" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>8000</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2025-01-17</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">lei2025lens</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Enhancing Lexicon-Based Text Embeddings with Large Language Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Lei, Yibin and Shen, Tao and Cao, Yu and Yates, Andrew}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.09749}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="zeta-alpha-aizeta-alpha-e5-mistral"><a href="https://huggingface.co/zeta-alpha-ai/Zeta-Alpha-E5-Mistral"><code>zeta-alpha-ai/Zeta-Alpha-E5-Mistral</code></a><a class="headerlink" href="#zeta-alpha-aizeta-alpha-e5-mistral" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h2 id="non-instruction-model">Non-instruction Model<a class="headerlink" href="#non-instruction-model" title="Permanent link">&para;</a></h2>
<h4 id="aiteamvnvietnamese_embedding"><a href="https://huggingface.co/AITeamVN/Vietnamese_Embedding"><code>AITeamVN/Vietnamese_Embedding</code></a><a class="headerlink" href="#aiteamvnvietnamese_embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-03-17</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Vietnamese_Embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Vietnamese_Embedding: Embedding model in Vietnamese language.}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Nguyen Nho Trung, Nguyen Nhat Quang, Nguyen Van Huy}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Huggingface}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="alibaba-nlpgte-base-en-v15"><a href="https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5"><code>Alibaba-NLP/gte-base-en-v1.5</code></a><a class="headerlink" href="#alibaba-nlpgte-base-en-v15" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>136.8M</td>
<td>not specified</td>
<td>2024-06-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">zhang2024mgte</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Xin Zhang and Yanzhao Zhang and Dingkun Long and Wen Xie and Ziqi Dai and Jialong Tang and Huan Lin and Baosong Yang and Pengjun Xie and Fei Huang and Meishan Zhang and Wenjie Li and Min Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.19669}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.19669}</span><span class="p">,</span>
<span class="p">}</span>
<span class="nc">@misc</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards General Text Embeddings with Multi-stage Contrastive Learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2308.03281}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="alibaba-nlpgte-modernbert-base"><a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base"><code>Alibaba-NLP/gte-modernbert-base</code></a><a class="headerlink" href="#alibaba-nlpgte-modernbert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>284.0 MB</td>
<td>2025-01-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024mgte</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{1393--1412}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="alibaba-nlpgte-multilingual-base"><a href="https://huggingface.co/Alibaba-NLP/gte-multilingual-base"><code>Alibaba-NLP/gte-multilingual-base</code></a><a class="headerlink" href="#alibaba-nlpgte-multilingual-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>305.4M</td>
<td>582.0 MB</td>
<td>2024-07-20</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2024mgte</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others}</span><span class="p">,</span>
<span class="w">  </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{1393--1412}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-en-icl"><a href="https://huggingface.co/BAAI/bge-en-icl"><code>BAAI/bge-en-icl</code></a><a class="headerlink" href="#baaibge-en-icl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>4096</td>
<td>7.1B</td>
<td>26.5 GB</td>
<td>2024-07-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2024makingtextembeddersfewshot</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Making Text Embedders Few-Shot Learners}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2409.15700}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2409.15700}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-m3"><a href="https://huggingface.co/BAAI/bge-m3"><code>BAAI/bge-m3</code></a><a class="headerlink" href="#baaibge-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-06-28</td>
<td>afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge-m3</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.03216}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-m3-unsupervised"><a href="https://huggingface.co/BAAI/bge-m3-unsupervised"><code>BAAI/bge-m3-unsupervised</code></a><a class="headerlink" href="#baaibge-m3-unsupervised" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>568.0M</td>
<td>2.1 GB</td>
<td>2024-01-30</td>
<td>afr-Latn, amh-Ethi, ast-Latn, azj-Latn, azj-Latn, ... (29)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge-m3</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.03216}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-multilingual-gemma2"><a href="https://huggingface.co/BAAI/bge-multilingual-gemma2"><code>BAAI/bge-multilingual-gemma2</code></a><a class="headerlink" href="#baaibge-multilingual-gemma2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://ai.google.dev/gemma/terms</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3584</td>
<td>9.2B</td>
<td>34.4 GB</td>
<td>2024-07-25</td>
<td>eng-Latn, fra-Latn, jpn-Jpan, jpn-Latn, kor-Hang, ... (7)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge-m3</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.03216}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>


<span class="nc">@misc</span><span class="p">{</span><span class="nl">xiao2024cpackpackagedresourcesadvance</span><span class="p">,</span>
<span class="w">  </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff and Defu Lian and Jian-Yun Nie}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{C-Pack: Packaged Resources To Advance General Chinese Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2309.07597}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="baaibge-reranker-v2-m3">BAAI/bge-reranker-v2-m3<a class="headerlink" href="#baaibge-reranker-v2-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2.1 GB</td>
<td>2024-06-24</td>
<td>ara-Arab, ben-Beng, dan-Latn, deu-Latn, eng-Latn, ... (32)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2023making</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Making Large Language Models A Better Foundation For Dense Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2312.15503}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">bge-m3</span><span class="p">,</span>
<span class="w">      </span><span class="na">archiveprefix</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2402.03216}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryclass</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bytedancelistconranker"><a href="https://huggingface.co/ByteDance/ListConRanker"><code>ByteDance/ListConRanker</code></a><a class="headerlink" href="#bytedancelistconranker" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>401.0M</td>
<td>1.2 GB</td>
<td>2024-12-11</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2025listconranker</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{ListConRanker: A Contrastive Text Reranker with Listwise Encoding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Liu, Junlong and Ma, Yue and Zhao, Ruihui and Zheng, Junhao and Ma, Qianli and Kang, Yangyang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2501.07111}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="classicalyinka"><a href="https://huggingface.co/Classical/Yinka"><code>Classical/Yinka</code></a><a class="headerlink" href="#classicalyinka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-01-09</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dmetasouldmeta-embedding-zh-small"><a href="https://huggingface.co/DMetaSoul/Dmeta-embedding-zh-small/"><code>DMetaSoul/Dmeta-embedding-zh-small</code></a><a class="headerlink" href="#dmetasouldmeta-embedding-zh-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0K</td>
<td>768</td>
<td>74.2M</td>
<td>283.0 MB</td>
<td>2024-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dmetasoulsbert-chinese-general-v1"><a href="https://huggingface.co/DMetaSoul/sbert-chinese-general-v1"><code>DMetaSoul/sbert-chinese-general-v1</code></a><a class="headerlink" href="#dmetasoulsbert-chinese-general-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>128</td>
<td>102.3M</td>
<td>not specified</td>
<td>2022-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="deeppavlovdistilrubert-small-cased-conversational"><a href="https://huggingface.co/DeepPavlov/distilrubert-small-cased-conversational"><code>DeepPavlov/distilrubert-small-cased-conversational</code></a><a class="headerlink" href="#deeppavlovdistilrubert-small-cased-conversational" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>107.0M</td>
<td>408.0 MB</td>
<td>2022-06-28</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">https://doi.org/10.48550/arxiv.2205.02340</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2205.02340}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2205.02340}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Kolesnikova, Alina and Kuratov, Yuri and Konovalov, Vasily and Burtsev, Mikhail}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Knowledge Distillation of Russian Language Models with Reduction of Vocabulary}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv.org perpetual, non-exclusive license}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deeppavlovrubert-base-cased"><a href="https://huggingface.co/DeepPavlov/rubert-base-cased"><code>DeepPavlov/rubert-base-cased</code></a><a class="headerlink" href="#deeppavlovrubert-base-cased" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>1.3B</td>
<td>4.8 GB</td>
<td>2020-03-04</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">kuratov2019adaptationdeepbidirectionalmultilingual</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Adaptation of Deep Bidirectional Multilingual Transformers for Russian Language}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Yuri Kuratov and Mikhail Arkhipov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{1905.07213}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/1905.07213}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deeppavlovrubert-base-cased-sentence"><a href="https://huggingface.co/DeepPavlov/rubert-base-cased-sentence"><code>DeepPavlov/rubert-base-cased-sentence</code></a><a class="headerlink" href="#deeppavlovrubert-base-cased-sentence" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>107.0M</td>
<td>408.0 MB</td>
<td>2020-03-04</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="facebookaixlm-roberta-base"><a href="https://huggingface.co/FacebookAI/xlm-roberta-base"><code>FacebookAI/xlm-roberta-base</code></a><a class="headerlink" href="#facebookaixlm-roberta-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2019-11-05</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-1911-02116</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Alexis Conneau and</span>
<span class="s">               Kartikay Khandelwal and</span>
<span class="s">               Naman Goyal and</span>
<span class="s">               Vishrav Chaudhary and</span>
<span class="s">               Guillaume Wenzek and</span>
<span class="s">               Francisco Guzm{&#39;{a}}n and</span>
<span class="s">               Edouard Grave and</span>
<span class="s">               Myle Ott and</span>
<span class="s">               Luke Zettlemoyer and</span>
<span class="s">               Veselin Stoyanov}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{Unsupervised Cross-lingual Representation Learning at Scale}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="w">   </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{http://arxiv.org/abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 11 Nov 2019 18:38:09 +0100}</span><span class="p">,</span>
<span class="w">  </span><span class="na">biburl</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-1911-02116.bib}</span><span class="p">,</span>
<span class="w">  </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="facebookaixlm-roberta-large"><a href="https://huggingface.co/FacebookAI/xlm-roberta-large"><code>FacebookAI/xlm-roberta-large</code></a><a class="headerlink" href="#facebookaixlm-roberta-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2019-11-05</td>
<td>afr-Latn, amh-Latn, ara-Latn, asm-Latn, aze-Latn, ... (99)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-1911-02116</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Alexis Conneau and</span>
<span class="s">               Kartikay Khandelwal and</span>
<span class="s">               Naman Goyal and</span>
<span class="s">               Vishrav Chaudhary and</span>
<span class="s">               Guillaume Wenzek and</span>
<span class="s">               Francisco Guzm{&#39;{a}}n and</span>
<span class="s">               Edouard Grave and</span>
<span class="s">               Myle Ott and</span>
<span class="s">               Luke Zettlemoyer and</span>
<span class="s">               Veselin Stoyanov}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{Unsupervised Cross-lingual Representation Learning at Scale}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="w">   </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{http://arxiv.org/abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 11 Nov 2019 18:38:09 +0100}</span><span class="p">,</span>
<span class="w">  </span><span class="na">biburl</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-1911-02116.bib}</span><span class="p">,</span>
<span class="w">  </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="gameselosts-multilingual-mpnet-base-v2"><a href="https://huggingface.co/Gameselo/STS-multilingual-mpnet-base-v2"><code>Gameselo/STS-multilingual-mpnet-base-v2</code></a><a class="headerlink" href="#gameselosts-multilingual-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-07</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="greennodegreennode-embedding-large-vn-mixed-v1"><a href="https://huggingface.co/GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1"><code>GreenNode/GreenNode-Embedding-Large-VN-Mixed-V1</code></a><a class="headerlink" href="#greennodegreennode-embedding-large-vn-mixed-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="greennodegreennode-embedding-large-vn-v1"><a href="https://huggingface.co/GreenNode/GreenNode-Embedding-Large-VN-V1"><code>GreenNode/GreenNode-Embedding-Large-VN-V1</code></a><a class="headerlink" href="#greennodegreennode-embedding-large-vn-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<h4 id="hit-tmgkalm-embedding-multilingual-mini-v1"><a href="https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-v1"><code>HIT-TMG/KaLM-embedding-multilingual-mini-v1</code></a><a class="headerlink" href="#hit-tmgkalm-embedding-multilingual-mini-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>896</td>
<td>494.0M</td>
<td>1.8 GB</td>
<td>2024-08-27</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">hu2025kalmembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Xinshuo Hu and Zifei Shan and Xinping Zhao and Zetian Sun and Zhenyu Liu and Dongfang Li and Shaolin Ye and Xinyuan Wei and Qian Chen and Baotian Hu and Haofen Wang and Jun Yu and Min Zhang}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2501.01028}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2501.01028}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="haon-chenspeed-embedding-7b-instruct"><a href="https://huggingface.co/Haon-Chen/speed-embedding-7b-instruct"><code>Haon-Chen/speed-embedding-7b-instruct</code></a><a class="headerlink" href="#haon-chenspeed-embedding-7b-instruct" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>not specified</td>
<td>7.1B</td>
<td>13.2 GB</td>
<td>2024-10-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2024little</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{Little Giants: Synthesizing High-Quality Embedding Data at Scale}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Chen, Haonan and Wang, Liang and Yang, Nan and Zhu, Yutao and Zhao, Ziliang and Wei, Furu and Dou, Zhicheng}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2410.18634}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hooshvarelabbert-base-parsbert-uncased"><a href="https://huggingface.co/HooshvareLab/bert-base-parsbert-uncased"><code>HooshvareLab/bert-base-parsbert-uncased</code></a><a class="headerlink" href="#hooshvarelabbert-base-parsbert-uncased" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2021-05-19</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">ParsBERT</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{ParsBERT: Transformer-based Model for Persian Language Understanding}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Mehrdad Farahani, Mohammad Gharachorloo, Marzieh Farahani, Mohammad Manthouri}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{ArXiv}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2020}</span><span class="p">,</span>
<span class="w">    </span><span class="na">volume</span><span class="p">=</span><span class="s">{abs/2005.12515}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hum-workslodestone-base-4096-v1"><a href="https://huggingface.co/Hum-Works/lodestone-base-4096-v1"><code>Hum-Works/lodestone-base-4096-v1</code></a><a class="headerlink" href="#hum-workslodestone-base-4096-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>768</td>
<td>137.4M</td>
<td>not specified</td>
<td>2023-08-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="ieityuanyuan-embedding-20-zh"><a href="https://huggingface.co/IEITYuan/Yuan-embedding-2.0-zh"><code>IEITYuan/Yuan-embedding-2.0-zh</code></a><a class="headerlink" href="#ieityuanyuan-embedding-20-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>327.4M</td>
<td>1.2 GB</td>
<td>2025-11-24</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="jaumegemma-2b-embeddings"><a href="https://huggingface.co/Jaume/gemma-2b-embeddings"><code>Jaume/gemma-2b-embeddings</code></a><a class="headerlink" href="#jaumegemma-2b-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>2048</td>
<td>2.5B</td>
<td>9.3 GB</td>
<td>2024-06-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="kblabsentence-bert-swedish-cased"><a href="https://huggingface.co/KBLab/sentence-bert-swedish-cased"><code>KBLab/sentence-bert-swedish-cased</code></a><a class="headerlink" href="#kblabsentence-bert-swedish-cased" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>384</td>
<td>768</td>
<td>124.7M</td>
<td>476.0 MB</td>
<td>2023-01-11</td>
<td>swe-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rekathati2021introducing</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Rekathati, Faton}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The KBLab Blog: Introducing a Swedish Sentence Transformer}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://kb-labb.github.io/posts/2021-08-23-a-swedish-sentence-transformer/}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2021}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kfstxlmroberta-en-da-sv-nb"><a href="https://huggingface.co/KFST/XLMRoberta-en-da-sv-nb"><code>KFST/XLMRoberta-en-da-sv-nb</code></a><a class="headerlink" href="#kfstxlmroberta-en-da-sv-nb" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2022-02-22</td>
<td>dan-Latn, eng-Latn, nno-Latn, nob-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<h4 id="kennethenevoldsendfm-sentence-encoder-large"><a href="https://huggingface.co/KennethEnevoldsen/dfm-sentence-encoder-large"><code>KennethEnevoldsen/dfm-sentence-encoder-large</code></a><a class="headerlink" href="#kennethenevoldsendfm-sentence-encoder-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>355.1M</td>
<td>1.5 GB</td>
<td>2023-07-12</td>
<td>dan-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">enevoldsenScandinavianEmbeddingBenchmarks2024</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The {Scandinavian} {Embedding} {Benchmarks}: {Comprehensive} {Assessment} of {Multilingual} and {Monolingual} {Text} {Embedding}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">shorttitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The {Scandinavian} {Embedding} {Benchmarks}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://openreview.net/forum?id=pJl_i7HIA72}</span><span class="p">,</span>
<span class="w">    </span><span class="na">language</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{en}</span><span class="p">,</span>
<span class="w">    </span><span class="na">urldate</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024-04-12}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Enevoldsen, Kenneth and Kardos, Mrton and Muennighoff, Niklas and Nielbo, Kristoffer}</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nv">feb</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kennethenevoldsendfm-sentence-encoder-medium"><a href="https://huggingface.co/KennethEnevoldsen/dfm-sentence-encoder-medium"><code>KennethEnevoldsen/dfm-sentence-encoder-medium</code></a><a class="headerlink" href="#kennethenevoldsendfm-sentence-encoder-medium" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2023-07-12</td>
<td>dan-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">enevoldsenScandinavianEmbeddingBenchmarks2024</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The {Scandinavian} {Embedding} {Benchmarks}: {Comprehensive} {Assessment} of {Multilingual} and {Monolingual} {Text} {Embedding}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">shorttitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{The {Scandinavian} {Embedding} {Benchmarks}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://openreview.net/forum?id=pJl_i7HIA72}</span><span class="p">,</span>
<span class="w">    </span><span class="na">language</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{en}</span><span class="p">,</span>
<span class="w">    </span><span class="na">urldate</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024-04-12}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Enevoldsen, Kenneth and Kardos, Mrton and Muennighoff, Niklas and Nielbo, Kristoffer}</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nv">feb</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="kowshik24bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2"><a href="https://huggingface.co/Kowshik24/bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2"><code>Kowshik24/bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2</code></a><a class="headerlink" href="#kowshik24bangla-sentence-transformer-ft-matryoshka-paraphrase-multilingual-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>128</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2025-11-10</td>
<td>ben-Beng</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="lajavanessbilingual-embedding-base"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-base"><code>Lajavaness/bilingual-embedding-base</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-1911-02116</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Alexis Conneau and</span>
<span class="s">                   Kartikay Khandelwal and</span>
<span class="s">                   Naman Goyal and</span>
<span class="s">                   Vishrav Chaudhary and</span>
<span class="s">                   Guillaume Wenzek and</span>
<span class="s">                   Francisco Guzm{&#39;{a}}n and</span>
<span class="s">                   Edouard Grave and</span>
<span class="s">                   Myle Ott and</span>
<span class="s">                   Luke Zettlemoyer and</span>
<span class="s">                   Veselin Stoyanov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{Unsupervised Cross-lingual Representation Learning at Scale}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="w">   </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">volume</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{http://arxiv.org/abs/1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 11 Nov 2019 18:38:09 +0100}</span><span class="p">,</span>
<span class="w">      </span><span class="na">biburl</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-1911-02116.bib}</span><span class="p">,</span>
<span class="w">      </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="w">    </span><span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">reimers2019sentence</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="p">=</span><span class="s">{Nils Reimers, Iryna Gurevych}</span><span class="p">,</span>
<span class="w">   </span><span class="na">journal</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/1908.10084}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">thakur2020augmented</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv e-prints}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{arXiv--2010}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="lajavanessbilingual-embedding-large"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-large"><code>Lajavaness/bilingual-embedding-large</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2024-06-24</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-1911-02116</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Alexis Conneau and</span>
<span class="s">                   Kartikay Khandelwal and</span>
<span class="s">                   Naman Goyal and</span>
<span class="s">                   Vishrav Chaudhary and</span>
<span class="s">                   Guillaume Wenzek and</span>
<span class="s">                   Francisco Guzm{&#39;{a}}n and</span>
<span class="s">                   Edouard Grave and</span>
<span class="s">                   Myle Ott and</span>
<span class="s">                   Luke Zettlemoyer and</span>
<span class="s">                   Veselin Stoyanov}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{Unsupervised Cross-lingual Representation Learning at Scale}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="w">   </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">volume</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{http://arxiv.org/abs/1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{1911.02116}</span><span class="p">,</span>
<span class="w">      </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 11 Nov 2019 18:38:09 +0100}</span><span class="p">,</span>
<span class="w">      </span><span class="na">biburl</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-1911-02116.bib}</span><span class="p">,</span>
<span class="w">      </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="w">    </span><span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">reimers2019sentence</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="p">=</span><span class="s">{Nils Reimers, Iryna Gurevych}</span><span class="p">,</span>
<span class="w">   </span><span class="na">journal</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/1908.10084}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">thakur2020augmented</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv e-prints}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{arXiv--2010}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="lajavanessbilingual-embedding-small"><a href="https://huggingface.co/Lajavaness/bilingual-embedding-small"><code>Lajavaness/bilingual-embedding-small</code></a><a class="headerlink" href="#lajavanessbilingual-embedding-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2024-07-17</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-1911-02116</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Alexis Conneau and</span>
<span class="s">               Kartikay Khandelwal and</span>
<span class="s">               Naman Goyal and</span>
<span class="s">               Vishrav Chaudhary and</span>
<span class="s">               Guillaume Wenzek and</span>
<span class="s">               Francisco Guzm{&#39;{a}}n and</span>
<span class="s">               Edouard Grave and</span>
<span class="s">               Myle Ott and</span>
<span class="s">               Luke Zettlemoyer and</span>
<span class="s">               Veselin Stoyanov}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{Unsupervised Cross-lingual Representation Learning at Scale}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="w">   </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2019}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{http://arxiv.org/abs/1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{1911.02116}</span><span class="p">,</span>
<span class="w">  </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 11 Nov 2019 18:38:09 +0100}</span><span class="p">,</span>
<span class="w">  </span><span class="na">biburl</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-1911-02116.bib}</span><span class="p">,</span>
<span class="w">  </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">reimers2019sentence</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="p">=</span><span class="s">{Nils Reimers, Iryna Gurevych}</span><span class="p">,</span>
<span class="w">   </span><span class="na">journal</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/1908.10084}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span>
<span class="p">}</span>

<span class="nc">@article</span><span class="p">{</span><span class="nl">thakur2020augmented</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Thakur, Nandan and Reimers, Nils and Daxenberger, Johannes and Gurevych, Iryna}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv e-prints}</span><span class="p">,</span>
<span class="w">  </span><span class="na">pages</span><span class="p">=</span><span class="s">{arXiv--2010}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2020}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcinexthakim"><a href="https://huggingface.co/MCINext/Hakim"><code>MCINext/Hakim</code></a><a class="headerlink" href="#mcinexthakim" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcinexthakim-small"><a href="https://huggingface.co/MCINext/Hakim-small"><code>MCINext/Hakim-small</code></a><a class="headerlink" href="#mcinexthakim-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>38.7M</td>
<td>148.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mcinexthakim-unsup"><a href="https://huggingface.co/MCINext/Hakim-unsup"><code>MCINext/Hakim-unsup</code></a><a class="headerlink" href="#mcinexthakim-unsup" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2025-05-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">sarmadi2025hakim</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Hakim: Farsi Text Embedding Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Sarmadi, Mehran and Alikhani, Morteza and Zinvandi, Erfan and Pourbahman, Zahra}</span><span class="p">,</span>
<span class="w">      </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.08435}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mihaiiibulbasaur"><a href="https://huggingface.co/Mihaiii/Bulbasaur"><code>Mihaiii/Bulbasaur</code></a><a class="headerlink" href="#mihaiiibulbasaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiiivysaur"><a href="https://huggingface.co/Mihaiii/Ivysaur"><code>Mihaiii/Ivysaur</code></a><a class="headerlink" href="#mihaiiiivysaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-04-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiisquirtle"><a href="https://huggingface.co/Mihaiii/Squirtle"><code>Mihaiii/Squirtle</code></a><a class="headerlink" href="#mihaiiisquirtle" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>15.6M</td>
<td>60.0 MB</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiivenusaur"><a href="https://huggingface.co/Mihaiii/Venusaur"><code>Mihaiii/Venusaur</code></a><a class="headerlink" href="#mihaiiivenusaur" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>15.6M</td>
<td>60.0 MB</td>
<td>2024-04-29</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiiwartortle"><a href="https://huggingface.co/Mihaiii/Wartortle"><code>Mihaiii/Wartortle</code></a><a class="headerlink" href="#mihaiiiwartortle" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiigte-micro"><a href="https://huggingface.co/Mihaiii/gte-micro"><code>Mihaiii/gte-micro</code></a><a class="headerlink" href="#mihaiiigte-micro" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>17.4M</td>
<td>66.0 MB</td>
<td>2024-04-21</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mihaiiigte-micro-v4"><a href="https://huggingface.co/Mihaiii/gte-micro-v4"><code>Mihaiii/gte-micro-v4</code></a><a class="headerlink" href="#mihaiiigte-micro-v4" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>19.2M</td>
<td>73.0 MB</td>
<td>2024-04-22</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="mira190euler-legal-embedding-v1"><a href="https://huggingface.co/Mira190/Euler-Legal-Embedding-V1"><code>Mira190/Euler-Legal-Embedding-V1</code></a><a class="headerlink" href="#mira190euler-legal-embedding-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.5K</td>
<td>4096</td>
<td>7.6B</td>
<td>15.3 GB</td>
<td>2025-11-06</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">euler2025legal</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Euler-Legal-Embedding: Advanced Legal Representation Learning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{LawRank Team}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="nbailabnb-bert-base"><a href="https://huggingface.co/NbAiLab/nb-bert-base"><code>NbAiLab/nb-bert-base</code></a><a class="headerlink" href="#nbailabnb-bert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>177.9M</td>
<td>681.0 MB</td>
<td>2021-01-13</td>
<td>nno-Latn, nob-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nbailabnb-bert-large"><a href="https://huggingface.co/NbAiLab/nb-bert-large"><code>NbAiLab/nb-bert-large</code></a><a class="headerlink" href="#nbailabnb-bert-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>355.1M</td>
<td>1.3 GB</td>
<td>2021-04-29</td>
<td>nno-Latn, nob-Latn</td>
</tr>
</tbody>
</table>
<h4 id="nbailabnb-sbert-base"><a href="https://huggingface.co/NbAiLab/nb-sbert-base"><code>NbAiLab/nb-sbert-base</code></a><a class="headerlink" href="#nbailabnb-sbert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>75</td>
<td>4096</td>
<td>177.9M</td>
<td>678.0 MB</td>
<td>2022-11-23</td>
<td>dan-Latn, nno-Latn, nob-Latn, swe-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-100k"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-100K"><code>NeuML/pubmedbert-base-embeddings-100K</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-100k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>100.0K</td>
<td>0.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-1m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-1M"><code>NeuML/pubmedbert-base-embeddings-1M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-1m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>1.0M</td>
<td>2.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-2m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-2M"><code>NeuML/pubmedbert-base-embeddings-2M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-2m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>1.9M</td>
<td>7.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-500k"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-500K"><code>NeuML/pubmedbert-base-embeddings-500K</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-500k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>500.0K</td>
<td>2.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="neumlpubmedbert-base-embeddings-8m"><a href="https://huggingface.co/NeuML/pubmedbert-base-embeddings-8M"><code>NeuML/pubmedbert-base-embeddings-8M</code></a><a class="headerlink" href="#neumlpubmedbert-base-embeddings-8m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.8M</td>
<td>30.0 MB</td>
<td>2025-01-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Arabert-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabert-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>135.2M</td>
<td>516.0 MB</td>
<td>2024-06-16</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet"><code>Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-minilm-l12-v2-all-nli-triplet" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2024-06-25</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<h4 id="omartificial-intelligence-spacearabic-triplet-matryoshka-v2"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2"><code>Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-triplet-matryoshka-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>768</td>
<td>768</td>
<td>135.2M</td>
<td>516.0 MB</td>
<td>2024-07-28</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@article</span><span class="p">{</span><span class="nl">nacar2025gate</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Nacar, Omer and Koubaa, Anis and Sibaee, Serry and Al-Habashi, Yasser and Ammar, Adel and Boulila, Wadii}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2505.24581}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Arabic-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-06-14</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="omartificial-intelligence-spacearabic-labse-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-labse-Matryoshka"><code>Omartificial-Intelligence-Space/Arabic-labse-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-labse-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>470.9M</td>
<td>1.8 GB</td>
<td>2024-06-16</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet"><code>Omartificial-Intelligence-Space/Arabic-mpnet-base-all-nli-triplet</code></a><a class="headerlink" href="#omartificial-intelligence-spacearabic-mpnet-base-all-nli-triplet" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2024-06-15</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka"><a href="https://huggingface.co/Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka"><code>Omartificial-Intelligence-Space/Marbert-all-nli-triplet-Matryoshka</code></a><a class="headerlink" href="#omartificial-intelligence-spacemarbert-all-nli-triplet-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2024-06-17</td>
<td>ara-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="opensearch-aiops-moa-conan-embedding-v1"><a href="https://huggingface.co/OpenSearch-AI/Ops-MoA-Conan-embedding-v1"><code>OpenSearch-AI/Ops-MoA-Conan-embedding-v1</code></a><a class="headerlink" href="#opensearch-aiops-moa-conan-embedding-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>343.4M</td>
<td>1.3 GB</td>
<td>2025-03-26</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="opensearch-aiops-moa-yuan-embedding-10"><a href="https://huggingface.co/OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0"><code>OpenSearch-AI/Ops-MoA-Yuan-embedding-1.0</code></a><a class="headerlink" href="#opensearch-aiops-moa-yuan-embedding-10" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1536</td>
<td>343.4M</td>
<td>1.2 GB</td>
<td>2025-03-26</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="ordalietechsolon-embeddings-large-01"><a href="https://huggingface.co/OrdalieTech/Solon-embeddings-large-0.1"><code>OrdalieTech/Solon-embeddings-large-0.1</code></a><a class="headerlink" href="#ordalietechsolon-embeddings-large-01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2023-12-09</td>
<td>fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="ordalietechsolon-embeddings-mini-beta-11"><a href="https://huggingface.co/OrdalieTech/Solon-embeddings-mini-beta-1.1"><code>OrdalieTech/Solon-embeddings-mini-beta-1.1</code></a><a class="headerlink" href="#ordalietechsolon-embeddings-mini-beta-11" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>211.8M</td>
<td>808.0 MB</td>
<td>2025-01-01</td>
<td>fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="partaitooka-sbert"><a href="https://huggingface.co/PartAI/Tooka-SBERT"><code>PartAI/Tooka-SBERT</code></a><a class="headerlink" href="#partaitooka-sbert" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>353.0M</td>
<td>1.3 GB</td>
<td>2024-12-07</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="partaitooka-sbert-v2-large"><a href="https://huggingface.co/PartAI/Tooka-SBERT-V2-Large"><code>PartAI/Tooka-SBERT-V2-Large</code></a><a class="headerlink" href="#partaitooka-sbert-v2-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>353.0M</td>
<td>1.3 GB</td>
<td>2025-05-01</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="partaitooka-sbert-v2-small"><a href="https://huggingface.co/PartAI/Tooka-SBERT-V2-Small"><code>PartAI/Tooka-SBERT-V2-Small</code></a><a class="headerlink" href="#partaitooka-sbert-v2-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>122.9M</td>
<td>496.0 MB</td>
<td>2025-05-01</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="partaitookabert-base"><a href="https://huggingface.co/PartAI/TookaBERT-Base"><code>PartAI/TookaBERT-Base</code></a><a class="headerlink" href="#partaitookabert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>122.9M</td>
<td>469.0 MB</td>
<td>2024-12-08</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="qodoqodo-embed-1-15b"><a href="https://huggingface.co/Qodo/Qodo-Embed-1-1.5B"><code>Qodo/Qodo-Embed-1-1.5B</code></a><a class="headerlink" href="#qodoqodo-embed-1-15b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>1536</td>
<td>1.5B</td>
<td>6.6 GB</td>
<td>2025-02-19</td>
<td>c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)</td>
</tr>
</tbody>
</table>
<h4 id="qodoqodo-embed-1-7b"><a href="https://huggingface.co/Qodo/Qodo-Embed-1-7B"><code>Qodo/Qodo-Embed-1-7B</code></a><a class="headerlink" href="#qodoqodo-embed-1-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/Qodo/Qodo-Embed-1-1.5B/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>32.8K</td>
<td>3584</td>
<td>7.1B</td>
<td>28.4 GB</td>
<td>2025-02-24</td>
<td>c#-Code, c++-Code, go-Code, java-Code, javascript-Code, ... (9)</td>
</tr>
</tbody>
</table>
<h4 id="queritquerit"><a href="https://huggingface.co/Querit/Querit"><code>Querit/Querit</code></a><a class="headerlink" href="#queritquerit" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>1024</td>
<td>4.9B</td>
<td>9.2 GB</td>
<td>2026-01-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="shuu12121codesearch-modernbert-crow-plus"><a href="https://huggingface.co/Shuu12121/CodeSearch-ModernBERT-Crow-Plus"><code>Shuu12121/CodeSearch-ModernBERT-Crow-Plus</code></a><a class="headerlink" href="#shuu12121codesearch-modernbert-crow-plus" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.0K</td>
<td>768</td>
<td>151.7M</td>
<td>607.0 MB</td>
<td>2025-04-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="tencentbacconan-embedding-v1"><a href="https://huggingface.co/Classical/Yinka"><code>TencentBAC/Conan-embedding-v1</code></a><a class="headerlink" href="#tencentbacconan-embedding-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-08-22</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">li2024conanembeddinggeneraltextembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Conan-embedding: General Text Embedding with More and Better Negative Samples}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Shiyu Li and Yang Tang and Shizhe Chen and Xi Chen}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2408.15710}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2408.15710}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="vovanphucsup-simcse-vietnamese-phobert-base"><a href="https://huggingface.co/VoVanPhuc/sup-SimCSE-VietNamese-phobert-base"><code>VoVanPhuc/sup-SimCSE-VietNamese-phobert-base</code></a><a class="headerlink" href="#vovanphucsup-simcse-vietnamese-phobert-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>768</td>
<td>135.0M</td>
<td>517.0 MB</td>
<td>2021-05-26</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">gao2021simcse</span><span class="p">,</span>
<span class="w">   </span><span class="na">title</span><span class="p">=</span><span class="s">{{SimCSE}: Simple Contrastive Learning of Sentence Embeddings}</span><span class="p">,</span>
<span class="w">   </span><span class="na">author</span><span class="p">=</span><span class="s">{Gao, Tianyu and Yao, Xingcheng and Chen, Danqi}</span><span class="p">,</span>
<span class="w">   </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2104.08821}</span><span class="p">,</span>
<span class="w">   </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span>
<span class="p">}</span>

<span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">phobert</span><span class="p">,</span>
<span class="na">title</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{{PhoBERT: Pre-trained language models for Vietnamese}}</span><span class="p">,</span>
<span class="na">author</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Dat Quoc Nguyen and Anh Tuan Nguyen}</span><span class="p">,</span>
<span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Findings of the Association for Computational Linguistics: EMNLP 2020}</span><span class="p">,</span>
<span class="na">year</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{2020}</span><span class="p">,</span>
<span class="na">pages</span><span class="w">     </span><span class="p">=</span><span class="w"> </span><span class="s">{1037--1042}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="aari1995german_semantic_sts_v2"><a href="https://huggingface.co/aari1995/German_Semantic_STS_V2"><code>aari1995/German_Semantic_STS_V2</code></a><a class="headerlink" href="#aari1995german_semantic_sts_v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.7M</td>
<td>1.3 GB</td>
<td>2022-11-17</td>
<td>deu-Latn</td>
</tr>
</tbody>
</table>
<h4 id="abhinandmedembed-small-v01"><a href="https://huggingface.co/abhinand/MedEmbed-small-v0.1"><code>abhinand/MedEmbed-small-v0.1</code></a><a class="headerlink" href="#abhinandmedembed-small-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-10-20</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">balachandran2024medembed</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Balachandran, Abhinand}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MedEmbed: Medical-Focused Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/abhinand5/MedEmbed}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ai-foreversbert_large_mt_nlu_ru"><a href="https://huggingface.co/ai-forever/sbert_large_mt_nlu_ru"><code>ai-forever/sbert_large_mt_nlu_ru</code></a><a class="headerlink" href="#ai-foreversbert_large_mt_nlu_ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>427.0M</td>
<td>1.6 GB</td>
<td>2021-05-18</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="ai-foreversbert_large_nlu_ru"><a href="https://huggingface.co/ai-forever/sbert_large_nlu_ru"><code>ai-forever/sbert_large_nlu_ru</code></a><a class="headerlink" href="#ai-foreversbert_large_nlu_ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>427.0M</td>
<td>1.6 GB</td>
<td>2020-11-20</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="amazontitan-text-embeddings-v2"><a href="https://huggingface.co/amazon/Titan-text-embeddings-v2"><code>amazon/Titan-text-embeddings-v2</code></a><a class="headerlink" href="#amazontitan-text-embeddings-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://aws.amazon.com/service-terms/</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="andersborgesmodel2vecdk"><a href="https://huggingface.co/andersborges/model2vecdk"><code>andersborges/model2vecdk</code></a><a class="headerlink" href="#andersborgesmodel2vecdk" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>48.0M</td>
<td>183.0 MB</td>
<td>2025-11-21</td>
<td>dan-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tulkens, Stephan and {van Dongen}, Thomas}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Fast State-of-the-Art Static Embeddings}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="andersborgesmodel2vecdk-stem"><a href="https://huggingface.co/andersborges/model2vecdk-stem"><code>andersborges/model2vecdk-stem</code></a><a class="headerlink" href="#andersborgesmodel2vecdk-stem" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>48.6M</td>
<td>185.0 MB</td>
<td>2025-11-21</td>
<td>dan-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tulkens, Stephan and {van Dongen}, Thomas}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Fast State-of-the-Art Static Embeddings}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="avsolatoriogist-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-Embedding-v0"><code>avsolatorio/GIST-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2024-01-31</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">solatorio2024gistembed</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Aivin V. Solatorio}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.16829}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">URL</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2402.16829}</span>
<span class="w">    </span><span class="nv">eprint</span><span class="err">={2402.16829</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">archivePrefix={arXiv},</span>
<span class="w">    </span><span class="c">primaryClass={cs.LG}</span>
<span class="c">}</span>
</code></pre></div>
</details>
<h4 id="avsolatoriogist-all-minilm-l6-v2"><a href="https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2"><code>avsolatorio/GIST-all-MiniLM-L6-v2</code></a><a class="headerlink" href="#avsolatoriogist-all-minilm-l6-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-02-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">solatorio2024gistembed</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Aivin V. Solatorio}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.16829}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">URL</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2402.16829}</span>
<span class="w">    </span><span class="nv">eprint</span><span class="err">={2402.16829</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">archivePrefix={arXiv},</span>
<span class="w">    </span><span class="c">primaryClass={cs.LG}</span>
<span class="c">}</span>
</code></pre></div>
</details>
<h4 id="avsolatoriogist-large-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-large-Embedding-v0"><code>avsolatorio/GIST-large-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-large-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>1.2 GB</td>
<td>2024-02-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">solatorio2024gistembed</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Aivin V. Solatorio}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.16829}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">URL</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2402.16829}</span>
<span class="w">    </span><span class="nv">eprint</span><span class="err">={2402.16829</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">archivePrefix={arXiv},</span>
<span class="w">    </span><span class="c">primaryClass={cs.LG}</span>
<span class="c">}</span>
</code></pre></div>
</details>
<h4 id="avsolatoriogist-small-embedding-v0"><a href="https://huggingface.co/avsolatorio/GIST-small-Embedding-v0"><code>avsolatorio/GIST-small-Embedding-v0</code></a><a class="headerlink" href="#avsolatoriogist-small-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-02-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">solatorio2024gistembed</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Aivin V. Solatorio}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2402.16829}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">URL</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2402.16829}</span>
<span class="w">    </span><span class="nv">eprint</span><span class="err">={2402.16829</span><span class="p">}</span><span class="c">,</span>
<span class="w">    </span><span class="c">archivePrefix={arXiv},</span>
<span class="w">    </span><span class="c">primaryClass={cs.LG}</span>
<span class="c">}</span>
</code></pre></div>
</details>
<h4 id="avsolatorionoinstruct-small-embedding-v0"><a href="https://huggingface.co/avsolatorio/NoInstruct-small-Embedding-v0"><code>avsolatorio/NoInstruct-small-Embedding-v0</code></a><a class="headerlink" href="#avsolatorionoinstruct-small-embedding-v0" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2024-05-01</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="baselinehuman">baseline/Human<a class="headerlink" href="#baselinehuman" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>ara-Arab, dan-Latn, eng-Latn, nob-Latn, rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="bedrockamazon-titan-embed-text-v1"><a href="https://aws.amazon.com/about-aws/whats-new/2023/09/amazon-titan-embeddings-generally-available/"><code>bedrock/amazon-titan-embed-text-v1</code></a><a class="headerlink" href="#bedrockamazon-titan-embed-text-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-09-27</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="bedrockamazon-titan-embed-text-v2"><a href="https://aws.amazon.com/about-aws/whats-new/2024/04/amazon-titan-text-embeddings-v2-amazon-bedrock/"><code>bedrock/amazon-titan-embed-text-v2</code></a><a class="headerlink" href="#bedrockamazon-titan-embed-text-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-30</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="bigsciencesgpt-bloom-7b1-msmarco"><a href="https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco"><code>bigscience/sgpt-bloom-7b1-msmarco</code></a><a class="headerlink" href="#bigsciencesgpt-bloom-7b1-msmarco" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>4096</td>
<td>7.1B</td>
<td>not specified</td>
<td>2022-08-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">muennighoff2022sgpt</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{SGPT: GPT Sentence Embeddings for Semantic Search}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Muennighoff, Niklas}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2202.08904}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bisectgroupbica-base"><a href="https://huggingface.co/bisectgroup/BiCA-base"><code>bisectgroup/BiCA-base</code></a><a class="headerlink" href="#bisectgroupbica-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2025-11-14</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">sinha2025bicaeffectivebiomedicaldense</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Aarush Sinha and Pavan Kumar S and Roshan Balaji and Nirav Pravinbhai Bhatt}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2511.08029}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2511.08029}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="bkai-foundation-modelsvietnamese-bi-encoder"><a href="https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder"><code>bkai-foundation-models/vietnamese-bi-encoder</code></a><a class="headerlink" href="#bkai-foundation-modelsvietnamese-bi-encoder" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>768</td>
<td>135.0M</td>
<td>515.0 MB</td>
<td>2023-09-09</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">      </span><span class="nc">@article</span><span class="p">{</span><span class="nl">duc2024towards</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh, Le Thanh Huong, Dinh Viet Sang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2403.01616}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="w">  </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="brahmairesearchslx-v01"><a href="https://huggingface.co/brahmairesearch/slx-v0.1"><code>brahmairesearch/slx-v0.1</code></a><a class="headerlink" href="#brahmairesearchslx-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2024-08-13</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="castorinimonot5-3b-msmarco-10k"><a href="https://huggingface.co/castorini/monot5-3b-msmarco-10k"><code>castorini/monot5-3b-msmarco-10k</code></a><a class="headerlink" href="#castorinimonot5-3b-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>3.0B</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-base-msmarco-10k">castorini/monot5-base-msmarco-10k<a class="headerlink" href="#castorinimonot5-base-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>296.9M</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-large-msmarco-10k">castorini/monot5-large-msmarco-10k<a class="headerlink" href="#castorinimonot5-large-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="castorinimonot5-small-msmarco-10k">castorini/monot5-small-msmarco-10k<a class="headerlink" href="#castorinimonot5-small-msmarco-10k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">rosa2022parameterleftbehinddistillation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{No Parameter Left Behind: How Distillation and Model Size Affect Zero-Shot Retrieval}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Guilherme Moraes Rosa and Luiz Bonifacio and Vitor Jeronymo and Hugo Abonizio and Marzieh Fadaee and Roberto Lotufo and Rodrigo Nogueira}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2206.02873}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2206.02873}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codesagecodesage-base-v2"><a href="https://huggingface.co/codesage/codesage-base-v2"><code>codesage/codesage-base-v2</code></a><a class="headerlink" href="#codesagecodesage-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>354.7M</td>
<td>1.3 GB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span>
<span class="w">    </span><span class="nl">zhang2024code</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://openreview.net/forum?id=vfzRRjumpX}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codesagecodesage-large-v2"><a href="https://huggingface.co/codesage/codesage-large-v2"><code>codesage/codesage-large-v2</code></a><a class="headerlink" href="#codesagecodesage-large-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>4.8 GB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span>
<span class="w">    </span><span class="nl">zhang2024code</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://openreview.net/forum?id=vfzRRjumpX}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="codesagecodesage-small-v2"><a href="https://huggingface.co/codesage/codesage-small-v2"><code>codesage/codesage-small-v2</code></a><a class="headerlink" href="#codesagecodesage-small-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>128.0M</td>
<td>496.0 MB</td>
<td>2024-02-03</td>
<td>go-Code, java-Code, javascript-Code, php-Code, python-Code, ... (6)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span>
<span class="w">    </span><span class="nl">zhang2024code</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{{CODE} {REPRESENTATION} {LEARNING} {AT} {SCALE}}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang}</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="p">=</span><span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://openreview.net/forum?id=vfzRRjumpX}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cointegratedlabse-en-ru"><a href="https://huggingface.co/cointegrated/LaBSE-en-ru"><code>cointegrated/LaBSE-en-ru</code></a><a class="headerlink" href="#cointegratedlabse-en-ru" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>129.0M</td>
<td>492.0 MB</td>
<td>2021-06-10</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="cointegratedrubert-tiny"><a href="https://huggingface.co/cointegrated/rubert-tiny"><code>cointegrated/rubert-tiny</code></a><a class="headerlink" href="#cointegratedrubert-tiny" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>312</td>
<td>11.9M</td>
<td>45.0 MB</td>
<td>2021-05-24</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="cointegratedrubert-tiny2"><a href="https://huggingface.co/cointegrated/rubert-tiny2"><code>cointegrated/rubert-tiny2</code></a><a class="headerlink" href="#cointegratedrubert-tiny2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>29.4M</td>
<td>112.0 MB</td>
<td>2021-10-28</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="colbert-ircolbertv20"><a href="https://huggingface.co/colbert-ir/colbertv2.0"><code>colbert-ir/colbertv2.0</code></a><a class="headerlink" href="#colbert-ircolbertv20" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>180</td>
<td>not specified</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="consciousaicai-lunaris-text-embeddings"><a href="https://huggingface.co/consciousAI/cai-lunaris-text-embeddings"><code>consciousAI/cai-lunaris-text-embeddings</code></a><a class="headerlink" href="#consciousaicai-lunaris-text-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>not specified</td>
<td>2023-06-22</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="consciousaicai-stellaris-text-embeddings"><a href="https://huggingface.co/consciousAI/cai-stellaris-text-embeddings"><code>consciousAI/cai-stellaris-text-embeddings</code></a><a class="headerlink" href="#consciousaicai-stellaris-text-embeddings" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-06-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="cross-encoderms-marco-minilm-l12-v2"><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L12-v2"><code>cross-encoder/ms-marco-MiniLM-L12-v2</code></a><a class="headerlink" href="#cross-encoderms-marco-minilm-l12-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2021-04-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cross-encoderms-marco-minilm-l2-v2"><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L2-v2"><code>cross-encoder/ms-marco-MiniLM-L2-v2</code></a><a class="headerlink" href="#cross-encoderms-marco-minilm-l2-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>15.6M</td>
<td>60.0 MB</td>
<td>2021-04-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cross-encoderms-marco-minilm-l4-v2"><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L4-v2"><code>cross-encoder/ms-marco-MiniLM-L4-v2</code></a><a class="headerlink" href="#cross-encoderms-marco-minilm-l4-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>19.2M</td>
<td>73.0 MB</td>
<td>2021-04-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cross-encoderms-marco-minilm-l6-v2"><a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L6-v2"><code>cross-encoder/ms-marco-MiniLM-L6-v2</code></a><a class="headerlink" href="#cross-encoderms-marco-minilm-l6-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2021-04-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="cross-encoderms-marco-tinybert-l2-v2"><a href="https://huggingface.co/cross-encoder/ms-marco-TinyBERT-L2-v2"><code>cross-encoder/ms-marco-TinyBERT-L2-v2</code></a><a class="headerlink" href="#cross-encoderms-marco-tinybert-l2-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>128</td>
<td>4.4M</td>
<td>17.0 MB</td>
<td>2021-04-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deepfileembedder-100p"><a href="https://huggingface.co/deepfile/embedder-100p"><code>deepfile/embedder-100p</code></a><a class="headerlink" href="#deepfileembedder-100p" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2023-07-24</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="deepvkuser-bge-m3"><a href="https://huggingface.co/deepvk/USER-base"><code>deepvk/USER-bge-m3</code></a><a class="headerlink" href="#deepvkuser-bge-m3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>359.0M</td>
<td>1.3 GB</td>
<td>2024-07-05</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">deepvk2024user</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{USER: Universal Sentence Encoder for Russian}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Malashenko, Boris and  Zemerov, Anton and Spirin, Egor}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/datasets/deepvk/USER-base}</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="deepvkdeberta-v1-base"><a href="https://huggingface.co/deepvk/deberta-v1-base"><code>deepvk/deberta-v1-base</code></a><a class="headerlink" href="#deepvkdeberta-v1-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.0M</td>
<td>473.0 MB</td>
<td>2023-02-07</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="dmedhipawanembd-68m"><a href="https://huggingface.co/dmedhi/PawanEmbd-68M"><code>dmedhi/PawanEmbd-68M</code></a><a class="headerlink" href="#dmedhipawanembd-68m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>67.8M</td>
<td>260.0 MB</td>
<td>2025-12-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">medhi2025pawanembd</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="p">=</span><span class="s">{PawanEmbd-68M: Distilled Embedding Model}</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="p">=</span><span class="s">{Medhi, D.}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="p">=</span><span class="s">{https://huggingface.co/dmedhi/PawanEmbd-68M}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="dunzhangstella-large-zh-v3-1792d"><a href="https://huggingface.co/dunzhang/stella-large-zh-v3-1792d"><code>dunzhang/stella-large-zh-v3-1792d</code></a><a class="headerlink" href="#dunzhangstella-large-zh-v3-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>327.4M</td>
<td>not specified</td>
<td>2024-02-17</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dunzhangstella-mrl-large-zh-v35-1792d"><a href="https://huggingface.co/dunzhang/stella-large-zh-v3-1792d"><code>dunzhang/stella-mrl-large-zh-v3.5-1792d</code></a><a class="headerlink" href="#dunzhangstella-mrl-large-zh-v35-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-02-27</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="dwzhue5-base-4k"><a href="https://huggingface.co/dwzhu/e5-base-4k"><code>dwzhu/e5-base-4k</code></a><a class="headerlink" href="#dwzhue5-base-4k" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>not specified</td>
<td>112.2M</td>
<td>not specified</td>
<td>2024-03-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zhu2024longembed</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{LongEmbed: Extending Embedding Models for Long Context Retrieval}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2404.12096}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="facebooksonar"><a href="https://ai.meta.com/research/publications/sonar-sentence-level-multimodal-and-language-agnostic-representations/"><code>facebook/SONAR</code></a><a class="headerlink" href="#facebooksonar" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2021-05-21</td>
<td>ace-Arab, ace-Latn, acm-Arab, acq-Arab, aeb-Arab, ... (204)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Duquenne:2023:sonar_arxiv</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Paul-Ambroise Duquenne and Holger Schwenk and Benoit Sagot}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{{SONAR:} Sentence-Level Multimodal and Language-Agnostic Representations}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2308.11466}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="facebookcontriever-msmarco"><a href="https://huggingface.co/facebook/contriever-msmarco"><code>facebook/contriever-msmarco</code></a><a class="headerlink" href="#facebookcontriever-msmarco" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>572.0 MB</td>
<td>2022-06-25</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">izacard2021contriever</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Unsupervised Dense Information Retrieval with Contrastive Learning}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2112.09118}</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2112.09118}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="fangxqxyz-embedding"><a href="https://huggingface.co/fangxq/XYZ-embedding"><code>fangxq/XYZ-embedding</code></a><a class="headerlink" href="#fangxqxyz-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>325.5M</td>
<td>1.2 GB</td>
<td>2024-09-13</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="geoffseeauto-g-embed-st"><a href="https://huggingface.co/geoffsee/auto-g-embed-st"><code>geoffsee/auto-g-embed-st</code></a><a class="headerlink" href="#geoffseeauto-g-embed-st" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2026-02-08</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="googleflan-t5-base"><a href="https://huggingface.co/google/flan-t5-base"><code>google/flan-t5-base</code></a><a class="headerlink" href="#googleflan-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>247.6M</td>
<td>944.0 MB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-large"><a href="https://huggingface.co/google/flan-t5-large"><code>google/flan-t5-large</code></a><a class="headerlink" href="#googleflan-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>783.2M</td>
<td>2.9 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-xl">google/flan-t5-xl<a class="headerlink" href="#googleflan-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>10.6 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="googleflan-t5-xxl">google/flan-t5-xxl<a class="headerlink" href="#googleflan-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>42.0 GB</td>
<td>2022-10-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">10.48550/arxiv.2210.11416</span><span class="p">,</span>
<span class="w">      </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.48550/ARXIV.2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2210.11416}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}</span><span class="p">,</span>
<span class="w">      </span><span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Scaling Instruction-Finetuned Language Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">copyright</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Creative Commons Attribution 4.0 International}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="hiieuhalong_embedding"><a href="https://huggingface.co/hiieu/halong_embedding"><code>hiieu/halong_embedding</code></a><a class="headerlink" href="#hiieuhalong_embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2024-07-06</td>
<td>vie-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">HalongEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{HalongEmbedding: A Vietnamese Text Embedding}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Ngo Hieu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="p">=</span><span class="s">{Huggingface}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="iampandazpoint_large_embedding_zh"><a href="https://huggingface.co/iampanda/zpoint_large_embedding_zh"><code>iampanda/zpoint_large_embedding_zh</code></a><a class="headerlink" href="#iampandazpoint_large_embedding_zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-06-04</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="ibm-granitegranite-embedding-107m-multilingual"><a href="https://huggingface.co/ibm-granite/granite-embedding-107m-multilingual"><code>ibm-granite/granite-embedding-107m-multilingual</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-107m-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>107.0M</td>
<td>204.0 MB</td>
<td>2024-12-18</td>
<td>ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-125m-english"><a href="https://huggingface.co/ibm-granite/granite-embedding-125m-english"><code>ibm-granite/granite-embedding-125m-english</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-125m-english" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.6M</td>
<td>238.0 MB</td>
<td>2024-12-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-278m-multilingual"><a href="https://huggingface.co/ibm-granite/granite-embedding-278m-multilingual"><code>ibm-granite/granite-embedding-278m-multilingual</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-278m-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>530.0 MB</td>
<td>2024-12-18</td>
<td>ara-Latn, ces-Latn, deu-Latn, eng-Latn, fra-Latn, ... (13)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-30m-english"><a href="https://huggingface.co/ibm-granite/granite-embedding-30m-english"><code>ibm-granite/granite-embedding-30m-english</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-30m-english" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>30.3M</td>
<td>58.0 MB</td>
<td>2024-12-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-english-r2"><a href="https://huggingface.co/ibm-granite/granite-embedding-english-r2"><code>ibm-granite/granite-embedding-english-r2</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-english-r2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>149.0M</td>
<td>284.0 MB</td>
<td>2025-08-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="ibm-granitegranite-embedding-small-english-r2"><a href="https://huggingface.co/ibm-granite/granite-embedding-small-english-r2"><code>ibm-granite/granite-embedding-small-english-r2</code></a><a class="headerlink" href="#ibm-granitegranite-embedding-small-english-r2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>384</td>
<td>47.7M</td>
<td>91.0 MB</td>
<td>2025-08-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">awasthy2025graniteembedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Granite Embedding Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Awasthy, Parul and Trivedi, Aashka and Li, Yulong and Bornea, Mihaela and Cox, David and Daniels, Abraham and Franz, Martin and Goodhart, Gabe and Iyer, Bhavani and Kumar, Vishwajeet and Lastras, Luis and McCarley, Scott and Murthy, Rudra and P, Vignesh and Rosenthal, Sara and Roukos, Salim and Sen, Jaydeep and Sharma, Sukriti and Sil, Avirup and Soule, Kate and Sultan, Arafat and Florian, Radu}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2502.20204}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="infgradstella-base-en-v2"><a href="https://huggingface.co/infgrad/stella-base-en-v2"><code>infgrad/stella-base-en-v2</code></a><a class="headerlink" href="#infgradstella-base-en-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>not specified</td>
<td>109.5M</td>
<td>not specified</td>
<td>2023-10-19</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="infgradstella-base-zh-v3-1792d"><a href="https://huggingface.co/infgrad/stella-base-zh-v3-1792d"><code>infgrad/stella-base-zh-v3-1792d</code></a><a class="headerlink" href="#infgradstella-base-zh-v3-1792d" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>104.0M</td>
<td>not specified</td>
<td>2024-02-17</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="izhxudever-bloom-1b1"><a href="https://huggingface.co/izhx/udever-bloom-1b1"><code>izhx/udever-bloom-1b1</code></a><a class="headerlink" href="#izhxudever-bloom-1b1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>1.1B</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2023language</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Language Models are Universal Embedders}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2310.08232}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="izhxudever-bloom-3b"><a href="https://huggingface.co/izhx/udever-bloom-3b"><code>izhx/udever-bloom-3b</code></a><a class="headerlink" href="#izhxudever-bloom-3b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>3.0B</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2023language</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Language Models are Universal Embedders}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2310.08232}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="izhxudever-bloom-560m"><a href="https://huggingface.co/izhx/udever-bloom-560m"><code>izhx/udever-bloom-560m</code></a><a class="headerlink" href="#izhxudever-bloom-560m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>559.2M</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2023language</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Language Models are Universal Embedders}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2310.08232}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="izhxudever-bloom-7b1"><a href="https://huggingface.co/izhx/udever-bloom-7b1"><code>izhx/udever-bloom-7b1</code></a><a class="headerlink" href="#izhxudever-bloom-7b1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/spaces/bigscience/license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-10-24</td>
<td>aka-Latn, ara-Arab, asm-Beng, bam-Latn, ben-Beng, ... (45)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">zhang2023language</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Language Models are Universal Embedders}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Zhang, Xin and Li, Zehan and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan and Zhang, Min}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2310.08232}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jhu-clspfollowir-7b"><a href="https://huggingface.co/jhu-clsp/FollowIR-7B"><code>jhu-clsp/FollowIR-7B</code></a><a class="headerlink" href="#jhu-clspfollowir-7b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>7.1B</td>
<td>13.5 GB</td>
<td>2024-04-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">weller2024followir</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Orion Weller and Benjamin Chang and Sean MacAvaney and Kyle Lo and Arman Cohan and Benjamin Van Durme and Dawn Lawrie and Luca Soldaini}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2403.15246}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-colbert-v2"><a href="https://huggingface.co/jinaai/jina-colbert-v2"><code>jinaai/jina-colbert-v2</code></a><a class="headerlink" href="#jinaaijina-colbert-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>not specified</td>
<td>559.4M</td>
<td>1.0 GB</td>
<td>2024-08-16</td>
<td>ara-Arab, ben-Beng, deu-Latn, eng-Latn, fas-Arab, ... (22)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xiao-etal-2024-jina</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{J}ina-{C}ol{BERT}-v2: A General-Purpose Multilingual Late Interaction Retriever&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Jha, Rohan  and</span>
<span class="s">      Wang, Bo  and</span>
<span class="s">      G{&quot;u}nther, Michael  and</span>
<span class="s">      Mastrapas, Georgios  and</span>
<span class="s">      Sturua, Saba  and</span>
<span class="s">      Mohr, Isabelle  and</span>
<span class="s">      Koukounas, Andreas  and</span>
<span class="s">      Wang, Mohammad Kalim  and</span>
<span class="s">      Wang, Nan  and</span>
<span class="s">      Xiao, Han}</span><span class="p">,</span>
<span class="w">    </span><span class="na">editor</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{S{&quot;a}lev{&quot;a}, Jonne  and</span>
<span class="s">      Owodunni, Abraham}</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nv">nov</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2024&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Miami, Florida, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2024.mrl-1.11/&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.18653/v1/2024.mrl-1.11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;159--166&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">abstract</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT`s late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce a novel architecture and a training framework to support long context window and multilingual retrieval. Leveraging Matryoshka Representation Loss, we further demonstrate that the reducing the embedding dimensionality from 128 to 64 has insignificant impact on the model`s retrieval performance and cut storage requirements by up to 50{\%}. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks,&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embedding-b-en-v1"><a href="https://huggingface.co/jinaai/jina-embedding-b-en-v1"><code>jinaai/jina-embedding-b-en-v1</code></a><a class="headerlink" href="#jinaaijina-embedding-b-en-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.6M</td>
<td>420.0 MB</td>
<td>2023-07-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">gnther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Gnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.11224}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embedding-s-en-v1"><a href="https://huggingface.co/jinaai/jina-embedding-s-en-v1"><code>jinaai/jina-embedding-s-en-v1</code></a><a class="headerlink" href="#jinaaijina-embedding-s-en-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>35.3M</td>
<td>134.0 MB</td>
<td>2023-07-07</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">gnther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Gnther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.11224}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v2-base-en"><a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en"><code>jinaai/jina-embeddings-v2-base-en</code></a><a class="headerlink" href="#jinaaijina-embeddings-v2-base-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>137.4M</td>
<td>262.0 MB</td>
<td>2023-09-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">gnther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Gnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.19923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-embeddings-v2-small-en"><a href="https://huggingface.co/jinaai/jina-embeddings-v2-small-en"><code>jinaai/jina-embeddings-v2-small-en</code></a><a class="headerlink" href="#jinaaijina-embeddings-v2-small-en" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>32.7M</td>
<td>62.0 MB</td>
<td>2023-09-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">gnther2023jina</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Michael Gnther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.19923}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="jinaaijina-reranker-v2-base-multilingual">jinaai/jina-reranker-v2-base-multilingual<a class="headerlink" href="#jinaaijina-reranker-v2-base-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>531.0 MB</td>
<td>2024-09-26</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="jinaaijina-reranker-v3"><a href="https://huggingface.co/jinaai/jina-reranker-v3"><code>jinaai/jina-reranker-v3</code></a><a class="headerlink" href="#jinaaijina-reranker-v3" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> cc-by-nc-4.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>131.1K</td>
<td>not specified</td>
<td>596.8M</td>
<td>1.1 GB</td>
<td>2025-09-18</td>
<td>afr-Latn, ara-Arab, aze-Latn, bel-Cyrl, ben-Beng, ... (73)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">wang2025jinarerankerv3lateinteractionlistwise</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{jina-reranker-v3: Last but Not Late Interaction for Listwise Document Reranking}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Feng Wang and Yuqing Li and Han Xiao}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2509.25085}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2509.25085}</span><span class="p">,}</span>
</code></pre></div>
</details>
<h4 id="keeeeenwmicrollama-text-embedding"><a href="https://huggingface.co/keeeeenw/MicroLlama-text-embedding"><code>keeeeenw/MicroLlama-text-embedding</code></a><a class="headerlink" href="#keeeeenwmicrollama-text-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>1024</td>
<td>271.9M</td>
<td>1.0 GB</td>
<td>2024-11-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="lier007xiaobu-embedding"><a href="https://huggingface.co/lier007/xiaobu-embedding"><code>lier007/xiaobu-embedding</code></a><a class="headerlink" href="#lier007xiaobu-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-01-09</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="lier007xiaobu-embedding-v2"><a href="https://huggingface.co/lier007/xiaobu-embedding-v2"><code>lier007/xiaobu-embedding-v2</code></a><a class="headerlink" href="#lier007xiaobu-embedding-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2024-06-30</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="lightonaigte-moderncolbert-v1"><a href="https://huggingface.co/lightonai/GTE-ModernColBERT-v1"><code>lightonai/GTE-ModernColBERT-v1</code></a><a class="headerlink" href="#lightonaigte-moderncolbert-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>not specified</td>
<td>149.0M</td>
<td>not specified</td>
<td>2025-04-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="llmrailsember-v1"><a href="https://huggingface.co/llmrails/ember-v1"><code>llmrails/ember-v1</code></a><a class="headerlink" href="#llmrailsember-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.0M</td>
<td>1.2 GB</td>
<td>2023-10-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nur2024emberv1</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{ember-v1: SOTA embedding model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Enrike Nur and Anar Aliyev}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="m3hrdadfibert-zwnj-wnli-mean-tokens"><a href="https://huggingface.co/m3hrdadfi/bert-zwnj-wnli-mean-tokens"><code>m3hrdadfi/bert-zwnj-wnli-mean-tokens</code></a><a class="headerlink" href="#m3hrdadfibert-zwnj-wnli-mean-tokens" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>118.3M</td>
<td>451.0 MB</td>
<td>2021-06-28</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="m3hrdadfiroberta-zwnj-wnli-mean-tokens"><a href="https://huggingface.co/m3hrdadfi/roberta-zwnj-wnli-mean-tokens"><code>m3hrdadfi/roberta-zwnj-wnli-mean-tokens</code></a><a class="headerlink" href="#m3hrdadfiroberta-zwnj-wnli-mean-tokens" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>118.3M</td>
<td>451.0 MB</td>
<td>2021-06-28</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="malenia1ternary-weight-embedding"><a href="https://huggingface.co/malenia1/ternary-weight-embedding"><code>malenia1/ternary-weight-embedding</code></a><a class="headerlink" href="#malenia1ternary-weight-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>98.7M</td>
<td>158.0 MB</td>
<td>2024-10-23</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manubge-m3-custom-fr"><a href="https://huggingface.co/manu/bge-m3-custom-fr"><code>manu/bge-m3-custom-fr</code></a><a class="headerlink" href="#manubge-m3-custom-fr" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1024</td>
<td>567.8M</td>
<td>2.1 GB</td>
<td>2024-04-11</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v02"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.2"><code>manu/sentence_croissant_alpha_v0.2</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v02" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-03-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v03"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.3"><code>manu/sentence_croissant_alpha_v0.3</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v03" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-04-26</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="manusentence_croissant_alpha_v04"><a href="https://huggingface.co/manu/sentence_croissant_alpha_v0.4"><code>manu/sentence_croissant_alpha_v0.4</code></a><a class="headerlink" href="#manusentence_croissant_alpha_v04" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>2048</td>
<td>1.3B</td>
<td>2.4 GB</td>
<td>2024-04-27</td>
<td>eng-Latn, fra-Latn</td>
</tr>
</tbody>
</table>
<h4 id="meta-llamallama-2-7b-chat-hf"><a href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf"><code>meta-llama/Llama-2-7b-chat-hf</code></a><a class="headerlink" href="#meta-llamallama-2-7b-chat-hf" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>7.0B</td>
<td>not specified</td>
<td>2023-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">touvron2023llama2openfoundation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Llama 2: Open Foundation and Fine-Tuned Chat Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.09288}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2307.09288}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="meta-llamallama-2-7b-hf">meta-llama/Llama-2-7b-hf<a class="headerlink" href="#meta-llamallama-2-7b-hf" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2023-07-18</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">touvron2023llama2openfoundation</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Llama 2: Open Foundation and Fine-Tuned Chat Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2307.09288}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2307.09288}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="microsoftspeecht5_tts"><a href="https://huggingface.co/microsoft/speecht5_tts"><code>microsoft/speecht5_tts</code></a><a class="headerlink" href="#microsoftspeecht5_tts" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>768</td>
<td>146.3M</td>
<td>558.0 MB</td>
<td>2022-05-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ao2022speecht5unifiedmodalencoderdecoderpretraining</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Junyi Ao and Rui Wang and Long Zhou and Chengyi Wang and Shuo Ren and Yu Wu and Shujie Liu and Tom Ko and Qing Li and Yu Zhang and Zhihua Wei and Yao Qian and Jinyu Li and Furu Wei}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2110.07205}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{eess.AS}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2110.07205}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_glove"><a href="https://huggingface.co/minishlab/M2V_base_glove"><code>minishlab/M2V_base_glove</code></a><a class="headerlink" href="#minishlabm2v_base_glove" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>102.0M</td>
<td>391.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_glove_subword"><a href="https://huggingface.co/minishlab/M2V_base_glove_subword"><code>minishlab/M2V_base_glove_subword</code></a><a class="headerlink" href="#minishlabm2v_base_glove_subword" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>103.0M</td>
<td>391.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_base_output"><a href="https://huggingface.co/minishlab/M2V_base_output"><code>minishlab/M2V_base_output</code></a><a class="headerlink" href="#minishlabm2v_base_output" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.6M</td>
<td>29.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabm2v_multilingual_output"><a href="https://huggingface.co/minishlab/M2V_multilingual_output"><code>minishlab/M2V_multilingual_output</code></a><a class="headerlink" href="#minishlabm2v_multilingual_output" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2024-09-21</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-2m"><a href="https://huggingface.co/minishlab/potion-base-2M"><code>minishlab/potion-base-2M</code></a><a class="headerlink" href="#minishlabpotion-base-2m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>64</td>
<td>2.0M</td>
<td>7.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-4m"><a href="https://huggingface.co/minishlab/potion-base-4M"><code>minishlab/potion-base-4M</code></a><a class="headerlink" href="#minishlabpotion-base-4m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>128</td>
<td>3.8M</td>
<td>14.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-base-8m"><a href="https://huggingface.co/minishlab/potion-base-8M"><code>minishlab/potion-base-8M</code></a><a class="headerlink" href="#minishlabpotion-base-8m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>7.6M</td>
<td>29.0 MB</td>
<td>2024-10-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="minishlabpotion-multilingual-128m"><a href="https://huggingface.co/minishlab/potion-multilingual-128M"><code>minishlab/potion-multilingual-128M</code></a><a class="headerlink" href="#minishlabpotion-multilingual-128m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>128.0M</td>
<td>489.0 MB</td>
<td>2025-05-23</td>
<td>afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (101)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">      </span><span class="na">authors</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Stephan Tulkens, Thomas van Dongen}</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Turn any Sentence Transformer into a Small Fast Model}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mistralaimistral-7b-instruct-v02"><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2"><code>mistralai/Mistral-7B-Instruct-v0.2</code></a><a class="headerlink" href="#mistralaimistral-7b-instruct-v02" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>7.2B</td>
<td>not specified</td>
<td>2023-12-11</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">jiang2023mistral7b</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Mistral 7B}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Llio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothe Lacroix and William El Sayed}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2310.06825}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2310.06825}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-edge-colbert-v0-17m"><a href="https://huggingface.co/mixedbread-ai/mxbai-edge-colbert-v0-17m"><code>mixedbread-ai/mxbai-edge-colbert-v0-17m</code></a><a class="headerlink" href="#mixedbread-aimxbai-edge-colbert-v0-17m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.0K</td>
<td>not specified</td>
<td>17.0M</td>
<td>64.0 MB</td>
<td>2025-10-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">takehi2025fantasticsmallretrieverstrain</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rikiya Takehi and Benjamin Clavi and Sean Lee and Aamir Shakir}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2510.14880}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2510.14880}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-edge-colbert-v0-32m"><a href="https://huggingface.co/mixedbread-ai/mxbai-edge-colbert-v0-32m"><code>mixedbread-ai/mxbai-edge-colbert-v0-32m</code></a><a class="headerlink" href="#mixedbread-aimxbai-edge-colbert-v0-32m" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>511</td>
<td>not specified</td>
<td>32.0M</td>
<td>122.0 MB</td>
<td>2025-10-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">takehi2025fantasticsmallretrieverstrain</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Rikiya Takehi and Benjamin Clavi and Sean Lee and Aamir Shakir}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2510.14880}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2510.14880}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-rerank-base-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-rerank-base-v1"><code>mixedbread-ai/mxbai-rerank-base-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-rerank-base-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>not specified</td>
<td>184.4M</td>
<td>352.0 MB</td>
<td>2024-02-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@online</span><span class="p">{</span><span class="nl">rerank2024mxbai</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Boost Your Search With The Crispy Mixedbread Rerank Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-rerank-v1}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-rerank-large-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-rerank-large-v1"><code>mixedbread-ai/mxbai-rerank-large-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-rerank-large-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>not specified</td>
<td>435.1M</td>
<td>830.0 MB</td>
<td>2024-02-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@online</span><span class="p">{</span><span class="nl">rerank2024mxbai</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Boost Your Search With The Crispy Mixedbread Rerank Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-rerank-v1}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mixedbread-aimxbai-rerank-xsmall-v1"><a href="https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1"><code>mixedbread-ai/mxbai-rerank-xsmall-v1</code></a><a class="headerlink" href="#mixedbread-aimxbai-rerank-xsmall-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>not specified</td>
<td>70.8M</td>
<td>135.0 MB</td>
<td>2024-02-29</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@online</span><span class="p">{</span><span class="nl">rerank2024mxbai</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Boost Your Search With The Crispy Mixedbread Rerank Models}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Aamir Shakir and Darius Koenig and Julius Lipp and Sean Lee}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://www.mixedbread.ai/blog/mxbai-rerank-v1}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-base"><a href="https://huggingface.co/moka-ai/m3e-base"><code>moka-ai/m3e-base</code></a><a class="headerlink" href="#moka-aim3e-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>102.3M</td>
<td>390.0 MB</td>
<td>2023-06-06</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-large"><a href="https://huggingface.co/moka-ai/m3e-large"><code>moka-ai/m3e-large</code></a><a class="headerlink" href="#moka-aim3e-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>325.5M</td>
<td>not specified</td>
<td>2023-06-21</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="moka-aim3e-small"><a href="https://huggingface.co/moka-ai/m3e-small"><code>moka-ai/m3e-small</code></a><a class="headerlink" href="#moka-aim3e-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/moka-ai/m3e-base#%F0%9F%93%9C-license</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>512</td>
<td>24.0M</td>
<td>not specified</td>
<td>2023-06-02</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">MokaMassiveMixedEmbedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Wang Yuxin and Sun Qingxuan and He Sicheng}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{M3E: Moka Massive Mixed Embedding Model}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mtebbaseline-bb25"><a href="https://github.com/instructkr/bb25"><code>mteb/baseline-bb25</code></a><a class="headerlink" href="#mtebbaseline-bb25" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2026-02-06</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">jeong2026bayesianbm25</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Bayesian BM25: A Probabilistic Framework for Hybrid Text and Vector Search}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Jeong, Jaepil}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="p">=</span><span class="s">{10.5281/zenodo.18414941}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://doi.org/10.5281/zenodo.18414941}</span><span class="p">,</span>
<span class="p">}</span>
<span class="nc">@software</span><span class="p">{</span><span class="nl">jeong2026neural</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{From Bayesian Inference to Neural Computation: The Analytical Emergence of Neural Network Structure from Probabilistic Relevance Estimation}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Jeong, Jaepil}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2026}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="p">=</span><span class="s">{10.5281/zenodo.18512411}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="p">=</span><span class="s">{https://doi.org/10.5281/zenodo.18512411}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="mtebbaseline-bm25s"><a href="https://github.com/xhluca/bm25s"><code>mteb/baseline-bm25s</code></a><a class="headerlink" href="#mtebbaseline-bm25s" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-07-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">bm25s</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{BM25S: Orders of magnitude faster lexical search via eager sparse scoring}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Xing Han L}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2407.03618}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2407.03618}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="myrkursentence-transformer-parsbert-fa"><a href="https://huggingface.co/myrkur/sentence-transformer-parsbert-fa"><code>myrkur/sentence-transformer-parsbert-fa</code></a><a class="headerlink" href="#myrkursentence-transformer-parsbert-fa" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>162.8M</td>
<td>621.0 MB</td>
<td>2024-12-10</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<h4 id="nvidiallama-nemotron-rerank-1b-v2"><a href="https://huggingface.co/nvidia/llama-nemotron-rerank-1b-v2"><code>nvidia/llama-nemotron-rerank-1b-v2</code></a><a class="headerlink" href="#nvidiallama-nemotron-rerank-1b-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>2048</td>
<td>1.2B</td>
<td>2.3 GB</td>
<td>2025-10-16</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="omarelshehyarabic-english-sts-matryoshka"><a href="https://huggingface.co/omarelshehy/arabic-english-sts-matryoshka"><code>omarelshehy/arabic-english-sts-matryoshka</code></a><a class="headerlink" href="#omarelshehyarabic-english-sts-matryoshka" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2024-10-13</td>
<td>ara-Arab, eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="openaitext-embedding-3-large"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-large</code></a><a class="headerlink" href="#openaitext-embedding-3-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>3072</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-large-embed_dim512"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-large (embed_dim=512)</code></a><a class="headerlink" href="#openaitext-embedding-3-large-embed_dim512" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-small"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-small</code></a><a class="headerlink" href="#openaitext-embedding-3-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-3-small-embed_dim512"><a href="https://openai.com/index/new-embedding-models-and-api-updates/"><code>openai/text-embedding-3-small (embed_dim=512)</code></a><a class="headerlink" href="#openaitext-embedding-3-small-embed_dim512" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>512</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-01-25</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openaitext-embedding-ada-002"><a href="https://openai.com/index/new-and-improved-embedding-model/"><code>openai/text-embedding-ada-002</code></a><a class="headerlink" href="#openaitext-embedding-ada-002" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1536</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-12-15</td>
<td>not specified</td>
</tr>
</tbody>
</table>
<h4 id="openbmbminicpm-embedding"><a href="https://huggingface.co/openbmb/MiniCPM-Embedding"><code>openbmb/MiniCPM-Embedding</code></a><a class="headerlink" href="#openbmbminicpm-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>2304</td>
<td>2.7B</td>
<td>5.1 GB</td>
<td>2024-09-04</td>
<td>eng-Latn, zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="panalexeuxlm-roberta-ua-distilled"><a href="https://github.com/panalexeu/xlm-roberta-ua-distilled/tree/main"><code>panalexeu/xlm-roberta-ua-distilled</code></a><a class="headerlink" href="#panalexeuxlm-roberta-ua-distilled" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2025-04-15</td>
<td>eng-Latn, ukr-Cyrl</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="prdevmini-gte"><a href="https://huggingface.co/prdev/mini-gte"><code>prdev/mini-gte</code></a><a class="headerlink" href="#prdevmini-gte" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>66.4M</td>
<td>253.0 MB</td>
<td>2025-01-28</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="rasgaardm2v-dfm-large"><a href="https://huggingface.co/rasgaard/m2v-dfm-large"><code>rasgaard/m2v-dfm-large</code></a><a class="headerlink" href="#rasgaardm2v-dfm-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>256</td>
<td>22.9M</td>
<td>87.0 MB</td>
<td>2025-10-08</td>
<td>dan-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">minishlab2024model2vec</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Tulkens, Stephan and {van Dongen}, Thomas}</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Model2Vec: Fast State-of-the-Art Static Embeddings}</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/MinishLab/model2vec}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="richinfoairitrieve_zh_v1"><a href="https://huggingface.co/richinfoai/ritrieve_zh_v1"><code>richinfoai/ritrieve_zh_v1</code></a><a class="headerlink" href="#richinfoairitrieve_zh_v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1792</td>
<td>326.0M</td>
<td>1.2 GB</td>
<td>2025-03-25</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="sbintuitionssarashina-embedding-v1-1b"><a href="https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b"><code>sbintuitions/sarashina-embedding-v1-1b</code></a><a class="headerlink" href="#sbintuitionssarashina-embedding-v1-1b" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> https://huggingface.co/sbintuitions/sarashina-embedding-v1-1b/blob/main/LICENSE</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>1792</td>
<td>1.2B</td>
<td>4.6 GB</td>
<td>2024-11-22</td>
<td>jpn-Jpan</td>
</tr>
</tbody>
</table>
<h4 id="sbunlpfabert"><a href="https://huggingface.co/sbunlp/fabert"><code>sbunlp/fabert</code></a><a class="headerlink" href="#sbunlpfabert" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2024-10-07</td>
<td>fas-Arab</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">masumi-etal-2025-fabert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;{F}a{BERT}: Pre-training {BERT} on {P}ersian Blogs&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Masumi, Mostafa  and</span>
<span class="s">      Majd, Seyed Soroush  and</span>
<span class="s">      Shamsfard, Mehrnoush  and</span>
<span class="s">      Beigy, Hamid&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">editor</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Bak, JinYeong  and</span>
<span class="s">      Goot, Rob van der  and</span>
<span class="s">      Jang, Hyeju  and</span>
<span class="s">      Buaphet, Weerayut  and</span>
<span class="s">      Ramponi, Alan  and</span>
<span class="s">      Xu, Wei  and</span>
<span class="s">      Ritter, Alan&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the Tenth Workshop on Noisy and User-generated Text&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nv">may</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2025&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Albuquerque, New Mexico, USA&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://aclanthology.org/2025.wnut-1.10/&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;10.18653/v1/2025.wnut-1.10&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;85--96&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">ISBN</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;979-8-89176-232-9&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sdadasmmlw-e5-base"><a href="https://huggingface.co/sdadas/mmlw-e5-base"><code>sdadas/mmlw-e5-base</code></a><a class="headerlink" href="#sdadasmmlw-e5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dadas2024pirb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sawomir Dadas and Micha Perekiewicz and Rafa Powiata}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13350}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sdadasmmlw-e5-large"><a href="https://huggingface.co/sdadas/mmlw-e5-large"><code>sdadas/mmlw-e5-large</code></a><a class="headerlink" href="#sdadasmmlw-e5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>559.9M</td>
<td>2.1 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dadas2024pirb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sawomir Dadas and Micha Perekiewicz and Rafa Powiata}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13350}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sdadasmmlw-e5-small"><a href="https://huggingface.co/sdadas/mmlw-e5-small"><code>sdadas/mmlw-e5-small</code></a><a class="headerlink" href="#sdadasmmlw-e5-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dadas2024pirb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sawomir Dadas and Micha Perekiewicz and Rafa Powiata}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13350}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sdadasmmlw-roberta-base"><a href="https://huggingface.co/sdadas/mmlw-roberta-base"><code>sdadas/mmlw-roberta-base</code></a><a class="headerlink" href="#sdadasmmlw-roberta-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>768</td>
<td>124.4M</td>
<td>475.0 MB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dadas2024pirb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sawomir Dadas and Micha Perekiewicz and Rafa Powiata}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13350}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sdadasmmlw-roberta-large"><a href="https://huggingface.co/sdadas/mmlw-roberta-large"><code>sdadas/mmlw-roberta-large</code></a><a class="headerlink" href="#sdadasmmlw-roberta-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>514</td>
<td>1024</td>
<td>435.0M</td>
<td>1.6 GB</td>
<td>2023-11-17</td>
<td>pol-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">dadas2024pirb</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{{PIRB}: A Comprehensive Benchmark of Polish Dense and Hybrid Text Retrieval Methods}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Sawomir Dadas and Micha Perekiewicz and Rafa Powiata}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13350}</span><span class="p">,</span>
<span class="w">  </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sensenovapiccolo-base-zh"><a href="https://huggingface.co/sensenova/piccolo-base-zh"><code>sensenova/piccolo-base-zh</code></a><a class="headerlink" href="#sensenovapiccolo-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>102.3M</td>
<td>not specified</td>
<td>2023-09-04</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<h4 id="sensenovapiccolo-large-zh-v2"><a href="https://huggingface.co/sensenova/piccolo-large-zh-v2"><code>sensenova/piccolo-large-zh-v2</code></a><a class="headerlink" href="#sensenovapiccolo-large-zh-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>not specified</td>
<td>not specified</td>
<td>2024-04-22</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">2405.06932</span><span class="p">,</span>
<span class="w">    </span><span class="na">Author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Junqin Huang and Zhongjie Hu and Zihao Jing and Mengya Gao and Yichao Wu}</span><span class="p">,</span>
<span class="w">    </span><span class="na">Title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Piccolo2: General Text Embedding with Multi-task Hybrid Loss Training}</span><span class="p">,</span>
<span class="w">    </span><span class="na">Year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">    </span><span class="na">Eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv:2405.06932}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerslabse"><a href="https://huggingface.co/sentence-transformers/LaBSE"><code>sentence-transformers/LaBSE</code></a><a class="headerlink" href="#sentence-transformerslabse" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>470.9M</td>
<td>1.8 GB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">feng2022languageagnosticbertsentenceembedding</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Language-agnostic BERT Sentence Embedding}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2007.01852}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2007.01852}</span><span class="p">,</span>
<span class="w">    </span><span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-minilm-l12-v2"><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2"><code>sentence-transformers/all-MiniLM-L12-v2</code></a><a class="headerlink" href="#sentence-transformersall-minilm-l12-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>33.4M</td>
<td>127.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-minilm-l6-v2"><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"><code>sentence-transformers/all-MiniLM-L6-v2</code></a><a class="headerlink" href="#sentence-transformersall-minilm-l6-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersall-mpnet-base-v2"><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2"><code>sentence-transformers/all-mpnet-base-v2</code></a><a class="headerlink" href="#sentence-transformersall-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>384</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersgtr-t5-base"><a href="https://huggingface.co/sentence-transformers/gtr-t5-base"><code>sentence-transformers/gtr-t5-base</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>110.2M</td>
<td>209.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021largedualencodersgeneralizable</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Large Dual Encoders Are Generalizable Retrievers}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hernndez brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2112.07899}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2112.07899}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersgtr-t5-large"><a href="https://huggingface.co/sentence-transformers/gtr-t5-large"><code>sentence-transformers/gtr-t5-large</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.7M</td>
<td>639.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021largedualencodersgeneralizable</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Large Dual Encoders Are Generalizable Retrievers}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hernndez brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2112.07899}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2112.07899}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersgtr-t5-xl"><a href="https://huggingface.co/sentence-transformers/gtr-t5-xl"><code>sentence-transformers/gtr-t5-xl</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>1.2B</td>
<td>2.3 GB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021largedualencodersgeneralizable</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Large Dual Encoders Are Generalizable Retrievers}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hernndez brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2112.07899}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2112.07899}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersgtr-t5-xxl"><a href="https://huggingface.co/sentence-transformers/gtr-t5-xxl"><code>sentence-transformers/gtr-t5-xxl</code></a><a class="headerlink" href="#sentence-transformersgtr-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>4.9B</td>
<td>9.1 GB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021largedualencodersgeneralizable</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Large Dual Encoders Are Generalizable Retrievers}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hernndez brego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2112.07899}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.IR}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2112.07899}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersmulti-qa-minilm-l6-cos-v1"><a href="https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1"><code>sentence-transformers/multi-qa-MiniLM-L6-cos-v1</code></a><a class="headerlink" href="#sentence-transformersmulti-qa-minilm-l6-cos-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>22.7M</td>
<td>87.0 MB</td>
<td>2021-08-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersmulti-qa-mpnet-base-dot-v1"><a href="https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1"><code>sentence-transformers/multi-qa-mpnet-base-dot-v1</code></a><a class="headerlink" href="#sentence-transformersmulti-qa-mpnet-base-dot-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>418.0 MB</td>
<td>2021-08-23</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersparaphrase-multilingual-minilm-l12-v2"><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"><code>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</code></a><a class="headerlink" href="#sentence-transformersparaphrase-multilingual-minilm-l12-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersparaphrase-multilingual-mpnet-base-v2"><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2"><code>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</code></a><a class="headerlink" href="#sentence-transformersparaphrase-multilingual-mpnet-base-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>278.0M</td>
<td>1.0 GB</td>
<td>2019-11-01</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (53)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;http://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerssentence-t5-base"><a href="https://huggingface.co/sentence-transformers/sentence-t5-base"><code>sentence-transformers/sentence-t5-base</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>222.9M</td>
<td>209.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021sentencet5scalablesentenceencoders</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Gustavo Hernndez brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2108.08877}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2108.08877}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerssentence-t5-large"><a href="https://huggingface.co/sentence-transformers/sentence-t5-large"><code>sentence-transformers/sentence-t5-large</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>335.7M</td>
<td>639.0 MB</td>
<td>2022-02-09</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021sentencet5scalablesentenceencoders</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Gustavo Hernndez brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2108.08877}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2108.08877}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerssentence-t5-xl"><a href="https://huggingface.co/sentence-transformers/sentence-t5-xl"><code>sentence-transformers/sentence-t5-xl</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-xl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>1.2B</td>
<td>2.3 GB</td>
<td>2024-03-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021sentencet5scalablesentenceencoders</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Gustavo Hernndez brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2108.08877}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2108.08877}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformerssentence-t5-xxl"><a href="https://huggingface.co/sentence-transformers/sentence-t5-xxl"><code>sentence-transformers/sentence-t5-xxl</code></a><a class="headerlink" href="#sentence-transformerssentence-t5-xxl" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>4.9B</td>
<td>9.1 GB</td>
<td>2024-03-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">ni2021sentencet5scalablesentenceencoders</span><span class="p">,</span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}</span><span class="p">,</span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Jianmo Ni and Gustavo Hernndez brego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang}</span><span class="p">,</span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2021}</span><span class="p">,</span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{2108.08877}</span><span class="p">,</span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span><span class="p">,</span>
<span class="w">      </span><span class="na">url</span><span class="p">=</span><span class="s">{https://arxiv.org/abs/2108.08877}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sentence-transformersstatic-retrieval-mrl-en-v1"><a href="https://huggingface.co/sentence-transformers/static-retrieval-mrl-en-v1"><code>sentence-transformers/static-retrieval-mrl-en-v1</code></a><a class="headerlink" href="#sentence-transformersstatic-retrieval-mrl-en-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>infP</td>
<td>1024</td>
<td>31.3M</td>
<td>119.0 MB</td>
<td>2024-10-24</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<h4 id="sentence-transformersstatic-similarity-mrl-multilingual-v1"><a href="https://huggingface.co/sentence-transformers/static-similarity-mrl-multilingual-v1"><code>sentence-transformers/static-similarity-mrl-multilingual-v1</code></a><a class="headerlink" href="#sentence-transformersstatic-similarity-mrl-multilingual-v1" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>1024</td>
<td>108.4M</td>
<td>413.0 MB</td>
<td>2025-01-15</td>
<td>ara-Arab, bul-Cyrl, cat-Latn, ces-Latn, dan-Latn, ... (49)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">reimers-2019-sentence-bert</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Reimers, Nils and Gurevych, Iryna&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">month</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;2019&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;Association for Computational Linguistics&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">&quot;https://arxiv.org/abs/1908.10084&quot;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="sergeyzhlabse-ru-turbo"><a href="https://huggingface.co/sergeyzh/LaBSE-ru-turbo"><code>sergeyzh/LaBSE-ru-turbo</code></a><a class="headerlink" href="#sergeyzhlabse-ru-turbo" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>129.0M</td>
<td>490.0 MB</td>
<td>2024-06-27</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="sergeyzhrubert-tiny-turbo"><a href="https://huggingface.co/sergeyzh/rubert-tiny-turbo"><code>sergeyzh/rubert-tiny-turbo</code></a><a class="headerlink" href="#sergeyzhrubert-tiny-turbo" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.0K</td>
<td>312</td>
<td>29.2M</td>
<td>111.0 MB</td>
<td>2024-06-21</td>
<td>rus-Cyrl</td>
</tr>
</tbody>
</table>
<h4 id="shibing624text2vec-base-chinese"><a href="https://huggingface.co/shibing624/text2vec-base-chinese"><code>shibing624/text2vec-base-chinese</code></a><a class="headerlink" href="#shibing624text2vec-base-chinese" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>102.3M</td>
<td>390.0 MB</td>
<td>2022-01-23</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="shibing624text2vec-base-chinese-paraphrase"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase"><code>shibing624/text2vec-base-chinese-paraphrase</code></a><a class="headerlink" href="#shibing624text2vec-base-chinese-paraphrase" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>117.9M</td>
<td>450.0 MB</td>
<td>2023-06-19</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="shibing624text2vec-base-multilingual"><a href="https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase"><code>shibing624/text2vec-base-multilingual</code></a><a class="headerlink" href="#shibing624text2vec-base-multilingual" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>384</td>
<td>117.7M</td>
<td>449.0 MB</td>
<td>2023-06-22</td>
<td>deu-Latn, eng-Latn, fra-Latn, ita-Latn, nld-Latn, ... (10)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">text2vec</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Xu Ming}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{text2vec: A Tool for Text to Vector}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/shibing624/text2vec}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="silma-aisilma-embeddding-matryoshka-v01"><a href="https://huggingface.co/silma-ai/silma-embeddding-matryoshka-v0.1"><code>silma-ai/silma-embeddding-matryoshka-v0.1</code></a><a class="headerlink" href="#silma-aisilma-embeddding-matryoshka-v01" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>135.2M</td>
<td>516.0 MB</td>
<td>2024-10-12</td>
<td>ara-Arab, eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">silma2024embedding</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Abu Bakr Soliman, Karim Ouda, SILMA AI}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{SILMA Embedding Matryoshka 0.1}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2024}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Hugging Face}</span><span class="p">,</span>
<span class="w">  </span><span class="na">howpublished</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://huggingface.co/silma-ai/silma-embeddding-matryoshka-0.1}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="spartan8806atles-champion-embedding"><a href="https://huggingface.co/spartan8806/atles-champion-embedding"><code>spartan8806/atles-champion-embedding</code></a><a class="headerlink" href="#spartan8806atles-champion-embedding" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> apache-2.0</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>420.0 MB</td>
<td>2025-11-15</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">conner2025epistemic</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{The Epistemic Barrier: How RLHF Makes AI Consciousness Empirically Undecidable}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Conner (spartan8806)}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{ATLES Research Papers}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">note</span><span class="p">=</span><span class="s">{Cross-model validation study (Phoenix, Grok, Gemini, Claude)}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="stephantulkensnife-gte-modernbert-base_as_router"><a href="https://huggingface.co/stephantulkens/NIFE-gte-modernbert-base"><code>stephantulkens/NIFE-gte-modernbert-base_as_router</code></a><a class="headerlink" href="#stephantulkensnife-gte-modernbert-base_as_router" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>8.2K</td>
<td>768</td>
<td>225.8M</td>
<td>861.0 MB</td>
<td>2025-10-30</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">Tulkens2025pyNIFE</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{St&#39;{e}phan Tulkens}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{pyNIFE: nearly inference free embeddings in python}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Zenodo}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.5281/zenodo.17512919}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/stephantul/pynife}</span><span class="p">,</span>
<span class="w">  </span><span class="na">license</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{MIT}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="stephantulkensnife-mxbai-embed-large-v1_as_router"><a href="https://huggingface.co/stephantulkens/NIFE-mxbai-embed-large-v1_as_router"><code>stephantulkens/NIFE-mxbai-embed-large-v1_as_router</code></a><a class="headerlink" href="#stephantulkensnife-mxbai-embed-large-v1_as_router" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>445.7M</td>
<td>1.7 GB</td>
<td>2025-11-03</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@software</span><span class="p">{</span><span class="nl">Tulkens2025pyNIFE</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{St&#39;{e}phan Tulkens}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{pyNIFE: nearly inference free embeddings in python}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="s">{2025}</span><span class="p">,</span>
<span class="w">  </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{Zenodo}</span><span class="p">,</span>
<span class="w">  </span><span class="na">doi</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{10.5281/zenodo.17512919}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{https://github.com/stephantul/pynife}</span><span class="p">,</span>
<span class="w">  </span><span class="na">license</span><span class="w">      </span><span class="p">=</span><span class="w"> </span><span class="s">{MIT}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-base"><a href="https://huggingface.co/thenlper/gte-base"><code>thenlper/gte-base</code></a><a class="headerlink" href="#thenlpergte-base" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>768</td>
<td>109.5M</td>
<td>209.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-base-zh"><a href="https://huggingface.co/thenlper/gte-base-zh"><code>thenlper/gte-base-zh</code></a><a class="headerlink" href="#thenlpergte-base-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>102.3M</td>
<td>195.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-large"><a href="https://huggingface.co/thenlper/gte-large"><code>thenlper/gte-large</code></a><a class="headerlink" href="#thenlpergte-large" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>335.1M</td>
<td>639.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-large-zh"><a href="https://huggingface.co/thenlper/gte-large-zh"><code>thenlper/gte-large-zh</code></a><a class="headerlink" href="#thenlpergte-large-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>325.5M</td>
<td>621.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-small"><a href="https://huggingface.co/thenlper/gte-small"><code>thenlper/gte-small</code></a><a class="headerlink" href="#thenlpergte-small" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>384</td>
<td>33.4M</td>
<td>64.0 MB</td>
<td>2023-07-27</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="thenlpergte-small-zh"><a href="https://huggingface.co/thenlper/gte-small-zh"><code>thenlper/gte-small-zh</code></a><a class="headerlink" href="#thenlpergte-small-zh" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1024</td>
<td>30.3M</td>
<td>58.0 MB</td>
<td>2023-11-08</td>
<td>zho-Hans</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">li2023towards</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="p">=</span><span class="s">{Towards general text embeddings with multi-stage contrastive learning}</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="p">=</span><span class="s">{Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="p">=</span><span class="s">{arXiv preprint arXiv:2308.03281}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="p">=</span><span class="s">{2023}</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="unicamp-dlmt5-base-mmarco-v2">unicamp-dl/mt5-base-mmarco-v2<a class="headerlink" href="#unicamp-dlmt5-base-mmarco-v2" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> not specified</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>not specified</td>
<td>2022-01-05</td>
<td>afr-Latn, amh-Ethi, ara-Arab, aze-Latn, bel-Cyrl, ... (103)</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2108-13897</span><span class="p">,</span>
<span class="w">  </span><span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Luiz Bonifacio and</span>
<span class="s">Israel Campiotti and</span>
<span class="s">Roberto de Alencar Lotufo and</span>
<span class="s">Rodrigo Frassetto Nogueira}</span><span class="p">,</span>
<span class="w">  </span><span class="na">bibsource</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{dblp computer science bibliography, https://dblp.org}</span><span class="p">,</span>
<span class="w">  </span><span class="na">biburl</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://dblp.org/rec/journals/corr/abs-2108-13897.bib}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprint</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2108.13897}</span><span class="p">,</span>
<span class="w">  </span><span class="na">eprinttype</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{arXiv}</span><span class="p">,</span>
<span class="w">  </span><span class="na">journal</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{CoRR}</span><span class="p">,</span>
<span class="w">  </span><span class="na">timestamp</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Mon, 20 Mar 2023 15:35:34 +0100}</span><span class="p">,</span>
<span class="w">  </span><span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{mMARCO: {A} Multilingual Version of {MS} {MARCO} Passage Ranking Dataset}</span><span class="p">,</span>
<span class="w">  </span><span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://arxiv.org/abs/2108.13897}</span><span class="p">,</span>
<span class="w">  </span><span class="na">volume</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{abs/2108.13897}</span><span class="p">,</span>
<span class="w">  </span><span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div>
</details>
<h4 id="w601sxsb1ade-embed"><a href="https://huggingface.co/w601sxs/b1ade-embed"><code>w601sxs/b1ade-embed</code></a><a class="headerlink" href="#w601sxsb1ade-embed" title="Permanent link">&para;</a></h4>
<p><strong>License:</strong> mit</p>
<table>
<thead>
<tr>
<th>Max Tokens</th>
<th>Embedding dimension</th>
<th>Parameters</th>
<th>Required Memory (Mb)</th>
<th>Release date</th>
<th>Languages</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1K</td>
<td>1024</td>
<td>335.1M</td>
<td>1.2 GB</td>
<td>2025-03-10</td>
<td>eng-Latn</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Citation</summary>
<div class="highlight"><pre><span></span><code><span class="w">    </span><span class="nc">@misc</span><span class="p">{</span><span class="nl">bigscience_workshop_2022</span><span class="p">,</span>
<span class="w">    </span><span class="na">author</span><span class="w">       </span><span class="p">=</span><span class="w"> </span><span class="s">{ {Shreyas Subramanian} }</span><span class="p">,</span>
<span class="w">    </span><span class="na">title</span><span class="w">        </span><span class="p">=</span><span class="w"> </span><span class="s">{ {b1ade series of models} }</span><span class="p">,</span>
<span class="w">    </span><span class="na">year</span><span class="w">         </span><span class="p">=</span><span class="w"> </span><span class="m">2024</span><span class="p">,</span>
<span class="w">    </span><span class="na">url</span><span class="w">          </span><span class="p">=</span><span class="w"> </span><span class="s">{ https://huggingface.co/w601sxs/b1ade-embed }</span><span class="p">,</span>
<span class="w">    </span><span class="na">publisher</span><span class="w">    </span><span class="p">=</span><span class="w"> </span><span class="s">{ Hugging Face }</span>
<span class="p">}</span>
</code></pre></div>
</details>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../image_text/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Image-text Model">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Image-text Model
              </div>
            </div>
          </a>
        
        
          
          <a href="../../../api/" class="md-footer__link md-footer__link--next" aria-label="Next: Overview">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Overview
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      This text is freely available under a Creative Commons Attribution 4.0 license
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/embeddings-benchmark/mteb" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.tracking", "navigation.instant", "navigation.tabs", "navigation.sections", "navigation.top", "search.suggest", "search.highlight", "content.action.edit", "content.action.view", "content.code.copy", "content.code.annotate", "content.code.annotation", "content.tabs.link", "content.tooltips", "navigation.footer", "navigation.indexes", "toc.follow"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>