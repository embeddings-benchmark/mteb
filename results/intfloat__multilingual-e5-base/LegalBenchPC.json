{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "mteb_dataset_name": "LegalBenchPC",
  "mteb_version": "1.7.7",
  "test": {
    "cos_sim": {
      "accuracy": 0.66015625,
      "accuracy_threshold": 0.7808122634887695,
      "ap": 0.7262082113371351,
      "f1": 0.7576150356448478,
      "f1_threshold": 0.764856219291687,
      "precision": 0.6268096514745308,
      "recall": 0.9574119574119574
    },
    "dot": {
      "accuracy": 0.66015625,
      "accuracy_threshold": 0.7808123826980591,
      "ap": 0.726208512243574,
      "f1": 0.7576150356448478,
      "f1_threshold": 0.7648563385009766,
      "precision": 0.6268096514745308,
      "recall": 0.9574119574119574
    },
    "euclidean": {
      "accuracy": 0.66015625,
      "accuracy_threshold": 0.6620992422103882,
      "ap": 0.7262082113371351,
      "f1": 0.7576150356448478,
      "f1_threshold": 0.685775101184845,
      "precision": 0.6268096514745308,
      "recall": 0.9574119574119574
    },
    "evaluation_time": 267.43,
    "manhattan": {
      "accuracy": 0.66064453125,
      "accuracy_threshold": 14.39570426940918,
      "ap": 0.7293593422078586,
      "f1": 0.7577437235083143,
      "f1_threshold": 15.084949493408203,
      "precision": 0.6294691224268689,
      "recall": 0.9516789516789517
    },
    "max": {
      "accuracy": 0.66064453125,
      "ap": 0.7293593422078586,
      "f1": 0.7577437235083143
    }
  }
}