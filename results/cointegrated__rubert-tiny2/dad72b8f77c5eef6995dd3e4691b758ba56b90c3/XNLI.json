{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 0.5870504379272461,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.25",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.6358974358974359,
          "accuracy_threshold": 0.6953425407409668,
          "ap": 0.6737823880386407,
          "f1": 0.6853868194842407,
          "f1_threshold": 0.592831015586853,
          "precision": 0.5625587958607714,
          "recall": 0.8768328445747801
        },
        "dot": {
          "accuracy": 0.6358974358974359,
          "accuracy_threshold": 0.6953425407409668,
          "ap": 0.6737823880386407,
          "f1": 0.6853868194842407,
          "f1_threshold": 0.5928310751914978,
          "precision": 0.5625587958607714,
          "recall": 0.8768328445747801
        },
        "euclidean": {
          "accuracy": 0.6358974358974359,
          "accuracy_threshold": 0.7805861830711365,
          "ap": 0.6737823880386407,
          "f1": 0.6853868194842407,
          "f1_threshold": 0.9024066925048828,
          "precision": 0.5625587958607714,
          "recall": 0.8768328445747801
        },
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6741113544969821,
        "manhattan": {
          "accuracy": 0.6410256410256411,
          "accuracy_threshold": 10.945795059204102,
          "ap": 0.6741113544969821,
          "f1": 0.6875349357182784,
          "f1_threshold": 12.842007637023926,
          "precision": 0.5555555555555556,
          "recall": 0.9017595307917888
        },
        "max": {
          "accuracy": 0.6410256410256411,
          "ap": 0.6741113544969821,
          "f1": 0.6875349357182784
        }
      }
    ],
    "validation": [
      {
        "cos_sim": {
          "accuracy": 0.6373626373626373,
          "accuracy_threshold": 0.6893622875213623,
          "ap": 0.6721862981729065,
          "f1": 0.6755504055619931,
          "f1_threshold": 0.5976014733314514,
          "precision": 0.5584291187739464,
          "recall": 0.8548387096774194
        },
        "dot": {
          "accuracy": 0.6373626373626373,
          "accuracy_threshold": 0.6893622875213623,
          "ap": 0.6721862981729065,
          "f1": 0.6755504055619931,
          "f1_threshold": 0.5976015329360962,
          "precision": 0.5584291187739464,
          "recall": 0.8548387096774194
        },
        "euclidean": {
          "accuracy": 0.6373626373626373,
          "accuracy_threshold": 0.7882102727890015,
          "ap": 0.6721862981729065,
          "f1": 0.6755504055619931,
          "f1_threshold": 0.8971047401428223,
          "precision": 0.5584291187739464,
          "recall": 0.8548387096774194
        },
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6744238856297844,
        "manhattan": {
          "accuracy": 0.6366300366300366,
          "accuracy_threshold": 11.116180419921875,
          "ap": 0.6744238856297844,
          "f1": 0.6759310728182322,
          "f1_threshold": 12.829548835754395,
          "precision": 0.5443151298119964,
          "recall": 0.8914956011730205
        },
        "max": {
          "accuracy": 0.6373626373626373,
          "ap": 0.6744238856297844,
          "f1": 0.6759310728182322
        }
      }
    ]
  },
  "task_name": "XNLI"
}