{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 7.0352561473846436,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.619384765625,
        "ap": 0.576739144098376,
        "ap_weighted": 0.576739144098376,
        "f1": 0.6179898369031541,
        "f1_weighted": 0.6179898369031541,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.619384765625,
        "scores_per_experiment": [
          {
            "accuracy": 0.6337890625,
            "ap": 0.5852972436621486,
            "ap_weighted": 0.5852972436621486,
            "f1": 0.6337205974932754,
            "f1_weighted": 0.6337205974932754
          },
          {
            "accuracy": 0.6708984375,
            "ap": 0.6140959874880267,
            "ap_weighted": 0.6140959874880267,
            "f1": 0.6708670489357886,
            "f1_weighted": 0.6708670489357886
          },
          {
            "accuracy": 0.6494140625,
            "ap": 0.5981295226050205,
            "ap_weighted": 0.5981295226050205,
            "f1": 0.6492213740458015,
            "f1_weighted": 0.6492213740458015
          },
          {
            "accuracy": 0.58349609375,
            "ap": 0.5507052815244667,
            "ap_weighted": 0.5507052815244667,
            "f1": 0.5783155010832112,
            "f1_weighted": 0.5783155010832112
          },
          {
            "accuracy": 0.61962890625,
            "ap": 0.5790714058393561,
            "ap_weighted": 0.5790714058393561,
            "f1": 0.6132509602716032,
            "f1_weighted": 0.6132509602716032
          },
          {
            "accuracy": 0.53076171875,
            "ap": 0.5164352599632753,
            "ap_weighted": 0.5164352599632753,
            "f1": 0.5295250448272755,
            "f1_weighted": 0.5295250448272755
          },
          {
            "accuracy": 0.67333984375,
            "ap": 0.6165706237852284,
            "ap_weighted": 0.6165706237852284,
            "f1": 0.6733378966921371,
            "f1_weighted": 0.6733378966921371
          },
          {
            "accuracy": 0.6533203125,
            "ap": 0.600305823305501,
            "ap_weighted": 0.600305823305501,
            "f1": 0.6533173368988343,
            "f1_weighted": 0.6533173368988343
          },
          {
            "accuracy": 0.59228515625,
            "ap": 0.5553321958969968,
            "ap_weighted": 0.5553321958969968,
            "f1": 0.591737633750402,
            "f1_weighted": 0.591737633750402
          },
          {
            "accuracy": 0.5869140625,
            "ap": 0.5514480969137396,
            "ap_weighted": 0.5514480969137396,
            "f1": 0.5866049750332127,
            "f1_weighted": 0.5866049750332127
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}