{
  "dataset_revision": "c1f9ee939b7d05667af864ee1cb066393154bf85",
  "evaluation_time": 24.602572917938232,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.435888671875,
        "f1": 0.4236578282497966,
        "f1_weighted": 0.4236322009989372,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.435888671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.4248046875,
            "f1": 0.4130979527798996,
            "f1_weighted": 0.4130616236209766
          },
          {
            "accuracy": 0.4306640625,
            "f1": 0.41788656927106055,
            "f1_weighted": 0.41787452898626054
          },
          {
            "accuracy": 0.4345703125,
            "f1": 0.4247610106419826,
            "f1_weighted": 0.42474425761058776
          },
          {
            "accuracy": 0.44970703125,
            "f1": 0.4353346812629991,
            "f1_weighted": 0.43528834831161795
          },
          {
            "accuracy": 0.44677734375,
            "f1": 0.4345352684967424,
            "f1_weighted": 0.4345015746209993
          },
          {
            "accuracy": 0.44384765625,
            "f1": 0.4363423915108509,
            "f1_weighted": 0.4363034928805297
          },
          {
            "accuracy": 0.43896484375,
            "f1": 0.4227225141518747,
            "f1_weighted": 0.4227319829709069
          },
          {
            "accuracy": 0.42724609375,
            "f1": 0.4083699766733034,
            "f1_weighted": 0.40833119040425203
          },
          {
            "accuracy": 0.41552734375,
            "f1": 0.4076822006122377,
            "f1_weighted": 0.4076586668951962
          },
          {
            "accuracy": 0.44677734375,
            "f1": 0.4358457170970154,
            "f1_weighted": 0.4358263436880449
          }
        ]
      }
    ]
  },
  "task_name": "YelpReviewFullClassification"
}