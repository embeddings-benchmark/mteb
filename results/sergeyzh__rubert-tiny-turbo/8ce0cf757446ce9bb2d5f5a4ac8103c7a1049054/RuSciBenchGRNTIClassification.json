{
  "dataset_revision": "673a610d6d3dd91a547a0d57ae1b56f37ebbf6a1",
  "evaluation_time": 6.65631365776062,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.529296875,
        "f1": 0.5136892216551846,
        "f1_weighted": 0.5138263945115431,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.529296875,
        "scores_per_experiment": [
          {
            "accuracy": 0.5634765625,
            "f1": 0.5450321199299915,
            "f1_weighted": 0.5451487969773701
          },
          {
            "accuracy": 0.533203125,
            "f1": 0.5252232207704194,
            "f1_weighted": 0.525360579653535
          },
          {
            "accuracy": 0.5078125,
            "f1": 0.48935928324029676,
            "f1_weighted": 0.4895610834328666
          },
          {
            "accuracy": 0.54150390625,
            "f1": 0.529700900543842,
            "f1_weighted": 0.5298318800030801
          },
          {
            "accuracy": 0.533203125,
            "f1": 0.5279846158689907,
            "f1_weighted": 0.52798774393941
          },
          {
            "accuracy": 0.5166015625,
            "f1": 0.4966346013236966,
            "f1_weighted": 0.49677949171762564
          },
          {
            "accuracy": 0.54443359375,
            "f1": 0.5220071715654007,
            "f1_weighted": 0.5222500843570264
          },
          {
            "accuracy": 0.51416015625,
            "f1": 0.49687826789440326,
            "f1_weighted": 0.4970322231983433
          },
          {
            "accuracy": 0.51513671875,
            "f1": 0.4988411313677607,
            "f1_weighted": 0.49895275545781553
          },
          {
            "accuracy": 0.5234375,
            "f1": 0.5052309040470451,
            "f1_weighted": 0.5053593063783592
          }
        ]
      }
    ]
  },
  "task_name": "RuSciBenchGRNTIClassification"
}