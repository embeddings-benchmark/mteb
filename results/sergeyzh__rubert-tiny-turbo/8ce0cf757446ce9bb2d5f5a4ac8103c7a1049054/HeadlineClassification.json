{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 4.5200018882751465,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.689013671875,
        "f1": 0.6880041842725984,
        "f1_weighted": 0.6880034868754101,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.689013671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.66162109375,
            "f1": 0.661746098912683,
            "f1_weighted": 0.6617567551797937
          },
          {
            "accuracy": 0.69384765625,
            "f1": 0.6906917634347239,
            "f1_weighted": 0.6907099319795769
          },
          {
            "accuracy": 0.69580078125,
            "f1": 0.697017564942763,
            "f1_weighted": 0.6970174952920494
          },
          {
            "accuracy": 0.7119140625,
            "f1": 0.7110746954520751,
            "f1_weighted": 0.7110468825791231
          },
          {
            "accuracy": 0.7109375,
            "f1": 0.7122892063531965,
            "f1_weighted": 0.7122860913918316
          },
          {
            "accuracy": 0.70947265625,
            "f1": 0.7096471963950944,
            "f1_weighted": 0.7096496223781761
          },
          {
            "accuracy": 0.6669921875,
            "f1": 0.6653556097792342,
            "f1_weighted": 0.6653127610534366
          },
          {
            "accuracy": 0.65771484375,
            "f1": 0.6565303908946779,
            "f1_weighted": 0.6565510179775272
          },
          {
            "accuracy": 0.66748046875,
            "f1": 0.662898367583047,
            "f1_weighted": 0.662914638438974
          },
          {
            "accuracy": 0.71435546875,
            "f1": 0.7127909489784886,
            "f1_weighted": 0.7127896724836131
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}