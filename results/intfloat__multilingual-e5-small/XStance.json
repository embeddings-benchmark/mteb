{
  "dataset_revision": "810604b9ad3aafdc6144597fdaa40f21a6f5f3de",
  "evaluation_time": 3.5373082160949707,
  "kg_co2_emissions": null,
  "mteb_version": "1.6.38",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.51806640625,
          "accuracy_threshold": 0.8497587442398071,
          "ap": 0.5038443561739071,
          "f1": 0.6712195121951219,
          "f1_threshold": 0.7853705883026123,
          "precision": 0.5053868756121449,
          "recall": 0.9990319457889641
        },
        "dot": {
          "accuracy": 0.51806640625,
          "accuracy_threshold": 0.8497587442398071,
          "ap": 0.5038443561739071,
          "f1": 0.6712195121951219,
          "f1_threshold": 0.7853705286979675,
          "precision": 0.5053868756121449,
          "recall": 0.9990319457889641
        },
        "euclidean": {
          "accuracy": 0.51806640625,
          "accuracy_threshold": 0.5481629371643066,
          "ap": 0.5038443561739071,
          "f1": 0.6712195121951219,
          "f1_threshold": 0.655178427696228,
          "precision": 0.5053868756121449,
          "recall": 0.9990319457889641
        },
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ],
        "main_score": 0.5040793387249269,
        "manhattan": {
          "accuracy": 0.51416015625,
          "accuracy_threshold": 8.64465045928955,
          "ap": 0.5040793387249269,
          "f1": 0.6714332141696457,
          "f1_threshold": 10.558012962341309,
          "precision": 0.5053816046966731,
          "recall": 1.0
        },
        "max": {
          "accuracy": 0.51806640625,
          "ap": 0.5040793387249269,
          "f1": 0.6714332141696457
        }
      },
      {
        "cos_sim": {
          "accuracy": 0.5712890625,
          "accuracy_threshold": 0.7847779989242554,
          "ap": 0.5942397553753017,
          "f1": 0.7265940902021772,
          "f1_threshold": 0.761985719203949,
          "precision": 0.570591108939912,
          "recall": 1.0
        },
        "dot": {
          "accuracy": 0.5712890625,
          "accuracy_threshold": 0.7847781181335449,
          "ap": 0.5942413209080255,
          "f1": 0.7265940902021772,
          "f1_threshold": 0.7619856595993042,
          "precision": 0.570591108939912,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.5712890625,
          "accuracy_threshold": 0.6560750603675842,
          "ap": 0.5942397553753018,
          "f1": 0.7265940902021772,
          "f1_threshold": 0.6898986101150513,
          "precision": 0.570591108939912,
          "recall": 1.0
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.5942413209080255,
        "manhattan": {
          "accuracy": 0.57177734375,
          "accuracy_threshold": 9.83655834197998,
          "ap": 0.5932090773271775,
          "f1": 0.7265940902021772,
          "f1_threshold": 10.73774242401123,
          "precision": 0.570591108939912,
          "recall": 1.0
        },
        "max": {
          "accuracy": 0.57177734375,
          "ap": 0.5942413209080255,
          "f1": 0.7265940902021772
        }
      },
      {
        "cos_sim": {
          "accuracy": 0.5408022130013831,
          "accuracy_threshold": 0.7930425405502319,
          "ap": 0.5182917988582176,
          "f1": 0.701123595505618,
          "f1_threshold": 0.7833303213119507,
          "precision": 0.5397923875432526,
          "recall": 1.0
        },
        "dot": {
          "accuracy": 0.5408022130013831,
          "accuracy_threshold": 0.7930425405502319,
          "ap": 0.5182917988582176,
          "f1": 0.701123595505618,
          "f1_threshold": 0.7833303213119507,
          "precision": 0.5397923875432526,
          "recall": 1.0
        },
        "euclidean": {
          "accuracy": 0.5408022130013831,
          "accuracy_threshold": 0.6433612108230591,
          "ap": 0.5182917988582176,
          "f1": 0.701123595505618,
          "f1_threshold": 0.6582847833633423,
          "precision": 0.5397923875432526,
          "recall": 1.0
        },
        "hf_subset": "it",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.5182917988582176,
        "manhattan": {
          "accuracy": 0.5401106500691563,
          "accuracy_threshold": 10.210434913635254,
          "ap": 0.5178128531012762,
          "f1": 0.7008547008547008,
          "f1_threshold": 10.210434913635254,
          "precision": 0.5398475398475399,
          "recall": 0.9987179487179487
        },
        "max": {
          "accuracy": 0.5408022130013831,
          "ap": 0.5182917988582176,
          "f1": 0.701123595505618
        }
      }
    ]
  },
  "task_name": "XStance"
}