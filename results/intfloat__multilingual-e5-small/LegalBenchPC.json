{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "mteb_dataset_name": "LegalBenchPC",
  "mteb_version": "1.7.7",
  "test": {
    "cos_sim": {
      "accuracy": 0.64453125,
      "accuracy_threshold": 0.7996842861175537,
      "ap": 0.6958654744262636,
      "f1": 0.7548711502199875,
      "f1_threshold": 0.771399974822998,
      "precision": 0.6124426313105559,
      "recall": 0.9836199836199836
    },
    "dot": {
      "accuracy": 0.64453125,
      "accuracy_threshold": 0.7996842861175537,
      "ap": 0.6958634391357681,
      "f1": 0.7548711502199875,
      "f1_threshold": 0.7713999152183533,
      "precision": 0.6124426313105559,
      "recall": 0.9836199836199836
    },
    "euclidean": {
      "accuracy": 0.64453125,
      "accuracy_threshold": 0.6329545378684998,
      "ap": 0.695863267997088,
      "f1": 0.7548711502199875,
      "f1_threshold": 0.6761655807495117,
      "precision": 0.6124426313105559,
      "recall": 0.9836199836199836
    },
    "evaluation_time": 105.69,
    "manhattan": {
      "accuracy": 0.640625,
      "accuracy_threshold": 9.960738182067871,
      "ap": 0.6955311761244476,
      "f1": 0.7561837455830388,
      "f1_threshold": 10.366462707519531,
      "precision": 0.622093023255814,
      "recall": 0.963963963963964
    },
    "max": {
      "accuracy": 0.64453125,
      "ap": 0.6958654744262636,
      "f1": 0.7561837455830388
    }
  }
}