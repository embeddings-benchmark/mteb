{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "mteb_dataset_name": "LegalBenchPC",
  "mteb_version": "1.7.7",
  "test": {
    "cos_sim": {
      "accuracy": 0.5736301369863014,
      "accuracy_threshold": 0.872885525226593,
      "ap": 0.5402842739866689,
      "f1": 0.6346604215456675,
      "f1_threshold": 0.751397430896759,
      "precision": 0.4648370497427101,
      "recall": 1.0
    },
    "dot": {
      "accuracy": 0.5736301369863014,
      "accuracy_threshold": 0.8728854656219482,
      "ap": 0.5402842739866689,
      "f1": 0.6346604215456675,
      "f1_threshold": 0.7513974905014038,
      "precision": 0.4648370497427101,
      "recall": 1.0
    },
    "euclidean": {
      "accuracy": 0.5736301369863014,
      "accuracy_threshold": 0.5042111873626709,
      "ap": 0.5402842739866689,
      "f1": 0.6346604215456675,
      "f1_threshold": 0.7051099538803101,
      "precision": 0.4648370497427101,
      "recall": 1.0
    },
    "evaluation_time": 80.01,
    "manhattan": {
      "accuracy": 0.5804794520547946,
      "accuracy_threshold": 8.489166259765625,
      "ap": 0.5412059665799563,
      "f1": 0.6349206349206349,
      "f1_threshold": 10.36783218383789,
      "precision": 0.4744525547445255,
      "recall": 0.959409594095941
    },
    "max": {
      "accuracy": 0.5804794520547946,
      "ap": 0.5412059665799563,
      "f1": 0.6349206349206349
    }
  }
}