{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 2.4368784427642822,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.47",
  "scores": {
    "test": [
      {
        "accuracy": 0.755908203125,
        "f1": 0.75571379942619,
        "f1_weighted": 0.7557266174101903,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.755908203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.71240234375,
            "f1": 0.717911348788057,
            "f1_weighted": 0.7179308401993338
          },
          {
            "accuracy": 0.748046875,
            "f1": 0.7477058170071742,
            "f1_weighted": 0.7477115561422353
          },
          {
            "accuracy": 0.77099609375,
            "f1": 0.7696101631102317,
            "f1_weighted": 0.7696349270813354
          },
          {
            "accuracy": 0.767578125,
            "f1": 0.7664120234200792,
            "f1_weighted": 0.7664085497172332
          },
          {
            "accuracy": 0.77587890625,
            "f1": 0.7754028026494842,
            "f1_weighted": 0.7754129412125276
          },
          {
            "accuracy": 0.76318359375,
            "f1": 0.7624506583031211,
            "f1_weighted": 0.7624598777978914
          },
          {
            "accuracy": 0.73779296875,
            "f1": 0.7366249150050685,
            "f1_weighted": 0.7366271426098568
          },
          {
            "accuracy": 0.77587890625,
            "f1": 0.7758638275964324,
            "f1_weighted": 0.7758711512471281
          },
          {
            "accuracy": 0.7431640625,
            "f1": 0.7391387739578494,
            "f1_weighted": 0.7391745613782144
          },
          {
            "accuracy": 0.76416015625,
            "f1": 0.7660176644244011,
            "f1_weighted": 0.766034626716148
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}