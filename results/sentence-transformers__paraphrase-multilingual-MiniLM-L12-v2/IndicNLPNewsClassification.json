{
  "dataset_revision": "3f23bd4a622a462adfb6989419cfadf7dc778f25",
  "mteb_dataset_name": "IndicNLPNewsClassification",
  "mteb_version": "1.7.50",
  "test": {
    "evaluation_time": 84.9,
    "gu": {
      "accuracy": 0.7580078125,
      "accuracy_stderr": 0.03183743526001344,
      "f1": 0.7579051770115244,
      "f1_stderr": 0.033271414383312575,
      "main_score": 0.7580078125
    },
    "kn": {
      "accuracy": 0.734375,
      "accuracy_stderr": 0.051248600638288774,
      "f1": 0.7332392360549641,
      "f1_stderr": 0.053080298537720734,
      "main_score": 0.734375
    },
    "mal": {
      "accuracy": 0.59375,
      "accuracy_stderr": 0.042607680395803496,
      "f1": 0.5912135773960228,
      "f1_stderr": 0.041858639805620684,
      "main_score": 0.59375
    },
    "mr": {
      "accuracy": 0.912841796875,
      "accuracy_stderr": 0.007690235922658197,
      "f1": 0.9132579448333551,
      "f1_stderr": 0.007659771443844076,
      "main_score": 0.912841796875
    },
    "ori": {
      "accuracy": 0.622802734375,
      "accuracy_stderr": 0.04549159469100328,
      "f1": 0.6175121687620425,
      "f1_stderr": 0.048596205840828646,
      "main_score": 0.622802734375
    },
    "pa": {
      "accuracy": 0.6431089743589744,
      "accuracy_stderr": 0.042448592589395845,
      "f1": 0.6443868391191396,
      "f1_stderr": 0.04336275520577154,
      "main_score": 0.6431089743589744
    },
    "ta": {
      "accuracy": 0.709130859375,
      "accuracy_stderr": 0.06199251132777149,
      "f1": 0.709264588053595,
      "f1_stderr": 0.06229727267154105,
      "main_score": 0.709130859375
    },
    "tel": {
      "accuracy": 0.753759765625,
      "accuracy_stderr": 0.03728119275071896,
      "f1": 0.7516550495715488,
      "f1_stderr": 0.03903453219430094,
      "main_score": 0.753759765625
    }
  }
}