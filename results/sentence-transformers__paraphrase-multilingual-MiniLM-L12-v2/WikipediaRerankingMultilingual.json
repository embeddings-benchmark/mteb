{
  "dataset_revision": "6268b37d6f975f2a134791ba2f250a91d0bdfb4f",
  "evaluation_time": 82.83001351356506,
  "kg_co2_emissions": null,
  "mteb_version": "1.10.2",
  "scores": {
    "test": [
      {
        "hf_subset": "bg",
        "languages": [
          "bul-Cyrl"
        ],
        "main_score": 0.8113743386243386,
        "map": 0.8113743386243386,
        "mrr": 0.8113743386243386
      },
      {
        "hf_subset": "bn",
        "languages": [
          "ben-Beng"
        ],
        "main_score": 0.4103452380952381,
        "map": 0.4103452380952381,
        "mrr": 0.4103452380952381
      },
      {
        "hf_subset": "cs",
        "languages": [
          "ces-Latn"
        ],
        "main_score": 0.8018489417989417,
        "map": 0.8018489417989417,
        "mrr": 0.8018489417989417
      },
      {
        "hf_subset": "da",
        "languages": [
          "dan-Latn"
        ],
        "main_score": 0.8124404761904761,
        "map": 0.8124404761904761,
        "mrr": 0.812468253968254
      },
      {
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ],
        "main_score": 0.8153772486772486,
        "map": 0.8153772486772486,
        "mrr": 0.8153772486772486
      },
      {
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.8558092592592593,
        "map": 0.8558092592592593,
        "mrr": 0.8558092592592593
      },
      {
        "hf_subset": "fa",
        "languages": [
          "fas-Arab"
        ],
        "main_score": 0.8080431216931216,
        "map": 0.8080431216931216,
        "mrr": 0.8080431216931216
      },
      {
        "hf_subset": "fi",
        "languages": [
          "fin-Latn"
        ],
        "main_score": 0.802108201058201,
        "map": 0.802108201058201,
        "mrr": 0.802108201058201
      },
      {
        "hf_subset": "hi",
        "languages": [
          "hin-Deva"
        ],
        "main_score": 0.7748285714285713,
        "map": 0.7748285714285713,
        "mrr": 0.7755841269841269
      },
      {
        "hf_subset": "it",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.8169097883597884,
        "map": 0.8169097883597884,
        "mrr": 0.8169097883597884
      },
      {
        "hf_subset": "nl",
        "languages": [
          "nld-Latn"
        ],
        "main_score": 0.8060235449735449,
        "map": 0.8060235449735449,
        "mrr": 0.8063568783068783
      },
      {
        "hf_subset": "pt",
        "languages": [
          "por-Latn"
        ],
        "main_score": 0.8135058201058201,
        "map": 0.8135058201058201,
        "mrr": 0.8135058201058201
      },
      {
        "hf_subset": "ro",
        "languages": [
          "ron-Latn"
        ],
        "main_score": 0.8141359788359789,
        "map": 0.8141359788359789,
        "mrr": 0.8144693121693122
      },
      {
        "hf_subset": "sr",
        "languages": [
          "srp-Cyrl"
        ],
        "main_score": 0.8187576719576718,
        "map": 0.8187576719576718,
        "mrr": 0.8190910052910052
      },
      {
        "hf_subset": "no",
        "languages": [
          "nor-Latn"
        ],
        "main_score": 0.7816222222222222,
        "map": 0.7816222222222222,
        "mrr": 0.7816222222222222
      },
      {
        "hf_subset": "sv",
        "languages": [
          "swe-Latn"
        ],
        "main_score": 0.8181380952380952,
        "map": 0.8181380952380952,
        "mrr": 0.8181380952380952
      }
    ]
  },
  "task_name": "WikipediaRerankingMultilingual"
}