{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "mteb_dataset_name": "LegalBenchPC",
  "mteb_version": "1.7.7",
  "test": {
    "cos_sim": {
      "accuracy": 0.6650390625,
      "accuracy_threshold": 0.25153490900993347,
      "ap": 0.7640598031906114,
      "f1": 0.7554179566563468,
      "f1_threshold": 0.15246719121932983,
      "precision": 0.6512455516014235,
      "recall": 0.8992628992628993
    },
    "dot": {
      "accuracy": 0.6484375,
      "accuracy_threshold": 3.606294631958008,
      "ap": 0.7123645322062555,
      "f1": 0.7486559139784946,
      "f1_threshold": 2.6156225204467773,
      "precision": 0.6347578347578348,
      "recall": 0.9123669123669124
    },
    "euclidean": {
      "accuracy": 0.6875,
      "accuracy_threshold": 5.562836647033691,
      "ap": 0.7931082137696056,
      "f1": 0.7619718309859155,
      "f1_threshold": 5.771557807922363,
      "precision": 0.6683137739345275,
      "recall": 0.8861588861588862
    },
    "evaluation_time": 73.6,
    "manhattan": {
      "accuracy": 0.685546875,
      "accuracy_threshold": 86.32853698730469,
      "ap": 0.7902235021809445,
      "f1": 0.7622149837133549,
      "f1_threshold": 88.31982421875,
      "precision": 0.6828793774319066,
      "recall": 0.8624078624078624
    },
    "max": {
      "accuracy": 0.6875,
      "ap": 0.7931082137696056,
      "f1": 0.7622149837133549
    }
  }
}