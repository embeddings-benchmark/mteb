{
  "dataset_revision": "12ca3b695563788fead87a982ad1a068284413f4",
  "mteb_dataset_name": "CitationPredictionLegalBenchPC",
  "mteb_version": "1.7.7",
  "test": {
    "cos_sim": {
      "accuracy": 0.5648148148148148,
      "accuracy_threshold": 0.10821166634559631,
      "ap": 0.49723470382503165,
      "f1": 0.6666666666666667,
      "f1_threshold": -0.04414784908294678,
      "precision": 0.5047619047619047,
      "recall": 0.9814814814814815
    },
    "dot": {
      "accuracy": 0.5555555555555556,
      "accuracy_threshold": 2.237715244293213,
      "ap": 0.4853033037963359,
      "f1": 0.6625000000000001,
      "f1_threshold": -1.3291501998901367,
      "precision": 0.5,
      "recall": 0.9814814814814815
    },
    "euclidean": {
      "accuracy": 0.5462962962962963,
      "accuracy_threshold": 5.220001220703125,
      "ap": 0.5486361926231341,
      "f1": 0.6583850931677019,
      "f1_threshold": 6.803489685058594,
      "precision": 0.4953271028037383,
      "recall": 0.9814814814814815
    },
    "evaluation_time": 3.88,
    "manhattan": {
      "accuracy": 0.5370370370370371,
      "accuracy_threshold": 81.57430267333984,
      "ap": 0.5336513734132683,
      "f1": 0.6583850931677019,
      "f1_threshold": 106.46473693847656,
      "precision": 0.4953271028037383,
      "recall": 0.9814814814814815
    },
    "max": {
      "accuracy": 0.5648148148148148,
      "ap": 0.5486361926231341,
      "f1": 0.6666666666666667
    }
  }
}