{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 8.756818294525146,
  "kg_co2_emissions": null,
  "mteb_version": "1.11.6",
  "scores": {
    "test": [
      {
        "accuracy": 0.581787109375,
        "ap": 0.5500034660932301,
        "f1": 0.5769633015450566,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.581787109375,
        "scores_per_experiment": [
          {
            "accuracy": 0.59912109375,
            "ap": 0.5593188310802619,
            "f1": 0.5991164104233052
          },
          {
            "accuracy": 0.59130859375,
            "ap": 0.5547076940283668,
            "f1": 0.5906682907714251
          },
          {
            "accuracy": 0.564453125,
            "ap": 0.5371161099137931,
            "f1": 0.5619763927772294
          },
          {
            "accuracy": 0.6044921875,
            "ap": 0.562579424676525,
            "f1": 0.6041747197526093
          },
          {
            "accuracy": 0.62158203125,
            "ap": 0.5773341444672131,
            "f1": 0.620507060141932
          },
          {
            "accuracy": 0.4775390625,
            "ap": 0.4894218059501263,
            "f1": 0.4707473529639076
          },
          {
            "accuracy": 0.61279296875,
            "ap": 0.5702115298581654,
            "f1": 0.6121863256651614
          },
          {
            "accuracy": 0.56494140625,
            "ap": 0.5356959484690067,
            "f1": 0.5543998052296427
          },
          {
            "accuracy": 0.56982421875,
            "ap": 0.5441062744590239,
            "f1": 0.5447100803619936
          },
          {
            "accuracy": 0.61181640625,
            "ap": 0.569542898029819,
            "f1": 0.61114657736336
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}