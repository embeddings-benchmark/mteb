{
  "dataset_revision": "14b13edfbc4046892f6011d114c29c0f83170589",
  "mteb_dataset_name": "TweetSentimentClassification",
  "mteb_version": "1.7.40",
  "test": {
    "arabic": {
      "accuracy": 0.428125,
      "accuracy_stderr": 0.03644344934278313,
      "f1": 0.4150515354479289,
      "f1_stderr": 0.042044791973811474,
      "main_score": 0.428125
    },
    "english": {
      "accuracy": 0.4640625,
      "accuracy_stderr": 0.03897646776020757,
      "f1": 0.4526982107712719,
      "f1_stderr": 0.03476162212189228,
      "main_score": 0.4640625
    },
    "evaluation_time": 9.35,
    "french": {
      "accuracy": 0.44765625,
      "accuracy_stderr": 0.03278458634118326,
      "f1": 0.4435231069614679,
      "f1_stderr": 0.034001046922440895,
      "main_score": 0.44765625
    },
    "german": {
      "accuracy": 0.496875,
      "accuracy_stderr": 0.04677071733467427,
      "f1": 0.49158929944119245,
      "f1_stderr": 0.04940566081017425,
      "main_score": 0.496875
    },
    "hindi": {
      "accuracy": 0.348828125,
      "accuracy_stderr": 0.035071512461371336,
      "f1": 0.3405460869610343,
      "f1_stderr": 0.03555657925349932,
      "main_score": 0.348828125
    },
    "italian": {
      "accuracy": 0.47734375,
      "accuracy_stderr": 0.06163463413536256,
      "f1": 0.47508665894167024,
      "f1_stderr": 0.06221424083081577,
      "main_score": 0.47734375
    },
    "portuguese": {
      "accuracy": 0.433984375,
      "accuracy_stderr": 0.034958209565359964,
      "f1": 0.42976637947800683,
      "f1_stderr": 0.03371352482544993,
      "main_score": 0.433984375
    },
    "spanish": {
      "accuracy": 0.448828125,
      "accuracy_stderr": 0.04046128369831616,
      "f1": 0.44069230613856447,
      "f1_stderr": 0.04471458030498882,
      "main_score": 0.448828125
    }
  }
}