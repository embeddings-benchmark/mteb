{
  "dataset_revision": "810604b9ad3aafdc6144597fdaa40f21a6f5f3de",
  "evaluation_time": 3.7826778888702393,
  "kg_co2_emissions": null,
  "mteb_version": "1.6.38",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.51123046875,
          "accuracy_threshold": 0.4590882360935211,
          "ap": 0.5075731046378607,
          "f1": 0.6705729166666666,
          "f1_threshold": 0.08755961060523987,
          "precision": 0.5051495831289848,
          "recall": 0.9970958373668926
        },
        "dot": {
          "accuracy": 0.5087890625,
          "accuracy_threshold": 2.410257339477539,
          "ap": 0.49235687867992717,
          "f1": 0.67012987012987,
          "f1_threshold": -1.0954264402389526,
          "precision": 0.504152418172936,
          "recall": 0.9990319457889641
        },
        "euclidean": {
          "accuracy": 0.521484375,
          "accuracy_threshold": 3.804874897003174,
          "ap": 0.5111616751683719,
          "f1": 0.67012987012987,
          "f1_threshold": 6.076992511749268,
          "precision": 0.504152418172936,
          "recall": 0.9990319457889641
        },
        "hf_subset": "de",
        "languages": [
          "deu-Latn"
        ],
        "main_score": 0.5127159499056744,
        "manhattan": {
          "accuracy": 0.52392578125,
          "accuracy_threshold": 58.5899543762207,
          "ap": 0.5127159499056744,
          "f1": 0.67012987012987,
          "f1_threshold": 95.33305358886719,
          "precision": 0.504152418172936,
          "recall": 0.9990319457889641
        },
        "max": {
          "accuracy": 0.52392578125,
          "ap": 0.5127159499056744,
          "f1": 0.6705729166666666
        }
      },
      {
        "cos_sim": {
          "accuracy": 0.57861328125,
          "accuracy_threshold": 0.23118528723716736,
          "ap": 0.5788378257134599,
          "f1": 0.7268201617921594,
          "f1_threshold": 0.007286369800567627,
          "precision": 0.570869990224829,
          "recall": 1.0
        },
        "dot": {
          "accuracy": 0.580078125,
          "accuracy_threshold": 3.5315213203430176,
          "ap": 0.5823969252545587,
          "f1": 0.7268749999999999,
          "f1_threshold": 1.4329934120178223,
          "precision": 0.5723425196850394,
          "recall": 0.9957191780821918
        },
        "euclidean": {
          "accuracy": 0.576171875,
          "accuracy_threshold": 5.129590034484863,
          "ap": 0.5807729598343339,
          "f1": 0.7267061389841072,
          "f1_threshold": 5.757790565490723,
          "precision": 0.5712885840274375,
          "recall": 0.9982876712328768
        },
        "hf_subset": "fr",
        "languages": [
          "fra-Latn"
        ],
        "main_score": 0.5823969252545587,
        "manhattan": {
          "accuracy": 0.5791015625,
          "accuracy_threshold": 80.20085144042969,
          "ap": 0.5806164228689288,
          "f1": 0.7267061389841072,
          "f1_threshold": 89.68399047851562,
          "precision": 0.5712885840274375,
          "recall": 0.9982876712328768
        },
        "max": {
          "accuracy": 0.580078125,
          "ap": 0.5823969252545587,
          "f1": 0.7268749999999999
        }
      },
      {
        "cos_sim": {
          "accuracy": 0.5428769017980636,
          "accuracy_threshold": 0.09806933999061584,
          "ap": 0.5148834304324035,
          "f1": 0.7017543859649124,
          "f1_threshold": 0.044192612171173096,
          "precision": 0.5405405405405406,
          "recall": 1.0
        },
        "dot": {
          "accuracy": 0.5442600276625172,
          "accuracy_threshold": 1.5229160785675049,
          "ap": 0.5095789685015906,
          "f1": 0.7019448213478064,
          "f1_threshold": 1.5229160785675049,
          "precision": 0.5422781271837875,
          "recall": 0.9948717948717949
        },
        "euclidean": {
          "accuracy": 0.5408022130013831,
          "accuracy_threshold": 5.5909929275512695,
          "ap": 0.5292217883504955,
          "f1": 0.701123595505618,
          "f1_threshold": 6.043132305145264,
          "precision": 0.5397923875432526,
          "recall": 1.0
        },
        "hf_subset": "it",
        "languages": [
          "ita-Latn"
        ],
        "main_score": 0.5292217883504955,
        "manhattan": {
          "accuracy": 0.5401106500691563,
          "accuracy_threshold": 83.67062377929688,
          "ap": 0.5286075698986751,
          "f1": 0.7003154574132493,
          "f1_threshold": 88.65383911132812,
          "precision": 0.5399583043780403,
          "recall": 0.9961538461538462
        },
        "max": {
          "accuracy": 0.5442600276625172,
          "ap": 0.5292217883504955,
          "f1": 0.7019448213478064
        }
      }
    ]
  },
  "task_name": "XStance"
}