{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 5.829885959625244,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.752294921875,
        "f1": 0.7529885470658287,
        "f1_weighted": 0.7529840030212294,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.752294921875,
        "scores_per_experiment": [
          {
            "accuracy": 0.703125,
            "f1": 0.70552976383629,
            "f1_weighted": 0.7055500439573419
          },
          {
            "accuracy": 0.75341796875,
            "f1": 0.7543442041604776,
            "f1_weighted": 0.7543458921230239
          },
          {
            "accuracy": 0.7666015625,
            "f1": 0.7671026276243748,
            "f1_weighted": 0.7671058897026122
          },
          {
            "accuracy": 0.751953125,
            "f1": 0.7498269276822986,
            "f1_weighted": 0.7498016127691709
          },
          {
            "accuracy": 0.75439453125,
            "f1": 0.7563698080834772,
            "f1_weighted": 0.7563606429887432
          },
          {
            "accuracy": 0.74951171875,
            "f1": 0.7504316529974737,
            "f1_weighted": 0.7504337242748097
          },
          {
            "accuracy": 0.7373046875,
            "f1": 0.7401642134373719,
            "f1_weighted": 0.7401726243778127
          },
          {
            "accuracy": 0.76806640625,
            "f1": 0.7699386811557482,
            "f1_weighted": 0.7699288327708634
          },
          {
            "accuracy": 0.74658203125,
            "f1": 0.7448571983818231,
            "f1_weighted": 0.7448399290637107
          },
          {
            "accuracy": 0.7919921875,
            "f1": 0.7913203932989535,
            "f1_weighted": 0.791300838184204
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}