{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 5.1637654304504395,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.755908203125,
        "f1": 0.7557078704584692,
        "f1_weighted": 0.7557207234062185,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.755908203125,
        "scores_per_experiment": [
          {
            "accuracy": 0.71240234375,
            "f1": 0.717911348788057,
            "f1_weighted": 0.7179308401993338
          },
          {
            "accuracy": 0.748046875,
            "f1": 0.7477058170071742,
            "f1_weighted": 0.7477115561422353
          },
          {
            "accuracy": 0.7705078125,
            "f1": 0.7690886378140765,
            "f1_weighted": 0.7691133711240769
          },
          {
            "accuracy": 0.767578125,
            "f1": 0.7664120234200792,
            "f1_weighted": 0.7664085497172332
          },
          {
            "accuracy": 0.77587890625,
            "f1": 0.7754028026494842,
            "f1_weighted": 0.7754129412125276
          },
          {
            "accuracy": 0.76318359375,
            "f1": 0.7624506583031211,
            "f1_weighted": 0.7624598777978914
          },
          {
            "accuracy": 0.73828125,
            "f1": 0.7370871506240162,
            "f1_weighted": 0.7370897585273973
          },
          {
            "accuracy": 0.77587890625,
            "f1": 0.7758638275964324,
            "f1_weighted": 0.7758711512471281
          },
          {
            "accuracy": 0.7431640625,
            "f1": 0.7391387739578494,
            "f1_weighted": 0.7391745613782144
          },
          {
            "accuracy": 0.76416015625,
            "f1": 0.7660176644244011,
            "f1_weighted": 0.766034626716148
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}