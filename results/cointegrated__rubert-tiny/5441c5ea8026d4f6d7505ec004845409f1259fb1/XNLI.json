{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 0.6114194393157959,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.25",
  "scores": {
    "test": [
      {
        "cos_sim": {
          "accuracy": 0.6065934065934065,
          "accuracy_threshold": 0.6747493743896484,
          "ap": 0.6148610063892326,
          "f1": 0.673,
          "f1_threshold": 0.47483354806900024,
          "precision": 0.5106221547799696,
          "recall": 0.9868035190615836
        },
        "dot": {
          "accuracy": 0.5912087912087912,
          "accuracy_threshold": 53.96538543701172,
          "ap": 0.5989100451788207,
          "f1": 0.6715101193565127,
          "f1_threshold": 40.61479949951172,
          "precision": 0.5196787148594377,
          "recall": 0.9486803519061584
        },
        "euclidean": {
          "accuracy": 0.580952380952381,
          "accuracy_threshold": 6.673254013061523,
          "ap": 0.5994763588246188,
          "f1": 0.6683316683316683,
          "f1_threshold": 9.83686637878418,
          "precision": 0.5068181818181818,
          "recall": 0.9809384164222874
        },
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6148610063892326,
        "manhattan": {
          "accuracy": 0.578021978021978,
          "accuracy_threshold": 93.78980255126953,
          "ap": 0.5962830748916279,
          "f1": 0.668958742632613,
          "f1_threshold": 157.28302001953125,
          "precision": 0.5029542097488922,
          "recall": 0.998533724340176
        },
        "max": {
          "accuracy": 0.6065934065934065,
          "ap": 0.6148610063892326,
          "f1": 0.673
        }
      }
    ],
    "validation": [
      {
        "cos_sim": {
          "accuracy": 0.6058608058608058,
          "accuracy_threshold": 0.6931620836257935,
          "ap": 0.6257443633561923,
          "f1": 0.6693670886075949,
          "f1_threshold": 0.4960152804851532,
          "precision": 0.5112142304717711,
          "recall": 0.969208211143695
        },
        "dot": {
          "accuracy": 0.5875457875457876,
          "accuracy_threshold": 56.548011779785156,
          "ap": 0.6028822045526122,
          "f1": 0.6714212939378503,
          "f1_threshold": 39.32011413574219,
          "precision": 0.5144418423106948,
          "recall": 0.966275659824047
        },
        "euclidean": {
          "accuracy": 0.5802197802197803,
          "accuracy_threshold": 6.8174285888671875,
          "ap": 0.6000967459170428,
          "f1": 0.6676470588235294,
          "f1_threshold": 11.248852729797363,
          "precision": 0.5014727540500736,
          "recall": 0.998533724340176
        },
        "hf_subset": "ru",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6257443633561923,
        "manhattan": {
          "accuracy": 0.5838827838827839,
          "accuracy_threshold": 94.84237670898438,
          "ap": 0.6025660181169511,
          "f1": 0.6698513800424628,
          "f1_threshold": 124.89250183105469,
          "precision": 0.5249584026622296,
          "recall": 0.9252199413489736
        },
        "max": {
          "accuracy": 0.6058608058608058,
          "ap": 0.6257443633561923,
          "f1": 0.6714212939378503
        }
      }
    ]
  },
  "task_name": "XNLI"
}