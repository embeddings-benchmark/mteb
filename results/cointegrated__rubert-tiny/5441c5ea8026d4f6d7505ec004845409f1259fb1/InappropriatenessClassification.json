{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 2.890075445175171,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.25",
  "scores": {
    "test": [
      {
        "accuracy": 0.544970703125,
        "ap": 0.5249790703948662,
        "f1": 0.538207308662605,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.544970703125,
        "scores_per_experiment": [
          {
            "accuracy": 0.54833984375,
            "ap": 0.5269361903901735,
            "f1": 0.5456009717384624
          },
          {
            "accuracy": 0.5830078125,
            "ap": 0.5467148841395864,
            "f1": 0.5718925227468323
          },
          {
            "accuracy": 0.52978515625,
            "ap": 0.5155751080015025,
            "f1": 0.5189762088557348
          },
          {
            "accuracy": 0.50390625,
            "ap": 0.5019787397540983,
            "f1": 0.4827701752065918
          },
          {
            "accuracy": 0.56298828125,
            "ap": 0.535079969522286,
            "f1": 0.5617468630026183
          },
          {
            "accuracy": 0.51220703125,
            "ap": 0.5062164600296077,
            "f1": 0.49944597694119086
          },
          {
            "accuracy": 0.59228515625,
            "ap": 0.5541507941632231,
            "f1": 0.591874043424957
          },
          {
            "accuracy": 0.55029296875,
            "ap": 0.52723358178888,
            "f1": 0.5451868301980969
          },
          {
            "accuracy": 0.53662109375,
            "ap": 0.5198693334988649,
            "f1": 0.534350857899951
          },
          {
            "accuracy": 0.5302734375,
            "ap": 0.5160356426604407,
            "f1": 0.5302286366116153
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}