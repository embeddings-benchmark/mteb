{
  "dataset_revision": "f6d2c31f4dc6b88f468552750bfec05b4b41b05a",
  "evaluation_time": 7.444393634796143,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.580078125,
        "f1": 0.5728133050610399,
        "f1_weighted": 0.5728087865988991,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.580078125,
        "scores_per_experiment": [
          {
            "accuracy": 0.60986328125,
            "f1": 0.5953365765532104,
            "f1_weighted": 0.5953284960014353
          },
          {
            "accuracy": 0.533203125,
            "f1": 0.5192865820084273,
            "f1_weighted": 0.5193020491324204
          },
          {
            "accuracy": 0.5517578125,
            "f1": 0.5560668590744411,
            "f1_weighted": 0.5560400037518488
          },
          {
            "accuracy": 0.6328125,
            "f1": 0.634043313570773,
            "f1_weighted": 0.6340505529417302
          },
          {
            "accuracy": 0.62939453125,
            "f1": 0.6298256748316163,
            "f1_weighted": 0.6298090395830505
          },
          {
            "accuracy": 0.591796875,
            "f1": 0.5791143288721026,
            "f1_weighted": 0.5791321646035887
          },
          {
            "accuracy": 0.58935546875,
            "f1": 0.5598187604069293,
            "f1_weighted": 0.5597946576818159
          },
          {
            "accuracy": 0.54736328125,
            "f1": 0.5423619033024347,
            "f1_weighted": 0.5423503428373241
          },
          {
            "accuracy": 0.4794921875,
            "f1": 0.47764827483883526,
            "f1_weighted": 0.4776286897754708
          },
          {
            "accuracy": 0.6357421875,
            "f1": 0.6346307771516285,
            "f1_weighted": 0.6346518696803068
          }
        ]
      }
    ]
  },
  "task_name": "RuReviewsClassification"
}