{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 6.551973819732666,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.584814453125,
        "ap": 0.5515273547085593,
        "ap_weighted": 0.5515273547085593,
        "f1": 0.5823864253991664,
        "f1_weighted": 0.5823864253991664,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.584814453125,
        "scores_per_experiment": [
          {
            "accuracy": 0.60693359375,
            "ap": 0.5724444768030794,
            "ap_weighted": 0.5724444768030794,
            "f1": 0.5907716098797242,
            "f1_weighted": 0.5907716098797242
          },
          {
            "accuracy": 0.59423828125,
            "ap": 0.555922619947967,
            "ap_weighted": 0.555922619947967,
            "f1": 0.5942304450669409,
            "f1_weighted": 0.5942304450669409
          },
          {
            "accuracy": 0.56591796875,
            "ap": 0.5368861360878199,
            "ap_weighted": 0.5368861360878199,
            "f1": 0.564684872859584,
            "f1_weighted": 0.564684872859584
          },
          {
            "accuracy": 0.54443359375,
            "ap": 0.5242861198183213,
            "ap_weighted": 0.5242861198183213,
            "f1": 0.5441935356903886,
            "f1_weighted": 0.5441935356903886
          },
          {
            "accuracy": 0.5732421875,
            "ap": 0.5428775448462415,
            "ap_weighted": 0.5428775448462415,
            "f1": 0.5710622700089241,
            "f1_weighted": 0.5710622700089241
          },
          {
            "accuracy": 0.54296875,
            "ap": 0.5231845773381295,
            "ap_weighted": 0.5231845773381295,
            "f1": 0.5421233662004128,
            "f1_weighted": 0.5421233662004128
          },
          {
            "accuracy": 0.6435546875,
            "ap": 0.5917987280597723,
            "ap_weighted": 0.5917987280597723,
            "f1": 0.6434781862181654,
            "f1_weighted": 0.6434781862181654
          },
          {
            "accuracy": 0.6123046875,
            "ap": 0.5682677650093808,
            "ap_weighted": 0.5682677650093808,
            "f1": 0.6121415657334217,
            "f1_weighted": 0.6121415657334217
          },
          {
            "accuracy": 0.5576171875,
            "ap": 0.5323794068408614,
            "ap_weighted": 0.5323794068408614,
            "f1": 0.5570697425712321,
            "f1_weighted": 0.5570697425712321
          },
          {
            "accuracy": 0.60693359375,
            "ap": 0.5672261723340188,
            "ap_weighted": 0.5672261723340188,
            "f1": 0.6041086597628695,
            "f1_weighted": 0.6041086597628695
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}