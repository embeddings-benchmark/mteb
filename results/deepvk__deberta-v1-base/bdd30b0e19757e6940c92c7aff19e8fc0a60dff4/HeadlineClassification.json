{
  "dataset_revision": "2fe05ee6b5832cda29f2ef7aaad7b7fe6a3609eb",
  "evaluation_time": 6.205138921737671,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.7875,
        "f1": 0.7870167906740788,
        "f1_weighted": 0.7870013739341392,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.7875,
        "scores_per_experiment": [
          {
            "accuracy": 0.75830078125,
            "f1": 0.7601316158780754,
            "f1_weighted": 0.7601115301159954
          },
          {
            "accuracy": 0.77294921875,
            "f1": 0.7721069174256852,
            "f1_weighted": 0.7720769949988773
          },
          {
            "accuracy": 0.78271484375,
            "f1": 0.7828183367239644,
            "f1_weighted": 0.7827994112599366
          },
          {
            "accuracy": 0.80712890625,
            "f1": 0.8079283862588125,
            "f1_weighted": 0.8078968068435939
          },
          {
            "accuracy": 0.80224609375,
            "f1": 0.8022290744466837,
            "f1_weighted": 0.8022171775410516
          },
          {
            "accuracy": 0.80029296875,
            "f1": 0.8002678330005479,
            "f1_weighted": 0.8002434554200487
          },
          {
            "accuracy": 0.80078125,
            "f1": 0.8007522624988473,
            "f1_weighted": 0.8007356661908323
          },
          {
            "accuracy": 0.7939453125,
            "f1": 0.7927820518033841,
            "f1_weighted": 0.7927849212464567
          },
          {
            "accuracy": 0.75341796875,
            "f1": 0.7482179968848316,
            "f1_weighted": 0.748236709699982
          },
          {
            "accuracy": 0.80322265625,
            "f1": 0.8029334318199549,
            "f1_weighted": 0.8029110660246178
          }
        ]
      }
    ]
  },
  "task_name": "HeadlineClassification"
}