{
  "dataset_revision": "601651fdc45ef243751676e62dd7a19f491c0285",
  "evaluation_time": 6.36400580406189,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "test": [
      {
        "accuracy": 0.6138671875,
        "ap": 0.5711020118497604,
        "ap_weighted": 0.5711020118497604,
        "f1": 0.6099462942803939,
        "f1_weighted": 0.6099462942803939,
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.6138671875,
        "scores_per_experiment": [
          {
            "accuracy": 0.64697265625,
            "ap": 0.5982007245111731,
            "ap_weighted": 0.5982007245111731,
            "f1": 0.6455664327160904,
            "f1_weighted": 0.6455664327160904
          },
          {
            "accuracy": 0.63427734375,
            "ap": 0.5824353866766777,
            "ap_weighted": 0.5824353866766777,
            "f1": 0.6313337651397624,
            "f1_weighted": 0.6313337651397624
          },
          {
            "accuracy": 0.63037109375,
            "ap": 0.5842485709884995,
            "ap_weighted": 0.5842485709884995,
            "f1": 0.6292820893819989,
            "f1_weighted": 0.6292820893819989
          },
          {
            "accuracy": 0.5732421875,
            "ap": 0.5415788230482852,
            "ap_weighted": 0.5415788230482852,
            "f1": 0.5725230509394237,
            "f1_weighted": 0.5725230509394237
          },
          {
            "accuracy": 0.62158203125,
            "ap": 0.5749774850632615,
            "ap_weighted": 0.5749774850632615,
            "f1": 0.6214151374314095,
            "f1_weighted": 0.6214151374314095
          },
          {
            "accuracy": 0.5693359375,
            "ap": 0.5392009812384899,
            "ap_weighted": 0.5392009812384899,
            "f1": 0.5689408799988546,
            "f1_weighted": 0.5689408799988546
          },
          {
            "accuracy": 0.671875,
            "ap": 0.6147470238095238,
            "ap_weighted": 0.6147470238095238,
            "f1": 0.6718221072541484,
            "f1_weighted": 0.6718221072541484
          },
          {
            "accuracy": 0.59912109375,
            "ap": 0.5562543532726214,
            "ap_weighted": 0.5562543532726214,
            "f1": 0.575922799854623,
            "f1_weighted": 0.575922799854623
          },
          {
            "accuracy": 0.5927734375,
            "ap": 0.5571611399755501,
            "ap_weighted": 0.5571611399755501,
            "f1": 0.5886111986219216,
            "f1_weighted": 0.5886111986219216
          },
          {
            "accuracy": 0.59912109375,
            "ap": 0.5622156299135219,
            "ap_weighted": 0.5622156299135219,
            "f1": 0.5940454814657077,
            "f1_weighted": 0.5940454814657077
          }
        ]
      }
    ]
  },
  "task_name": "InappropriatenessClassification"
}