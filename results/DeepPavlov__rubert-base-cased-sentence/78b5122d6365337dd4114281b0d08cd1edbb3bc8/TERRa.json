{
  "dataset_revision": "7b58f24536063837d644aab9a023c62199b2a612",
  "evaluation_time": 0.6721987724304199,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.49",
  "scores": {
    "dev": [
      {
        "cosine": {
          "accuracy": 0.5830618892508144,
          "accuracy_threshold": 0.7970056533813477,
          "ap": 0.5911700831394415,
          "f1": 0.6854460093896714,
          "f1_threshold": 0.5531601905822754,
          "precision": 0.5347985347985348,
          "recall": 0.954248366013072
        },
        "dot": {
          "accuracy": 0.5732899022801303,
          "accuracy_threshold": 297.14422607421875,
          "ap": 0.5891414022488688,
          "f1": 0.6872037914691943,
          "f1_threshold": 206.76393127441406,
          "precision": 0.5390334572490706,
          "recall": 0.9477124183006536
        },
        "euclidean": {
          "accuracy": 0.5863192182410424,
          "accuracy_threshold": 11.772513389587402,
          "ap": 0.5904749299082525,
          "f1": 0.6854460093896714,
          "f1_threshold": 18.283950805664062,
          "precision": 0.5347985347985348,
          "recall": 0.954248366013072
        },
        "hf_subset": "default",
        "languages": [
          "rus-Cyrl"
        ],
        "main_score": 0.5911700831394415,
        "manhattan": {
          "accuracy": 0.5830618892508144,
          "accuracy_threshold": 256.449951171875,
          "ap": 0.5901460695332876,
          "f1": 0.6853146853146853,
          "f1_threshold": 403.99346923828125,
          "precision": 0.532608695652174,
          "recall": 0.9607843137254902
        },
        "max": {
          "accuracy": 0.5863192182410424,
          "ap": 0.5911700831394415,
          "f1": 0.6872037914691943
        },
        "similarity": {
          "accuracy": 0.5830618892508144,
          "accuracy_threshold": 0.7970057129859924,
          "ap": 0.5911700831394415,
          "f1": 0.6854460093896714,
          "f1_threshold": 0.5531602501869202,
          "precision": 0.5347985347985348,
          "recall": 0.954248366013072
        }
      }
    ]
  },
  "task_name": "TERRa"
}