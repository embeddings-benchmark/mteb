{
  "dataset_revision": "502695fe1a141108650e3c5b91c8b5e0ff84ed49",
  "evaluation_time": 1878.3961374759674,
  "kg_co2_emissions": null,
  "mteb_version": "1.14.15",
  "scores": {
    "test": [
      {
        "accuracy": 0.677899169921875,
        "ap": 0.6171958412862929,
        "ap_weighted": 0.6171958412862929,
        "f1": 0.6716262837899631,
        "f1_weighted": 0.6716104944451556,
        "hf_subset": "default",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.677899169921875,
        "scores_per_experiment": [
          {
            "accuracy": 0.7061767578125,
            "ap": 0.6366412143481036,
            "ap_weighted": 0.6366412143481036,
            "f1": 0.7010964063919244,
            "f1_weighted": 0.7010797573001903
          },
          {
            "accuracy": 0.635162353515625,
            "ap": 0.5809307261507256,
            "ap_weighted": 0.5809307261507256,
            "f1": 0.6234755309662483,
            "f1_weighted": 0.6234471894599793
          },
          {
            "accuracy": 0.7178955078125,
            "ap": 0.6544682544786682,
            "ap_weighted": 0.6544682544786682,
            "f1": 0.7177918531207379,
            "f1_weighted": 0.7177895423473547
          },
          {
            "accuracy": 0.6851806640625,
            "ap": 0.6264741993887054,
            "ap_weighted": 0.6264741993887054,
            "f1": 0.6851780733405203,
            "f1_weighted": 0.6851776874883105
          },
          {
            "accuracy": 0.64508056640625,
            "ap": 0.5874648120652616,
            "ap_weighted": 0.5874648120652616,
            "f1": 0.6305895551303842,
            "f1_weighted": 0.6305582956299431
          }
        ]
      }
    ]
  },
  "task_name": "PatchCamelyon"
}